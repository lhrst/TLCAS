index,year,title,author,abstract,,,,name
1,2013,A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation,Yang Liu,"We introduce a shift-reduce parsing algorithm for phrase-based string-to-dependency translation. As the algorithm generates dependency trees for partial translations left-to-right in decoding, it allows for efficient integration of both n -gram and dependency language mod- els. To resolve conflicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data. As our approach combines the merits of phrase-based and string-to- dependency models, it achieves significant improvements over the two baselines on the NIST Chinese-English datasets. ",,,,ACL
2,2013,Integrating Translation Memory into Phrase-Based Machine Translation during Decoding,"Kun Wang, Chengqing Zong, Keh-Yih Su","Since statistical machine translation (SMT) and translation memory (TM) complement each other in matched and unmatched regions, integrated models are proposed in this paper to incorporate TM information into phrase-based SMT. Unlike previous multi-stage pipeline approaches, which directly merge TM result into the final output, the proposed models refer to the corresponding TM information associat- ed with each phrase at SMT decoding. On a Chinese – English TM database, our experi- ments show that the proposed integrated Mod- el-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Be- sides, the proposed models also outperform previous approaches significantly. ",,,,ACL
3,2013,Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment,Thomas Schoenemann,"We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and first order parameters, are nondeficient. Subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training. The arising M-step energies are non-trivial and handled via projected gradient ascent. Our evaluation on gold alignments shows substantial improvements (in weighted F- measure) for the IBM-3. For the IBM- 4 there are no consistent improvements. Training the nondeficient IBM-5 in the regular way gives surprisingly good re- sults. Using the resulting alignments for phrase- based translation systems offers no clear insights w.r.t. BLEU scores. ",,,,ACL
4,2013,Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation,"Trevor Cohn, Lucia Specia","Annotating linguistic data is often a complex, time consuming and expensive endeavour. Even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpretations of the task and lev- els of consistency. We present novel tech- niques for learning from the outputs of multiple annotators while accounting for annotator specific behaviour. These tech- niques use multi-task Gaussian Processes to learn jointly a series of annotator and metadata specific models, while explicitly representing correlations between models which can be learned directly from data. Our experiments on two machine translation quality estimation datasets show uniform significant accuracy gains from multi-task learning, and consistently out- perform strong baselines. ",,,,ACL
5,2013,Smoothed marginal distribution constraints for language modeling,"Brian Roark, Cyril Allauzen, Michael Riley","We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of well-known Kneser-Ney (1995) smoothing. Unlike Kneser-Ney, our approach is de- signed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retain- ing the benefits of such marginal distribu- tion constraints. We present experimen- tal results for heavily pruned backoff n- gram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm has been released as part of the OpenGrm ngram library. 1 ",,,,ACL
6,2013,Grounded Language Learning from Video Described with Sentences,"Haonan Yu, Jeffrey Mark Siskind","We present a method that learns representations for word meanings from short video clips paired with sentences. Unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple com- plex objects in outdoor environments. Un- like prior computer-vision approaches that learn from videos with verb labels or im- ages with noun labels, our labels are sentences containing nouns, verbs, preposi- tions, adjectives, and adverbs. The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts simultaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video. ",,,,ACL
7,2013,"Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation","Mehdi Manshadi, Daniel Gildea, James Allen","Recent work on statistical quantifier scope disambiguation (QSD) has improved upon earlier work by scoping an arbitrary num- ber and type of noun phrases. No corpus- based method, however, has yet addressed QSD when incorporating the implicit uni- versal of plurals and/or operators such as negation. In this paper we report early, though promising, results for automatic QSD when handling both phenomena. We also present a general model for learning to build partial orders from a set of pair- wise preferences. We give an n log n algo- rithm for finding a guaranteed approxima- tion of the optimal solution, which works very well in practice. Finally, we signifi- cantly improve the performance of the pre- vious model using a rich set of automati- cally generated features. ",,,,ACL
8,2013,Joint Event Extraction via Structured Prediction with Global Features,"Qi Li, Heng Ji, Liang Huang","Traditional approaches to the task of ACE event extraction usually rely on sequential pipelines with multiple stages, which suffer from error propagation since event trig- gers and arguments are predicted in isola- tion by independent local classifiers. By contrast, we propose a joint framework based on structured prediction which ex- tracts triggers and arguments together so that the local predictions can be mutu- ally improved. In addition, we propose to incorporate global features which ex- plicitly capture the dependencies of multi- ple triggers and arguments. Experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features fur- ther improves the performance signifi- cantly. Our approach advances state-of- the-art sentence-level event extraction, and even outperforms previous argument la- beling methods which use external knowl- edge from other sentences and documents. ",,,,ACL
9,2013,Language-Independent Discriminative Parsing of Temporal Expressions,"Gabor Angeli, Jakob Uszkoreit","Temporal resolution systems are traditionally tuned to a particular language, requiring significant human effort to trans- late them to new languages. We present a language independent semantic parser for learning the interpretation of tempo- ral phrases given only a corpus of utter- ances and the times they reference. We make use of a latent parse that encodes a language-flexible representation of time, and extract rich features over both the parse and associated temporal semantics. The parameters of the model are learned using a weakly supervised bootstrapping approach, without the need for manually tuned parameters or any other language expertise. We achieve state-of-the-art ac- curacy on all languages in the TempEval- 2 temporal normalization task, reporting a 4% improvement in both English and Spanish accuracy, and to our knowledge the first results for four other languages. ",,,,ACL
10,2013,Graph-based Local Coherence Modeling,"Camille Guinaudeau, Michael Strube","We propose a computationally efficient graph-based approach for local coherence modeling. We evaluate our system on three tasks: sentence ordering, summary coherence rating and readability assessment. The performance is comparable to entity grid based approaches though these rely on a computationally expensive train- ing phase and face data sparsity problems. ",,,,ACL
11,2013,Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms,"Elijah Mayfield, David Adamson, Carolyn Penstein Rosé","Automated annotation of social behavior in conversation is necessary for large-scale analysis of real-world conversational data. Important behavioral categories, though, are often sparse and often appear only in specific subsections of a conversation. This makes supervised machine learning difficult, through a combination of noisy features and unbalanced class distribu- tions. We propose within-instance con- tent selection, using cue features to selec- tively suppress sections of text and bias- ing the remaining representation towards minority classes. We show the effective- ness of this technique in automated anno- tation of empowerment language in online support group chatrooms. Our technique is significantly more accurate than multi- ple baselines, especially when prioritizing high precision. ",,,,ACL
12,2013,Decentralized Entity-Level Modeling for Coreference Resolution,"Greg Durrett, David Hall, Dan Klein","Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We de- scribe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent men- tions. This model can be represented as a factor graph for each document that ad- mits efficient inference via belief propaga- tion. We show that our method can use entity-level information to outperform a basic pairwise system. ",,,,ACL
13,2013,Chinese Parsing Exploiting Characters,"Meishan Zhang, Yue Zhang, Wanxiang Che, Ting Liu","Characters play an important role in the Chinese language, yet computational processing of Chinese has been dominated by word-based approaches, with leaves in syntax trees being words. We investigate Chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. We demonstrate the importance of character-level information to Chinese processing by building a joint segmen- tation, part-of-speech (POS) tagging and phrase-structure parsing system that inte- grates character-structure features. Our joint system significantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test, and gives the best published results for Chinese parsing. ",,,,ACL
14,2013,A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy,"Francesco Sartorio, Giorgio Satta, Joakim Nivre","We present a novel transition-based, greedy dependency parser which implements a flexible mix of bottom-up and top-down strategies. The new strategy allows the parser to postpone difficult decisions until the relevant information becomes available. The novel parser has a ～ 12% error reduc- tion in unlabeled attachment score over an arc-eager parser, with a slow-down factor of 2.8. ",,,,ACL
15,2013,General binarization for parsing and translation,"Matthias Büchse, Alexander Koller, Heiko Vogler",Binarization of grammars is crucial for im- proving the complexity and performance of parsing and translation. We present a versatile binarization algorithm that can be tailored to a number of grammar formalisms by simply varying a formal pa- rameter. We apply our algorithm to bi- narizing tree-to-string transducers used in syntax-based machine translation. ,,,,ACL
16,2013,Distortion Model Considering Rich Context for Statistical Machine Translation,"Isao Goto, Masao Utiyama, Eiichiro Sumita, Akihiro Tamura","This paper proposes new distortion mod- els for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by dis- criminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. ",,,,ACL
17,2013,Word Alignment Modeling with Context Dependent Deep Neural Network,"Nan Yang, Shujie Liu, Mu Li, Ming Zhou","In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNN- HMM (Dahl et al., 2012) method intro- duced in speech recognition to the HMM- based word alignment model, in which bilingual word embedding is discrimina- tively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being ca- pable to model the rich bilingual corre- spondence, our method generates a very compact model with much fewer parame- ters. Experiments on a large scale English- Chinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score. ",,,,ACL
18,2013,Microblogs as Parallel Corpora,"Wang Ling, Guang Xiang, Chris Dyer, Alan Black","In the ever-expanding sea of microblog data, there is a surprising amount of naturally occurring par- allel text: some users create post multilingual messages targeting international audiences while oth- ers “retweet” translations. We present an efficient method for detecting these messages and extract- ing parallel segments from them. We have been able to extract over 1M Chinese-English parallel segments from Sina Weibo (the Chinese counter- part of Twitter) using only their public APIs. As a supplement to existing parallel training data, our automatically extracted parallel data yields sub- stantial translation quality improvements in trans- lating microblog text and modest improvements in translating edited news commentary. The re- sources in described in this paper are available at http://www.cs.cmu.edu/ ～ lingwang/utopia. ",,,,ACL
19,2013,Improved Bayesian Logistic Supervised Topic Models with Data Augmentation,"Jun Zhu, Xun Zheng, Bo Zhang",Supervised topic models with a logistic likelihood have two issues that potential- ly limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-field assumptions. We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by intro- ducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restrict- ing assumptions and can be easily paral- lelized. Empirical results demonstrate sig- nificant improvements on prediction per- formance and time efficiency. ,,,,ACL
20,2013,Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning,"Miguel Almeida, André Martins","We present a dual decomposition framework for multi-document summarization, using a model that jointly extracts and compresses sentences. Compared with previous work based on integer linear programming, our approach does not require external solvers, is significantly faster, and is modular in the three qualities a sum- mary should have: conciseness, informa- tiveness, and grammaticality. In addition, we propose a multi-task learning frame- work to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC- 2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers. ",,,,ACL
21,2013,Unsupervised Transcription of Historical Documents,"Taylor Berg-Kirkpatrick, Greg Durrett, Dan Klein","We present a generative probabilistic model, inspired by historical printing processes, for transcribing images of docu- ments from the printing press era. By jointly modeling the text of the docu- ment and the noisy (but regular) process of rendering glyphs, our unsupervised sys- tem is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially out- performs state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading com- mercial system for historical transcription, and a 47% relative reduction over Tesser- act, Google’s open source OCR system. ",,,,ACL
22,2013,Adapting Discriminative Reranking to Grounded Language Learning,"Joohyun Kim, Raymond Mooney","We adapt discriminative reranking to im- prove the performance of grounded language acquisition, specifically the task of learning to follow navigation instructions from observation. Unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded set- ting. Instead, we show how the weak su- pervision of response feedback (e.g. suc- cessful task completion) can be used as an alternative, experimentally demonstrat- ing that its performance is comparable to training on gold-standard parse trees. ",,,,ACL
23,2013,Universal Conceptual Cognitive Annotation (UCCA),"Omri Abend, Ari Rappoport","Syntactic structures, by their nature, reflect first and foremost the formal con- structions used for expressing meanings. This renders them sensitive to formal vari- ation both within and across languages, and limits their value to semantic ap- plications. We present UCCA, a novel multi-layered framework for semantic rep- resentation that aims to accommodate the semantic distinctions expressed through linguistic utterances. We demonstrate UCCA’s portability across domains and languages, and its relative insensitivity to meaning-preserving syntactic variation. We also show that UCCA can be ef- fectively and quickly learned by annota- tors with no linguistic background, and describe the compilation of a UCCA- annotated corpus. ",,,,ACL
24,2013,Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media,"Weiwei Guo, Hao Li, Heng Ji, Mona Diab","Many current Natural Language Processing [NLP] techniques work well assuming a large context of text as input data. However they become ineffective when applied to short texts such as Twitter feeds. To overcome the issue, we want to find a related newswire document to a given tweet to provide contextual support for NLP tasks. This requires robust model- ing and understanding of the semantics of short texts. The contribution of the paper is two-fold: 1. we introduce the Linking-Tweets-to- News task as well as a dataset of linked tweet-news pairs, which can benefit many NLP applications; 2. in contrast to previ- ous research which focuses on lexical fea- tures within the short texts (text-to-word information), we propose a graph based latent variable model that models the in- ter short text correlations (text-to-text in- formation). This is motivated by the ob- servation that a tweet usually only cov- ers one aspect of an event. We show that using tweet specific feature (hashtag) and news specific feature (named entities) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. Our experiments show significant im- provement of our new model over base- lines with three evaluation metrics in the new task. ",,,,ACL
25,2013,A computational approach to politeness with application to social factors,"Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec","We propose a computational framework for identifying linguistic aspects of polite- ness. Our starting point is a new corpus of requests annotated for politeness, which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context. These findings guide our construction of a classifier with domain-independent lexi- cal and syntactic features operationalizing key components of politeness theory, such as indirection, deference, impersonaliza- tion and modality. Our classifier achieves close to human performance and is effec- tive across domains. We use our frame- work to study the relationship between po- liteness and social power, showing that po- lite Wikipedia editors are more likely to achieve high status through elections, but, once elevated, they become less polite. We see a similar negative correlation between politeness and power on Stack Exchange, where users at the top of the reputation scale are less polite than those at the bot- tom. Finally, we apply our classifier to a preliminary analysis of politeness vari- ation by gender and community. ",,,,ACL
26,2013,Modeling Thesis Clarity in Student Essays,"Isaac Persing, Vincent Ng","Recently, researchers have begun exploring methods of scoring student essays with respect to particular dimensions of qual- ity such as coherence, technical errors, and relevance to prompt, but there is relatively little work on modeling thesis clar- ity. We present a new annotated corpus and propose a learning-based approach to scoring essays along the thesis clarity di- mension. Additionally, in order to pro- vide more valuable feedback on why an essay is scored as it is, we propose a sec- ond learning-based approach to identify- ing what kinds of errors an essay has that may lower its thesis clarity score. ",,,,ACL
27,2013,Translating Italian connectives into Italian Sign Language,"Camillo Lugaresi, Barbara Di Eugenio","We present a corpus analysis of how Ital- ian connectives are translated into LIS, the Italian Sign Language. Since corpus re- sources are scarce, we propose an alignment method between the syntactic trees of the Italian sentence and of its LIS trans- lation. This method, and clustering ap- plied to its outputs, highlight the differ- ent ways a connective can be rendered in LIS: with a corresponding sign, by affect- ing the location or shape of other signs, or being omitted altogether. We translate these findings into a computational model that will be integrated into the pipeline of an existing Italian-LIS rendering system. Initial experiments to learn the four possi- ble translations with Decision Trees give promising results. ",,,,ACL
28,2013,Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing,"David Mareček, Milan Straka","Even though the quality of unsupervised dependency parsers grows, they often fail in recognition of very basic dependencies. In this paper, we exploit a prior knowledge of STOP-probabilities (whether a given word has any children in a given direc- tion), which is obtained from a large raw corpus using the reducibility principle. By incorporating this knowledge into Depen- dency Model with Valence, we managed to considerably outperform the state-of-the- art results in terms of average attachment score over 20 treebanks from CoNLL 2006 and 2007 shared tasks. ",,,,ACL
29,2013,Transfer Learning for Constituency-Based Grammars,"Yuan Zhang, Regina Barzilay, Amir Globerson","In this paper, we consider the problem of cross-formalism transfer in parsing. We are interested in parsing constituency- based grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quan- tity of coarse CFG annotations from the Penn Treebank. While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and se- mantic features. To handle this appar- ent discrepancy, we design a probabilistic model that jointly generates CFG and tar- get formalism parses. The model includes features of both parses, allowing trans- fer between the formalisms, while pre- serving parsing efficiency. We evaluate our approach on three constituency-based grammars — CCG, HPSG, and LFG, aug- mented with the Penn Treebank-1. Our ex- periments show that across all three for- malisms, the target parsers significantly benefit from the coarse annotations. 1 ",,,,ACL
30,2013,A Context Free TAG Variant,"Ben Swanson, Elif Yamangil, Eugene Charniak, Stuart Shieber","We propose a new variant of Tree-Adjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity. We provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters. This collapsed context-free form is used to implement efficient gram- mar estimation and parsing algorithms. We perform parsing experiments the Penn Treebank and draw comparisons to Tree- Substitution Grammars and between dif- ferent variations in probabilistic model de- sign. Examination of the most probable derivations reveals examples of the lin- guistically relevant structure that our vari- ant makes possible. ",,,,ACL
31,2013,Fast and Adaptive Online Training of Feature-Rich Translation Models,"Spence Green, Sida Wang, Daniel Cer, Christopher D. Manning","We present a fast and scalable online method for tuning statistical machine trans- lation models with large feature sets. The standard tuning algorithm—MERT—only scales to tens of features. Recent discriminative algorithms that accommodate sparse features have produced smaller than ex- pected translation quality gains in large systems. Our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs. Large-scale experiments on Arabic-English and Chinese-English show that our method produces significant trans- lation quality gains by exploiting sparse fea- tures. Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and ap- plies to other recent discriminative methods for machine translation. ",,,,ACL
32,2013,Advancements in Reordering Models for Statistical Machine Translation,"Minwei Feng, Jan-Thorsten Peter, Hermann Ney","In this paper, we propose a novel re- ordering model based on sequence labeling techniques. Our model converts the reordering problem into a sequence label- ing problem, i.e. a tagging task. Results on five Chinese-English NIST tasks show that our model improves the baseline sys- tem by 1.32 B LEU and 1.53 T ER on av- erage. Results of comparative study with other seven widely used reordering mod- els will also be reported. ",,,,ACL
33,2013,A Markov Model of Machine Translation using Non-parametric Bayesian Inference,"Yang Feng, Trevor Cohn","Most modern machine translation systems use phrase pairs as translation units, allowing for accurate modelling of phrase-internal translation and reordering. How- ever phrase-based approaches are much less able to model sentence level effects between different phrase-pairs. We pro- pose a new model to address this im- balance, based on a word-based Markov model of translation which generates tar- get translations left-to-right. Our model encodes word and phrase level phenom- ena by conditioning translation decisions on previous decisions and uses a hierar- chical Pitman-Yor Process prior to pro- vide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over com- petitive baselines, of up to +3.4 BLEU. ",,,,ACL
34,2013,Scaling Semi-supervised Naive Bayes with Feature Marginals,"Michael Lucas, Doug Downey","Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL tech- niques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statis- tics calculated from the unlabeled corpus. In experiments with text topic classifica- tion and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previ- ous work. ",,,,ACL
35,2013,Learning Latent Personas of Film Characters,"David Bamman, Brendan O’Connor, Noah A. Smith","We present two latent variable models for learning character types, or personas, in film, in which a persona is defined as a set of mixtures over latent lexical classes. These lexical classes capture the stereo- typical actions of which a character is the agent and patient, as well as attributes by which they are described. As the first attempt to solve this problem explicitly, we also present a new dataset for the text-driven analysis of film, along with a benchmark testbed to help drive future work in this area. ",,,,ACL
36,2013,Scalable Decipherment for Machine Translation via Hash Sampling,Sujith Ravi,"In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only non- parallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorpo- rate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then de- rive a hash sampling strategy that is in- spired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex mod- els (for the first time) and large vocab- ulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving sig- nificant computational speedups (several orders faster). We also report for the first time—BLEU score results for a large- scale MT task using only non-parallel data (EMEA corpus). ",,,,ACL
37,2013,Automatic Interpretation of the English Possessive,"Stephen Tratz, Eduard Hovy","The English ’s possessive construction occurs frequently in text and can encode several different semantic relations; how- ever, it has received limited attention from the computational linguistics community. This paper describes the creation of a se- mantic relation inventory covering the use of ’s, an inter-annotator agreement study to calculate how well humans can agree on the relations, a large collection of pos- sessives annotated according to the rela- tions, and an accurate automatic annota- tion system for labeling new examples. Our 21,938 example dataset is by far the largest annotated possessives dataset we are aware of, and both our automatic clas- sification system, which achieves 87.4% accuracy in our classification experiment, and our annotation data are publicly avail- able. ",,,,ACL
38,2013,Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web,"Katsuma Narisawa, Yotaro Watanabe, Junta Mizuno, Naoaki Okazaki","This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e.g., three billion) is large, small, or nor- mal for a given context (e.g., number of people facing a water shortage). We first discuss the necessity of numerical com- mon sense in solving textual entailment problems. We explore two approaches for acquiring numerical common sense. Both approaches start with extracting numeri- cal expressions and their context from the Web. One approach estimates the distribu- tion of numbers co-occurring within a con- text and examines whether a given value is large, small, or normal, based on the distri- bution. Another approach utilizes textual patterns with which speakers explicitly ex- presses their judgment about the value of a numerical expression. Experimental re- sults demonstrate the effectiveness of both approaches. ",,,,ACL
39,2013,Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors,"Jackie Chi Kit Cheung, Gerald Penn","Generative probabilistic models have been used for content modelling and template induction, and are typically trained on small corpora in the target domain. In contrast, vector space models of distributional semantics are trained on large cor- pora, but are typically applied to domain- general lexical disambiguation tasks. We introduce Distributional Semantic Hidden Markov Models, a novel variant of a hid- den Markov model that integrates these two approaches by incorporating contex- tualized distributional semantic vectors into a generative model as observed emis- sions. Experiments in slot induction show that our approach yields improvements in learning coherent entity clusters in a do- main. In a subsequent extrinsic evalua- tion, we show that these improvements are also reflected in multi-document summa- rization. ",,,,ACL
40,2013,Extracting bilingual terminologies from comparable corpora,"Ahmet Aker, Monica Paramita, Rob Gaizauskas","In this paper we present a method for extracting bilingual terminologies from comparable corpora. In our approach we treat bilingual term extrac- tion as a classification problem. For classification we use an SVM binary classifier and training data taken from the EUROVOC thesaurus. We test our approach on a held-out test set from EUROVOC and perform precision, recall and f-measure eval- uations for 20 European language pairs. The per- formance of our classifier reaches the 100% pre- cision level for many language pairs. We also perform manual evaluation on bilingual terms ex- tracted from English-German term-tagged compa- rable corpora. The results of this manual evalu- ation showed 60-83% of the term pairs generated are exact translations and over 90% exact or partial translations. ",,,,ACL
41,2013,The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis,"Kashyap Popat, Balamurali A.R, Pushpak Bhattacharyya, Gholamreza Haffari","Expensive feature engineering based on Word Net senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. In this paper, the problem of data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA. ",,,,ACL
42,2013,Large-scale Semantic Parsing via Schema Matching and Lexicon Extension,"Qingqing Cai, Alexander Yates","Supervised training procedures for semantic parsers produce high-quality semantic parsers, but they have difficulty scaling to large databases because of the sheer number of logical constants for which they must see labeled training data. We present a technique for developing seman- tic parsers for large databases based on a reduction to standard supervised train- ing algorithms, schema matching, and pat- tern learning. Leveraging techniques from each of these areas, we develop a semantic parser for Freebase that is capable of pars- ing questions with an F1 that improves by 0.42 over a purely-supervised learning al- gorithm. ",,,,ACL
43,2013,Fast and Accurate Shift-Reduce Constituent Parsing,"Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang","Shift-reduce dependency parsers give comparable accuracies to their chart- based counterparts, yet the best shift-reduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. ",,,,ACL
44,2013,Nonconvex Global Optimization for Latent-Variable Models,"Matthew R. Gormley, Jason Eisner","Many models in NLP involve latent variables, such as unknown parses, tags, or alignments. Finding the optimal model parameters is then usually a difficult noncon- vex optimization problem. The usual practice is to settle for local optimization meth- ods such as EM or gradient ascent. We explore how one might instead search for a global optimum in parameter space, using branch-and-bound. Our method would eventually find the global maxi- mum (up to a user-specified ) if run for long enough, but at any point can return a suboptimal solution together with an up- per bound on the global maximum. As an illustrative case, we study a gener- ative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to for- mulate this as a mixed integer quadratic programming problem with nonlinear con- straints. We use the Reformulation Lin- earization Technique to produce convex relaxations during branch-and-bound. Al- though these techniques do not yet pro- vide a practical solution to our instance of this NP-hard problem, they sometimes find better solutions than Viterbi EM with random restarts, in the same time. ",,,,ACL
45,2013,Parsing with Compositional Vector Grammars,"Richard Socher, John Bauer, Christopher D. Manning, Andrew Y. Ng","Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this rep- resentation does not capture the full syn- tactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting cate- gories only partly address the problem at the cost of huge feature spaces and sparse- ness. Instead, we introduce a Compo- sitional Vector Grammar (CVG), which combines PCFGs with a syntactically un- tied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments. ",,,,ACL
46,2013,Discriminative state tracking for spoken dialog systems,"Angeliki Metallinou, Dan Bohus, Jason Williams","In spoken dialog systems, statistical state tracking aims to improve robustness to speech recognition errors by tracking a posterior distribution over hidden dialog states. Current approaches based on generative or discriminative models have differ- ent but important shortcomings that limit their accuracy. In this paper we discuss these limitations and introduce a new ap- proach for discriminative state tracking that overcomes them by leveraging the problem structure. An offline evaluation with dialog data collected from real users shows improvements in both state track- ing accuracy and the quality of the pos- terior probabilities. Features that encode speech recognition error patterns are par- ticularly helpful, and training requires rel- atively few dialogs. ",,,,ACL
47,2013,Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition,"Man Lan, Yu Xu, Zhengyu Niu","To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by remov- ing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learn- ing tasks, can achieve an averaged F 1 im- provement of 5.86% over baseline models. ",,,,ACL
48,2013,Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis,"Shafiq Joty, Giuseppe Carenini, Raymond Ng, Yashar Mehdad","We propose a novel approach for develop- ing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Con- ditional Random Fields: one for intra- sentential parsing and the other for multi- sentential parsing. We present two ap- proaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the state- of-the-art, often by a wide margin. ",,,,ACL
49,2013,Improving pairwise coreference models through feature space hierarchy learning,"Emmanuel Lassalle, Pascal Denis","This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we con- struct distinct classification models. In ef- fect, our approach finds an optimal fea- ture space (derived from a base feature set and indicator set) for discriminating coref- erential mention pairs. Although our ap- proach explores a very large space of pos- sible feature spaces, it remains tractable by exploiting the structure of the hierar- chies built from the indicators. Our exper- iments on the CoNLL-2012 Shared Task English datasets (gold mentions) indicate that our method is robust relative to dif- ferent clustering strategies and evaluation metrics, showing large and consistent im- provements over a single pairwise model using the same base features. Our best system obtains a competitive 67.2 of aver- age F1 over MUC, B 3 , and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets. ",,,,ACL
50,2013,Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval,"K. Tamsin Maxwell, Jon Oberlander, W. Bruce Croft","Techniques that compare short text segments using dependency paths (or simply, paths) appear in a wide range of automated language processing applications including question answering (QA). However, few models in ad hoc information retrieval (IR) use paths for document ranking due to the prohibitive cost of parsing a retrieval collection. In this paper, we introduce a flexible notion of paths that describe chains of words on a dependency path. These chains, or catenae, are readily applied in standard IR models. Informative catenae are selected using supervised machine learning with linguistically informed fea- tures and compared to both non-linguistic terms and catenae selected heuristically with filters derived from work on paths. Automatically selected catenae of 1-2 words deliver significant performance gains on three TREC collections. ",,,,ACL
51,2013,Coordination Structures in Dependency Treebanks,"Martin Popel, David Mareček, Jan Štěpánek, Daniel Zeman","Paratactic syntactic structures are noto- riously difficult to represent in dependency formalisms. This has painful consequences such as high frequency of parsing errors related to coordination. In other words, coordination is a pending prob- lem in dependency analysis of natural lan- guages. This paper tries to shed some light on this area by bringing a system- atizing view of various formal means de- veloped for encoding coordination struc- tures. We introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages. In addition, empirical obser- vations on convertibility between selected styles of representations are shown too. ",,,,ACL
52,2013,GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web,"Flavio De Benedictis, Stefano Faralli, Roberto Navigli","We present GlossBoot, an effective minimally-supervised approach to acquiring wide-coverage domain glossaries for many languages. For each language of interest, given a small number of hypernymy relation seeds concerning a target domain, we bootstrap a glossary from the Web for that domain by means of iteratively acquired term/gloss extraction patterns. Our experiments show high performance in the acquisition of domain terminologies and glossaries for three different languages. ",,,,ACL
53,2013,Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model,"Ulle Endriss, Raquel Fernández","Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective anno- tation, taking inspiration from the field of social choice theory. We formulate a gen- eral formal model for collective annotation and propose several aggregation methods that go beyond the commonly used major- ity rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation. ",,,,ACL
54,2013,ParGramBank: The ParGram Parallel Treebank,"Sebastian Sulger, Miriam Butt, Tracy Holloway King, Paul Meurer","This paper discusses the construction of a parallel treebank currently involving ten languages from six language families. The treebank is based on deep LFG (Lexical- Functional Grammar) grammars that were developed within the framework of the ParGram (Parallel Grammar) effort. The grammars produce output that is maxi- mally parallelized across languages and language families. This output forms the basis of a parallel treebank covering a diverse set of phenomena. The treebank is publicly available via the INESS tree- banking environment, which also allows for the alignment of language pairs. We thus present a unique, multilayered paral- lel treebank that represents more and dif- ferent types of languages than are avail- able in other treebanks, that represents deep linguistic knowledge and that allows for the alignment of sentences at sev- eral levels: dependency structures, con- stituency structures and POS information. ",,,,ACL
55,2013,Identifying Bad Semantic Neighbors for Improving Distributional Thesauri,Olivier Ferret,"Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an impor- tant issue that is mainly tackled indirectly through the improvement of semantic sim- ilarity measures. In this article, we pro- pose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification re- lies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by ap- plying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies. ",,,,ACL
56,2013,Models of Semantic Representation with Visual Attributes,"Carina Silberer, Vittorio Ferrari, Mirella Lapata",We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual at- tributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classi- fiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word as- sociation data compared to amodal models and word representations based on hand- crafted norming data. ,,,,ACL
57,2013,Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages,"Dan Garrette, Jason Mielens, Jason Baldridge","Developing natural language processing tools for low-resource languages often re- quires creating resources from scratch. While a variety of semi-supervised methods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary. We dis- cuss a series of experiments designed to shed light on such questions in the con- text of part-of-speech tagging. We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy (as well as English) and eval- uate how the amounts of various kinds of data affect performance of a trained POS -tagger. Our results show that an- notation of word types is the most im- portant, provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. We also show that finite- state morphological analyzers are effective sources of type information when few la- beled examples are available. ",,,,ACL
58,2013,Using subcategorization knowledge to improve case prediction for translation to German,"Marion Weller, Alexander Fraser, Sabine Schulte im Walde","This paper demonstrates the need and im- pact of subcategorization information for SMT. We combine (i) features on source- side syntactic subcategorization and (ii) an external knowledge base with quantitative, dependency-based information about target-side subcategorization frames. A manual evaluation of an English-to- German translation task shows that the subcategorization information has a posi- tive impact on translation quality through better prediction of case. ",,,,ACL
59,2013,Name-aware Machine Translation,"Haibo Li, Jing Zheng, Heng Ji, Qi Li","We propose a Name-aware Machine Translation (MT) approach which can tightly integrate name processing into MT model, by jointly annotating parallel corpora, extracting name-aware translation grammar and rules, adding name phrase table and name translation driven decod- ing. Additionally, we also propose a new MT metric to appropriately evaluate the translation quality of informative words, by assigning different weights to differ- ent words according to their importance values in a document. Experiments on Chinese-English translation demonstrated the effectiveness of our approach on en- hancing the quality of overall translation, name translation and word alignment over a high-quality MT baseline 1 . ",,,,ACL
60,2013,Decipherment Complexity in 1:1 Substitution Ciphers,"Malte Nuhn, Hermann Ney","In this paper we show that even for the case of 1:1 substitution ciphers—which encipher plaintext symbols by exchang- ing them with a unique substitute—finding the optimal decipherment with respect to a bigram language model is NP-hard. We show that in this case the decipherment problem is equivalent to the quadratic as- signment problem (QAP). To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature be- fore. ",,,,ACL
61,2013,Non-Monotonic Sentence Alignment via Semisupervised Learning,"Xiaojun Quan, Chunyu Kit, Yan Song","This paper studies the problem of non- monotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisuper- vised learning approach based on two as- sumptions: (1) sentences with high affinity in one language tend to have their counter- parts with similar relatedness in the other; and (2) initial alignment is readily avail- able with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally op- timal solution. The evaluation with real- world legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data. ",,,,ACL
62,2013,Bootstrapping Entity Translation on Weakly Comparable Corpora,"Taesung Lee, Seung-won Hwang","This paper studies the problem of mining named entity translations from comparable corpora with some “asymmetry”. Unlike the previous approaches relying on the “symmetry” found in parallel corpora, the proposed method is tolerant to asymme- try often found in comparable corpora, by distinguishing different semantics of rela- tions of entity pairs to selectively prop- agate seed entity translations on weakly comparable corpora. Our experimental results on English-Chinese corpora show that our selective propagation approach outperforms the previous approaches in named entity translation in terms of the mean reciprocal rank by up to 0.16 for or- ganization names, and 0.14 in a low com- parability case. ",,,,ACL
63,2013,Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia,"Zhigang Wang, Zhixing Li, Juanzi Li, Jie Tang","Wikipedia infoboxes are a valuable source of structured knowledge for global knowledge sharing. However, infobox information is very incomplete and imbalanced among the Wikipedias in different languages. It is a promising but challenging problem to utilize the rich structured knowledge from a source language Wikipedia to help complete the missing in- foboxes for a target language. In this paper, we formulate the problem of cross-lingual knowledge extraction from multilingual Wikipedia sources, and present a novel framework, called Wiki- CiKE, to solve this problem. An instance-based transfer learning method is utilized to overcome the problems of topic drift and translation errors. Our experimen- tal results demonstrate that Wiki CiKE out- performs the monolingual knowledge ex- traction method and the translation-based method. ",,,,ACL
64,2013,Bridging Languages through Etymology: The case of cross language text categorization,"Vivi Nastase, Carlo Strapparava","We propose the hypothesis that word etymology is useful for NLP applications as a bridge between languages. We support this hypothesis with experiments in cross- language (English-Italian) document cat- egorization. In a straightforward bag-of- words experimental set-up we add etymo- logical ancestors of the words in the docu- ments, and investigate the performance of a model built on English data, on Italian test data (and viceversa). The results show not only statistically significant, but a large improvement – a jump of almost 40 points in F1-score – over the raw (vanilla bag-of- words) representation. ",,,,ACL
65,2013,Creating Similarity: Lateral Thinking for Vertical Similarity Judgments,"Tony Veale, Guofu Li","Just as observing is more than just see- ing, comparing is far more than mere matching. It takes understanding, and even inventiveness, to discern a useful basis for judging two ideas as similar in a particular context, especially when our perspective is shaped by an act of linguis- tic creativity such as metaphor, simile or analogy. Structured resources such as WordNet offer a convenient hierarchical means for converging on a common ground for comparison, but offer little support for the divergent thinking that is needed to creatively view one concept as another. We describe such a means here, by showing how the web can be used to harvest many divergent views for many familiar ideas. These lateral views com- plement the vertical views of WordNet, and support a system for idea exploration called Thesaurus Rex. We show also how Thesaurus Rex supports a novel, genera- tive similarity measure for WordNet. ",,,,ACL
66,2013,Discovering User Interactions in Ideological Discussions,"Arjun Mukherjee, Bing Liu","Online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest. In forums where users discuss social, political, or religious issues, there are often heated debates among users or participants. Existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points. In this paper, we focus on identifying the nature of interactions among user pairs. The central questions are: How does each pair of users interact with each other? Does the pair of users mostly agree or disagree? What is the lexicon that people often use to express agreement and disagreement? We present a topic model based approach to answer these questions. Since agreement and disagreement expressions are usually multi- word phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling. After modeling, we use the modeling results to classify the nature of interaction of each user pair. Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques. ",,,,ACL
67,2013,Multilingual Affect Polarity and Valence Prediction in Metaphor-Rich Texts,Zornitsa Kozareva,"Metaphor is an important way of conveying the affect of people, hence understanding how people use metaphors to convey affect is important for the communication between individuals and increases cohesion if the perceived affect of the con- crete example is the same for the two in- dividuals. Therefore, building computa- tional models that can automatically iden- tify the affect in metaphor-rich texts like “The team captain is a rock.”, “Time is money.”, “My lawyer is a shark.” is an important challenging problem, which has been of great interest to the research com- munity. To solve this task, we have collected and manually annotated the affect of metaphor-rich texts for four languages. We present novel algorithms that integrate triggers for cognitive, affective, perceptual and social processes with stylistic and lex- ical information. By running evaluations on datasets in English, Spanish, Russian and Farsi, we show that the developed af- fect polarity and valence prediction tech- nology of metaphor-rich texts is portable and works equally well for different lan- guages. ",,,,ACL
68,2013,Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language,"Tiberiu Boros, Radu Ion, Dan Tufis","Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories). For this reason, a number of alternative methods have been proposed over the years. One of the most successful methods used for this task, exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. A second phase is aimed at recovering the full set of morpho-syntactic features. In this paper we present an alternative method to Tiered Tagging, based on local optimizations with Neural Networks and we show how, by properly encoding the input sequence in a general Neural Network architecture, we achieve results similar to the Tiered Tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method. ",,,,ACL
69,2013,Learning to lemmatise Polish noun phrases,Adam Radziszewski,"We present a novel approach to noun phrase lemmatisation where the main phase is cast as a tagging problem. The idea draws on the observation that the lemmatisation of almost all Polish noun phrases may be decomposed into trans- formation of singular words (tokens) that make up each phrase. We perform eval- uation, which shows results similar to those obtained earlier by a rule-based sys- tem, while our approach allows to separate chunking from lemmatisation. ",,,,ACL
70,2013,Using Conceptual Class Attributes to Characterize Social Media Users,"Shane Bergsma, Benjamin Van Durme","We describe a novel approach for automatically predicting the hidden demographic properties of social media users. Building on prior work in common-sense knowledge acquisition from third-person text, we first learn the distinguishing attributes of certain classes of people. For exam- ple, we learn that people in the Female class tend to have maiden names and en- gagement rings. We then show that this knowledge can be used in the analysis of first-person communication; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new train- ing examples. Our novel approach enables substantial improvements on the widely- studied task of user gender prediction, ob- taining a 20% relative error reduction over the current state-of-the-art. ",,,,ACL
71,2013,The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia,"Oliver Ferschke, Iryna Gurevych, Marc Rittberger","With the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key chal- lenge. However, only a small amount of annotated data is available for training quality assessment systems. Wikipedia contains a large amount of texts anno- tated with cleanup templates which iden- tify quality flaws. We show that the dis- tribution of these labels is topically bi- ased, since they cannot be applied freely to any arbitrary article. We argue that it is necessary to consider the topical restric- tions of each label in order to avoid a sam- pling bias that results in a skewed classifier and overly optimistic evaluation results. We factor out the topic bias by extracting reliable training instances from the revi- sion history which have a topic distribu- tion similar to the labeled articles. This ap- proach better reflects the situation a classi- fier would face in a real-life application. ",,,,ACL
72,2013,Mining Informal Language from Chinese Microtext: Joint Word Recognition and Segmentation,"Aobo Wang, Min-Yen Kan","We address the problem of informal word recognition in Chinese microblogs. A key problem is the lack of word delimiters in Chinese. We exploit this reliance as an opportunity: recognizing the relation between informal word recognition and Chi- nese word segmentation, we propose to model the two tasks jointly. Our joint in- ference method significantly outperforms baseline systems that conduct the tasks in- dividually or sequentially. ",,,,ACL
73,2013,Generating Synthetic Comparable Questions for News Articles,"Oleg Rokhlenko, Idan Szpektor","We introduce the novel task of automati- cally generating questions that are relevant to a text but do not appear in it. One motivating example of its application is for increasing user engagement around news articles by suggesting relevant compara- ble questions, such as “is Beyonce a bet- ter singer than Madonna?”, for the user to answer. We present the first algorithm for the task, which consists of: (a) of- fline construction of a comparable ques- tion template database; (b) ranking of rel- evant templates to a given article; and (c) instantiation of templates only with enti- ties in the article whose comparison un- der the template’s relation makes sense. We tested the suggestions generated by our algorithm via a Mechanical Turk ex- periment, which showed a significant im- provement over the strongest baseline of more than 45% in all metrics. ",,,,ACL
74,2013,Punctuation Prediction with Transition-based Parsing,"Dongdong Zhang, Shuangzhi Wu, Nan Yang, Mu Li","Punctuations are not available in automatic speech recognition outputs, which could create barriers to many subsequent text pro- cessing tasks. This paper proposes a novel method to predict punctuation symbols for the stream of words in transcribed speech texts. Our method jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features when processing words from left to right. It can exploit a global view to capture long-range dependencies for punc- tuation prediction with linear complexity. The experimental results on the test data sets of IWSLT and TDT4 show that our method can achieve high-level performance in punctuation prediction over the stream of words in tran- scribed speech text. ",,,,ACL
75,2013,Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study,"Wenbin Jiang, Meng Sun, Yajuan Lü, Yating Yang","Structural information in web text provides natural annotations for NLP problems such as word segmentation and parsing. In this paper we propose a discrim- inative learning algorithm to take advantage of the linguistic knowledge in large amounts of natural annotations on the In- ternet. It utilizes the Internet as an external corpus with massive (although slight and sparse) natural annotations, and enables a classifier to evolve on the large-scaled and real-time updated web text. With Chinese word segmentation as a case study, exper- iments show that the segmenter enhanced with the Chinese wikipedia achieves sig- nificant improvement on a series of testing sets from different domains, even with a single classifier and local features. ",,,,ACL
76,2013,Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging,"Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, Isabel Trancoso","This paper introduces a graph-based semi-supervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor simi- larity graph over all trigrams of labeled and unlabeled data for propagating syn- tactic information, i.e., label distribution- s. The derived label distributions are re- garded as virtual evidences to regular- ize the learning of linear conditional ran- dom fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better result- s than the supervised baselines and other competitive semi-supervised CRFs in this task. ",,,,ACL
77,2013,An Infinite Hierarchical Bayesian Model of Phrasal Translation,"Trevor Cohn, Gholamreza Haffari","Modern phrase-based machine translation systems make extensive use of word- based translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many trans- lation phenomena that do not decompose into word-for-word translation. This pa- per presents a novel method for induc- ing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminat- ing at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competi- tive baseline systems. ",,,,ACL
78,2013,Additive Neural Networks for Statistical Machine Translation,"Lemao Liu, Taro Watanabe, Eiichiro Sumita, Tiejun Zhao","Most statistical machine translation (SMT) systems are modeled using a log-linear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential. A neural network is a reasonable method to address these pitfalls. However, modeling SMT with a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embed- ding features on Chinese-to-English and Japanese-to-English translation tasks. ",,,,ACL
79,2013,Hierarchical Phrase Table Combination for Machine Translation,"Conghui Zhu, Taro Watanabe, Eiichiro Sumita, Tiejun Zhao","Typical statistical machine translation systems are batch trained with a given train- ing data and their performances are large- ly influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training ev- ery time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In par- ticular, we train a Bayesian phrasal inver- sion transduction grammars for each do- main separately. The learned phrase ta- bles are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the tra- ditional batch training method. Further- more, each phrase table is trained sepa- rately in each domain, and while compu- tational overhead is significantly reduced by training them in parallel. ",,,,ACL
80,2013,Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation,"Fabienne Braune, Nina Seemann, Daniel Quernheim, Andreas Maletti","We present a new translation model integrating the shallow local multi bottom-up tree transducer. We perform a large-scale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree base- line on the WMT 2009 English → German translation task. As an additional contribu- tion we make the developed software and complete tool-chain publicly available for further experimentation. ",,,,ACL
81,2013,Enlisting the Ghost: Modeling Empty Categories for Machine Translation,"Bing Xiang, Xiaoqiang Luo, Bowen Zhou","Empty categories (EC) are artificial elements in Penn Treebanks motivated by the government-binding (GB) theory to explain certain language phenomena such as pro-drop. ECs are ubiquitous in languages like Chinese, but they are tacitly ignored in most machine translation (MT) work because of their elusive nature. In this paper we present a comprehensive treat- ment of ECs by first recovering them with a structured MaxEnt model with a rich set of syntactic and lexical features, and then incorporating the predicted ECs into a Chinese-to-English machine translation task through multiple approaches, includ- ing the extraction of EC-specific sparse features. We show that the recovered empty categories not only improve the word alignment quality, but also lead to significant improvements in a large-scale state-of-the-art syntactic MT system. ",,,,ACL
82,2013,A Multi-Domain Translation Model Framework for Statistical Machine Translation,"Rico Sennrich, Holger Schwenk, Walid Aransa","While domain adaptation techniques for SMT have proven to be effective at improving translation quality, their practicality for a multi-domain environment is of ten limited because of the computational and human costs of developing and main- taining multiple systems adapted to differ- ent domains. We present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also de- scribe a method for unsupervised adapta- tion with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effec- tiveness of both our translation model ar- chitecture and automatic clustering, with gains of up to ",,,,ACL
83,2013,Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation,"Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Hiroya Takamura","This paper proposes a nonparametric Bayesian method for inducing Part-of- Speech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilin- gual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned tar- get word, either jointly (joint model), or independently (independent model). Eval- uations of Japanese-to-English translation on the NTCIR-9 data show that our in- duced Japanese POS tags for dependency trees improve the performance of a forest- to-string SMT system. Our independent model gains over 1 point in BLEU by re- solving the sparseness problem introduced in the joint model. ",,,,ACL
84,2013,Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization,"Guangyou Zhou, Fang Liu, Yang Liu, Shizhu He","Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implic- itly expanding the queried questions with additional words or phrases using mono- lingual translation models. While use- ful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they are troubled by noise issue. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advan- tage of potentially rich semantic informa- tion drawn from other languages. Our pro- posed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other lan- guages via matrix factorization. Experi- ments conducted on a real CQA data show that our proposed approach is promising. ",,,,ACL
85,2013,Improved Lexical Acquisition through DPP-based Verb Clustering,"Roi Reichart, Anna Korhonen","Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicate argument structure. We present the first unified framework for unsupervised learn- ing of these three types of information. We show how to utilize Determinantal Point Processes (DPPs), elegant proba- bilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high qual- ity and diverse subsets, for clustering. Our novel clustering algorithm constructs a joint SCF-DPP DPP kernel matrix and uti- lizes the efficient sampling algorithms of DPPs to cluster together verbs with sim- ilar SCFs and SPs. We evaluate the in- duced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1 . ",,,,ACL
86,2013,Semantic Frames to Predict Stock Price Movement,"Boyi Xie, Rebecca J. Passonneau, Leon Wu, Germán G. Creamer","Semantic frames are a rich linguistic re- source. There has been much work on semantic frame parsers, but less that applies them to general NLP problems. We address a task to predict change in stock price from financial news. Seman- tic frames help to generalize from spe- cific sentences to scenarios, and to de- tect the (positive or negative) roles of spe- cific companies. We introduce a novel tree representation, and use it to train predic- tive models with tree kernels using sup- port vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that fea- tures derived from semantic frame pars- ing have significantly better performance across years on the polarity task. ",,,,ACL
87,2013,Density Maximization in Context-Sense Metric Space for All-words WSD,"Koichi Tanigaki, Mitsuteru Shiba, Tatsuji Munaka, Yoshinori Sagisaka","This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity result- ing from a large variety of linguistic con- text and sense, as well as to exploit sense- interdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic ker- nels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superi- ority of the proposed method over conven- tional ones by showing the better perfor- mances beyond most-frequent-sense base- line performance where none of SemEval- 2 unsupervised systems reached. ",,,,ACL
88,2013,The Role of Syntax in Vector Space Models of Compositional Semantics,"Karl Moritz Hermann, Phil Blunsom","Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a funda- mental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Com- binatory Categorial Autoencoders. This model leverages the CCG combinatory op- erators to guide a non-linear transforma- tion of meaning within a sentence. We use this model to learn high dimensional em- beddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a con- cise model to learn representations that are both effective and general. ",,,,ACL
89,2013,Margin-based Decomposed Amortized Inference,"Gourab Kundu, Vivek Srikumar, Dan Roth","Given that structured output prediction is typically performed over entire datasets, one natural question is whether it is pos- sible to re-use computation from earlier inference instances to speed up inference for future instances. Amortized inference has been proposed as a way to accomplish this. In this paper, first, we introduce a new amortized inference algorithm called the Margin-based Amortized Inference, which uses the notion of structured margin to identify inference problems for which pre- vious solutions are provably optimal. Sec- ond, we introduce decomposed amortized inference, which is designed to address very large inference problems, where ear- lier amortization methods become less ef- fective. This approach works by decom- posing the output structure and applying amortization piece-wise, thus increasing the chance that we can re-use previous so- lutions for parts of the output structure. These parts are then combined to a global coherent solution using Lagrangian relax- ation. In our experiments, using the NLP tasks of semantic role labeling and entity- relation extraction, we demonstrate that with the margin-based algorithm, we need to call the inference engine only for a third of the test examples. Further, we show that the decomposed variant of margin-based amortized inference achieves a greater re- duction in the number of inference calls. ",,,,ACL
90,2013,Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression,"Asli Celikyilmaz, Dilek Hakkani-Tur, Gokhan Tur, Ruhi Sarikaya","Finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity. Furthermore, data mismatch issues, which arise when the expected test (target) data does not exactly match the training data, aggra- vate this scarcity problem. To deal with these issues, we describe an efficient semi- supervised learning (SSL) approach which has two components: (i) Markov Topic Regression is a new probabilistic model to cluster words into semantic tags (con- cepts). It can efficiently handle seman- tic ambiguity by extending standard topic models with two new features. First, it en- codes word n-gram features from labeled source and unlabeled target data. Sec- ond, by going beyond a bag-of-words ap- proach, it takes into account the inherent sequential nature of utterances to learn se- mantic classes based on context. (ii) Ret- rospective Learner is a new learning tech- nique that adapts to the unlabeled target data. Our new SSL approach improves semantic tagging performance by 3% ab- solute over the baseline models, and also compares favorably on semi-supervised syntactic tagging. ",,,,ACL
91,2013,Parsing Graphs with Hyperedge Replacement Grammars,"David Chiang, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann","Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimiza- tion analogous to binarization of context- free grammars, and some important im- plementation details, resulting in an algo- rithm that is practical for natural-language applications. The algorithm is part of Boli- nas, a new software toolkit for HRG pro- cessing. ",,,,ACL
92,2013,Grounded Unsupervised Semantic Parsing,Hoifung Poon,"We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. Our GUSP system produces a semantic parse by annotat- ing the dependency-tree nodes and edges with latent states, and learns a proba- bilistic grammar using EM. To compen- sate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by su- pervised approaches. ",,,,ACL
93,2013,Automatic detection of deception in child-produced speech using syntactic complexity features,"Maria Yancheva, Frank Rudzicz","It is important that the testimony of children be admissible in court, especially given allegations of abuse. Unfortunately, children can be misled by interrogators or might offer false information, with dire consequences. In this work, we evalu- ate various parameterizations of five clas- sifiers (including support vector machines, neural networks, and random forests) in deciphering truth from lies given tran- scripts of interviews with 198 victims of abuse between the ages of 4 and 7. These evaluations are performed using a novel set of syntactic features, including mea- sures of complexity. Our results show that sentence length, the mean number of clauses per utterance, and the Stajner- Mitkov measure of complexity are highly informative syntactic features, that classi- fication accuracy varies greatly by the age of the speaker, and that accuracy up to 91.7 % can be achieved by support vec- tor machines given a sufficient amount of data. ",,,,ACL
94,2013,Sentiment Relevance,"Christian Scheible, Hinrich Schütze","A number of different notions, including subjectivity, have been proposed for dis- tinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demon- strate experimentally that sentiment rele- vance and subjectivity are related, but dif- ferent. Since no large amount of labeled training data for our new notion of sen- timent relevance is available, we investi- gate two semi-supervised methods for cre- ating sentiment relevance classifiers: a dis- tant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge trans- fer. We show that both methods learn sen- timent relevance classifiers that perform well. ",,,,ACL
95,2013,Predicting and Eliciting Addressee’s Emotion in Online Dialogue,"Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga, Masashi Toyoda","While there have been many attempts to estimate the emotion of an addresser from her/his utterance, few studies have ex- plored how her/his utterance affects the emotion of the addressee. This has motivated us to investigate two novel tasks: predicting the emotion of the addressee and generating a response that elicits a specific emotion in the addressee’s mind. We target Japanese Twitter posts as a source of dialogue data and automatically build training data for learning the pre- dictors and generators. The feasibility of our approaches is assessed by using 1099 utterance-response pairs that are built by five human workers. ",,,,ACL
96,2013,Utterance-Level Multimodal Sentiment Analysis,"Verónica Pérez-Rosas, Rada Mihalcea, Louis-Philippe Morency","During real-life interactions, people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions. With the recent growth of social websites such as YouTube, Facebook, and Amazon, video reviews are emerging as a new source of multimodal and natural opinions that has been left al- most untapped by automatic opinion anal- ysis techniques. This paper presents a method for multimodal sentiment classi- fication, which can identify the sentiment expressed in utterance-level visual datas- treams. Using a new multimodal dataset consisting of sentiment annotated utter- ances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality. ",,,,ACL
97,2013,Probabilistic Sense Sentiment Similarity through Hidden Emotions,"Mitra Mohtarami, Man Lan, Chew Lim Tan","Sentiment Similarity of word pairs reflects the distance between the words regarding their underlying sentiments. This paper aims to in- fer the sentiment similarity between word pairs with respect to their senses. To achieve this aim, we propose a probabilistic emotion- based approach that is built on a hidden emo- tional model. The model aims to predict a vec- tor of basic human emotions for each sense of the words. The resultant emotional vectors are then employed to infer the sentiment similarity of word pairs. We apply the proposed ap- proach to address two main NLP tasks, name- ly, Indirect yes/no Question Answer Pairs in- ference and Sentiment Orientation prediction. Extensive experiments demonstrate the effec- tiveness of the proposed approach. ",,,,ACL
98,2013,A user-centric model of voting intention from Social Media,"Vasileios Lampos, Daniel Preoţiuc-Pietro, Trevor Cohn","Social Media contain a multitude of user opinions which can be used to predict real-world phenomena in many domains including politics, finance and health. Most existing methods treat these problems as linear regression, learning to relate word frequencies and other simple features to a known response variable (e.g., voting intention polls or financial indicators). These techniques require very careful fil- tering of the input texts, as most Social Media posts are irrelevant to the task. In this paper, we present a novel approach which performs high quality filtering au- tomatically, through modelling not just words but also users, framed as a bilin- ear model with a sparse regulariser. We also consider the problem of modelling groups of related output variables, us- ing a structured multi-task regularisation method. Our experiments on voting inten- tion prediction demonstrate strong perfor- mance over large-scale input from Twitter on two distinct case studies, outperform- ing competitive baselines. ",,,,ACL
99,2013,Using Supervised Bigram-based ILP for Extractive Summarization,"Chen Li, Xian Qian, Yang Liu","In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference sum- mary. The regression model uses a vari- ety of indicative features and is trained dis- criminatively to minimize the distance be- tween the estimated and the ground truth bigram frequency in the reference sum- mary. During testing, the sentence selec- tion problem is formulated as an ILP prob- lem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also con- ducted various analysis to show the im- pact of bigram selection, weight estima- tion, and ILP setup. ",,,,ACL
100,2013,Summarization Through Submodularity and Dispersion,"Anirban Dasgupta, Ravi Kumar, Sujith Ravi","We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the sum- marization desideratum is expressed as a sum of a submodular function and a non- submodular function, which we call dis- persion; the latter uses inter-sentence dis- similarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion func- tions and show that a greedy algorithm can obtain an approximately optimal sum- mary in all three cases. We conduct ex- periments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algo- rithm outperforms those that rely only on submodularity. ",,,,ACL
101,2013,Subtree Extractive Summarization via Submodular Maximization,"Hajime Morita, Ryohei Sasano, Hiroya Takamura, Manabu Okumura","This study proposes a text summarization model that simultaneously performs sentence extraction and compression. We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster. We also encode obligatory case constraints as must-link dependency constraints in or- der to guarantee the readability of the gen- erated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 1 2 (1 ? e ? 1 ) . Our experiments with the NTCIR ACLIA test collections show that our approach outper- forms a state-of-the-art algorithm. ",,,,ACL
102,2013,The effect of non-tightness on Bayesian estimation of PCFGs,"Shay B. Cohen, Mark Johnson","Probabilistic context-free grammars have the unusual property of not always defining tight distributions (i.e., the sum of the “probabilities” of the trees the grammar generates can be less than one). This paper reviews how this non-tightness can arise and discusses its impact on Bayesian es- timation of PCFGs. We begin by present- ing the notion of “almost everywhere tight grammars” and show that linear CFGs fol- low it. We then propose three different ways of reinterpreting non-tight PCFGs to make them tight, show that the Bayesian estimators in Johnson et al. (2007) are correct under one of them, and provide MCMC samplers for the other two. We conclude with a discussion of the impact of tightness empirically. ",,,,ACL
103,2013,Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources,"Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka, Yusuke Miyao","This paper describes a method of inducing wide-coverage CCG resources for Japanese. While deep parsers with corpus- induced grammars have been emerging for some languages, those for Japanese have not been widely studied, mainly be- cause most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based cor- pora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexi- con and the accuracy of parsing. ",,,,ACL
104,2013,Transition-based Dependency Parsing with Selectional Branching,"Jinho D. Choi, Andrew McCallum","We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected lin- ear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transition- based parser that uses beam search. ",,,,ACL
105,2013,Bilingually-Guided Monolingual Dependency Grammar Induction,"Kai Liu, Yajuan Lü, Wenbin Jiang, Qun Liu","This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed coun- terpart language, and simultaneously min- ing the underlying syntactic structure of the language considered, it effectively in- tegrates the advantages of bilingual pro- jection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised in- duction. We induced dependency gram- mar for five different languages under the guidance of dependency information pro- jected from the parsed English translation, experiments show that the bilingually- guided method achieves a significant improvement of 28.5% over the unsuper- vised baseline and 3.0% over the best pro- jection baseline on average. ",,,,ACL
106,2013,Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition,"Mengqiu Wang, Wanxiang Che, Christopher D. Manning","Translated bi-texts contain complemen- tary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions be- tween the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mis- takes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two uni- directional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tag- ging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the com- bined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields signif- icant improvements in both NER and word alignment over state-of-the-art monolin- gual baselines. ",,,,ACL
107,2013,Resolving Entity Morphs in Censored Data,"Hongzhao Huang, Zhen Wen, Dian Yu, Heng Ji","In some societies, internet users have to create information morphs (e.g. “Peace West King” to refer to “Bo Xilai”) to avoid active censorship or achieve other communication goals. In this paper we aim to solve a new problem of resolving en- tity morphs to their real targets. We ex- ploit temporal constraints to collect cross- source comparable corpora relevant to any given morph query and identify target can- didates. Then we propose various novel similarity measurements including surface features, meta-path based semantic fea- tures and social correlation features and combine them in a learning-to-rank frame- work. Experimental results on Chinese Sina Weibo data demonstrate that our ap- proach is promising and significantly out- performs baseline methods 1 . ",,,,ACL
108,2013,Learning to Extract International Relations from Political Context,"Brendan O’Connor, Brandon M. Stewart, Noah A. Smith","We describe a new probabilistic model for extracting events between major political actors from news corpora. Our unsupervised model brings together familiar components in natural language processing (like parsers and topic models) with contextual political information— temporal and dyad dependence—to infer latent event classes. We quantitatively evaluate the model’s performance on political science benchmarks: recover- ing expert-assigned event class valences, and detecting real-world conflict. We also conduct a small case study based on our model’s inferences. A supplementary appendix, and replica- tion software/data are available online, at: http://brenocon.com/irevents ",,,,ACL
109,2013,Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation,"Majid Razmara, Maryam Siahbani, Reza Haffari, Anoop Sarkar","Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language transla- tions. Experimental results show that our graph propagation method significantly improves per- formance over two strong baselines under intrin- sic and extrinsic evaluation metrics. ",,,,ACL
110,2013,Online Relative Margin Maximization for Statistical Machine Translation,"Vladimir Eidelman, Yuval Marton, Philip Resnik","Recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization, such as the spread of the data. However, these so- lutions are impractical in complex struc- tured prediction problems such as statis- tical machine translation. We present an online gradient-based algorithm for rela- tive margin maximization, which bounds the spread of the projected data while max- imizing the margin. We evaluate our op- timizer on Chinese-English and Arabic- English translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant im- provements of 1.2-2 B LEU and 1.7-4.3 TER on average over state-of-the-art opti- mizers with the large feature set. ",,,,ACL
111,2013,Handling Ambiguities of Bilingual Predicate-Argument Structures for Statistical Machine Translation,"Feifei Zhai, Jiajun Zhang, Yu Zhou, Chengqing Zong","Predicate-argument structure (PAS) has been demonstrated to be very effective in improving SMT performance. However, since a source-side PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation. In this pa- per, we group PAS ambiguities into two types: role ambiguity and gap ambiguity. Then we propose two novel methods to handle the two PAS ambiguities for SMT accordingly: 1) in- side context integration; 2) a novel maximum entropy PAS disambiguation (MEPD) model. In this way, we incorporate rich context in- formation of PAS for disambiguation. Then we integrate the two methods into a PAS- based translation framework. Experiments show that our approach helps to achieve sig- nificant improvements on translation quality. ",,,,ACL
112,2013,Reconstructing an Indo-European Family Tree from Non-native English Texts,"Ryo Nagata, Edward Whittaker","Mother tongue interference is the phenomenon where linguistic systems of a mother tongue are transferred to another language. Although there has been plenty of work on mother tongue interference, very little is known about how strongly it is transferred to another language and about what relation there is across mother tongues. To address these questions, this paper explores and visualizes mother tongue interference preserved in English texts written by Indo-European language speakers. This paper further explores lin- guistic features that explain why certain relations are preserved in English writing, and which contribute to related tasks such as native language identification. ",,,,ACL
113,2013,Word Association Profiles and their Use for Automated Scoring of Essays,"Beata Beigman Klebanov, Michael Flor","We describe a new representation of the content vocabulary of a text we call word association profile that captures the pro- portions of highly associated, mildly associated, unassociated, and dis-associated pairs of words that co-exist in the given text. We illustrate the shape of the dis- tirbution and observe variation with genre and target audience. We present a study of the relationship between quality of writ- ing and word association profiles. For a set of essays written by college graduates on a number of general topics, we show that the higher scoring essays tend to have higher percentages of both highly asso- ciated and dis-associated pairs, and lower percentages of mildly associated pairs of words. Finally, we use word association profiles to improve a system for automated scoring of essays. ",,,,ACL
114,2013,Adaptive Parser-Centric Text Normalization,"Congle Zhang, Tyler Baldwin, Howard Ho, Benny Kimelfeld","Text normalization is an important first step towards enabling many Natural Language Processing (NLP) tasks over infor- mal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly define the task in the word-to-word sense; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of nor- malization on the parser, we tie normal- ization performance directly to parser per- formance. Additionally, we design a cus- tomizable framework to address the often overlooked concept of domain adaptabil- ity, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimen- tal study over datasets from three domains demonstrates that our approach outper- forms not only the state-of-the-art word- to-word normalization techniques, but also manual word-to-word annotations. ",,,,ACL
115,2013,A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation,"Zhenhua Tian, Hengheng Xiang, Ziqi Liu, Qinghua Zheng","This paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences. Based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predi- cate’s smoothed preferences. Experimen- tal results show that this approach out- performs several state-of-the-art method- s on the pseudo-disambiguation task, and it better correlates with human plausibility judgements. ",,,,ACL
116,2013,ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling,"Egoitz Laparra, German Rigau","This paper presents a novel deterministic algorithm for implicit Semantic Role Labeling. The system exploits a very simple but relevant discursive property, the ar- gument coherence over different instances of a predicate. The algorithm solves the implicit arguments sequentially, exploit- ing not only explicit but also the implicit arguments previously solved. In addition, we empirically demonstrate that the algo- rithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data. ",,,,ACL
117,2013,Cross-lingual Transfer of Semantic Role Labeling Models,"Mikhail Kozhevnikov, Ivan Titov","Semantic Role Labeling (SRL) has become one of the standard tasks of natural language processing and proven useful as a source of information for a number of other applications. We address the prob- lem of transferring an SRL model from one language to another using a shared feature representation. This approach is then evaluated on three language pairs, demonstrating competitive performance as compared to a state-of-the-art unsuper- vised SRL system and a cross-lingual an- notation projection baseline. We also con- sider the contribution of different aspects of the feature representation to the perfor- mance of the model and discuss practical applicability of this method. ",,,,ACL
118,2013,DErivBase: Inducing and Evaluating a Derivational Morphology Resource for German,"Britta Zeller, Jan Šnajder, Sebastian Padó","Derivational models are still an underresearched area in computational morphol- ogy. Even for German, a rather resource- rich language, there is a lack of large-coverage derivational knowledge. This paper describes a rule-based framework for inducing derivational families (i.e., clus- ters of lemmas in derivational relation- ships) and its application to create a high- coverage German resource, DE RIV B ASE , mapping over 280k lemmas into more than 17k non-singleton clusters. We focus on the rule component and a qualitative and quan- titative evaluation. Our approach achieves up to 93% precision and 71% recall. We attribute the high precision to the fact that our rules are based on information from grammar books. ",,,,ACL
119,2013,Crowdsourcing Interaction Logs to Understand Text Reuse from the Web,"Martin Potthast, Matthias Hagen, Michael Völske, Benno Stein","We report on the construction of the Webis text reuse corpus 2012 for advanced research on text reuse. The corpus compiles manually written documents obtained from a completely controlled, yet representative environment that emulates the web. Each of the 297 documents in the corpus is about one of the 150 topics used at the TREC Web Tracks 2009–2011, thus forming a strong connection with existing evaluation efforts. Writers, hired at the crowdsourc- ing platform oDesk, had to retrieve sources for a given topic and to reuse text from what they found. Part of the corpus are detailed interaction logs that consistently cover the search for sources as well as the creation of documents. This will allow for in-depth analyses of how text is composed if a writer is at liberty to reuse texts from a third party—a setting which has not been studied so far. In addition, the corpus pro- vides an original resource for the evalua- tion of text reuse and plagiarism detectors, where currently only less realistic resources are employed. ",,,,ACL
120,2013,SPred: Large-scale Harvesting of Semantic Predicates,"Tiziano Flati, Roberto Navigli","We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ) and learn the semantic classes that best fit the ? argument. To do this, we extract all the occurrences in Wikipedia which match the predicate and abstract its argu- ments to general semantic classes (e.g., break B ODY P ART , break A GREEMENT , etc.). Our experiments show that we are able to create a large collection of seman- tic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach. ",,,,ACL
121,2013,Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain,"Jackie Chi Kit Cheung, Gerald Penn","In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with re- dundancy avoidance and some sentence compression, to produce mostly extrac- tive summaries. In this paper, we investi- gate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the se- mantic level of caseframes. We show that model summaries (1) are more abstrac- tive and make use of more sentence aggre- gation, (2) do not contain as many topi- cal caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improve- ments are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed. ",,,,ACL
122,2013,HEADY: News headline abstraction through event pattern clustering,"Enrique Alfonseca, Daniele Pighin, Guillermo Garrido","This paper presents H EADY : a novel, abstractive approach for headline generation from news collections. From a web-scale corpus of English news, we mine syntac- tic patterns that a Noisy-OR model generalizes into event descriptions. At inference time, we query the model with the patterns observed in an unseen news collection, identify the event that better captures the gist of the collection and retrieve the most appropriate pattern to generate a head- line. H EADY improves over a state-of-the- art open-domain title abstraction method, bridging half of the gap that separates it from extractive methods using human- generated titles in manual evaluations, and performs comparably to human-generated headlines as evaluated with ROUGE. ",,,,ACL
123,2013,Conditional Random Fields for Responsive Surface Realisation using Global Features,"Nina Dethlefs, Helen Hastie, Heriberto Cuayáhuitl, Oliver Lemon","Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisa- tion. It also allows generation from partial and modified inputs and is therefore ap- plicable to incremental surface realisation. Results from a human rating study confirm that users are sensitive to this extended no- tion of context and assign ratings that are significantly higher (up to 14% ) than those for taking only local context into account. ",,,,ACL
124,2013,Two-Neighbor Orientation Model with Cross-Boundary Global Contexts,"Hendra Setiawan, Bowen Zhou, Bing Xiang, Libin Shen","Long distance reordering remains one of the greatest challenges in statistical ma- chine translation research as the key contextual information may well be beyond the confine of translation units. In this paper, we propose Two-Neighbor Orien- tation (TNO) model that jointly models the orientation decisions between anchors and two neighboring multi-unit chunks which may cross phrase or rule bound- aries. We explicitly model the longest span of such chunks, referred to as Max- imal Orientation Span, to serve as a global parameter that constrains under- lying local decisions. We integrate our proposed model into a state-of-the-art string-to-dependency translation system and demonstrate the efficacy of our pro- posal in a large-scale Chinese-to-English translation task. On NIST MT08 set, our most advanced model brings around +2.0 BLEU and -1.0 TER improvement. ",,,,ACL
125,2013,Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation,"Karthik Visweswariah, Mitesh M. Khapra, Ananthakrishnan Ramanathan","Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation perfor- mance. In this paper, we focus on further improving the performance of the reorder- ing model (and thereby machine transla- tion) by using a larger corpus of sentence aligned data for which manual word align- ments are not available but automatic ma- chine generated alignments are available. The main challenge we tackle is to gen- erate quality data for training the reorder- ing model in spite of the machine align- ments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorder- ings produced given noisy alignments and also improves word alignments using in- formation from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline su- pervised aligner. The data generated al- lows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English eval- uation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline. ",,,,ACL
126,2013,Vector Space Model for Adaptation in Statistical Machine Translation,"Boxing Chen, Roland Kuhn, George Foster","This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a di- mensionality equal to the number of train- ing subcorpora; each entry in the vector re- flects the contribution of a particular sub- corpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the train- ing data, we create a vector with features defined in the same way, and calculate its similarity score with the vector represent- ing the dev set. Thus, we obtain a de- coding feature whose value represents the phrase pair’s closeness to the dev. This is a simple, computationally cheap form of instance weighting for phrase pairs. Ex- periments on large scale NIST evaluation data show improvements over strong base- lines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and signifi- cant improvements in most circumstances over baselines with linear mixture model adaptation. An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre. ",,,,ACL
127,2013,From Natural Language Specifications to Program Input Parsers,"Tao Lei, Fan Long, Regina Barzilay, Martin Rinard","We present a method for automatically generating input parsers from English specifications of input file formats. We use a Bayesian generative model to capture relevant natural language phenomena and translate the English specification into a specification tree, which is then trans- lated into a C++ input parser. We model the problem as a joint dependency pars- ing and semantic role labeling task. Our method is based on two sources of infor- mation: (1) the correlation between the text and the specification tree and (2) noisy supervision as determined by the success of the generated C++ parser in reading in- put examples. Our results show that our approach achieves 80.0% F-Score accu- racy compared to an F-Score of 66.7% produced by a state-of-the-art semantic parser on a dataset of input format speci- fications from the ACM International Col- legiate Programming Contest (which were written in English for humans with no in- tention of providing support for automated processing). 1 ",,,,ACL
128,2013,Entity Linking for Tweets,"Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou","We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. Particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method. ",,,,ACL
129,2013,Identification of Speakers in Novels,"Hua He, Denilson Barbosa, Grzegorz Kondrak","Speaker identification is the task of attributing utterances to characters in a literary narrative. It is challenging to auto- mate because the speakers of the majority of utterances are not explicitly identified in novels. In this paper, we present a super- vised machine learning approach for the task that incorporates several novel fea- tures. The experimental results show that our method is more accurate and general than previous approaches to the problem. ",,,,ACL
130,2013,Language Acquisition and Probabilistic Models: keeping it simple,"Aline Villavicencio, Marco Idiart, Robert Berwick, Igor Malioutov","Hierarchical Bayesian Models (HBMs) have been used with some success to capture empirically observed patterns of under and overgeneralization in child language acquisition. However, as is well known, HBMs are “ideal” learning systems, assuming ac- cess to unlimited computational re- sources that may not be available to child language learners. Conse- quently, it remains crucial to carefully assess the use of HBMs along with al- ternative, possibly simpler, candidate models. This paper presents such an evaluation for a language acquisi- tion domain where explicit HBMs have been proposed: the acquisition of En- glish dative constructions. In particu- lar, we present a detailed, empirically- grounded model-selection compari- son of HBMs vs. a simpler alternative based on clustering along with max- imum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance with- out incurring on the high computa- tional costs associated with HBMs. ",,,,ACL
131,2013,A Two Level Model for Context Sensitive Inference Rules,"Oren Melamud, Jonathan Berant, Ido Dagan, Jacob Goldberger","Automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words, operating at the word space level. A recent line of work, which addresses context sensitivity of rules, represented contexts in a latent topic space and computed similar- ity over topic vectors. We propose a novel two-level model, which computes simi- larities between word-level vectors that are biased by topic-level context repre- sentations. Evaluations on a naturally- distributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set. ",,,,ACL
132,2013,"Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity","Mohammad Taher Pilehvar, David Jurgens, Roberto Navigli","Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which re- quires adapting the method for each data type. We present a unified approach to se- mantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-of- the-art performance on three tasks: seman- tic textual similarity, word similarity, and word sense coarsening. ",,,,ACL
133,2013,Linking and Extending an Open Multilingual Wordnet,"Francis Bond, Ryan Foster","We create an open multilingual wordnet with large wordnets for over 26 languages and smaller ones for 57 languages. It is made by combining wordnets with open licences, data from Wiktionary and the Unicode Common Locale Data Repository. Overall there are over 2 million senses for over 100 thousand concepts, linking over 1.4 million words in hundreds of lan- guages. ",,,,ACL
134,2013,FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection,"Silvana Hartmann, Iryna Gurevych","We present a new bilingual FrameNet lex- icon for English and German. It is created through a simple, but powerful ap- proach to construct a FrameNet in any language using Wiktionary as an interlingual representation. Our approach is based on a sense alignment of FrameNet and Wiktionary, and subsequent transla- tion disambiguation into the target lan- guage. We perform a detailed evaluation of the created resource and a discussion of Wiktionary as an interlingual connection for the cross-language transfer of lexical- semantic resources. The created resource is publicly available at http://www. ukp.tu-darmstadt.de/fnwkde/ . ",,,,ACL
135,2013,Dirt Cheap Web-Scale Parallel Text from the Common Crawl,"Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn","Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon’s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the result- ing data boosts translation performance across the board for five different language pairs in the news domain, and on open do- main test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource. 1 ",,,,ACL
136,2013,A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization,"Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Florian","We consider the problem of using sentence compression techniques to facilitate query- focused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search de- coder is proposed to efficiently find highly probable compressions. Under this frame- work, we show how to integrate various in- dicative metrics such as linguistic motivation and query relevance into the compression pro- cess by deriving a novel formulation of a com- pression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ",,,,ACL
137,2013,Domain-Independent Abstract Generation for Focused Meeting Summarization,"Lu Wang, Claire Cardie","We address the challenge of generating natu- ral language abstractive summaries for spoken meetings in a domain-independent fashion. We apply Multiple-Sequence Alignment to induce abstract generation templates that can be used for different domains. An Overgenerate- and-Rank strategy is utilized to produce and rank candidate abstracts. Experiments us- ing in-domain and out-of-domain training on disparate corpora show that our system uni- formly outperforms state-of-the-art supervised extract-based approaches. In addition, human judges rate our system summaries significantly higher than compared systems in fluency and overall quality. ",,,,ACL
138,2013,A Statistical NLG Framework for Aggregated Planning and Realization,"Ravi Kondadadi, Blake Howald, Frank Schilder","We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target do- main. First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis. Each sentence is then organized into semanti- cally similar groups (representing a do- main specific concept) by k-means cluster- ing. After this semi-automatic processing (human review of cluster assignments), a number of corpus–level statistics are com- piled and used as features by a ranking SVM to develop model weights from a training corpus. At generation time, a set of input data, the collection of semanti- cally organized templates, and the model weights are used to select optimal tem- plates. Our system is evaluated with au- tomatic, non–expert crowdsourced and ex- pert evaluation metrics. We also introduce a novel automatic metric – syntactic vari- ability – that represents linguistic variation as a measure of unique template sequences across a collection of automatically gener- ated documents. The metrics for generated weather and biography texts fall within ac- ceptable ranges. In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based ar- chitectures and readily adapts to different domains with reduced development time. ?*Ravi Kondadadi is now affiliated with Nuance Commu- nications, Inc. ",,,,ACL
139,2013,Models of Translation Competitions,"Mark Hopkins, Jonathan May","What do we want to learn from a trans- lation competition and how do we learn it with confidence? We argue that a disproportionate focus on ranking competition participants has led to lots of different rankings, but little insight about which rankings we should trust. In response, we provide the first framework that allows an empirical comparison of different analy- ses of competition results. We then use this framework to compare several analyt- ical models on data from the Workshop on Machine Translation (WMT). ",,,,ACL
140,2013,Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation,"Jiajun Zhang, Chengqing Zong","Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for in- ducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effec- tive method to induce a phrase-based model from the monolingual corpora given an au- tomatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. ",,,,ACL
141,2013,SenseSpotting: Never let your parallel data tie you to an old domain,"Marine Carpuat, Hal Daumé III, Katharine Henry, Ann Irvine","Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensi- tive to lexical semantics. We define a task, S ENSE S POTTING , in which we build sys- tems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a gold- standard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for ma- chine translation. Our system is able to achieve F-measures of as much as 80% , when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains. ",,,,ACL
142,2013,BRAINSUP: Brainstorming Support for Creative Sentence Generation,"Gözde Özbal, Daniele Pighin, Carlo Strapparava","We present B RAIN S UP , an extensible framework for the generation of creative sentences in which users are able to force several words to appear in the sentences and to control the generation process across several semantic dimensions, namely emotions, colors, domain related- ness and phonetic properties. We evalu- ate its performance on a creative sentence generation task, showing its capability of generating well-formed, catchy and effec- tive sentences that have all the good qual- ities of slogans produced by human copy- writers. ",,,,ACL
143,2013,Grammatical Error Correction Using Integer Linear Programming,"Yuanbin Wu, Hwee Tou Ng","We propose a joint inference algorithm for grammatical error correction. Different from most previous work where different error types are corrected independently, our proposed inference process considers all possible errors in a unied framework. We use integer linear programming (ILP) to model the inference process, which can easily incorporate both the power of exist- ing error classiers and prior knowledge on grammatical error correction. Exper- imental results on the Helping Our Own shared task show that our method is com- petitive with state-of-the-art systems. ",,,,ACL
144,2013,Text-Driven Toponym Resolution using Indirect Supervision,"Michael Speriosu, Jason Baldridge","Toponym resolvers identify the specific lo- cations referred to by ambiguous placenames in text. Most resolvers are based on heuristics using spatial relationships between multiple toponyms in a document, or metadata such as population. This pa- per shows that text-driven disambiguation for toponyms is far more effective. We ex- ploit document-level geotags to indirectly generate training instances for text classi- fiers for toponym resolution, and show that textual cues can be straightforwardly in- tegrated with other commonly used ones. Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles. ",,,,ACL
145,2013,Argument Inference from Relevant Event Mentions in Chinese Argument Extraction,"Peifeng Li, Qiaoming Zhu, Guodong Zhou","As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those inter- sentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. ",,,,ACL
146,2013,Fine-grained Semantic Typing of Emerging Entities,"Ndapandula Nakashole, Tomasz Tylenda, Gerhard Weikum","Methods for information extraction (IE) and knowledge base (KB) construction have been intensively studied. However, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging. In this paper, we present a method for discovering and se- mantically typing newly emerging out-of- KB entities, thus improving the freshness and recall of ontology-based IE and im- proving the precision and semantic rigor of open IE. Our method is based on a prob- abilistic model that feeds weights into in- teger linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints. Our experimental evaluation, based on crowd- sourced user studies, show our method performing significantly better than prior work. ",,,,ACL
147,2013,Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction,"Barbara Plank, Alessandro Moschitti","Relation Extraction (RE) is the task of extracting semantic relationships between entities in text. Recent studies on relation extraction are mostly supervised. The clear drawback of supervised methods is the need of training data: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. This is the problem of domain adapta- tion. In this paper, we propose to combine (i) term generalization approaches such as word clustering and latent semantic anal- ysis (LSA) and (ii) structured kernels to improve the adaptability of relation ex- tractors to new text genres/domains. The empirical evaluation on ACE 2005 do- mains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation. ",,,,ACL
148,2013,A joint model of word segmentation and phonological variation for English word-final /t/-deletion,"Benjamin Börschinger, Mark Johnson, Katherine Demuth","Word-final /t/-deletion refers to a common phenomenon in spoken English where words such as / wEst / “west” are pronounced as [ wEs ] “wes” in certain con- texts. Phonological variation like this is common in naturally occurring speech. Current computational models of unsu- pervised word segmentation usually assume idealized input that is devoid of these kinds of variation. We extend a non-parametric model of word segmenta- tion by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-defined joint model as a first step towards han- dling variation and segmentation in a sin- gle model. We analyse how our model handles /t/-deletion on a large corpus of transcribed speech, and show that the joint model can perform word segmentation and recover underlying /t/s. We find that Bi- gram dependencies are important for per- forming well on real data and for learning appropriate deletion probabilities for dif- ferent contexts. 1 ",,,,ACL
149,2013,Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics,"Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli, Marco Baroni","Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional meth- ods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations con- structed in this way beat a strong baseline and can be of higher quality than represen- tations directly constructed from corpus data. Our results constitute a novel evalua- tion of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics. ",,,,ACL
150,2013,Unsupervised Consonant-Vowel Prediction over Hundreds of Languages,"Young-Bum Kim, Benjamin Snyder","In this paper, we present a solution to one aspect of the decipherment task: the prediction of consonants and vowels for an unknown language and alphabet. Adopting a classical Bayesian perspective, we performs posterior inference over hun- dreds of languages, leveraging knowledge of known languages and alphabets to un- cover general linguistic patterns of typo- logically coherent language clusters. We achieve average accuracy in the unsuper- vised consonant/vowel prediction task of 99% across 503 languages. We further show that our methodology can be used to predict more fine-grained phonetic dis- tinctions. On a three-way classification task between vowels, nasals, and non- nasal consonants, our model yields unsu- pervised accuracy of 89% across the same set of languages. ",,,,ACL
151,2013,Improving Text Simplification Language Modeling Using Unsimplified Text Data,David Kauchak,"In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the lan- guage model. We explore the relation- ship between normal English and simpli- fied English and compare language mod- els trained on varying amounts of text from each. We evaluate the models intrin- sically with perplexity and extrinsically on the lexical simplification task from Se- mEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc anal- ysis shows that the additional unsimplified data provides better coverage for unseen and rare n -grams. ",,,,ACL
152,2013,Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures,"Sina Zarrieß, Jonas Kuhn","We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of Ger- man articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture. ",,,,ACL
153,2013,Named Entity Recognition using Cross-lingual Resources: Arabic as an Example,Kareem Darwish,"Some languages lack large knowledge bases and good discriminative features for Name Entity Recognition (NER) that can generalize to previously unseen named entities. One such language is Arabic, which: a) lacks a capitalization feature; and b) has relatively small knowledge bases, such as Wikipedia. In this work we address both problems by in- corporating cross-lingual features and knowl- edge bases from English using cross-lingual links. We show that such features have a dramatic positive effect on recall. We show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in F- measure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and mi- croblogs test sets respectively. ",,,,ACL
154,2013,Beam Search for Solving Substitution Ciphers,"Malte Nuhn, Julian Schamper, Hermann Ney","In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a concep- tually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ci- phers and is able to use high order n -gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3 -gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6 -gram language model. We also apply our approach to the famous Zodiac- 408 cipher and obtain slightly bet- ter (and near to optimal) results than pre- viously published. Unlike the previous state-of-the-art approach that uses addi- tional word lists to evaluate possible deci- pherments, our approach only uses a letter- based 6 -gram language model. Further- more we use our algorithm to solve large vocabulary substitution ciphers and im- prove the best published decipherment er- ror rate based on the Gigaword corpus of 7.8% to 6.0% error rate. ",,,,ACL
155,2013,Social Text Normalization using Contextual Graph Random Walks,"Hany Hassan, Arul Menezes","We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text. The proposed system is based on unsupervised learning of the normalization equivalences from unla- beled text. The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. We show that the proposed approach has a very high precision of (92.43) and a rea- sonable recall of (56.4). When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP appli- cation to handle social media text. ",,,,ACL
156,2013,Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation,"ThuyLinh Nguyen, Stephan Vogel","Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We pro- pose an extension of Hiero called Phrasal-Hiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypoth- esis space as the original Hiero but in- corporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero transla- tion derivation, find its corresponding dis- continuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for Arabic- English, Chinese-English and German- English translation. ",,,,ACL
157,2013,Machine Translation Detection from Monolingual Web-Text,"Yuki Arase, Ming Zhou","We propose a method for automatically detecting low-quality Web-text translated by statistical machine translation (SMT) systems. We focus on the phrase salad phenomenon that is observed in existing SMT results and propose a set of computa- tionally inexpensive features to effectively detect such machine-translated sentences from a large-scale Web-mined text. Un- like previous approaches that require bilin- gual data, our method uses only monolin- gual text as input; therefore it is applicable for refining data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. ",,,,ACL
158,2013,Paraphrase-Driven Learning for Open Question Answering,"Anthony Fader, Luke Zettlemoyer, Oren Etzioni","We study question answering as a machine learning problem, and induce a function that maps open-domain questions to queries over a database of web extrac- tions. Given a large, community-authored, question-paraphrase corpus, we demon- strate that it is possible to learn a se- mantic lexicon and linear ranking func- tion without manually annotating ques- tions. Our approach automatically gener- alizes a seed lexicon and includes a scal- able, parallelized perceptron parameter es- timation scheme. Experiments show that our approach more than quadruples the re- call of the seed lexicon, with only an 8% loss in precision. ",,,,ACL
159,2013,Aid is Out There: Looking for Help from Tweets during a Large Scale Disaster,"István Varga, Motoki Sano, Kentaro Torisawa, Chikara Hashimoto","The 2011 Great East Japan Earthquake caused a wide range of problems, and as countermeasures, many aid activities were carried out. Many of these problems and aid activities were reported via Twitter. However, most problem reports and corre- sponding aid messages were not success- fully exchanged between victims and lo- cal governments or humanitarian organi- zations, overwhelmed by the vast amount of information. As a result, victims could not receive necessary aid and humanitar- ian organizations wasted resources on re- dundant efforts. In this paper, we propose a method for discovering matches between problem reports and aid messages. Our system contributes to problem-solving in a large scale disaster situation by facilitat- ing communication between victims and humanitarian organizations. ",,,,ACL
160,2013,"A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations","Angeliki Lazaridou, Ivan Titov, Caroline Sporleder","We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by in- corporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of dis- course, as we induce types of discourse re- lations and associated discourse cues rel- evant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and as- pect shifters. The quantitative analysis that we conducted indicated that the integra- tion of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the in- duced representations encode a meaning- ful discourse structure. ",,,,ACL
161,2013,Joint Inference for Fine-grained Opinion Extraction,"Bishan Yang, Claire Cardie","This paper addresses the task of finegrained opinion extraction – the identification of opinion-related entities: the opinion expressions, the opinion holders, and the targets of the opinions, and the relations between opinion expressions and their targets and holders. Most ex- isting approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner, where the inter- dependencies among different extraction stages are not captured. We propose a joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction, and seeks a glob- ally optimal solution. Experimental re- sults demonstrate that our joint inference approach significantly outperforms tradi- tional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction. ",,,,ACL
162,2013,Linguistic Models for Analyzing and Detecting Biased Language,"Marta Recasens, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky","Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits de- signed to remove bias from Wikipedia ar- ticles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and episte- mological bias, related to whether propo- sitions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective inten- sifiers. These insights help us develop fea- tures for a model to solve a new prediction task of practical importance: given a bi- ased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task. ",,,,ACL
163,2013,Evaluating a City Exploration Dialogue System with Integrated Question-Answering and Pedestrian Navigation,"Srinivasan Janarthanam, Oliver Lemon, Phil Bartie, Tiphaine Dalmas","We present a city navigation and tourist information mobile dialogue app with integrated question-answering (QA) and geographic information system (GIS) modules that helps pedestrian users to navigate in and learn about urban environ- ments. In contrast to existing mobile apps which treat these problems independently, our Android app addresses the prob- lem of navigation and touristic question- answering in an integrated fashion using a shared dialogue context. We evaluated our system in comparison with Samsung S-Voice (which interfaces to Google nav- igation and Google search) with 17 users and found that users judged our system to be significantly more interesting to inter- act with and learn from. They also rated our system above Google search (with the Samsung S-Voice interface) for tourist in- formation tasks. ",,,,ACL
164,2013,Lightly Supervised Learning of Procedural Dialog Systems,"Svitlana Volkova, Pallavi Choudhury, Chris Quirk, Bill Dolan","Procedural dialog systems can help users achieve a wide range of goals. However, such systems are challenging to build, currently requiring manual engineering of substantial domain-specific task knowledge and dialog management strategies. In this paper, we demonstrate that it is pos- sible to learn procedural dialog systems given only light supervision, of the type that can be provided by non-experts. We consider domains where the required task knowledge exists in textual form (e.g., in- structional web pages) and where system builders have access to statements of user intent (e.g., search query logs or dialog interactions). To learn from such tex- tual resources, we describe a novel ap- proach that first automatically extracts task knowledge from instructions, then learns a dialog manager over this task knowledge to provide assistance. Evaluation in a Mi- crosoft Office domain shows that the indi- vidual components are highly accurate and can be integrated into a dialog system that provides effective help to users. ",,,,ACL
165,2013,Public Dialogue: Analysis of Tolerance in Online Discussions,"Arjun Mukherjee, Vivek Venkataraman, Bing Liu, Sharon Meraz","Social media platforms have enabled people to freely express their views and discuss issues of interest with others. While it is important to dis- cover the topics in discussions, it is equally useful to mine the nature of such discussions or de- bates and the behavior of the participants. There are many questions that can be asked. One key question is whether the participants give rea- soned arguments with justifiable claims via constructive debates or exhibit dogmatism and egotistic clashes of ideologies. The central idea of this question is tolerance, which is a key concept in the field of communications. In this work, we perform a computational study of tol- erance in the context of online discussions. We aim to identify tolerant vs. intolerant partici- pants and investigate how disagreement affects tolerance in discussions in a quantitative framework. To the best of our knowledge, this is the first such study. Our experiments using real-life discussions demonstrate the effective- ness of the proposed technique and also provide some key insights into the psycholinguistic phenomenon of tolerance in online discussions. ",,,,ACL
166,2013,Offspring from Reproduction Problems: What Replication Failure Teaches Us,"Antske Fokkens, Marieke van Erp, Marten Postma, Ted Pedersen","Repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing work. We present two concrete use cases involving key techniques in the NLP domain for which we show that reproduc- ing results is still difficult. We show that the deviation that can be found in repro- duction efforts leads to questions about how our results should be interpreted. Moreover, investigating these deviations provides new insights and a deeper under- standing of the examined techniques. We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers. Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field. ",,,,ACL
167,2013,Evaluating Text Segmentation using Boundary Edit Distance,Chris Fournier,"This work proposes a new segmentation evaluation metric, named boundary similarity (B), an inter-coder agreement coef- ficient adaptation, and a confusion-matrix for segmentation that are all based upon an adaptation of the boundary edit distance in Fournier and Inkpen (2012). Existing seg- mentation metrics such as P k , WindowD- iff, and Segmentation Similarity (S) are all able to award partial credit for near misses between boundaries, but are biased towards segmentations containing few or tightly clustered boundaries. Despite S’s improvements, its normalization also pro- duces cosmetically high values that over- estimate agreement & performance, lead- ing this work to propose a solution. ",,,,ACL
168,2013,Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation,"Rohan Ramanath, Monojit Choudhury, Kalika Bali, Rishiraj Saha Roy","Query segmentation, like text chunking, is the first step towards query understanding. In this study, we explore the effectiveness of crowdsourcing for this task. Through carefully designed control experiments and Inter Annotator Agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal (often only two) parts. Sim- ilarly, in the case of hierarchical or nested segmentation, turkers have a strong prefer- ence towards balanced binary trees. ",,,,ACL
169,2013,Deceptive Answer Prediction with User Preference Graph,"Fangtao Li, Yang Gao, Shuchang Zhou, Xiance Si","In Community question answering (QA) sites, malicious users may provide deceptive answers to promote their products or services. It is important to identify and fil- ter out these deceptive answers. In this paper, we first solve this problem with the traditional supervised learning meth- ods. Two kinds of features, including tex- tual and contextual features, are investi- gated for this task. We further propose to exploit the user relationships to identify the deceptive answers, based on the hy- pothesis that similar users will have simi- lar behaviors to post deceptive or authentic answers. To measure the user similarity, we propose a new user preference graph based on the answer preference expressed by users, such as “helpful” voting and “best answer” selection. The user prefer- ence graph is incorporated into traditional supervised learning framework with the graph regularization technique. The ex- periment results demonstrate that the user preference graph can indeed help improve the performance of deceptive answer pre- diction. ",,,,ACL
170,2013,Why-Question Answering using Intra- and Inter-Sentential Causal Relations,"Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Motoki Sano","In this paper, we explore the utility of intra and inter-sentential causal relations between terms or clauses as evidence for answering why-questions. To the best of our knowledge, this is the first work that uses both intra- and inter-sentential causal relations for why-QA. We also propose a method for assessing the appropriate- ness of causal relations as answers to a given question using the semantic orienta- tion of excitation proposed by Hashimoto et al. (2012). By applying these ideas to Japanese why-QA, we improved preci- sion by 4.4% against all the questions in our test set over the current state-of-the- art system for Japanese why-QA. In addi- tion, unlike the state-of-the-art system, our system could achieve very high precision (83.2%) for 25% of all the questions in the test set by restricting its output to the con- fident answers only. ",,,,ACL
171,2013,Question Answering Using Enhanced Lexical Semantic Models,"Wen-tau Yih, Ming-Wei Chang, Christopher Meek, Andrzej Pastusiak","In this paper, we study the answer sentence selection problem for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance us- ing models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, re- gardless of the choice of learning algo- rithms. When evaluated on a bench- mark dataset, the MAP and MRR scores are increased by 8 to 10 points, com- pared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin. ",,,,ACL
172,2013,Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews,"Kang Liu, Liheng Xu, Jun Zhao","Mining opinion targets is a fundamental and important task for opinion mining from online reviews. To this end, there are usually two kinds of methods: syntax based and alignment based methods. Syntax based methods usually ex- ploited syntactic patterns to extract opin- ion targets, which were however prone to suffer from parsing errors when dealing with online informal texts. In contrast, alignment based methods used word align- ment model to fulfill this task, which could avoid parsing errors without using pars- ing. However, there is no research fo- cusing on which kind of method is more better when given a certain amount of re- views. To fill this gap, this paper empiri- cally studies how the performance of these two kinds of methods vary when chang- ing the size, domain and language of the corpus. We further combine syntactic pat- terns with alignment model by using a par- tially supervised framework and investi- gate whether this combination is useful or not. In our experiments, we verify that our combination is effective on the corpus with small and medium size. ",,,,ACL
173,2013,Mining Opinion Words and Opinion Targets in a Two-Stage Framework,"Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen","This paper proposes a novel two-stage method for mining opinion words and opinion targets. In the first stage, we propose a Sentiment Graph Walking algorithm, which naturally incorporates syntactic patterns in a Sentiment Graph to ex- tract opinion word/target candidates. Then random walking is employed to estimate confidence of candidates, which improves extraction accuracy by considering confi- dence of patterns. In the second stage, we adopt a self-learning strategy to refine the results from the first stage, especially for filtering out high-frequency noise terms and capturing the long-tail terms, which are not investigated by previous meth- ods. The experimental results on three real world datasets demonstrate the effective- ness of our approach compared with state- of-the-art unsupervised methods. ",,,,ACL
174,2013,Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning,"Song Feng, Jun Seok Kang, Polina Kuznetsova, Yejin Choi","Understanding the connotation of words plays an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text, as seemingly objective statements often allude nuanced sentiment of the writer, and even purpose- fully conjure emotion from the readers’ minds. The focus of this paper is draw- ing nuanced, connotative sentiments from even those words that are objective on the surface, such as “intelligence”, “human”, and “cheesecake”. We propose induction algorithms encoding a diverse set of lin- guistic insights (semantic prosody, distri- butional similarity, semantic parallelism of coordination) and prior knowledge drawn from lexical resources, resulting in the first broad-coverage connotation lexicon. ",,,,ACL
1,2014,Learning Ensembles of Structured Prediction Rules,"Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri","We present a series of algorithms with the- oretical guarantees for learning accurate ensembles of several structured prediction rules for which no prior knowledge is as- sumed. This includes a number of ran- domized and deterministic algorithms de- vised by converting on-line learning al- gorithms to batch ones, and a boosting- style algorithm applicable in the context of structured prediction with a large number of labels. We also report the results of ex- tensive experiments with these algorithms. ",,,,ACL
2,2014,Representation Learning for Text-level Discourse Parsing,"Yangfeng Ji, Jacob Eisenstein","Text-level discourse parsing is notoriously difficult, as distinctions between discourse relations require subtle semantic judg- ments that are not easily captured using standard features. In this paper, we present a representation learning approach, in which we transform surface features into a latent space that facilitates RST dis- course parsing. By combining the machin- ery of large-margin transition-based struc- tured prediction with representation learn- ing, our method jointly learns to parse dis- course while at the same time learning a discourse-driven projection of surface fea- tures. The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art in pre- dicting relations and nuclearity on the RST Treebank. ",,,,ACL
3,2014,Text-level Discourse Dependency Parsing,"Sujian Li, Liang Wang, Ziqiang Cao, Wenjie Li","Previous researches on Text-level discourse parsing mainly made use of constituency structure to parse the whole document into one discourse tree. In this paper, we present the limitations of constituency based dis- course parsing and first propose to use de- pendency structure to directly represent the relations between elementary discourse units (EDUs). The state-of-the-art depend- ency parsing techniques, the Eisner algo- rithm and maximum spanning tree (MST) algorithm, are adopted to parse an optimal discourse dependency tree based on the arc- factored model and the large-margin learn- ing techniques. Experiments show that our discourse dependency parsers achieve a competitive performance on text-level dis- course parsing. ",,,,ACL
4,2014,Discovering Latent Structure in Task-Oriented Dialogues,"Ke Zhai, Jason D. Williams","A key challenge for computational conver- sation models is to discover latent struc- ture in task-oriented dialogue, since it pro- vides a basis for analysing, evaluating, and building conversational systems. We pro- pose three new unsupervised models to discover latent structures in task-oriented dialogues. Our methods synthesize hidden Markov models (for underlying state) and topic models (to connect words to states). We apply them to two real, non-trivial datasets: human-computer spoken dia- logues in bus query service, and human- human text-based chats from a live tech- nical support service. We show that our models extract meaningful state represen- tations and dialogue structures consistent with human annotations. Quantitatively, we show our models achieve superior per- formance on held-out log likelihood eval- uation and an ordering task. ",,,,ACL
5,2014,Learning Structured Perceptrons for Coreference Resolution with Latent Antecedents and Non-local Features,"Anders Björkelund, Jonas Kuhn","We investigate different ways of learning structured perceptron models for coref- erence resolution when using non-local features and beam search. Our experi- mental results indicate that standard tech- niques such as early updates or Learning as Search Optimization (LaSO) perform worse than a greedy baseline that only uses local features. By modifying LaSO to de- lay updates until the end of each instance we obtain significant improvements over the baseline. Our model obtains the best results to date on recent shared task data for Arabic, Chinese, and English. ",,,,ACL
6,2014,Multilingual Models for Compositional Distributed Semantics,"Karl Moritz Hermann, Phil Blunsom","We present a novel technique for learn- ing semantic representations, which ex- tends the distributional hypothesis to mul- tilingual data and joint-space embeddings. Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences, while maintaining sufficient distance between those of dissimilar sentences. The mod- els do not rely on word alignments or any syntactic information and are success- fully applied to a number of diverse lan- guages. We extend our approach to learn semantic representations at the document level, too. We evaluate these models on two cross-lingual document classification tasks, outperforming the prior state of the art. Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic rela- tionships across languages without paral- lel data. ",,,,ACL
7,2014,Simple Negation Scope Resolution through Deep Parsing: A Semantic Solution to a Semantic Problem,"Woodley Packard, Emily M. Bender, Jonathon Read, Stephan Oepen","In this work, we revisit Shared Task 1 from the 2012 *SEM Conference: the au- tomated analysis of negation. Unlike the vast majority of participating systems in 2012, our approach works over explicit and formal representations of proposi- tional semantics, i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning rep- resentations. We relate the task-specific interpretation of (negation) scope to the concept of (quantifier and operator) scope in mainstream underspecified semantics. With reference to an explicit encoding of semantic predicate-argument structure, we can operationalize the annotation deci- sions made for the 2012 *SEM task, and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system. In a system combination setting, our approach improves over the best pub- lished results on this task to date. ",,,,ACL
8,2014,Logical Inference on Dependency-based Compositional Semantics,"Ran Tian, Yusuke Miyao, Takuya Matsuzaki","Dependency-based Compositional Se- mantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics. In this paper, we equip the DCS framework with logical inference, by defining ab- stract denotations as an abstraction of the computing process of denotations in original DCS. An inference engine is built to achieve inference on abstract denota- tions. Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation. Experiments on FraCaS and PASCAL RTE datasets show promising results. ",,,,ACL
9,2014,A practical and linguistically-motivated approach to compositional distributional semantics,"Denis Paperno, Nghia The Pham, Marco Baroni","Distributional semantic methods to ap- proximate word meaning with context vectors have been very successful empir- ically, and the last years have seen a surge of interest in their compositional exten- sion to phrases and sentences. We present here a new model that, like those of Co- ecke et al. (2010) and Baroni and Zam- parelli (2010), closely mimics the standard Montagovian semantic treatment of com- position in distributional terms. However, our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged, real-life sentences. We test the model on a variety of empirical tasks, showing that it consistently outperforms a set of competitive rivals. ",,,,ACL
10,2014,Lattice Desegmentation for Statistical Machine Translation,"Mohammad Salameh, Colin Cherry, Grzegorz Kondrak","Morphological segmentation is an effec- tive sparsity reduction strategy for statis- tical machine translation (SMT) involv- ing morphologically complex languages. When translating into a segmented lan- guage, an extra step is required to deseg- ment the output; previous studies have de- segmented the 1-best output from the de- coder. In this paper, we expand our trans- lation options by desegmenting n-best lists or lattices. Our novel lattice desegmenta- tion algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs, which allows for inclusion of features related to the de- segmentation process, as well as an un- segmented language model (LM). We in- vestigate this technique in the context of English-to-Arabic and English-to-Finnish translation, showing significant improve- ments in translation quality over deseg- mentation of 1-best decoder outputs. ",,,,ACL
11,2014,Bilingually-constrained Phrase Embeddings for Machine Translation,"Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou","We propose Bilingually-constrained Re- cursive Auto-encoders (BRAE) to learn semantic phrase embeddings (compact vector representations for phrases), which can distinguish the phrases with differ- ent semantic meanings. The BRAE is trained in a way that minimizes the seman- tic distance of translation equivalents and maximizes the semantic distance of non- translation pairs simultaneously. After training, the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other. We evaluate our proposed method on two end-to-end SMT tasks (phrase ta- ble pruning and decoding with phrasal se- mantic similarities) which need to mea- sure semantic similarity between a source phrase and its translation candidates. Ex- tensive experiments show that the BRAE is remarkably effective in these two tasks. ",,,,ACL
12,2014,Learning New Semi-Supervised Deep Auto-encoder Features for Statistical Machine Translation,"Shixiang Lu, Zhenbiao Chen, Bo Xu","In this paper, instead of designing new fea- tures based on intuition, linguistic knowl- edge and domain, we learn some new and effective features using the deep auto- encoder (DAE) paradigm for phrase-based translation model. Using the unsupervised pre-trained deep belief net (DBN) to ini- tialize DAE’s parameters and using the in- put original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsuper- vised DBN features. Moreover, to learn high dimensional feature representation, we introduce a natural horizontal compo- sition of more DAEs for large hidden lay- ers feature learning. On two Chinese- English tasks, our semi-supervised DAE features obtain statistically significant im- provements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the unsupervised DBN features and the base- line features, respectively. ",,,,ACL
13,2014,Learning Topic Representation for SMT with Neural Networks,"Lei Cui, Dongdong Zhang, Shujie Liu, Qiming Chen","Statistical Machine Translation (SMT) usually utilizes contextual information to disambiguate translation candidates. However, it is often limited to contexts within sentence boundaries, hence broader topical information cannot be leveraged. In this paper, we propose a novel approach to learning topic representation for paral- lel data using a neural network architec- ture, where abundant topical contexts are embedded via topic relevant monolingual data. By associating each translation rule with the topic representation, topic rele- vant rules are selected according to the dis- tributional similarity with the source text during SMT decoding. Experimental re- sults show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task com- pared to a state-of-the-art baseline. ",,,,ACL
14,2014,Tagging The Web: Building A Robust Web Tagger with Neural Network,"Ji Ma, Yue Zhang, Jingbo Zhu","In this paper, we address the problem of web-domain POS tagging using a two- phase approach. The first phase learns rep- resentations that capture regularities un- derlying web text. The representation is integrated as features into a neural network that serves as a scorer for an easy-first POS tagger. Parameters of the neural network are trained using guided learning in the second phase. Experiment on the SANCL 2012 shared task show that our approach achieves 93.15% average tagging accu- racy, which is the best accuracy reported so far on this data set, higher than those given by ensembled syntactic parsers. ",,,,ACL
15,2014,Unsupervised Solution Post Identification from Discussion Forums,"Deepak P, Karthik Visweswariah","Discussion forums have evolved into a de- pendable source of knowledge to solve common problems. However, only a mi- nority of the posts in discussion forums are solution posts. Identifying solution posts from discussion forums, hence, is an important research problem. In this pa- per, we present a technique for unsuper- vised solution post identification leverag- ing a so far unexplored textual feature, that of lexical correlations between problems and solutions. We use translation mod- els and language models to exploit lex- ical correlations and solution post char- acter respectively. Our technique is de- signed to not rely much on structural fea- tures such as post metadata since such features are often not uniformly available across forums. Our clustering-based itera- tive solution identification approach based on the EM-formulation performs favor- ably in an empirical evaluation, beating the only unsupervised solution identifica- tion technique from literature by a very large margin. We also show that our unsu- pervised technique is competitive against methods that require supervision, outper- forming one such technique comfortably. ",,,,ACL
16,2014,Weakly Supervised User Profile Extraction from Twitter,"Jiwei Li, Alan Ritter, Eduard Hovy","While user attribute extraction on social media has received considerable attention, existing approaches, mostly supervised, encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates (e.g., gen- der). In this paper, we present a weakly- supervised approach to user profile extrac- tion from Twitter. Users’ profiles from so- cial media websites such as Facebook or Google Plus are used as a distant source of supervision for extraction of their at- tributes from user-generated text. In addi- tion to traditional linguistic features used in distant supervision for information ex- traction, our approach also takes into ac- count network information, a unique op- portunity offered by social media. We test our algorithm on three attribute domains: spouse, education and job; experimental results demonstrate our approach is able to make accurate predictions for users’ at- tributes based on their tweets. 1 ",,,,ACL
17,2014,The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter,"Chenhao Tan, Lillian Lee, Bo Pang","Consider a person trying to spread an important message on a social network. He/she can spend hours trying to craft the message. Does it actually matter? While there has been extensive prior work look- ing into predicting popularity of social- media content, the effect of wording per se has rarely been studied since it is of- ten confounded with the popularity of the author and the topic. To control for these confounding factors, we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording. Given such pairs, we ask: which version attracts more retweets? This turns out to be a more difficult task than predicting popular topics. Still, hu- mans can answer this question better than chance (but far from perfectly), and the computational methods we develop can do better than both an average human and a strong competing method trained on non- controlled data. ",,,,ACL
18,2014,Inferring User Political Preferences from Streaming Communications,"Svitlana Volkova, Glen Coppersmith, Benjamin Van Durme","Existing models for social media per- sonal analytics assume access to thou- sands of messages per user, even though most users author content only sporadi- cally over time. Given this sparsity, we: (i) leverage content from the local neigh- borhood of a user; (ii) evaluate batch mod- els as a function of size and the amount of messages in various types of neighbor- hoods; and (iii) estimate the amount of time and tweets required for a dynamic model to predict user preferences. We show that even when limited or no self- authored data is available, language from friend, retweet and user mention commu- nications provide sufficient evidence for prediction. When updating models over time based on Twitter, we find that polit- ical preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author’s tweeting frequency. ",,,,ACL
19,2014,Steps to Excellence: Simple Inference with Refined Scoring of Dependency Trees,"Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola","Much of the recent work on depen- dency parsing has been focused on solv- ing inherent combinatorial problems as- sociated with rich scoring functions. In contrast, we demonstrate that highly ex- pressive scoring functions can be used with substantially simpler inference pro- cedures. Specifically, we introduce a sampling-based parser that can easily han- dle arbitrary global features. Inspired by SampleRank, we learn to take guided stochastic steps towards a high scoring parse. We introduce two samplers for traversing the space of trees, Gibbs and Metropolis-Hastings with Random Walk. The model outperforms state-of-the-art re- sults when evaluated on 14 languages of non-projective CoNLL datasets. Our sampling-based approach naturally ex- tends to joint prediction scenarios, such as joint parsing and POS correction. The resulting method outperforms the best re- ported results on the CATiB dataset, ap- proaching performance of parsing with gold tags. 1 ",,,,ACL
20,2014,"Sparser, Better, Faster GPU Parsing","David Hall, Taylor Berg-Kirkpatrick, Dan Klein","Due to their origin in computer graph- ics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points. Natural lan- guage processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, Canny et al. (2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computa- tional power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per sec- ond on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning ap- proach to the constraints of a GPU. The resulting system is capable of computing over 404 Viterbi parses per second—more than a 2x speedup—on the same hard- ware. Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improv- ing throughput for this more accurate algo- rithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning—nearly a 6x speedup. ",,,,ACL
21,2014,Shift-Reduce CCG Parsing with a Dependency Model,"Wenduan Xu, Stephen Clark, Yue Zhang","This paper presents the first dependency model for a shift-reduce CCG parser. Mod- elling dependencies is desirable for a num- ber of reasons, including handling the “spurious” ambiguity of CCG ; fitting well with the theory of CCG ; and optimizing for structures which are evaluated at test time. We develop a novel training tech- nique using a dependency oracle, in which all derivations are hidden. A challenge arises from the fact that the oracle needs to keep track of exponentially many gold- standard derivations, which is solved by integrating a packed parse forest with the beam-search decoder. Standard CCGBank tests show the model achieves up to 1.05 labeled F-score improvements over three existing, competitive CCG parsing models. ",,,,ACL
22,2014,"Less Grammar, More Features","David Hall, Greg Durrett, Dan Klein","We present a parser that relies primar- ily on extracting information directly from surface spans rather than on propagat- ing information through enriched gram- mar structure. For example, instead of cre- ating separate grammar symbols to mark the definiteness of an NP, our parser might instead capture the same information from the first word of the NP. Moving context out of the grammar and onto surface fea- tures can greatly simplify the structural component of the parser: because so many deep syntactic cues have surface reflexes, our system can still parse accurately with context-free backbones as minimal as X- bar grammars. Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks. On the SPMRL 2013 multilingual con- stituency parsing shared task (Seddah et al., 2013), our system outperforms the top single parser system of Bj¨orkelund et al. (2013) on a range of languages. In addi- tion, despite being designed for syntactic analysis, our system also achieves state- of-the-art numbers on the structural senti- ment task of Socher et al. (2013). Finally, we show that, in both syntactic parsing and sentiment analysis, many broad linguistic trends can be captured via surface features. ",,,,ACL
23,2014,"Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors","Marco Baroni, Georgiana Dinu, Germán Kruszewski","Context-predicting models (more com- monly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the litera- ture is still lacking a systematic compari- son of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counter- parts. ",,,,ACL
24,2014,Metaphor Detection with Cross-Lingual Model Transfer,"Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric Nyberg","We show that it is possible to reliably dis- criminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by piv- oting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages. We pro- vide results on three new test sets in Span- ish, Farsi, and Russian. The results sup- port the hypothesis that metaphors are conceptual, rather than lexical, in nature. ",,,,ACL
25,2014,"Learning Word Sense Distributions, Detecting Unattested Senses and Identifying Novel Senses Using Topic Models","Jey Han Lau, Paul Cook, Diana McCarthy, Spandana Gella","Unsupervised word sense disambiguation ( WSD ) methods are an attractive approach to all-words WSD due to their non-reliance on expensive annotated data. Unsuper- vised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distri- butions. This paper presents a fully unsu- pervised topic modelling-based approach to sense frequency estimation, which is highly portable to different corpora and sense inventories, in being applicable to any part of speech, and not requiring a hi- erarchical sense inventory, parsing or par- allel text. We demonstrate the effective- ness of the method over the tasks of pre- dominant sense learning and sense distri- bution acquisition, and also the novel tasks of detecting senses which aren’t attested in the corpus, and identifying novel senses in the corpus which aren’t captured in the sense inventory. ",,,,ACL
26,2014,Learning to Automatically Solve Algebra Word Problems,"Nate Kushman, Yoav Artzi, Luke Zettlemoyer, Regina Barzilay","We present an approach for automatically learning to solve algebra word problems. Our algorithm reasons across sentence boundaries to construct and solve a sys- tem of linear equations, while simultane- ously recovering an alignment of the vari- ables and numbers in these equations to the problem text. The learning algorithm uses varied supervision, including either full equations or just the final answers. We evaluate performance on a newly gathered corpus of algebra word problems, demon- strating that the system can correctly an- swer almost 70% of the questions in the dataset. This is, to our knowledge, the first learning result for this task. ",,,,ACL
27,2014,Modelling function words improves unsupervised word segmentation,"Mark Johnson, Anne Christophe, Emmanuel Dupoux, Katherine Demuth","Inspired by experimental psychological findings suggesting that function words play a special role in word learning, we make a simple modification to an Adaptor Grammar based Bayesian word segmenta- tion model to allow it to learn sequences of monosyllabic “function words” at the beginnings and endings of collocations of (possibly multi-syllabic) words. This modification improves unsupervised word segmentation on the standard Bernstein- Ratner (1987) corpus of child-directed En- glish by more than 4% token f-score com- pared to a model identical except that it does not special-case “function words”, setting a new state-of-the-art of 92.4% to- ken f-score. Our function word model as- sumes that function words appear at the left periphery, and while this is true of languages such as English, it is not true universally. We show that a learner can use Bayesian model selection to determine the location of function words in their lan- guage, even though the input to the model only consists of unsegmented sequences of phones. Thus our computational models support the hypothesis that function words play a special role in word learning. ",,,,ACL
28,2014,Max-Margin Tensor Neural Network for Chinese Word Segmentation,"Wenzhe Pei, Tao Ge, Baobao Chang","Recently, neural network models for nat- ural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering. In this paper, we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network (MMTNN). By exploiting tag embeddings and tensor- based transformation, MMTNN has the ability to model complicated interactions between tags and context characters. Fur- thermore, a new tensor factorization ap- proach is proposed to speed up the model and avoid overfitting. Experiments on the benchmark dataset show that our model achieves better performances than previ- ous neural network models and that our model can achieve a competitive perfor- mance with minimal feature engineering. Despite Chinese word segmentation being a specific case, MMTNN can be easily generalized and applied to other sequence labeling tasks. ",,,,ACL
29,2014,An Empirical Study on the Effect of Negation Words on Sentiment,"Xiaodan Zhu, Hongyu Guo, Saif Mohammad, Svetlana Kiritchenko","Negation words, such as no and not, play a fundamental role in modifying sentiment of textual expressions. We will refer to a negation word as the negator and the text span within the scope of the negator as the argument. Commonly used heuristics to estimate the sentiment of negated expres- sions rely simply on the sentiment of ar- gument (and not on the negator or the ar- gument itself). We use a sentiment tree- bank to show that these existing heuristics are poor estimators of sentiment. We then modify these heuristics to be dependent on the negators and show that this improves prediction. Next, we evaluate a recently proposed composition model (Socher et al., 2013) that relies on both the negator and the argument. This model learns the syntax and semantics of the negator’s ar- gument with a recursive neural network. We show that this approach performs bet- ter than those mentioned above. In ad- dition, we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting er- rors. ",,,,ACL
30,2014,Extracting Opinion Targets and Opinion Words from Online Reviews with Graph Co-ranking,"Kang Liu, Liheng Xu, Jun Zhao","Extracting opinion targets and opinion words from online reviews are two fun- damental tasks in opinion mining. This paper proposes a novel approach to col- lectively extract them with graph co- ranking. First, compared to previous methods which solely employed opinion relations among words, our method con- structs a heterogeneous graph to model two types of relations, including seman- tic relations and opinion relations. Next, a co-ranking algorithm is proposed to es- timate the confidence of each candidate, and the candidates with higher confidence will be extracted as opinion targets/words. In this way, different relations make coop- erative effects on candidates’ confidence estimation. Moreover, word preference is captured and incorporated into our co- ranking algorithm. In this way, our co- ranking is personalized and each candi- date’s confidence is only determined by its preferred collocations. It helps to improve the extraction precision. The experimen- tal results on three data sets with differ- ent sizes and languages show that our ap- proach achieves better performance than state-of-the-art methods. ",,,,ACL
31,2014,Context-aware Learning for Sentence-level Sentiment Analysis with Posterior Regularization,"Bishan Yang, Claire Cardie","This paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences. Most ex- isting machine learning approaches suf- fer from limitations in the modeling of complex linguistic structures across sen- tences and often fail to capture non- local contextual cues that are important for sentiment interpretation. In contrast, our approach allows structured modeling of sentiment while taking into account both local and global contextual infor- mation. Specifically, we encode intu- itive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization. The context-aware constraints provide ad- ditional power to the CRF model and can guide semi-supervised learning when la- beled data is limited. Experiments on standard product review datasets show that our method outperforms the state-of-the- art methods in both the supervised and semi-supervised settings. ",,,,ACL
32,2014,Product Feature Mining: Semantic Clues versus Syntactic Constituents,"Liheng Xu, Kang Liu, Siwei Lai, Jun Zhao","Product feature mining is a key subtask in fine-grained opinion mining. Previ- ous works often use syntax constituents in this task. However, syntax-based methods can only use discrete contextual informa- tion, which may suffer from data sparsity. This paper proposes a novel product fea- ture mining method which leverages lexi- cal and contextual semantic clues. Lexical semantic clue verifies whether a candidate term is related to the target product, and contextual semantic clue serves as a soft pattern miner to find candidates, which ex- ploits semantics of each word in context so as to alleviate the data sparsity prob- lem. We build a semantic similarity graph to encode lexical semantic clue, and em- ploy a convolutional neural model to cap- ture contextual semantic clue. Then Label Propagation is applied to combine both se- mantic clues. Experimental results show that our semantics-based method signif- icantly outperforms conventional syntax- based approaches, which not only mines product features more accurately, but also extracts more infrequent product features. ",,,,ACL
33,2014,Aspect Extraction with Automated Prior Knowledge Learning,"Zhiyuan Chen, Arjun Mukherjee, Bing Liu","Aspect extraction is an important task in sentiment analysis. Topic modeling is a popular method for the task. However, unsupervised topic models often generate incoherent aspects. To address the is- sue, several knowledge-based models have been proposed to incorporate prior knowl- edge provided by the user to guide mod- eling. In this paper, we take a major step forward and show that in the big data era, without any user input, it is possi- ble to learn prior knowledge automatically from a large amount of review data avail- able on the Web. Such knowledge can then be used by a topic model to discover more coherent aspects. There are two key challenges: (1) learning quality knowl- edge from reviews of diverse domains, and (2) making the model fault-tolerant to handle possibly wrong knowledge. A novel approach is proposed to solve these problems. Experimental results using re- views from 36 domains show that the pro- posed approach achieves significant im- provements over state-of-the-art baselines. ",,,,ACL
34,2014,Anchors Regularized: Adding Robustness and Extensibility to Scalable Topic-Modeling Algorithms,"Thang Nguyen, Yuening Hu, Jordan Boyd-Graber","Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expec- tation maximization. However, these new methods lack the rich priors associated with probabilistic models. We examine Arora et al.’s anchor words algorithm for topic mod- eling and develop new, regularized algo- rithms that not only mathematically resem- ble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be com- bined with informed priors. ",,,,ACL
35,2014,A Bayesian Mixed Effects Model of Literary Character,"David Bamman, Ted Underwood, Noah A. Smith","We consider the problem of automatically inferring latent character types in a collec- tion of 15,099 English novels published between 1700 and 1899. Unlike prior work in which character types are assumed responsible for probabilistically generat- ing all text associated with a character, we introduce a model that employs mul- tiple effects to account for the influence of extra-linguistic information (such as au- thor). In an empirical evaluation, we find that this method leads to improved agree- ment with the preregistered judgments of a literary scholar, complementing the results of alternative models. ",,,,ACL
36,2014,Collective Tweet Wikification based on Semi-supervised Graph Regularization,"Hongzhao Huang, Yunbo Cao, Xiaojiang Huang, Heng Ji","Wikification for tweets aims to automat- ically identify each concept mention in a tweet and link it to a concept referent in a knowledge base (e.g., Wikipedia). Due to the shortness of a tweet, a collective inference model incorporating global ev- idence from multiple mentions and con- cepts is more appropriate than a non- collecitve approach which links each men- tion at a time. In addition, it is chal- lenging to generate sufficient high quality labeled data for supervised models with low cost. To tackle these challenges, we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multi- ple tweets through three fine-grained re- lations. In order to identify semantically- related mentions for collective inference, we detect meta path-based semantic rela- tions through social networks. Compared to the state-of-the-art supervised model trained from 100% labeled data, our pro- posed approach achieves comparable per- formance with 31% labeled data and ob- tains 5% absolute F1 gain with 50% la- beled data. ",,,,ACL
37,2014,Zero-shot Entity Extraction from Web Pages,"Panupong Pasupat, Percy Liang","In order to extract entities of a fine-grained category from semi-structured data in web pages, existing information extraction sys- tems rely on seed examples or redundancy across multiple web pages. In this paper, we consider a new zero-shot learning task of extracting entities specified by a natural language query (in place of seeds) given only a single web page. Our approach de- fines a log-linear model over latent extrac- tion predicates, which select lists of enti- ties from the web page. The main chal- lenge is to define features on widely vary- ing candidate entity lists. We tackle this by abstracting list elements and using aggre- gate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline. ",,,,ACL
38,2014,Incremental Joint Extraction of Entity Mentions and Relations,"Qi Li, Heng Ji","We present an incremental joint frame- work to simultaneously extract entity men- tions and relations using structured per- ceptron with efficient beam-search. A segment-based decoder based on the idea of semi-Markov chain is adopted to the new framework as opposed to traditional token-based tagging. In addition, by virtue of the inexact search, we developed a num- ber of new and effective global features as soft constraints to capture the inter- dependency among entity mentions and relations. Experiments on Automatic Con- tent Extraction (ACE) 1 corpora demon- strate that our joint model significantly outperforms a strong pipelined baseline, which attains better performance than the best-reported end-to-end system. ",,,,ACL
39,2014,That’s Not What I Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text,"Manjuan Duan, Michael White","We investigate whether parsers can be used for self-monitoring in surface real- ization in order to avoid egregious errors involving “vicious” ambiguities, namely those where the intended interpretation fails to be considerably more likely than alternative ones. Using parse accuracy in a simple reranking strategy for self- monitoring, we find that with a state- of-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make. How- ever, by using an SVM ranker to combine the realizer’s model score together with features from multiple parsers, including ones designed to make the ranker more ro- bust to parsing mistakes, we show that sig- nificant increases in BLEU scores can be achieved. Moreover, via a targeted man- ual analysis, we demonstrate that the SVM reranker frequently manages to avoid vi- cious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy. ",,,,ACL
40,2014,Surface Realisation from Knowledge-Bases,"Bikash Gyawali, Claire Gardent","We present a simple, data-driven approach to generation from knowledge bases (KB). A key feature of this approach is that grammar induction is driven by the ex- tended domain of locality principle of TAG (Tree Adjoining Grammar); and that it takes into account both syntactic and semantic information. The resulting ex- tracted TAG includes a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data. Experimental evaluation on the KBGen data shows that our model outper- forms a data-driven generate-and-rank ap- proach based on an automatically induced probabilistic grammar; and is comparable with a handcrafted symbolic approach. ",,,,ACL
41,2014,Hybrid Simplification using Deep Semantics and Machine Translation,"Shashi Narayan, Claire Gardent","We present a hybrid approach to sentence simplification which combines deep se- mantics and monolingual machine transla- tion to derive simple sentences from com- plex ones. The approach differs from pre- vious work in two main ways. First, it is semantic based in that it takes as in- put a deep semantic representation rather than e.g., a sentence or a parse tree. Sec- ond, it combines a simplification model for splitting and deletion with a monolin- gual translation model for phrase substi- tution and reordering. When compared against current state of the art methods, our model yields significantly simpler out- put that is both grammatical and meaning preserving. ",,,,ACL
42,2014,Grammatical Relations in Chinese: GB-Ground Extraction and Data-Driven Parsing,"Weiwei Sun, Yantao Du, Xin Kou, Shuoyang Ding","This paper is concerned with building linguistic re- sources and statistical parsers for deep grammatical relation (GR) analysis of Chinese texts. A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general di- rected dependency graphs. The reliability of this linguistically-motivated GR extraction procedure is highlighted by manual evaluation. Based on the converted corpus, we study transition-based, data- driven models for GR parsing. We present a novel transition system which suits GR graphs better than existing systems. The key idea is to introduce a new type of transition that reorders top k elements in the memory module. Evaluation gauges how successful GR parsing for Chinese can be by applying data- driven models. ",,,,ACL
43,2014,Ambiguity-aware Ensemble Training for Semi-supervised Dependency Parsing,"Zhenghua Li, Min Zhang, Wenliang Chen","This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1- best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. This framework offers two promising advantages. 1) ambiguity encoded in parse forests compromises noise in 1-best parse trees. During training, the parser is aware of these ambiguous structures, and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves. 2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. ",,,,ACL
44,2014,A Robust Approach to Aligning Heterogeneous Lexical Resources,"Mohammad Taher Pilehvar, Roberto Navigli","Lexical resource alignment has been an active field of research over the last decade. However, prior methods for align- ing lexical resources have been either spe- cific to a particular pair of resources, or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned. Here we present a unified approach that can be applied to an arbitrary pair of lexical resources, includ- ing machine-readable dictionaries with no network structure. Our approach leverages a similarity measure that enables the struc- tural comparison of senses across lexical resources, achieving state-of-the-art per- formance on the task of aligning WordNet to three different collaborative resources: Wikipedia, Wiktionary and OmegaWiki. ",,,,ACL
45,2014,Predicting the relevance of distributional semantic similarity with contextual information,"Philippe Muller, Cécile Fabre, Clémentine Adam","Using distributional analysis methods to compute semantic proximity links be- tween words has become commonplace in NLP. The resulting relations are often noisy or difficult to interpret in general. This paper focuses on the issues of eval- uating a distributional resource and filter- ing the relations it contains, but instead of considering it in abstracto, we focus on pairs of words in context. In a dis- course, we are interested in knowing if the semantic link between two items is a by- product of textual coherence or is irrele- vant. We first set up a human annotation of semantic links with or without contex- tual information to show the importance of the textual context in evaluating the rele- vance of semantic similarity, and to assess the prevalence of actual semantic relations between word tokens. We then built an ex- periment to automatically predict this rel- evance, evaluated on the reliable reference data set which was the outcome of the first annotation. We show that in-document in- formation greatly improve the prediction made by the similarity level alone. ",,,,ACL
46,2014,Interpretable Semantic Vectors from a Joint Model of Brain- and Text- Based Meaning,"Alona Fyshe, Partha P. Talukdar, Brian Murphy, Tom M. Mitchell","Vector space models (VSMs) represent word meanings as points in a high dimen- sional space. VSMs are typically created using a large text corpora, and so repre- sent word semantics as observed in text. We present a new algorithm (JNNSE) that can incorporate a measure of semantics not previously used to create VSMs: brain activation data recorded while people read words. The resulting model takes advan- tage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representa- tion of semantics. Evaluations show that the model 1) matches a behavioral mea- sure of semantics more closely, 2) can be used to predict corpus data for unseen words and 3) has predictive power that generalizes across brain imaging technolo- gies and across subjects. We believe that the model is thus a more faithful represen- tation of mental vocabularies. ",,,,ACL
47,2014,Single-Agent vs. Multi-Agent Techniques for Concurrent Reinforcement Learning of Negotiation Dialogue Policies,"Kallirroi Georgila, Claire Nelson, David Traum","We use single-agent and multi-agent Rein- forcement Learning (RL) for learning dia- logue policies in a resource allocation ne- gotiation scenario. Two agents learn con- currently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from. In particular, we compare the Q- learning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the sce- nario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate. Our re- sults show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly. We also show that very high gradually decreasing exploration rates are required for convergence. We conclude that multi- agent RL of dialogue policies is a promis- ing alternative to using single-agent RL and SUs or learning directly from corpora. ",,,,ACL
48,2014,A Linear-Time Bottom-Up Discourse Parser with Constraints and Post-Editing,"Vanessa Wei Feng, Graeme Hirst","Text-level discourse parsing remains a challenge. The current state-of-the-art overall accuracy in relation assignment is 55.73%, achieved by Joty et al. (2013). However, their model has a high order of time complexity, and thus cannot be ap- plied in practice. In this work, we develop a much faster model whose time complex- ity is linear in the number of sentences. Our model adopts a greedy bottom-up ap- proach, with two linear-chain CRFs ap- plied in cascade as local classifiers. To en- hance the accuracy of the pipeline, we add additional constraints in the Viterbi decod- ing of the first CRF. In addition to effi- ciency, our parser also significantly out- performs the state of the art. Moreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy. ",,,,ACL
49,2014,Negation Focus Identification with Contextual Discourse Information,"Bowei Zou, Guodong Zhou, Qiaoming Zhu","Negative expressions are common in natural language text and play a critical role in in- formation extraction. However, the perfor- mances of current systems are far from satis- faction, largely due to its focus on intra- sentence information and its failure to con- sider inter-sentence information. In this paper, we propose a graph model to enrich intra- sentence features with inter-sentence features from both lexical and topic perspectives. Evaluation on the *SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus iden- tification and justifies the effectiveness of our graph model in capturing such global infor- mation. * ",,,,ACL
50,2014,New Word Detection for Sentiment Analysis,"Minlie Huang, Borui Ye, Yichen Wang, Haiqiang Chen","Automatic extraction of new words is an indispensable precursor to many NLP tasks such as Chinese word segmentation, named entity extraction, and sentimen- t analysis. This paper aims at extract- ing new sentiment words from large-scale user-generated content. We propose a ful- ly unsupervised, purely data-driven frame- work for this purpose. We design statisti- cal measures respectively to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word. The method is almost free of linguistic re- sources (except POS tags), and requires no elaborated linguistic rules. We also demonstrate how new sentiment word will benefit sentiment analysis. Experiment re- sults demonstrate the effectiveness of the proposed method. ",,,,ACL
51,2014,ReNew: A Semi-Supervised Framework for Generating Domain-Specific Lexicons and Sentiment Analysis,"Zhe Zhang, Munindar P. Singh","The sentiment captured in opinionated text provides interesting and valuable informa- tion for social media services. However, due to the complexity and diversity of linguistic representations, it is challeng- ing to build a framework that accurately extracts such sentiment. We propose a semi-supervised framework for generat- ing a domain-specific sentiment lexicon and inferring sentiments at the segment level. Our framework can greatly reduce the human effort for building a domain- specific sentiment lexicon with high qual- ity. Specifically, in our evaluation, work- ing with just 20 manually labeled reviews, it generates a domain-specific sentiment lexicon that yields weighted average F- Measure gains of 3%. Our sentiment clas- sification model achieves approximately 1% greater accuracy than a state-of-the-art approach based on elementary discourse units. ",,,,ACL
52,2014,A Decision-Theoretic Approach to Natural Language Generation,"Nathan McKinley, Soumya Ray","We study the problem of generating an En- glish sentence given an underlying prob- abilistic grammar, a world and a com- municative goal. We model the genera- tion problem as a Markov decision process with a suitably defined reward function that reflects the communicative goal. We then use probabilistic planning to solve the MDP and generate a sentence that, with high probability, accomplishes the com- municative goal. We show empirically that our approach can generate complex sen- tences with a speed that generally matches or surpasses the state of the art. Further, we show that our approach is anytime and can handle complex communicative goals, including negated goals. ",,,,ACL
53,2014,Generating Code-switched Text for Lexical Learning,"Igor Labutov, Hod Lipson","A vast majority of L1 vocabulary acqui- sition occurs through incidental learning during reading (Nation, 2001; Schmitt et al., 2001). We propose a probabilistic ap- proach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading. Our model that takes as input a bilingual dictionary and an English text, and gener- ates a code-switched text that optimizes a defined “learnability” metric by construct- ing a factor graph over lexical mentions. Using an artificial language vocabulary, we evaluate a set of algorithms for gener- ating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence com- pletion task. ",,,,ACL
54,2014,Omni-word Feature and Soft Constraint for Chinese Relation Extraction,"Yanping Chen, Qinghua Zheng, Wei Zhang","Chinese is an ancient hieroglyphic. It is inat- tentive to structure. Therefore, segmenting and parsing Chinese are more difficult and less accurate. In this paper, we propose an Omni- word feature and a soft constraint method for Chinese relation extraction. The Omni-word feature uses every potential word in a sentence as lexicon feature, reducing errors caused by word segmentation. In order to utilize the structure information of a relation instance, we discuss how soft constraint can be used to cap- ture the local dependency. Both Omni-word feature and soft constraint make a better use of sentence information and minimize the in- fluences caused by Chinese word segmenta- tion and parsing. We test these methods on the ACE 2005 RDC Chinese corpus. The re- sults show a significant improvement in Chi- nese relation extraction, outperforming other methods in F-score by 10% in 6 relation types and 15% in 18 relation subtypes. ",,,,ACL
55,2014,Bilingual Active Learning for Relation Classification via Pseudo Parallel Corpora,"Longhua Qian, Haotian Hui, Ya’nan Hu, Guodong Zhou","Active learning (AL) has been proven ef- fective to reduce human annotation ef- forts in NLP. However, previous studies on AL are limited to applications in a single language. This paper proposes a bilingual active learning paradigm for re- lation classification, where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle. Instead of using a parallel corpus, labeled and unlabeled instances in one language are translated into ones in the other language and all instances in both languages are then fed into a bilin- gual active learning engine as pseudo parallel corpora. Experimental results on the ACE RDC 2005 Chinese and English corpora show that bilingual active learn- ing for relation classification signifi- cantly outperforms monolingual active learning. ",,,,ACL
56,2014,Learning Soft Linear Constraints with Application to Citation Field Extraction,"Sam Anzaroot, Alexandre Passos, David Belanger, Andrew McCallum","Accurately segmenting a citation string into fields for authors, titles, etc. is a chal- lenging task because the output typically obeys various global constraints. Previous work has shown that modeling soft con- straints, where the model is encouraged, but not require to obey the constraints, can substantially improve segmentation per- formance. On the other hand, for impos- ing hard constraints, dual decomposition is a popular technique for efficient predic- tion given existing algorithms for uncon- strained inference. We extend dual decom- position to perform prediction subject to soft constraints. Moreover, with a tech- nique for performing inference given soft constraints, it is easy to automatically gen- erate large families of constraints and learn their costs with a simple convex optimiza- tion problem during training. This allows us to obtain substantial gains in accuracy on a new, challenging citation extraction dataset. ",,,,ACL
57,2014,A Study of Concept-based Weighting Regularization for Medical Records Search,"Yue Wang, Xitong Liu, Hui Fang","An important search task in the biomedical domain is to find medical records of pa- tients who are qualified for a clinical trial. One commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept- based representation. However, the map- ping results are not perfect, and none of previous work studied how to deal with them in the retrieval process. In this pa- per, we focus on addressing the limitations caused by the imperfect mapping results and study how to further improve the re- trieval performance of the concept-based ranking methods. In particular, we ap- ply axiomatic approaches and propose two weighting regularization methods that ad- just the weighting based on the relations among the concepts. Experimental results show that the proposed methods are effec- tive to improve the retrieval performance, and their performances are comparable to other top-performing systems in the TREC Medical Records Track. ",,,,ACL
58,2014,Learning to Predict Distributions of Words Across Domains,"Danushka Bollegala, David Weir, John Carroll","Although the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been lim- ited to a single domain because the dis- tribution of a word can vary between do- mains as the word’s predominant mean- ing changes. However, if it were pos- sible to predict how the distribution of a word changes from one domain to an- other, the predictions could be used to adapt a system trained in one domain to work in another. We propose an unsuper- vised method to predict the distribution of a word in one domain, given its distribu- tion in another domain. We evaluate our method on two tasks: cross-domain part- of-speech tagging and cross-domain sen- timent classification. In both tasks, our method significantly outperforms compet- itive baselines and returns results that are statistically comparable to current state- of-the-art methods, while requiring no task-specific customisations. ",,,,ACL
59,2014,How to make words with vectors: Phrase generation in distributional semantics,"Georgiana Dinu, Marco Baroni","We introduce the problem of generation in distributional semantics: Given a distri- butional vector representing some mean- ing, how can we generate the phrase that best expresses that meaning? We mo- tivate this novel challenge on theoretical and practical grounds and propose a sim- ple data-driven approach to the estimation of generation functions. We test this in a monolingual scenario (paraphrase gen- eration) as well as in a cross-lingual set- ting (translation by synthesizing adjective- noun phrase vectors in English and gener- ating the equivalent expressions in Italian). ",,,,ACL
60,2014,Vector space semantics with frequency-driven motifs,"Shashank Srivastava, Eduard Hovy","Traditional models of distributional se- mantics suffer from computational issues such as data sparsity for individual lex- emes and complexities of modeling se- mantic composition when dealing with structures larger than single lexical items. In this work, we present a frequency- driven paradigm for robust distributional semantics in terms of semantically cohe- sive lineal constituents, or motifs. The framework subsumes issues such as dif- ferential compositional as well as non- compositional behavior of phrasal con- situents, and circumvents some problems of data sparsity by design. We design a segmentation model to optimally par- tition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically dis- ambiguated. Hellinger PCA embeddings learnt using the framework show competi- tive results on empirical tasks. ",,,,ACL
61,2014,Lexical Inference over Multi-Word Predicates: A Distributional Approach,"Omri Abend, Shay B. Cohen, Mark Steedman","Representing predicates in terms of their argument distribution is common practice in NLP. Multi-word predicates (MWPs) in this context are often either disregarded or considered as fixed expressions. The lat- ter treatment is unsatisfactory in two ways: (1) identifying MWPs is notoriously diffi- cult, (2) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts. We propose a novel approach that integrates the distributional representation of multiple sub-sets of the MWP’s words. We assume a latent distri- bution over sub-sets of the MWP, and esti- mate it relative to a downstream prediction task. Focusing on the supervised identi- fication of lexical inference relations, we compare against state-of-the-art baselines that consider a single sub-set of an MWP, obtaining substantial improvements. To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics. ",,,,ACL
62,2014,A Convolutional Neural Network for Modelling Sentences,"Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom","The ability to accurately represent sen- tences is central to language understand- ing. We describe a convolutional architec- ture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pool- ing, a global pooling operation over lin- ear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily ap- plicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment predic- tion, six-way question classification and Twitter sentiment prediction by distant su- pervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline. ",,,,ACL
63,2014,Online Learning in Tensor Space,"Yuan Cao, Sanjeev Khudanpur","We propose an online learning algorithm based on tensor-space models. A tensor- space model represents data in a compact way, and via rank-1 approximation the weight tensor can be made highly struc- tured, resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models. This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is de- fined but very limited resources are avail- able for training. We apply with the pro- posed algorithm to a parsing task, and show that even with very little training data the learning algorithm based on a ten- sor model performs well, and gives signif- icantly better results than standard learn- ing algorithms based on traditional vector- space models. ",,,,ACL
64,2014,Graph-based Semi-Supervised Learning of Translation Models from Monolingual Data,"Avneesh Saluja, Hany Hassan, Kristina Toutanova, Chris Quirk","Statistical phrase-based translation learns translation rules from bilingual corpora, and has traditionally only used monolin- gual evidence to construct features that rescore existing translation candidates. In this work, we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data. The proposed tech- nique first constructs phrase graphs using both source and target language mono- lingual corpora. Next, graph propaga- tion identifies translations of phrases that were not observed in the bilingual cor- pus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrase- based systems, leading to consistent im- provements between 1 and 4 BLEU points on standard evaluation sets. ",,,,ACL
65,2014,Using Discourse Structure Improves Machine Translation Evaluation,"Francisco Guzmán, Shafiq Joty, Lluís Màrquez, Preslav Nakov","We present experiments in using dis- course structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level. Rather than proposing a single new metric, we show that discourse information is com- plementary to the state-of-the-art evalu- ation metrics, and thus should be taken into account in the development of future richer evaluation metrics. ",,,,ACL
66,2014,Learning Continuous Phrase Representations for Translation Modeling,"Jianfeng Gao, Xiaodong He, Wen-tau Yih, Li Deng","This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representa- tions, whose distributed nature enables the sharing of related phrases in their represen- tations. A pair of source and target phrases are projected into continuous-valued vec- tor representations in a low-dimensional latent space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a neural network whose weights are learned on parallel training data. Experimental evaluation has been performed on two WMT translation tasks. Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points. ",,,,ACL
67,2014,Adaptive Quality Estimation for Machine Translation,"Marco Turchi, Antonios Anastasopoulos, José G. C. de Souza, Matteo Negri","The automatic estimation of machine translation (MT) output quality is a hard task in which the selection of the appro- priate algorithm and the most predictive features over reasonably sized training sets plays a crucial role. When moving from controlled lab evaluations to real-life sce- narios the task becomes even harder. For current MT quality estimation (QE) sys- tems, additional complexity comes from the difficulty to model user and domain changes. Indeed, the instability of the sys- tems with respect to data coming from dif- ferent distributions calls for adaptive so- lutions that react to new operating con- ditions. To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes. Contrastive exper- iments in different testing conditions in- volving user and domain changes demon- strate the effectiveness of our approach. ",,,,ACL
68,2014,Learning Grounded Meaning Representations with Autoencoders,"Carina Silberer, Mirella Lapata","In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from tex- tual and visual input. The two modali- ties are encoded as vectors of attributes and are obtained automatically from text and images, respectively. We evaluate our model on its ability to simulate similar- ity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models. ",,,,ACL
69,2014,Joint POS Tagging and Transition-based Constituent Parsing in Chinese with Non-local Features,"Zhiguo Wang, Nianwen Xue","We propose three improvements to ad- dress the drawbacks of state-of-the-art transition-based constituent parsers. First, to resolve the error propagation problem of the traditional pipeline approach, we incorporate POS tagging into the syntac- tic parsing process. Second, to allevi- ate the negative influence of size differ- ences among competing action sequences, we align parser states during beam-search decoding. Third, to enhance the pow- er of parsing models, we enlarge the fea- ture set with non-local features and semi- supervised word cluster features. Exper- imental results show that these modifica- tions improve parsing performance signif- icantly. Evaluated on the Chinese Tree- Bank (CTB), our final performance reach- es 86.3% (F1) when trained on CTB 5.1, and 87.1% when trained on CTB 6.0, and these results outperform all state-of-the-art parsers. ",,,,ACL
70,2014,Strategies for Contiguous Multiword Expression Analysis and Dependency Parsing,"Marie Candito, Matthieu Constant","In this paper, we investigate various strate- gies to predict both syntactic dependency parsing and contiguous multiword expres- sion (MWE) recognition, testing them on the dependency version of French Tree- bank (Abeill′e and Barrier, 2004), as in- stantiated in the SPMRL Shared Task (Seddah et al., 2013). Our work focuses on using an alternative representation of syntactically regular MWEs, which cap- tures their syntactic internal structure. We obtain a system with comparable perfor- mance to that of previous works on this dataset, but which predicts both syntactic dependencies and the internal structure of MWEs. This can be useful for capturing the various degrees of semantic composi- tionality of MWEs. ",,,,ACL
71,2014,Correcting Preposition Errors in Learner English Using Error Case Frames and Feedback Messages,"Ryo Nagata, Mikko Vilenius, Edward Whittaker","This paper presents a novel framework called error case frames for correcting preposition errors. They are case frames specially designed for describing and cor- recting preposition errors. Their most dis- tinct advantage is that they can correct er- rors with feedback messages explaining why the preposition is erroneous. This pa- per proposes a method for automatically generating them by comparing learner and native corpora. Experiments show (i) au- tomatically generated error case frames achieve a performance comparable to con- ventional methods; (ii) error case frames are intuitively interpretable and manually modifiable to improve them; (iii) feedback messages provided by error case frames are effective in language learning assis- tance. Considering these advantages and the fact that it has been difficult to provide feedback messages by automatically gen- erated rules, error case frames will likely be one of the major approaches for prepo- sition error correction. ",,,,ACL
72,2014,Kneser-Ney Smoothing on Expected Counts,"Hui Zhang, David Chiang","Widely used in speech and language pro- cessing, Kneser-Ney (KN) smoothing has consistently been shown to be one of the best-performing smoothing methods. However, KN smoothing assumes integer counts, limiting its potential uses—for ex- ample, inside Expectation-Maximization. In this paper, we propose a generaliza- tion of KN smoothing that operates on fractional counts, or, more precisely, on distributions over counts. We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts, and apply it to two tasks where KN smoothing was not applicable before: one in language model adaptation, and the other in word alignment. In both cases, our method improves performance signifi- cantly. ",,,,ACL
73,2014,Robust Entity Clustering via Phylogenetic Inference,"Nicholas Andrews, Jason Eisner, Mark Dredze","Entity clustering must determine when two named-entity mentions refer to the same entity. Typical approaches use a pipeline ar- chitecture that clusters the mentions using fixed or learned measures of name and con- text similarity. In this paper, we propose a model for cross-document coreference res- olution that achieves robustness by learn- ing similarity from unlabeled data. The generative process assumes that each entity mention arises from copying and option- ally mutating an earlier name from a sim- ilar context. Clustering the mentions into entities depends on recovering this copying tree jointly with estimating models of the mutation process and parent selection pro- cess. We present a block Gibbs sampler for posterior inference and an empirical evalu- ation on several datasets. ",,,,ACL
74,2014,Linguistic Structured Sparsity in Text Categorization,"Dani Yogatama, Noah A. Smith","We introduce three linguistically moti- vated structured regularizers based on parse trees, topics, and hierarchical word clusters for text categorization. These regularizers impose linguistic bias in fea- ture weights, enabling us to incorporate prior knowledge into conventional bag- of-words models. We show that our structured regularizers consistently im- prove classification accuracies compared to standard regularizers that penalize fea- tures in isolation (such as lasso, ridge, and elastic net regularizers) on a range of datasets for various text prediction prob- lems: topic classification, sentiment anal- ysis, and forecasting. ",,,,ACL
75,2014,Perplexity on Reduced Corpora,Hayato Kobayashi,"This paper studies the idea of remov- ing low-frequency words from a corpus, which is a common practice to reduce computational costs, from a theoretical standpoint. Based on the assumption that a corpus follows Zipf’s law, we derive trade- off formulae of the perplexity of k -gram models and topic models with respect to the size of the reduced vocabulary. In ad- dition, we show an approximate behavior of each formula under certain conditions. We verify the correctness of our theory on synthetic corpora and examine the gap be- tween theory and practice on real corpora. ",,,,ACL
76,2014,Robust Domain Adaptation for Relation Extraction via Clustering Consistency,"Minh Luan Nguyen, Ivor W. Tsang, Kian Ming A. Chai, Hai Leong Chieu","We propose a two-phase framework to adapt existing relation extraction classi- fiers to extract relations for new target do- mains. We address two challenges: neg- ative transfer when knowledge in source domains is used without considering the differences in relation distributions; and lack of adequate labeled samples for rarer relations in the new domain, due to a small labeled data set and imbalance rela- tion distributions. Our framework lever- ages on both labeled and unlabeled data in the target domain. First, we determine the relevance of each source domain to the target domain for each relation type, using the consistency between the clus- tering given by the target domain labels and the clustering given by the predic- tors trained for the source domain. To overcome the lack of labeled samples for rarer relations, these clusterings operate on both the labeled and unlabeled data in the target domain. Second, we trade-off between using relevance-weighted source- domain predictors and the labeled target data. Again, to overcome the imbalance distribution, the source-domain predictors operate on the unlabeled target data. Our method outperforms numerous baselines and a weakly-supervised relation extrac- tion method on ACE 2004 and YAGO. ",,,,ACL
77,2014,Encoding Relation Requirements for Relation Extraction via Joint Inference,"Liwei Chen, Yansong Feng, Songfang Huang, Yong Qin","Most existing relation extraction models make predictions for each entity pair lo- cally and individually, while ignoring im- plicit global clues available in the knowl- edge base, sometimes leading to conflicts among local predictions from different en- tity pairs. In this paper, we propose a joint inference framework that utilizes these global clues to resolve disagree- ments among local predictions. We ex- ploit two kinds of clues to generate con- straints which can capture the implicit type and cardinality requirements of a relation. Experimental results on three datasets, in both English and Chinese, show that our framework outperforms the state-of-the- art relation extraction models when such clues are applicable to the datasets. And, we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human. ",,,,ACL
78,2014,Medical Relation Extraction with Manifold Models,"Chang Wang, James Fan","In this paper, we present a manifold model for medical relation extraction. Our model is built upon a medical corpus containing 80M sentences (11 gigabyte text) and de- signed to accurately and ef?ciently detect the key medical relations that can facilitate clinical decision making. Our approach integrates domain speci?c parsing and typ- ing systems, and can utilize labeled as well as unlabeled examples. To provide users with more ?exibility, we also take label weight into consideration. Effectiveness of our model is demonstrated both theo- retically with a proof to show that the so- lution is a closed-form solution and exper- imentally with positive results in experi- ments. ",,,,ACL
79,2014,Distant Supervision for Relation Extraction with Matrix Completion,"Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu","The essence of distantly supervised rela- tion extraction is that it is an incomplete multi-label classification problem with s- parse and noisy features. To tackle the s- parsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a s- parse matrix that concatenates training and testing textual features with training label- s. Our algorithmic framework is based on the assumption that the rank of item-by- feature and item-by-label joint matrix is low. We apply two optimization model- s to recover the underlying low-rank ma- trix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the glob- al optimum. Experiments on two wide- ly used datasets with different dimension- s of textual features demonstrate that our low-rank matrix completion approach sig- nificantly outperforms the baseline and the state-of-the-art methods. ",,,,ACL
80,2014,Enhancing Grammatical Cohesion: Generating Transitional Expressions for SMT,"Mei Tu, Yu Zhou, Chengqing Zong","Transitional expressions provide glue that holds ideas together in a text and enhance the logical organization, which together help im- prove readability of a text. However, in most current statistical machine translation (SMT) systems, the outputs of compound-complex sentences still lack proper transitional expres- sions. As a result, the translations are often hard to read and understand. To address this issue, we propose two novel models to en- courage generating such transitional expres- sions by introducing the source compound- complex sentence structure (CSS). Our models include a CSS-based translation model, which generates new CSS-based translation rules, and a generative transfer model, which en- courages producing transitional expressions during decoding. The two models are integrat- ed into a hierarchical phrase-based translation system to evaluate their effectiveness. The ex- perimental results show that significant im- provements are achieved on various test data meanwhile the translations are more cohesive and smooth. ",,,,ACL
81,2014,Adaptive HTER Estimation for Document-Specific MT Post-Editing,"Fei Huang, Jian-Ming Xu, Abraham Ittycheriah, Salim Roukos","We present an adaptive translation qual- ity estimation (QE) method to predict the human-targeted translation error rate (HTER) for a document-specific machine translation model. We first introduce fea- tures derived internal to the translation de- coding process as well as externally from the source sentence analysis. We show the effectiveness of such features in both classification and regression of MT qual- ity. By dynamically training the QE model for the document-specific MT model, we are able to achieve consistency and pre- diction quality across multiple documents, demonstrated by the higher correlation co- efficient and F-scores in finding Good sen- tences. Additionally, the proposed method is applied to IBM English-to-Japanese MT post editing field study and we observe strong correlation with human preference, with a 10% increase in human translators’ productivity. ",,,,ACL
82,2014,Translation Assistance by Translation of L1 Fragments in an L2 Context,"Maarten van Gompel, Antal van den Bosch",In this paper we present new research in translation assistance. We describe a sys- tem capable of translating native language (L1) fragments to foreign language (L2) fragments in an L2 context. Practical ap- plications of this research can be framed in the context of second language learning. The type of translation assistance system under investigation here encourages lan- guage learners to write in their target lan- guage while allowing them to fall back to their native language in case the correct word or expression is not known. These code switches are subsequently translated to L2 given the L2 context. We study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense dis- ambiguation baselines. A classification- based approach is presented that is in- deed found to improve significantly over these baselines by making use of a contex- tual window spanning a small number of neighbouring words. ,,,,ACL
83,2014,Response-based Learning for Grounded Machine Translation,"Stefan Riezler, Patrick Simianer, Carolin Haas","We propose a novel learning approach for statistical machine translation (SMT) that allows to extract supervision signals for structured learning from an extrinsic re- sponse to a translation input. We show how to generate responses by grounding SMT in the task of executing a seman- tic parse of a translated query against a database. Experiments on the G EO - QUERY database show an improvement of about 6 points in F1-score for response- based learning over learning from refer- ences only on returning the correct an- swer from a semantic parse of a translated query. In general, our approach alleviates the dependency on human reference trans- lations and solves the reachability problem in structured learning for SMT. ",,,,ACL
84,2014,"Modelling Events through Memory-based, Open-IE Patterns for Abstractive Summarization","Daniele Pighin, Marco Cornolti, Enrique Alfonseca, Katja Filippova","Abstractive text summarization of news requires a way of representing events, such as a collection of pattern clusters in which every cluster represents an event (e.g., marriage) and every pattern in the clus- ter is a way of expressing the event (e.g., X married Y, X and Y tied the knot). We compare three ways of extracting event patterns: heuristics-based, compression- based and memory-based. While the for- mer has been used previously in multi- document abstraction, the latter two have never been used for this task. Compared with the first two techniques, the memory- based method allows for generating sig- nificantly more grammatical and informa- tive sentences, at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances. To this end, we introduce a data structure and a search method that make it possible to efficiently extrapolate from every sentence the parse sub-trees that match against any of the stored utterances. ",,,,ACL
85,2014,Hierarchical Summarization: Scaling Up Multi-Document Summarization,"Janara Christensen, Stephen Soderland, Gagan Bansal, Mausam","Multi-document summarization (MDS) systems have been designed for short, un- structured summaries of 10-15 documents, and are inadequate for larger document collections. We propose a new approach to scaling up summarization called hierar- chical summarization, and present the first implemented system, S UMMA . S UMMA produces a hierarchy of relatively short summaries, in which the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest. S UMMA optimizes for coherence as well as cover- age of salient information. In an Amazon Mechanical Turk evaluation, users pref- ered S UMMA ten times as often as flat MDS and three times as often as timelines. ",,,,ACL
86,2014,Query-Chain Focused Summarization,"Tal Baumel, Raphael Cohen, Michael Elhadad","Update summarization is a form of multi- document summarization where a document set must be summarized in the context of other documents assumed to be known. Efficient update summarization must focus on identify- ing new information and avoiding repetition of known information. In Query-focused summa- rization, the task is to produce a summary as an answer to a given query. We introduce a new task, Query-Chain Summarization, which combines aspects of the two previous tasks: starting from a given document set, increas- ingly specific queries are considered, and a new summary is produced at each step. This process models exploratory search: a user ex- plores a new topic by submitting a sequence of queries, inspecting a summary of the result set and phrasing a new query at each step. We present a novel dataset comprising 22 query- chains sessions of length up to 3 with 3 match- ing human summaries each in the consumer- health domain. Our analysis demonstrates that summaries produced in the context of such exploratory process are different from in- formative summaries. We present an algorithm for Query-Chain Summarization based on a new LDA topic model variant. Evaluation in- dicates the algorithm improves on strong base- lines. ",,,,ACL
87,2014,Exploiting Timelines to Enhance Multi-document Summarization,"Jun-Ping Ng, Yan Chen, Min-Yen Kan, Zhoujun Li","We study the use of temporal information in the form of timelines to enhance multi- document summarization. We employ a fully automated temporal processing sys- tem to generate a timeline for each in- put document. We derive three features from these timelines, and show that their use in supervised summarization lead to a significant 4.1% improvement in ROUGE performance over a state-of-the-art base- line. In addition, we propose T IME MMR, a modification to Maximal Marginal Rel- evance that promotes temporal diversity by way of computing time span similar- ity, and show its utility in summarizing certain document sets. We also propose a filtering metric to discard noisy timelines generated by our automatic processes, to purify the timeline input for summariza- tion. By selectively using timelines guided by filtering, overall summarization perfor- mance is increased by a significant 5.9%. ",,,,ACL
88,2014,A chance-corrected measure of inter-annotator agreement for syntax,Arne Skjærholt,"Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used. With this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncor- rected parser evaluation metrics such as bracket F 1 (for phrase structure) and ac- curacy scores (for dependencies). In this work we present a chance-corrected metric based on Krippendorff’s α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications. To evaluate our metric we first present a number of synthetic experi- ments to better control the sources of noise and gauge the metric’s responses, before finally contrasting the behaviour of our chance-corrected metric with that of un- corrected parser evaluation metrics on real corpora. 1 ",,,,ACL
89,2014,Two Is Bigger (and Better) Than One: the Wikipedia Bitaxonomy Project,"Tiziano Flati, Daniele Vannella, Tommaso Pasini, Roberto Navigli","We present WiBi, an approach to the automatic creation of a bitaxonomy for Wikipedia, that is, an integrated taxon- omy of Wikipage pages and categories. We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show higher quality and cov- erage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet and WikiTaxonomy. WiBi is available at http://wibitaxonomy.org. ",,,,ACL
90,2014,Information Extraction over Structured Data: Question Answering with Freebase,"Xuchen Yao, Benjamin Van Durme","Answering natural language questions us- ing the Freebase knowledge base has re- cently been explored as a platform for ad- vancing the state of the art in open do- main semantic parsing. Those efforts map questions to sophisticated meaning repre- sentations that are then attempted to be matched against viable answer candidates in the knowledge base. Here we show that relatively modest information extrac- tion techniques, when paired with a web- scale corpus, can outperform these sophis- ticated approaches by roughly 34% rela- tive gain. ",,,,ACL
91,2014,Knowledge-Based Question Answering as Machine Translation,"Junwei Bao, Nan Duan, Ming Zhou, Tiejun Zhao","A typical knowledge-based question an- swering (KB-QA) system faces two chal- lenges: one is to transform natural lan- guage questions into their meaning repre- sentations (MRs); the other is to retrieve answers from knowledge bases (KBs) us- ing generated MRs. Unlike previous meth- ods which treat them in a cascaded man- ner, we present a translation-based ap- proach to solve these two tasks in one u- nified framework. We translate questions to answers based on CYK parsing. An- swers as translations of the span covered by each CYK cell are obtained by a ques- tion translation method, which first gener- ates formal triple queries as MRs for the span based on question patterns and re- lation expressions, and then retrieves an- swers from a given KB based on triple queries generated. A linear model is de- fined over derivations, and minimum er- ror rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system us- ing a state-of-the-art semantic parser, our method achieves better results. ",,,,ACL
92,2014,Discourse Complements Lexical Semantics for Non-factoid Answer Reranking,"Peter Jansen, Mihai Surdeanu, Peter Clark","We propose a robust answer reranking model for non-factoid questions that inte- grates lexical semantics with discourse in- formation, driven by two representations of discourse: a shallow representation cen- tered around discourse markers, and a deep one based on Rhetorical Structure Theory. We evaluate the proposed model on two corpora from different genres and domains: one from Yahoo! Answers and one from the biology domain, and two types of non-factoid questions: manner and reason. We experimentally demon- strate that the discourse structure of non- factoid answers provides information that is complementary to lexical semantic sim- ilarity between question and answer, im- proving performance up to 24% (relative) over a state-of-the-art model that exploits lexical semantic similarity alone. We fur- ther demonstrate excellent domain transfer of discourse information, suggesting these discourse features have general utility to non-factoid question answering. ",,,,ACL
93,2014,"Toward Future Scenario Generation: Extracting Event Causality Exploiting Semantic Relation, Context, and Association Features","Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer, Motoki Sano","We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture → exacerbate desertification from the web using se- mantic relation (between nouns), context, and association features. Experiments show that our method outperforms base- lines that are based on state-of-the-art methods. We also propose methods of generating future scenarios like conduct slash-and-burn agriculture → exacerbate desertification → increase Asian dust (from China) → asthma gets worse. Experi- ments show that we can generate 50,000 scenarios with 68% precision. We also generated a scenario deforestation con- tinues → global warming worsens → sea temperatures rise → vibrio parahaemolyti- cus fouls (water), which is written in no document in our input web corpus crawled in 2007. But the vibrio risk due to global warming was observed in Baker-Austin et al. (2013). Thus, we “predicted” the future event sequence in a sense. ",,,,ACL
94,2014,Cross-narrative Temporal Ordering of Medical Events,"Preethi Raghavan, Eric Fosler-Lussier, Noémie Elhadad, Albert M. Lai","Cross-narrative temporal ordering of med- ical events is essential to the task of gen- erating a comprehensive timeline over a patient’s history. We address the prob- lem of aligning multiple medical event se- quences, corresponding to different clin- ical narratives, comparing the following approaches: (1) A novel weighted finite state transducer representation of medi- cal event sequences that enables compo- sition and search for decoding, and (2) Dynamic programming with iterative pair- wise alignment of multiple sequences us- ing global and local alignment algorithms. The cross-narrative coreference and tem- poral relation weights used in both these approaches are learned from a corpus of clinical narratives. We present results us- ing both approaches and observe that the finite state transducer approach performs performs significantly better than the dy- namic programming one by 6.8% for the problem of multiple-sequence alignment. ",,,,ACL
95,2014,Language-Aware Truth Assessment of Fact Candidates,"Ndapandula Nakashole, Tom M. Mitchell","This paper introduces FactChecker, language-aware approach to truth-finding. FactChecker differs from prior approaches in that it does not rely on iterative peer voting, instead it leverages language to infer believability of fact candidates. In particular, FactChecker makes use of lin- guistic features to detect if a given source objectively states facts or is speculative and opinionated. To ensure that fact candidates mentioned in similar sources have similar believability, FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate. Our experiments on various datasets show that FactChecker yields higher accuracy than existing approaches. ",,,,ACL
96,2014,That’s sick dude!: Automatic identification of word sense change across different timescales,"Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Biemann","In this paper, we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books. We construct distribu- tional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the differ- ent time points. Subsequently, we com- pare these sense clusters of two different time points to find if (i) there is birth of a new sense or (ii) if an older sense has got split into more than one sense or (iii) if a newer sense has been formed from the joining of older senses or (iv) if a partic- ular sense has died. We conduct a thor- ough evaluation of the proposed method- ology both manually as well as through comparison with WordNet. Manual eval- uation indicates that the algorithm could correctly identify 60.4% birth cases from a set of 48 randomly picked samples and 57% split/join cases from a set of 21 ran- domly picked samples. Remarkably, in 44% cases the birth of a novel sense is attested by WordNet, while in 46% cases and 43% cases split and join are respec- tively confirmed by WordNet. Our ap- proach can be applied for lexicography, as well as for applications like word sense disambiguation or semantic search. ",,,,ACL
97,2014,A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes,"Daisuke Kawahara, Daniel W. Peterson, Martha Palmer","We present an unsupervised method for in- ducing verb classes from verb uses in giga- word corpora. Our method consists of two clustering steps: verb-specific seman- tic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only generate verb classes based on a massive amount of verb uses in a scalable manner, but also deal with verb polysemy, which is bypassed by most of the previous studies on verb clustering. In our exper- iments, we acquire semantic frames and verb classes from two giga-word corpora, the larger comprising 20 billion words. The effectiveness of our approach is veri- fied through quantitative evaluations based on polysemy-aware gold-standard data. ",,,,ACL
98,2014,Structured Learning for Taxonomy Induction with Belief Propagation,"Mohit Bansal, David Burkett, Gerard de Melo, Dan Klein","We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation. Our model incorporates heterogeneous re- lational evidence about both hypernymy and siblinghood, captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia ab- stracts. For efficient inference over tax- onomy structures, we use loopy belief propagation along with a directed span- ning tree algorithm for the core hyper- nymy factor. To train the system, we ex- tract sub-structures of WordNet and dis- criminatively learn to reproduce them, us- ing adaptive subgradient stochastic opti- mization. On the task of reproducing sub-hierarchies of WordNet, our approach achieves a 51% error reduction over a chance baseline, including a 15% error re- duction due to the non-hypernym-factored sibling features. On a comparison setup, we find up to 29% relative error reduction over previous work on ancestor F1. ",,,,ACL
99,2014,A Provably Correct Learning Algorithm for Latent-Variable PCFGs,"Shay B. Cohen, Michael Collins","We introduce a provably correct learning algorithm for latent-variable PCFGs. The algorithm relies on two steps: first, the use of a matrix-decomposition algorithm ap- plied to a co-occurrence matrix estimated from the parse trees in a training sample; second, the use of EM applied to a convex objective derived from the training sam- ples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and ef- fective in practice. ",,,,ACL
100,2014,Spectral Unsupervised Parsing with Additive Tree Metrics,"Ankur P. Parikh, Shay B. Cohen, Eric P. Xing","We propose a spectral approach for un- supervised constituent parsing that comes with theoretical guarantees on latent struc- ture recovery. Our approach is grammar- less – we directly learn the bracketing structure of a given sentence without us- ing a grammar model. The main algorithm is based on lifting the concept of additive tree metrics for structure learning of la- tent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples. Although finding the “minimal” latent tree is NP-hard in general, for the case of pro- jective trees we find that it can be found using bilexical parsing algorithms. Empir- ically, our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization. ",,,,ACL
101,2014,Weak semantic context helps phonetic learning in a model of infant language acquisition,"Stella Frank, Naomi H. Feldman, Sharon Goldwater","Learning phonetic categories is one of the first steps to learning a language, yet is hard to do using only distributional phonetic in- formation. Semantics could potentially be useful, since words with different mean- ings have distinct phonetics, but it is un- clear how many word meanings are known to infants learning phonetic categories. We show that attending to a weaker source of semantics, in the form of a distribution over topics in the current context, can lead to improvements in phonetic category learn- ing. In our model, an extension of a pre- vious model of joint word-form and pho- netic category inference, the probability of word-forms is topic-dependent, enabling the model to find significantly better pho- netic vowel categories and word-forms than a model with no semantic knowledge. ",,,,ACL
102,2014,Bootstrapping into Filler-Gap: An Acquisition Story,"Marten van Schijndel, Micha Elsner","Analyses of filler-gap dependencies usu- ally involve complex syntactic rules or heuristics; however recent results suggest that filler-gap comprehension begins ear- lier than seemingly simpler constructions such as ditransitives or passives. Therefore, this work models filler-gap acquisition as a byproduct of learning word orderings (e.g. SVO vs OSV), which must be done at a very young age anyway in order to extract meaning from language. Specifically, this model, trained on part-of-speech tags, rep- resents the preferred locations of semantic roles relative to a verb as Gaussian mix- tures over real numbers. This approach learns role assignment in filler-gap constructions in a manner con- sistent with current developmental findings and is extremely robust to initialization variance. Additionally, this model is shown to be able to account for a characteristic er- ror made by learners during this period (A and B gorped interpreted as A gorped B ). ",,,,ACL
103,2014,Nonparametric Learning of Phonological Constraints in Optimality Theory,"Gabriel Doyle, Klinton Bicknell, Roger Levy","We present a method to jointly learn fea- tures and weights directly from distri- butional data in a log-linear framework. Specifically, we propose a non-parametric Bayesian model for learning phonologi- cal markedness constraints directly from the distribution of input-output mappings in an Optimality Theory (OT) setting. The model uses an Indian Buffet Process prior to learn the feature values used in the log- linear method, and is the first algorithm for learning phonological constraints with- out presupposing constraint structure. The model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis, with a violation struc- ture corresponding to the standard con- straints. These results suggest an alterna- tive data-driven source for constraints in- stead of a fully innate constraint set. ",,,,ACL
104,2014,Active Learning with Efficient Feature Weighting Methods for Improving Data Quality and Classification Accuracy,"Justin Martineau, Lu Chen, Doreen Cheng, Amit Sheth","Many machine learning datasets are noisy with a substantial number of mislabeled instances. This noise yields sub-optimal classification performance. In this paper we study a large, low quality annotated dataset, created quickly and cheaply us- ing Amazon Mechanical Turk to crowd- source annotations. We describe compu- tationally cheap feature weighting tech- niques and a novel non-linear distribution spreading algorithm that can be used to it- eratively and interactively correcting mis- labeled instances to significantly improve annotation quality at low cost. Eight dif- ferent emotion extraction experiments on Twitter data demonstrate that our approach is just as effective as more computation- ally expensive techniques. Our techniques save a considerable amount of time. ",,,,ACL
105,2014,Political Ideology Detection Using Recursive Neural Networks,"Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, Philip Resnik","An individual’s words often reveal their po- litical ideology. Existing automated tech- niques to identify ideology from text focus on bags of words or wordlists, ignoring syn- tax. Taking inspiration from recent work in sentiment analysis that successfully models the compositional aspect of language, we apply a recursive neural network (RNN) framework to the task of identifying the po- litical position evinced by a sentence. To show the importance of modeling subsen- tential elements, we crowdsource political annotations at a phrase and sentence level. Our model outperforms existing models on our newly annotated dataset and an existing dataset. ",,,,ACL
106,2014,A Unified Model for Soft Linguistic Reordering Constraints in Statistical Machine Translation,"Junhui Li, Yuval Marton, Philip Resnik, Hal Daumé III","This paper explores a simple and effec- tive unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation sys- tem: 1) a syntactic reordering model that explores reorderings for context free grammar rules; and 2) a semantic re- ordering model that focuses on the re- ordering of predicate-argument structures. We develop novel features based on both models and use them as soft constraints to guide the translation process. Ex- periments on Chinese-English translation show that the reordering approach can sig- nificantly improve a state-of-the-art hier- archical phrase-based translation system. However, the gain achieved by the seman- tic reordering model is limited in the pres- ence of the syntactic reordering model, and we therefore provide a detailed analy- sis of the behavior differences between the two. ",,,,ACL
107,2014,Are Two Heads Better than One? Crowdsourced Translation via a Two-Step Collaboration of Non-Professional Translators and Editors,"Rui Yan, Mingkun Gao, Ellie Pavlick, Chris Callison-Burch","Crowdsourcing is a viable mechanism for creating training data for machine trans- lation. It provides a low cost, fast turn- around way of processing large volumes of data. However, when compared to pro- fessional translation, naive collection of translations from non-professionals yields low-quality results. Careful quality con- trol is necessary for crowdsourcing to work well. In this paper, we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals. We develop graph- based ranking models that automatically select the best output from multiple redun- dant versions of translations and edits, and improves translation quality closer to pro- fessionals. ",,,,ACL
108,2014,A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser Ney Smoothing,"Rene Pickhardt, Thomas Gottron, Martin Körner, Paul Georg Wagner","We introduce a novel approach for build- ing language models based on a system- atic, recursive exploration of skip n-gram models which are interpolated using modi- fied Kneser-Ney smoothing. Our approach generalizes language models as it contains the classical interpolation with lower or- der models as a special case. In this pa- per we motivate, formalize and present our approach. In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1% and 12.7% in comparison to traditional language mod- els using modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consis- tent improvements. Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data. Using a very small train- ing data set of only 736 KB text we yield improvements of even 25.7% reduction of perplexity. ",,,,ACL
109,2014,A Semiparametric Gaussian Copula Regression Model for Predicting Financial Risks from Earnings Calls,"William Yang Wang, Zhenhao Hua","Earnings call summarizes the financial performance of a company, and it is an important indicator of the future financial risks of the company. We quantitatively study how earnings calls are correlated with the financial risks, with a special fo- cus on the financial crisis of 2009. In par- ticular, we perform a text regression task: given the transcript of an earnings call, we predict the volatility of stock prices from the week after the call is made. We pro- pose the use of copula: a powerful statis- tical framework that separately models the uniform marginals and their complex mul- tivariate stochastic dependencies, while not requiring any prior assumptions on the distributions of the covariate and the de- pendent variable. By performing probabil- ity integral transform, our approach moves beyond the standard count-based bag-of- words models in NLP, and improves pre- vious work on text regression by incor- porating the correlation among local fea- tures in the form of semiparametric Gaus- sian copula. In experiments, we show that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings. ",,,,ACL
110,2014,Polylingual Tree-Based Topic Models for Translation Domain Adaptation,"Yuening Hu, Ke Zhai, Vladimir Eidelman, Jordan Boyd-Graber","Topic models, an unsupervised technique for inferring translation domains improve machine translation quality. However, pre- vious work uses only the source language and completely ignores the target language, which can disambiguate domains. We pro- pose new polylingual tree-based topic mod- els to extract domain knowledge that con- siders both source and target languages and derive three different inference schemes. We evaluate our model on a Chinese to En- glish translation task and obtain up to 1.2 BLEU improvement over strong baselines. ",,,,ACL
111,2014,Low-Resource Semantic Role Labeling,"Matthew R. Gormley, Margaret Mitchell, Benjamin Van Durme, Mark Dredze","We explore the extent to which high- resource manual annotations such as tree- banks are necessary for the task of se- mantic role labeling (SRL). We examine how performance changes without syntac- tic supervision, comparing both joint and pipelined methods to induce latent syn- tax. This work highlights a new applica- tion of unsupervised grammar induction and demonstrates several approaches to SRL in the absence of supervised syntax. Our best models obtain competitive results in the high-resource setting and state-of- the-art results in the low resource setting, reaching 72.48% F1 averaged across lan- guages. We release our code for this work along with a larger toolkit for specifying arbitrary graphical structure. 1 ",,,,ACL
112,2014,Joint Syntactic and Semantic Parsing with Combinatory Categorial Grammar,"Jayant Krishnamurthy, Tom M. Mitchell","We present an approach to training a joint syntactic and semantic parser that com- bines syntactic training information from CCGbank with semantic training informa- tion from a knowledge base via distant su- pervision. The trained parser produces a full syntactic parse of any sentence, while simultaneously producing logical forms for portions of the sentence that have a se- mantic representation within the parser’s predicate vocabulary. We demonstrate our approach by training a parser whose se- mantic representation contains 130 pred- icates from the NELL ontology. A seman- tic evaluation demonstrates that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach. A syn- tactic evaluation on CCGbank demon- strates that the parser’s dependency F- score is within 2.5% of state-of-the-art. ",,,,ACL
113,2014,Learning Semantic Hierarchies via Word Embeddings,"Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che","Semantic hierarchy construction aims to build structures of concepts linked by hypernym–hyponym (“is-a”) relations. A major challenge for this task is the automatic discovery of such relations. This paper proposes a novel and effec- tive method for the construction of se- mantic hierarchies based on word em- beddings, which can be used to mea- sure the semantic relationship between words. We identify whether a candidate word pair has hypernym–hyponym rela- tion by using the word-embedding-based semantic projections between words and their hypernyms. Our result, an F-score of 73.74%, outperforms the state-of-the- art methods on a manually labeled test dataset. Moreover, combining our method with a previous manually-built hierarchy extension method can further improve F- score to 80.29%. ",,,,ACL
114,2014,Probabilistic Soft Logic for Semantic Textual Similarity,"Islam Beltagy, Katrin Erk, Raymond Mooney","Probabilistic Soft Logic (PSL) is a re- cently developed framework for proba- bilistic logic. We use PSL to combine logical and distributional representations of natural-language meaning, where distri- butional information is represented in the form of weighted inference rules. We ap- ply this framework to the task of Seman- tic Textual Similarity (STS) (i.e. judg- ing the semantic similarity of natural- language sentences), and show that PSL gives improved results compared to a pre- vious approach based on Markov Logic Networks (MLNs) and a purely distribu- tional approach. ",,,,ACL
115,2014,Abstractive Summarization of Spoken and Written Conversations Based on Phrasal Queries,"Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng","We propose a novel abstractive query- based summarization system for conversa- tions, where queries are defined as phrases reflecting a user information needs. We rank and extract the utterances in a con- versation based on the overall content and the phrasal query information. We clus- ter the selected sentences based on their lexical similarity and aggregate the sen- tences in each cluster by means of a word graph model. We propose a ranking strat- egy to select the best path in the con- structed graph as a query-based abstract sentence for each cluster. A resulting sum- mary consists of abstractive sentences rep- resenting the phrasal query information and the overall content of the conversa- tion. Automatic and manual evaluation results over meeting, chat and email con- versations show that our approach signifi- cantly outperforms baselines and previous extractive models. ",,,,ACL
116,2014,Comparing Multi-label Classification with Reinforcement Learning for Summarisation of Time-series Data,"Dimitra Gkatzia, Helen Hastie, Oliver Lemon","We present a novel approach for automatic report generation from time-series data, in the context of student feedback genera- tion. Our proposed methodology treats content selection as a multi-label (ML) classification problem, which takes as in- put time-series data and outputs a set of templates, while capturing the dependen- cies between selected templates. We show that this method generates output closer to the feedback that lecturers actually gener- ated, achieving 3.5% higher accuracy and 15% higher F-score than multiple simple classifiers that keep a history of selected templates. Furthermore, we compare a ML classifier with a Reinforcement Learn- ing (RL) approach in simulation and using ratings from real student users. We show that the different methods have different benefits, with ML being more accurate for predicting what was seen in the training data, whereas RL is more exploratory and slightly preferred by the students. ",,,,ACL
117,2014,Approximation Strategies for Multi-Structure Sentence Compression,Kapil Thadani,"Sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objec- tives but this typically requires expensive integer programming. We explore instead the use of Lagrangian relaxation to decou- ple the two subproblems and solve them separately. While dynamic programming is viable for bigram-based sentence com- pression, finding optimal compressed trees within graphs is NP-hard. We recover ap- proximate solutions to this problem us- ing LP relaxation and maximum spanning tree algorithms, yielding techniques that can be combined with the efficient bigram- based inference approach using Lagrange multipliers. Experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a sig- nificant improvement in runtime. ",,,,ACL
118,2014,Opinion Mining on YouTube,"Aliaksei Severyn, Alessandro Moschitti, Olga Uryupina, Barbara Plank",This paper defines a systematic approach to Opinion Mining (OM) on YouTube comments by (i) modeling classifiers for predicting the opinion polarity and the type of comment and (ii) proposing ro- bust shallow syntactic structures for im- proving model adaptability. We rely on the tree kernel technology to automatically ex- tract and learn features with better gener- alization power than bag-of-words. An ex- tensive empirical evaluation on our manu- ally annotated YouTube comments corpus shows a high classification accuracy and highlights the benefits of structural mod- els in a cross-domain setting. ,,,,ACL
119,2014,Automatic Keyphrase Extraction: A Survey of the State of the Art,"Kazi Saidul Hasan, Vincent Ng","While automatic keyphrase extraction has been examined extensively, state-of-the- art performance on this task is still much lower than that on many core natural lan- guage processing tasks. We present a sur- vey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead. ",,,,ACL
120,2014,Pattern Dictionary of English Prepositions,Ken Litkowski,"We present a new lexical resource for the study of preposition behavior, the Pattern Dictionary of English Prepositions (PDEP). This dictionary, which follows principles laid out in Hanks’ theory of norms and exploit a- tions, is linked to 81,509 sentences for 304 prepositions, which have been made available under The Preposition Project (TPP). Nota- bly, 47,285 sentences, initially untagged, provide a representative sample of preposi- tion use, unlike the tagged sentences used in previous studies. Each sentence has been parsed with a dependency parser and our sys- tem has near-instantaneous access to features developed with this parser to explore and an- notate properties of individual senses. The features make extensive use of WordNet. We have extended feature exploration to include lookup of FrameNet lexical units and VerbNet classes for use in characterizing preposition behavior. We have designed our system to allow public access to any of the data available in the system. ",,,,ACL
121,2014,Looking at Unbalanced Specialized Comparable Corpora for Bilingual Lexicon Extraction,"Emmanuel Morin, Amir Hazem","The main work in bilingual lexicon ex- traction from comparable corpora is based on the implicit hypothesis that corpora are balanced. However, the historical context- based projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus. Within this context, we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments. Moreover, we have introduced a regression model that boosts the observations of word co- occurrences used in the context-based pro- jection method. Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons. ",,,,ACL
122,2014,Validating and Extending Semantic Knowledge Bases using Video Games with a Purpose,"Daniele Vannella, David Jurgens, Daniele Scarfini, Domenico Toscani","Large-scale knowledge bases are impor- tant assets in NLP. Frequently, such re- sources are constructed through automatic mergers of complementary resources, such as WordNet and Wikipedia. However, manually validating these resources is pro- hibitively expensive, even when using methods such as crowdsourcing. We pro- pose a cost-effective method of validat- ing and extending knowledge bases using video games with a purpose. Two video games were created to validate concept- concept and concept-image relations. In experiments comparing with crowdsourc- ing, we show that video game-based vali- dation consistently leads to higher-quality annotations, even when players are not compensated. ",,,,ACL
123,2014,Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring,"Suma Bhat, Huichao Xue, Su-Youn Yoon","Designing measures that capture various aspects of language ability is a central task in the design of systems for auto- matic scoring of spontaneous speech. In this study, we address a key aspect of lan- guage proficiency assessment – syntactic complexity. We propose a novel measure of syntactic complexity for spontaneous speech that shows optimum empirical per- formance on real world data in multiple ways. First, it is both robust and reliable, producing automatic scores that agree well with human rating compared to the state- of-the-art. Second, the measure makes sense theoretically, both from algorithmic and native language acquisition points of view. ",,,,ACL
124,2014,Can You Repeat That? Using Word Repetition to Improve Spoken Term Detection,"Jonathan Wintrode, Sanjeev Khudanpur","We aim to improve spoken term detec- tion performance by incorporating con- textual information beyond traditional N- gram language models. Instead of taking a broad view of topic context in spoken doc- uments, variability of word co-occurrence statistics across corpora leads us to fo- cus instead the on phenomenon of word repetition within single documents. We show that given the detection of one in- stance of a term we are more likely to find additional instances of that term in the same document. We leverage this bursti- ness of keywords by taking the most con- fident keyword hypothesis in each docu- ment and interpolating with lower scor- ing hits. We then develop a principled approach to select interpolation weights using only the ASR training data. Us- ing this re-weighting approach we demon- strate consistent improvement in the term detection performance across all five lan- guages in the BABEL program. ",,,,ACL
125,2014,Character-Level Chinese Dependency Parsing,"Meishan Zhang, Yue Zhang, Wanxiang Che, Ting Liu","Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words, enabling character- level analysis of Chinese syntactic struc- tures. In this paper, we investigate the problem of character-level Chinese depen- dency parsing, building dependency trees over characters. Character-level infor- mation can benefit downstream applica- tions by offering flexible granularities for word segmentation while improving word- level dependency parsing accuracies. We present novel adaptations of two ma- jor shift-reduce dependency parsing algo- rithms to character-level parsing. Exper- imental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. ",,,,ACL
126,2014,Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization,"Xuezhe Ma, Fei Xia","We present a novel approach for induc- ing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resource- rich language. We train probabilistic pars- ing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy reg- ularization. Our method can be used as a purely monolingual dependency parser, requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages. We perform experiments on three Data sets — Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages. We obtain state- of-the art performance of all the three data sets when compared with previously studied unsupervised and projected pars- ing systems. ",,,,ACL
127,2014,Unsupervised Morphology-Based Vocabulary Expansion,"Mohammad Sadegh Rasooli, Thomas Lippincott, Nizar Habash, Owen Rambow","We present a novel way of generating un- seen words, which is useful for certain ap- plications such as automatic speech recog- nition or optical character recognition in low-resource languages. We test our vo- cabulary generator on seven low-resource languages by measuring the decrease in out-of-vocabulary word rate on a held-out test set. The languages we study have very different morphological properties; we show how our results differ depend- ing on the morphological complexity of the language. In our best result (on As- samese), our approach can predict 29% of the token-based out-of-vocabulary with a small amount of unlabeled training data. ",,,,ACL
128,2014,Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints,"Xiaodong Zeng, Lidia S. Chao, Derek F. Wong, Isabel Trancoso","This study investigates on building a better Chinese word segmentation mod- el for statistical machine translation. It aims at leveraging word boundary infor- mation, automatically learned by bilin- gual character-based alignments, to induce a preferable segmentation model. We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRF- s model, trained by the treebank data (la- beled), on the bilingual data (unlabeled). The induced word boundary information is encoded as a graph propagation con- straint. The constrained model induction is accomplished by using posterior reg- ularization algorithm. The experiments on a Chinese-to-English machine transla- tion task reveal that the proposed model can bring positive segmentation effects to translation quality. ",,,,ACL
129,2014,Fast and Robust Neural Network Joint Models for Statistical Machine Translation,"Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar","Recent work has shown success in us- ing neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexi- calized and can be integrated into any MT decoder. We also present several varia- tions of the NNJM which provide signif- icant additive improvements. Although the model is quite simple, it yields strong empirical results. On the NIST OpenMT12 Arabic-English condi- tion, the NNJM features produce a gain of +3.0 BLEU on top of a powerful, feature- rich baseline which already includes a target-only NNLM. The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chi- ang’s (2007) original Hiero implementa- tion. Additionally, we describe two novel tech- niques for overcoming the historically high cost of using NNLM-style models in MT decoding. These techniques speed up NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM. This work was supported by DARPA/I2O Contract No. HR0011-12-C-0014 under the BOLT program (Approved for Public Release, Distribution Unlimited). The views, opin- ions, and/or findings contained in this article are those of the author and should not be interpreted as representing the of- ficial views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Depart- ment of Defense. ",,,,ACL
130,2014,Low-Rank Tensors for Scoring Dependency Structures,"Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay","Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich, high- dimensional feature representations. A small subset of such features is often se- lected manually. This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features. In this paper, we use tensors to map high-dimensional fea- ture vectors into low dimensional repre- sentations. We explicitly maintain the pa- rameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage mod- ularity in the tensor for easy training with online algorithms. Our parser consistently outperforms the Turbo and MST parsers across 14 different languages. We also ob- tain the best published UAS results on 5 languages. 1 ",,,,ACL
131,2014,CoSimRank: A Flexible & Efficient Graph-Theoretic Similarity Measure,"Sascha Rothe, Hinrich Schütze","We present CoSimRank, a graph-theoretic similarity measure that is efficient because it can compute a single node similarity without having to compute the similarities of the entire graph. We present equivalent formalizations that show CoSimRank’s close relationship to Personalized Page- Rank and SimRank and also show how we can take advantage of fast matrix mul- tiplication algorithms to compute CoSim- Rank. Another advantage of CoSimRank is that it can be flexibly extended from ba- sic node-node similarity to several other graph-theoretic similarity measures. In an experimental evaluation on the tasks of synonym extraction and bilingual lexicon extraction, CoSimRank is faster or more accurate than previous approaches. ",,,,ACL
132,2014,Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world,"Angeliki Lazaridou, Elia Bruni, Marco Baroni","Following up on recent work on estab- lishing a mapping between vector-based semantic embeddings of words and the visual representations of the correspond- ing objects from natural images, we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning, in which an image of a previously unseen object is mapped to a linguistic representation denoting its word. We then introduce fast mapping, a challenging and more cognitively plausi- ble variant of the zero-shot task, in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts. By combining prior linguistic and visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence avail- able, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simula- tions of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts. ",,,,ACL
133,2014,Semantic Parsing via Paraphrasing,"Jonathan Berant, Percy Liang","A central challenge in semantic parsing is handling the myriad ways in which knowl- edge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this pa- per, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically gener- ate a set of candidate logical forms with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best para- phrases the input, and output the corre- sponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system P ARA S EMPRE improves state- of-the-art accuracies on two recently re- leased question-answering datasets. ",,,,ACL
134,2014,A Discriminative Graph-Based Parser for the Abstract Meaning Representation,"Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer","Meaning Representation (AMR) is a semantic formalism for which a grow- ing set of annotated examples is avail- able. We introduce the first approach to parse sentences into this representa- tion, providing a strong baseline for fu- ture improvement. The method is based on a novel algorithm for finding a maxi- mum spanning, connected subgraph, em- bedded within a Lagrangian relaxation of an optimization problem that imposes lin- guistically inspired constraints. Our ap- proach is described in the general frame- work of structured prediction, allowing fu- ture incorporation of additional features and constraints, and may extend to other formalisms as well. Our open-source sys- tem, JAMR, is available at: http://github.com/jflanigan/jamr ",,,,ACL
135,2014,Context-dependent Semantic Parsing for Time Expressions,"Kenton Lee, Yoav Artzi, Jesse Dodge, Luke Zettlemoyer","We present an approach for learning context-dependent semantic parsers to identify and interpret time expressions. We use a Combinatory Categorial Gram- mar to construct compositional meaning representations, while considering contex- tual cues, such as the document creation time and the tense of the governing verb, to compute the final time values. Exper- iments on benchmark datasets show that our approach outperforms previous state- of-the-art systems, with error reductions of 13% to 21% in end-to-end performance. ",,,,ACL
136,2014,Semantic Frame Identification with Distributed Word Representations,"Karl Moritz Hermann, Dipanjan Das, Jason Weston, Kuzman Ganchev","We present a novel technique for semantic frame identification using distributed rep- resentations of predicates and their syntac- tic context; this technique leverages auto- matic syntactic parses and a generic set of word embeddings. Given labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. The latter is used for se- mantic frame identification; with a stan- dard argument identification method in- spired by prior work, we achieve state-of- the-art results on FrameNet-style frame- semantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. ",,,,ACL
137,2014,A Sense-Based Translation Model for Statistical Machine Translation,"Deyi Xiong, Min Zhang","The sense in which a word is used deter- mines the translation of the word. In this paper, we propose a sense-based transla- tion model to integrate word senses into statistical machine translation. We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language. The pro- posed sense-based translation model en- ables the decoder to select appropriate translations for source words according to the inferred senses for these words us- ing maximum entropy classifiers. Our method is significantly different from pre- vious word sense disambiguation reformu- lated for machine translation in that the lat- ter neglects word senses in nature. We test the effectiveness of the proposed sense- based translation model on a large-scale Chinese-to-English translation task. Re- sults show that the proposed model sub- stantially outperforms not only the base- line but also the previous reformulated word sense disambiguation. ",,,,ACL
138,2014,Recurrent Neural Networks for Word Alignment Model,"Akihiro Tamura, Taro Watanabe, Eiichiro Sumita","This study proposes a word alignment model based on a recurrent neural net- work (RNN), in which an unlimited alignment history is represented by re- currently connected hidden layers. We perform unsupervised learning using noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which utilizes artificially generated negative samples. Our alignment model is directional, similar to the generative IBM models (Brown et al., 1993). To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that en- sures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable transla- tion performance to those baselines for Japanese-English and Chinese-English translation tasks. ",,,,ACL
139,2014,A Constrained Viterbi Relaxation for Bidirectional Word Alignment,"Yin-Wen Chang, Alexander M. Rush, John DeNero, Michael Collins","Bidirectional models of word alignment are an appealing alternative to post-hoc combinations of directional word align- ers. Unfortunately, most bidirectional formulations are NP-Hard to solve, and a previous attempt to use a relaxation- based decoder yielded few exact solu- tions (6%). We present a novel relax- ation for decoding the bidirectional model of DeNero and Macherey (2011). The relaxation can be solved with a mod- ified version of the Viterbi algorithm. To find optimal solutions on difficult instances, we alternate between incre- mentally adding constraints and applying optimality-preserving coarse-to-fine prun- ing. The algorithm finds provably ex- act solutions on 86% of sentence pairs and shows improvements over directional models. ",,,,ACL
140,2014,A Recursive Recurrent Neural Network for Statistical Machine Translation,"Shujie Liu, Nan Yang, Mu Li, Ming Zhou","In this paper, we propose a novel recursive recurrent neural network (R 2 NN) to mod- el the end-to-end decoding process for s- tatistical machine translation. R 2 NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neu- ral networks, so that language model and translation model can be integrated natu- rally; (2) a tree structure can be built, as recursive neural networks, so as to gener- ate the translation candidates in a bottom up manner. A semi-supervised training ap- proach is proposed to train the parameter- s, and the phrase pair embedding is ex- plored to model translation confidence di- rectly. Experiments on a Chinese to En- glish translation task show that our pro- posed R 2 NN can outperform the state- of-the-art baseline by about 1.5 points in BLEU. ",,,,ACL
141,2014,Predicting Instructor’s Intervention in MOOC forums,"Snigdha Chaturvedi, Dan Goldwasser, Hal Daumé III","Instructor intervention in student discus- sion forums is a vital component in Massive Open Online Courses (MOOCs), where personalized interaction is limited. This paper introduces the problem of pre- dicting instructor interventions in MOOC forums. We propose several prediction models designed to capture unique aspects of MOOCs, combining course informa- tion, forum structure and posts content. Our models abstract contents of individ- ual posts of threads using latent categories, learned jointly with the binary interven- tion prediction problem. Experiments over data from two Coursera MOOCs demon- strate that incorporating the structure of threads into the learning problem leads to better predictive performance. ",,,,ACL
142,2014,A Joint Graph Model for Pinyin-to-Chinese Conversion with Typo Correction,"Zhongye Jia, Hai Zhao","It is very import for Chinese language pro- cessing with the aid of an efficient input method engine (IME), of which pinyin- to-Chinese (PTC) conversion is the core part. Meanwhile, though typos are in- evitable during user pinyin inputting, ex- isting IMEs paid little attention to such big inconvenience. In this paper, motivated by a key equivalence of two decoding algo- rithms, we propose a joint graph model to globally optimize PTC and typo correction for IME. The evaluation results show that the proposed method outperforms both ex- isting academic and commercial IMEs. ",,,,ACL
143,2014,Smart Selection,"Patrick Pantel, Michael Gamon, Ariel Fuxman","Natural touch interfaces, common now in devices such as tablets and smartphones, make it cumbersome for users to select text. There is a need for a new text selec- tion paradigm that goes beyond the high acuity selection-by-mouse that we have re- lied on for decades. In this paper, we in- troduce such a paradigm, called Smart Se- lection, which aims to recover a user’s in- tended text selection from her touch input. We model the problem using an ensemble learning approach, which leverages mul- tiple linguistic analysis techniques com- bined with information from a knowledge base and a Web graph. We collect a dataset of true intended user selections and simu- lated user touches via a large-scale crowd- sourcing task, which we release to the academic community. We show that our model effectively addresses the smart se- lection task and significantly outperforms various baselines and standalone linguistic analysis techniques. ",,,,ACL
144,2014,Modeling Prompt Adherence in Student Essays,"Isaac Persing, Vincent Ng","Recently, researchers have begun explor- ing methods of scoring student essays with respect to particular dimensions of qual- ity such as coherence, technical errors, and prompt adherence. The work on modeling prompt adherence, however, has been focused mainly on whether individ- ual sentences adhere to the prompt. We present a new annotated corpus of essay- level prompt adherence scores and pro- pose a feature-rich approach to scoring es- says along the prompt adherence dimen- sion. Our approach significantly outper- forms a knowledge-lean baseline prompt adherence scoring system yielding im- provements of up to 16.6%. ",,,,ACL
145,2014,ConnotationWordNet: Learning Connotation over the Word+Sense Network,"Jun Seok Kang, Song Feng, Leman Akoglu, Yejin Choi","We introduce ConnotationWordNet, a con- notation lexicon over the network of words in conjunction with senses. We formulate the lexicon induction problem as collec- tive inference over pairwise-Markov Ran- dom Fields, and present a loopy belief propagation algorithm for inference. The key aspect of our method is that it is the first unified approach that assigns the polarity of both word- and sense-level connotations, exploiting the innate bipar- tite graph structure encoded in WordNet. We present comprehensive evaluation to demonstrate the quality and utility of the resulting lexicon in comparison to existing connotation and sentiment lexicons. ",,,,ACL
146,2014,Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification,"Duyu Tang, Furu Wei, Nan Yang, Ming Zhou","We present a method that learns word em- bedding for Twitter sentiment classifica- tion in this paper. Most existing algorithm- s for learning continuous word represen- tations typically only model the syntactic context of words but ignore the sentimen- t of text. This is problematic for senti- ment analysis as they usually map word- s with similar syntactic context but oppo- site sentiment polarity, such as good and bad, to neighboring word vectors. We address this issue by learning sentiment- specific word embedding (SSWE), which encodes sentiment information in the con- tinuous representation of words. Specif- ically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sen- tences or tweets) in their loss function- s. To obtain large scale training corpora, we learn the sentiment-specific word em- bedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SS- WE to a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the perfor- mance is further improved by concatenat- ing SSWE with existing feature set. ",,,,ACL
147,2014,Towards a General Rule for Identifying Deceptive Opinion Spam,"Jiwei Li, Myle Ott, Claire Cardie, Eduard Hovy","Consumers’ purchase decisions are in- creasingly influenced by user-generated online reviews. Accordingly, there has been growing concern about the poten- tial for posting deceptive opinion spam— fictitious reviews that have been deliber- ately written to sound authentic, to de- ceive the reader. In this paper, we ex- plore generalized approaches for identify- ing online deceptive opinion spam based on a new gold standard dataset, which is comprised of data from three different do- mains (i.e. Hotel, Restaurant, Doctor), each of which contains three types of re- views, i.e. customer generated truthful re- views, Turker generated deceptive reviews and employee (domain-expert) generated deceptive reviews. Our approach tries to capture the general difference of language usage between deceptive and truthful re- views, which we hope will help customers when making purchase decisions and re- view portal operators, such as TripAdvisor or Yelp, investigate possible fraudulent ac- tivity on their sites. 1 ",,,,ACL
1,2015,On Using Very Large Target Vocabulary for Neural Machine Translation,"Sébastien Jean, Kyunghyun Cho, Roland Memisevic, Yoshua Bengio","Neural machine translation, a recently proposed approach to machine transla- tion based purely on neural networks, has shown promising results compared to the existing approaches such as phrase- based statistical machine translation. De- spite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complex- ity as well as decoding complexity in- crease proportionally to the number of tar- get words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vo- cabulary without increasing training com- plexity. We show that decoding can be efficiently done even with the model hav- ing a very large target vocabulary by se- lecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases out- perform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Fur- thermore, when we use an ensemble of a few models with very large target vo- cabularies, we achieve performance com- parable to the state of the art (measured by BLEU) on both the English→German and English→French translation tasks of WMT’14. ",,,,ACL
2,2015,Addressing the Rare Word Problem in Neural Machine Translation,"Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals","Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are com- parable to traditional approaches. A sig- nificant weakness in conventional NMT systems is their inability to correctly trans- late very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and im- plement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sen- tence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our exper- iments on the WMT’14 English to French translation task show that this method pro- vides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT sys- tem is the first to surpass the best result achieved on a WMT’14 contest task. ",,,,ACL
3,2015,Encoding Source Language with Convolutional Neural Network for Machine Translation,"Fandong Meng, Zhengdong Lu, Mingxuan Wang, Hang Li","The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target lan- guage model with a heuristically cho- sen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more sys- tematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With dif- ferent guiding signals during decod- ing, our specifically designed convolu- tion+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of en- tire source sentence to form a unified representation. This representation, to- gether with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English trans- lation tasks show that the proposed model can achieve significant improve- ments over the previous NNJM by up to +1.08 BLEU points on average. ",,,,ACL
4,2015,Statistical Machine Translation Features with Multitask Tensor Networks,"Hendra Setiawan, Zhongqiang Huang, Jacob Devlin, Thomas Lamar","We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various non- local translation phenomena. Second, we augment the architecture of the neural net- work with tensor layers that capture im- portant higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improve- ments that are complementary. The over- all improvement is +2.7 and +1.8 BLEU points for Arabic-English and Chinese- English translation over a state-of-the-art system that already includes neural net- work features. ",,,,ACL
5,2015,Describing Images using Inferred Visual Dependency Representations,"Desmond Elliott, Arjen de Vries","The Visual Dependency Representation (VDR) is an explicit model of the spa- tial relationships between objects in an im- age. In this paper we present an approach to training a VDR Parsing Model without the extensive human supervision used in previous work. Our approach is to find the objects mentioned in a given descrip- tion using a state-of-the-art object detec- tor, and to use successful detections to pro- duce training data. The description of an unseen image is produced by first predict- ing its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR. The performance of our approach is comparable to a state-of- the-art multimodal deep neural network in images depicting actions. ",,,,ACL
6,2015,Text to 3D Scene Generation with Rich Lexical Grounding,"Angel Chang, Will Monroe, Manolis Savva, Christopher Potts","The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, educa- tion, and robotics. However, prior work on the text to 3D scene generation task has used manually specified object cate- gories and language that identifies them. We introduce a dataset of 3D scenes an- notated with natural language descriptions and learn from this data how to ground tex- tual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method im- proves 3D scene generation over previ- ous work using purely rule-based meth- ods. We evaluate the fidelity and plau- sibility of 3D scenes generated with our grounding approach through human judg- ments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments. ",,,,ACL
7,2015,MultiGranCNN: An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity,"Wenpeng Yin, Hinrich Schütze","We present MultiGranCNN, a general deep learning architecture for matching text chunks. MultiGranCNN supports multigranular comparability of represen- tations: shorter sequences in one chunk can be directly compared to longer se- quences in the other chunk. Multi- GranCNN also contains a flexible and modularized match feature component that is easily adaptable to different types of chunk matching. We demonstrate state- of-the-art performance of MultiGranCNN on clause coherence and paraphrase iden- tification tasks. ",,,,ACL
8,2015,Weakly Supervised Models of Aspect-Sentiment for Online Course Discussion Forums,"Arti Ramesh, Shachi H. Kumar, James Foulds, Lise Getoor","Massive open online courses (MOOCs) are redefining the education system and transcending boundaries posed by tradi- tional courses. With the increase in pop- ularity of online courses, there is a cor- responding increase in the need to under- stand and interpret the communications of the course participants. Identifying top- ics or aspects of conversation and inferring sentiment in online course forum posts can enable instructor interventions to meet the needs of the students, rapidly address course-related issues, and increase student retention. Labeled aspect-sentiment data for MOOCs are expensive to obtain and may not be transferable between courses, suggesting the need for approaches that do not require labeled data. We develop a weakly supervised joint model for aspect- sentiment in online courses, modeling the dependencies between various aspects and sentiment using a recently developed scal- able class of statistical relational models called hinge-loss Markov random fields. We validate our models on posts sam- pled from twelve online courses, each con- taining an average of 10,000 posts, and demonstrate that jointly modeling aspect with sentiment improves the prediction ac- curacy for both aspect and sentiment. ",,,,ACL
9,2015,Semantically Smooth Knowledge Graph Embedding,"Shu Guo, Quan Wang, Bin Wang, Lihong Wang","This paper considers the problem of em- bedding Knowledge Graphs (KGs) con- sisting of entities and relations into low- dimensional vector spaces. Most of the existing methods perform this task based solely on observed facts. The only re- quirement is that the learned embeddings should be compatible within each individ- ual fact. In this paper, aiming at further discovering the intrinsic geometric struc- ture of the embedding space, we propose Semantically Smooth Embedding (SSE). The key idea of SSE is to take full ad- vantage of additional semantic informa- tion and enforce the embedding space to be semantically smooth, i.e., entities be- longing to the same semantic category will lie close to each other in the embedding s- pace. Two manifold learning algorithms Laplacian Eigenmaps and Locally Linear Embedding are used to model the smooth- ness assumption. Both are formulated as geometrically based regularization terms to constrain the embedding task. We em- pirically evaluate SSE in two benchmark tasks of link prediction and triple classi- fication, and achieve significant and con- sistent improvements over state-of-the-art methods. Furthermore, SSE is a general framework. The smoothness assumption can be imposed to a wide variety of em- bedding models, and it can also be con- structed using other information besides entities’ semantic categories. ",,,,ACL
10,2015,SensEmbed: Learning Sense Embeddings for Word and Relational Similarity,"Ignacio Iacobacci, Mohammad Taher Pilehvar, Roberto Navigli","Word embeddings have recently gained considerable popularity for modeling words in different Natural Language Processing (NLP) tasks including seman- tic similarity measurement. However, notwithstanding their success, word embeddings are by their very nature unable to capture polysemy, as different meanings of a word are conflated into a single representation. In addition, their learning process usually relies on massive corpora only, preventing them from taking advantage of structured knowledge. We address both issues by proposing a multi- faceted approach that transforms word embeddings to the sense level and lever- ages knowledge from a large semantic network for effective semantic similarity measurement. We evaluate our approach on word similarity and relational similar- ity frameworks, reporting state-of-the-art performance on multiple datasets. ",,,,ACL
11,2015,Revisiting Word Embedding for Contrasting Meaning,"Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen","Contrasting meaning is a basic aspect of semantics. Recent word-embedding mod- els based on distributional semantics hy- pothesis are known to be weak for mod- eling lexical contrast. We present in this paper the embedding models that achieve an F-score of 92% on the widely-used, publicly available dataset, the GRE “most contrasting word” questions (Mohammad et al., 2008). This is the highest perfor- mance seen so far on this dataset. Sur- prisingly at the first glance, unlike what was suggested in most previous work, where relatedness statistics learned from corpora is claimed to yield extra gains over lexicon-based models, we obtained our best result relying solely on lexical re- sources (Roget’s and WordNet)—corpora statistics did not lead to further improve- ment. However, this should not be sim- ply taken as that distributional statistics is not useful. We examine several basic con- cerns in modeling contrasting meaning to provide detailed analysis, with the aim to shed some light on the future directions for this basic semantics modeling problem. ",,,,ACL
12,2015,Joint Models of Disagreement and Stance in Online Debate,"Dhanya Sridhar, James Foulds, Bert Huang, Lise Getoor","Online debate forums present a valu- able opportunity for the understanding and modeling of dialogue. To understand these debates, a key challenge is inferring the stances of the participants, all of which are interrelated and dependent. While collectively modeling users’ stances has been shown to be effective (Walker et al., 2012c; Hasan and Ng, 2013), there are many modeling decisions whose ramifi- cations are not well understood. To in- vestigate these choices and their effects, we introduce a scalable unified probabilis- tic modeling framework for stance clas- sification models that 1) are collective, 2) reason about disagreement, and 3) can model stance at either the author level or at the post level. We comprehensively evaluate the possible modeling choices on eight topics across two online debate cor- pora, finding accuracy improvements of up to 11.5 percentage points over a local classifier. Our results highlight the im- portance of making the correct modeling choices for online dialogues, and having a unified probabilistic modeling framework that makes this possible. ",,,,ACL
13,2015,Low-Rank Regularization for Sparse Conjunctive Feature Spaces: An Application to Named Entity Classification,"Audi Primadhanty, Xavier Carreras, Ariadna Quattoni","Entity classification, like many other important problems in NLP, involves learning classifiers over sparse high- dimensional feature spaces that result from the conjunction of elementary fea- tures of the entity mention and its context. In this paper we develop a low-rank reg- ularization framework for training max- entropy models in such sparse conjunctive feature spaces. Our approach handles con- junctive feature spaces using matrices and induces an implicit low-dimensional rep- resentation via low-rank constraints. We show that when learning entity classifiers under minimal supervision, using a seed set, our approach is more effective in con- trolling model capacity than standard tech- niques for linear classifiers. ",,,,ACL
14,2015,Learning Word Representations by Jointly Modeling Syntagmatic and Paradigmatic Relations,"Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu","Vector space representation of words has been widely used to capture fine-grained linguistic regularities, and proven to be successful in various natural language pro- cessing tasks in recent years. However, existing models for learning word repre- sentations focus on either syntagmatic or paradigmatic relations alone. In this pa- per, we argue that it is beneficial to jointly modeling both relations so that we can not only encode different types of linguistic properties in a unified way, but also boost the representation learning due to the mu- tual enhancement between these two types of relations. We propose two novel dis- tributional models for word representation using both syntagmatic and paradigmatic relations via a joint training objective. The proposed models are trained on a public Wikipedia corpus, and the learned rep- resentations are evaluated on word anal- ogy and word similarity tasks. The re- sults demonstrate that the proposed mod- els can perform significantly better than all the state-of-the-art baseline methods on both tasks. ",,,,ACL
15,2015,Learning Dynamic Feature Selection for Fast Sequential Prediction,"Emma Strubell, Luke Vilnis, Kate Silverstein, Andrew McCallum","We present paired learning and inference algorithms for significantly reducing com- putation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are or- dered such that high confidence can of- ten be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. Our approach is simpler and better suited to NLP than other related cascade methods. We present experiments in left-to-right part-of-speech tagging, named entity recognition, and transition-based dependency parsing. On the typical benchmarking datasets we can preserve POS tagging accuracy above 97% and parsing LAS above 88.5% both with over a five-fold reduction in run-time, and NER F1 above 88 with more than 2x in- crease in speed. ",,,,ACL
16,2015,Compositional Vector Space Models for Knowledge Base Completion,"Arvind Neelakantan, Benjamin Roth, Andrew McCallum","Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by infer- ring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop re- lational path treated as an atomic feature, like bornIn(X,Z) → containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implica- tions of a path using a recurrent neural network (RNN) that takes as inputs vec- tor embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assem- ble a new dataset of over 52M relational triples, and show that our method im- proves over a traditional classifier by 11%, and a method leveraging pre-trained em- beddings by 7%. ",,,,ACL
17,2015,Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks,"Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng","Traditional approaches to the task of ACE event extraction primarily rely on elabo- rately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack gener- alization, take a large amount of human effort and are prone to error propaga- tion and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically ex- tract lexical-level and sentence-level fea- tures without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic reg- ularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neu- ral network (DMCNN), which uses a dy- namic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimen- tal results show that our approach signif- icantly outperforms other state-of-the-art methods. ",,,,ACL
18,2015,Stacked Ensembles of Information Extractors for Knowledge-Base Population,"Vidhoon Viswanathan, Nazneen Fatema Rajani, Yinon Bentor, Raymond Mooney","We present results on using stacking to en- semble multiple systems for the Knowl- edge Base Population English Slot Fill- ing (KBP-ESF) task. In addition to us- ing the output and confidence of each sys- tem as input to the stacked classifier, we also use features capturing how well the systems agree about the provenance of the information they extract. We demon- strate that our stacking approach outper- forms the best system from the 2014 KBP- ESF competition as well as alternative en- sembling methods employed in the 2014 KBP Slot Filler Validation task and several other ensembling baselines. Additionally, we demonstrate that including provenance information further increases the perfor- mance of stacking. ",,,,ACL
19,2015,Generative Event Schema Induction with Entity Disambiguation,"Kiem-Hieu Nguyen, Xavier Tannier, Olivier Ferret, Romaric Besançon","This paper presents a generative model to event schema induction. Previous meth- ods in the literature only use head words to represent entities. However, elements other than head words contain useful in- formation. For instance, an armed man is more discriminative than man. Our model takes into account this information and precisely represents it using proba- bilistic topic distributions. We illustrate that such information plays an important role in parameter estimation. Mostly, it makes topic distributions more coherent and more discriminative. Experimental results on benchmark dataset empirically confirm this enhancement. ",,,,ACL
20,2015,Syntax-based Simultaneous Translation through Prediction of Unseen Syntactic Constituents,"Yusuke Oda, Graham Neubig, Sakriani Sakti, Tomoki Toda","Simultaneous translation is a method to reduce the latency of communication through machine translation (MT) by di- viding the input into short segments be- fore performing translation. However, short segments pose problems for syntax- based translation methods, as it is diffi- cult to generate accurate parse trees for sub-sentential segments. In this paper, we perform the first experiments applying syntax-based SMT to simultaneous trans- lation, and propose two methods to pre- vent degradations in accuracy: a method to predict unseen syntactic constituents that help generate complete parse trees, and a method that waits for more input when the current utterance is not enough to gener- ate a fluent translation. Experiments on English-Japanese translation show that the proposed methods allow for improvements in accuracy, particularly with regards to word order of the target sentences. ",,,,ACL
21,2015,Efficient Top-Down BTG Parsing for Machine Translation Preordering,Tetsuji Nakagawa,"We present an efficient incremental top- down parsing method for preordering based on Bracketing Transduction Gram- mar (BTG). The BTG-based preordering framework (Neubig et al., 2012) can be applied to any language using only par- allel text, but has the problem of compu- tational efficiency. Our top-down parsing algorithm allows us to use the early up- date technique easily for the latent vari- able structured Perceptron algorithm with beam search, and solves the problem. Experimental results showed that the top- down method is more than 10 times faster than a method using the CYK algorithm. A phrase-based machine translation sys- tem with the top-down method had statis- tically significantly higher BLEU scores for 7 language pairs without relying on supervised syntactic parsers, compared to baseline systems using existing preorder- ing methods. ",,,,ACL
22,2015,Online Multitask Learning for Machine Translation Quality Estimation,"José G. C. de Souza, Matteo Negri, Elisa Ricci, Marco Turchi","We present a method for predicting ma- chine translation output quality geared to the needs of computer-assisted translation. These include the capability to: i) con- tinuously learn and self-adapt to a stream of data coming from multiple translation jobs, ii) react to data diversity by ex- ploiting human feedback, and iii) leverage data similarity by learning and transferring knowledge across domains. To achieve these goals, we combine two supervised machine learning paradigms, online and multitask learning, adapting and unifying them in a single framework. We show the effectiveness of our approach in a re- gression task (HTER prediction), in which online multitask learning outperforms the competitive online single-task and pooling methods used for comparison. This in- dicates the feasibility of integrating in a CAT tool a single QE component capa- ble to simultaneously serve (and continu- ously learn from) multiple translation jobs involving different domains and users. ",,,,ACL
23,2015,A Context-Aware Topic Model for Statistical Machine Translation,"Jinsong Su, Deyi Xiong, Yang Liu, Xianpei Han","Lexical selection is crucial for statistical ma- chine translation. Previous studies separately exploit sentence-level contexts and document- level topics for lexical selection, neglecting their correlations. In this paper, we propose a context-aware topic model for lexical selec- tion, which not only models local contexts and global topics but also captures their correla- tions. The model uses target-side translations as hidden variables to connect document top- ics and source-side local contextual words. In order to learn hidden variables and distribu- tions from data, we introduce a Gibbs sam- pling algorithm for statistical estimation and inference. A new translation probability based on distributions learned by the model is inte- grated into a translation system for lexical se- lection. Experiment results on NIST Chinese- English test sets demonstrate that 1) our model significantly outperforms previous lexical se- lection methods and 2) modeling correlations between local words and global topics can fur- ther improve translation quality. ",,,,ACL
24,2015,Learning Answer-Entailing Structures for Machine Comprehension,"Mrinmaya Sachan, Kumar Dubey, Eric Xing, Matthew Richardson","Understanding open-domain text is one of the primary challenges in NLP. Ma- chine comprehension evaluates the sys- tem’s ability to understand text through a series of question-answering tasks on short pieces of text such that the correct answer can be found only in the given text. For this task, we posit that there is a hid- den (latent) structure that explains the rela- tion between the question, correct answer, and text. We call this the answer-entailing structure; given the structure, the correct- ness of the answer is evident. Since the structure is latent, it must be inferred. We present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs), and uses what it learns to answer machine comprehension questions on novel texts. We extend this framework to incorporate multi-task learning on the different sub- tasks that are required to perform machine comprehension. Evaluation on a publicly available dataset shows that our frame- work outperforms various IR and neural- network baselines, achieving an overall accuracy of 67.8% (vs. 59.9%, the best previously-published result.) ",,,,ACL
25,2015,Learning Continuous Word Embedding with Metadata for Question Retrieval in Community Question Answering,"Guangyou Zhou, Tingting He, Jun Zhao, Po Hu","Community question answering (cQA) has become an important issue due to the popularity of c QA archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in c QA archives aims to find the exist- ing questions that are semantically equiv- alent or relevant to the queried questions. However, the lexical gap problem brings about new challenge for question retrieval in cQA. In this paper, we propose to learn continuous word embeddings with meta- data of category information within cQA pages for question retrieval. To deal with the variable size of word embedding vec- tors, we employ the framework of fisher kernel to aggregated them into the fixed- length vectors. Experimental results on large-scale real world cQA data set show that our approach can significantly out- perform state-of-the-art translation models and topic-based models for question re- trieval in cQA. ",,,,ACL
26,2015,Question Answering over Freebase with Multi-Column Convolutional Neural Networks,"Li Dong, Furu Wei, Ming Zhou, Ke Xu","Answering natural language questions over a knowledge base is an important and challenging task. Most of existing sys- tems typically rely on hand-crafted fea- tures and rules to conduct question under- standing and/or answer ranking. In this pa- per, we introduce multi-column convolu- tional neural networks (MCCNNs) to un- derstand questions from three different as- pects (namely, answer path, answer con- text, and answer type) and learn their dis- tributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use F REEBASE as the knowledge base and conduct exten- sive experiments on the W EB Q UESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline sys- tems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn. ",,,,ACL
27,2015,Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning,"Angeliki Lazaridou, Georgiana Dinu, Marco Baroni","Zero-shot methods in language, vision and other domains rely on a cross-space map- ping function that projects vectors from the relevant feature space (e.g., visual- feature-based image representations) to a large semantic word space (induced in an unsupervised way from corpus data), where the entities of interest (e.g., objects images depict) are labeled with the words associated to the nearest neighbours of the mapped vectors. Zero-shot cross-space mapping methods hold great promise as a way to scale up annotation tasks well be- yond the labels in the training data (e.g., recognizing objects that were never seen in training). However, the current perfor- mance of cross-space mapping functions is still quite low, so that the strategy is not yet usable in practical applications. In this paper, we explore some general properties, both theoretical and empirical, of the cross-space mapping function, and we build on them to propose better meth- ods to estimate it. In this way, we attain large improvements over the state of the art, both in cross-linguistic (word trans- lation) and cross-modal (image labeling) zero-shot experiments. ",,,,ACL
28,2015,A Generalisation of Lexical Functions for Composition in Distributional Semantics,"Antoine Bride, Tim Van de Cruys, Nicholas Asher","Over the last two decades, numerous algo- rithms have been developed that success- fully capture something of the semantics of single words by looking at their distri- bution in text and comparing these distri- butions in a vector space model. How- ever, it is not straightforward to construct meaning representations beyond the level of individual words – i.e. the combina- tion of words into larger units – using dis- tributional methods. Our contribution is twofold. First of all, we carry out a large- scale evaluation, comparing different com- position methods within the distributional framework for the cases of both adjective- noun and noun-noun composition, making use of a newly developed dataset. Sec- ondly, we propose a novel method for composition, which generalises the ap- proach by Baroni and Zamparelli (2010). The performance of our novel method is also evaluated on our new dataset and proves competitive with the best methods. ",,,,ACL
29,2015,Simple Learning and Compositional Application of Perceptually Grounded Word Meanings for Incremental Reference Resolution,"Casey Kennington, David Schlangen","An elementary way of using language is to refer to objects. Often, these objects are physically present in the shared envi- ronment and reference is done via men- tion of perceivable properties of the ob- jects. This is a type of language use that is modelled well neither by logical semantics nor by distributional semantics, the former focusing on inferential relations between expressed propositions, the latter on simi- larity relations between words or phrases. We present an account of word and phrase meaning that is perceptually grounded, trainable, compositional, and ‘dialogue- plausible’ in that it computes meanings word-by-word. We show that the approach performs well (with an accuracy of 65% on a 1-out-of-32 reference resolution task) on direct descriptions and target/landmark descriptions, even when trained with less than 800 training examples and automati- cally transcribed utterances. ",,,,ACL
30,2015,Neural CRF Parsing,"Greg Durrett, Dan Klein","This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear fea- turization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but in- stead of linear potential functions based on sparse features, we use nonlinear po- tentials computed via a feedforward neu- ral network. Because potentials are still local to anchored rules, structured infer- ence (CKY) is unchanged from the sparse case. Computing gradients during learn- ing involves backpropagating an error sig- nal formed from standard CRF sufficient statistics (expected rule counts). Us- ing only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system 1 achieves 91.1 F 1 on section 23 of the Penn Tree- bank, and more generally outperforms the best prior single parser results on a range of languages. ",,,,ACL
31,2015,An Effective Neural Network Model for Graph-based Dependency Parsing,"Wenzhe Pei, Tao Ge, Baobao Chang","Most existing graph-based parsing models rely on millions of hand-crafted features, which limits their generalization ability and slows down the parsing speed. In this paper, we propose a general and effective Neural Network model for graph-based dependency parsing. Our model can auto- matically learn high-order feature combi- nations using only atomic features by ex- ploiting a novel activation function tanh- cube. Moreover, we propose a simple yet effective way to utilize phrase-level infor- mation that is expensive to use in conven- tional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. ",,,,ACL
32,2015,Structured Training for Neural Network Transition-Based Parsing,"David Weiss, Chris Alberti, Michael Collins, Slav Petrov","We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automat- ically parsed sentences. Given this fixed network representation, we learn a final layer using the struc- tured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% un- labeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide in- depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy. ",,,,ACL
33,2015,Transition-Based Dependency Parsing with Stack Long Short-Term Memory,"Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews","We propose a technique for learning rep- resentations of parser states in transition- based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks— the stack LSTM. Like the conventional stack data structures used in transition- based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formu- late an efficient parsing model that cap- tures three facets of a parser’s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their inter- nal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance. ",,,,ACL
34,2015,Leveraging Linguistic Structure For Open Domain Information Extraction,"Gabor Angeli, Melvin Jose Johnson Premkumar, Christopher D. Manning","Relation triples produced by open domain information extraction (open IE) systems are useful for question answering, infer- ence, and other IE tasks. Traditionally these are extracted using a large set of pat- terns; however, this approach is brittle on out-of-domain text and long-range depen- dencies, and gives no insight into the sub- structure of the arguments. We replace this large pattern set with a few patterns for canonically structured sentences, and shift the focus to a classifier which learns to extract self-contained clauses from longer sentences. We then run natural logic infer- ence over these short clauses to determine the maximally specific arguments for each candidate triple. We show that our ap- proach outperforms a state-of-the-art open IE system on the end-to-end TAC-KBP 2013 Slot Filling task. ",,,,ACL
35,2015,Joint Information Extraction and Reasoning: A Scalable Statistical Relational Learning Approach,"William Yang Wang, William W. Cohen","A standard pipeline for statistical rela- tional learning involves two steps: one first constructs the knowledge base (KB) from text, and then performs the learn- ing and reasoning tasks using probabilis- tic first-order logics. However, a key is- sue is that information extraction (IE) er- rors from text affect the quality of the KB, and propagate to the reasoning task. In this paper, we propose a statistical rela- tional learning model for joint information extraction and reasoning. More specifi- cally, we incorporate context-based entity extraction with structure learning (SL) in a scalable probabilistic logic framework. We then propose a latent context inven- tion (LCI) approach to improve the per- formance. In experiments, we show that our approach outperforms state-of-the-art baselines over three real-world Wikipedia datasets from multiple domains; that joint learning and inference for IE and SL sig- nificantly improve both tasks; that latent context invention further improves the re- sults. ",,,,ACL
36,2015,A Knowledge-Intensive Model for Prepositional Phrase Attachment,"Ndapandula Nakashole, Tom M. Mitchell","Prepositional phrases (PPs) express cru- cial information that knowledge base con- struction methods need to extract. How- ever, PPs are a major source of syntactic ambiguity and still pose problems in pars- ing. We present a method for resolving ambiguities arising from PPs, making ex- tensive use of semantic knowledge from various resources. As training data, we use both labeled and unlabeled data, utilizing an expectation maximization algorithm for parameter estimation. Experiments show that our method yields improvements over existing methods including a state of the art dependency parser. ",,,,ACL
37,2015,A Convolution Kernel Approach to Identifying Comparisons in Text,"Maksim Tkachenko, Hady Lauw","Comparisons in text, such as in online re- views, serve as useful decision aids. In this paper, we focus on the task of iden- tifying whether a comparison exists be- tween a specific pair of entity mentions in a sentence. This formulation is trans- formative, as previous work only seeks to determine whether a sentence is com- parative, which is presumptuous in the event the sentence mentions multiple en- tities and is comparing only some, not all, of them. Our approach leverages not only lexical features such as salient words, but also structural features expressing the re- lationships among words and entity men- tions. To model these features seamlessly, we rely on a dependency tree representa- tion, and investigate the applicability of a series of tree kernels. This leads to the de- velopment of a new context-sensitive tree kernel: Skip-node Kernel (SNK). We fur- ther describe both its exact and approxi- mate computations. Through experiments on real-life datasets, we evaluate the effec- tiveness of our kernel-based approach for comparison identification, as well as the utility of SNK and its approximations. ",,,,ACL
38,2015,It Depends: Dependency Parser Comparison Using A Web-based Evaluation Tool,"Jinho D. Choi, Joel Tetreault, Amanda Stent","The last few years have seen a surge in the number of accurate, fast, publicly avail- able dependency parsers. At the same time, the use of dependency parsing in NLP applications has increased. It can be difficult for a non-expert to select a good “off-the-shelf” parser. We present a com- parative analysis of ten leading statistical dependency parsers on a multi-genre cor- pus of English. For our analysis, we de- veloped a new web-based tool that gives a convenient way of comparing depen- dency parser outputs. Our analysis will help practitioners choose a parser to op- timize their desired speed/accuracy trade- off, and our tool will help practitioners ex- amine and compare parser output. ",,,,ACL
39,2015,Generating High Quality Proposition Banks for Multilingual Semantic Role Labeling,"Alan Akbik, Laura Chiticariu, Marina Danilevsky, Yunyao Li","Semantic role labeling (SRL) is crucial to natural language understanding as it identi- fies the predicate-argument structure in text with semantic labels. Unfortunately, re- sources required to construct SRL models are expensive to obtain and simply do not exist for most languages. In this paper, we present a two-stage method to enable the construction of SRL models for resource- poor languages by exploiting monolingual SRL and multilingual parallel data. Exper- imental results show that our method out- performs existing methods. We use our method to generate Proposition Banks with high to reasonable quality for 7 languages in three language families and release these resources to the research community. ",,,,ACL
40,2015,Aligning Opinions: Cross-Lingual Opinion Mining with Dependencies,"Mariana S. C. Almeida, Cláudia Pinto, Helena Figueira, Pedro Mendes","We propose a cross-lingual framework for fine-grained opinion mining using bitext projection. The only requirements are a running system in a source language and word-aligned parallel data. Our method projects opinion frames from the source to the target language, and then trains a sys- tem on the target language using the auto- matic annotations. Key to our approach is a novel dependency-based model for opin- ion mining, which we show, as a byprod- uct, to be on par with the current state of the art for English, while avoiding the need for integer programming or rerank- ing. In cross-lingual mode (English to Por- tuguese), our approach compares favor- ably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. ",,,,ACL
41,2015,Learning to Adapt Credible Knowledge in Cross-lingual Sentiment Analysis,"Qiang Chen, Wenjie Li, Yu Lei, Xule Liu","Cross-lingual sentiment analysis is a task of identifying sentiment polarities of texts in a low-resource language by using sen- timent knowledge in a resource-abundant language. While most existing approaches are driven by transfer learning, their performance does not reach to a promising level due to the transferred errors. In this paper, we propose to integrate into knowl- edge transfer a knowledge validation mod- el, which aims to prevent the negative influence from the wrong knowledge by distinguishing highly credible knowledge. Experiment results demonstrate the neces- sity and effectiveness of the model. ",,,,ACL
42,2015,Learning Bilingual Sentiment Word Embeddings for Cross-language Sentiment Classification,"HuiWei Zhou, Long Chen, Fulin Shi, Degen Huang","The sentiment classification performance relies on high-quality sentiment resources. However, these resources are imbalanced in different languages. Cross-language sentiment classification (CLSC) can lever- age the rich resources in one language (source language) for sentiment classifica- tion in a resource-scarce language (target language). Bilingual embeddings could eliminate the semantic gap between two languages for CLSC, but ignore the senti- ment information of text. This paper pro- poses an approach to learning bilingual sentiment word embeddings (BSWE) for English-Chinese CLSC. The proposed B- SWE incorporate sentiment information of text into bilingual embeddings. Further- more, we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on large- scale parallel corpora. Experiments on NLP&CC 2013 CLSC dataset show that our approach outperforms the state-of-the- art systems. ",,,,ACL
43,2015,Content Models for Survey Generation: A Factoid-Based Evaluation,"Rahul Jha, Catherine Finegan-Dollak, Ben King, Reed Coke","We present a new factoid-annotated dataset for evaluating content models for scientific survey article generation containing 3,425 sentences from 7 topics in natural language processing. We also introduce a novel HITS-based content model for automated survey article gen- eration called H IT S UM that exploits the lexical network structure between sen- tences from citing and cited papers. Using the factoid-annotated data, we conduct a pyramid evaluation and compare H IT S UM with two previous state-of-the-art content models: C-Lexrank, a network based con- tent model, and T OPIC S UM , a Bayesian content model. Our experiments show that our new content model captures useful survey-worthy information and outper- forms C-Lexrank by 4% and T OPIC S UM by 7% in pyramid evaluation. ",,,,ACL
44,2015,Training a Natural Language Generator From Unaligned Data,"Ondřej Dušek, Filip Jurčíček","We present a novel syntax-based natural language generation system that is train- able from unaligned pairs of input mean- ing representations and output sentences. It is divided into sentence planning, which incrementally builds deep-syntactic de- pendency trees, and surface realization. Sentence planner is based on A* search with a perceptron ranker that uses novel differing subtree updates and a simple fu- ture promise estimation; surface realiza- tion uses a rule-based pipeline from the Treex NLP toolkit. Our first results show that training from unaligned data is feasible, the outputs of our generator are mostly fluent and rele- vant. ",,,,ACL
45,2015,Event-Driven Headline Generation,"Rui Sun, Yue Zhang, Meishan Zhang, Donghong Ji","We propose an event-driven model for headline generation. Given an input document, the system identifies a key event chain by extracting a set of structural events that describe them. Then a novel multi-sentence compression algorithm is used to fuse the extracted events, generating a headline for the document. Our model can be viewed as a novel combination of extractive and abstractive headline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-the-art systems. ",,,,ACL
46,2015,New Transfer Learning Techniques for Disparate Label Sets,"Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, Minwoo Jeong","In natural language understanding (NLU), a user utterance can be labeled differently depending on the domain or application (e.g., weather vs. calendar). Standard domain adaptation techniques are not di- rectly applicable to take advantage of the existing annotations because they assume that the label set is invariant. We propose a solution based on label embeddings in- duced from canonical correlation analysis (CCA) that reduces the problem to a stan- dard domain adaptation task and allows use of a number of transfer learning tech- niques. We also introduce a new trans- fer learning technique based on pretrain- ing of hidden-unit CRFs (HUCRFs). We perform extensive experiments on slot tag- ging on eight personal digital assistant do- mains and demonstrate that the proposed methods are superior to strong baselines. ",,,,ACL
47,2015,Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding,"Yun-Nung Chen, William Yang Wang, Anatole Gershman, Alexander Rudnicky","Spoken dialogue systems (SDS) typically require a predefined semantic ontology to train a spoken language understanding (SLU) module. In addition to the anno- tation cost, a key challenge for design- ing such an ontology is to define a coher- ent slot set while considering their com- plex relations. This paper introduces a novel matrix factorization (MF) approach to learn latent feature vectors for utter- ances and semantic elements without the need of corpus annotations. Specifically, our model learns the semantic slots for a domain-specific SDS in an unsupervised fashion, and carries out semantic pars- ing using latent MF techniques. To fur- ther consider the global semantic struc- ture, such as inter-word and inter-slot re- lations, we augment the latent MF-based model with a knowledge graph propaga- tion model based on a slot-based seman- tic graph and a word-based lexical graph. Our experiments show that the proposed MF approaches produce better SLU mod- els that are able to predict semantic slots and word patterns taking into account their relations and domain-specificity in a joint manner. ",,,,ACL
48,2015,Efficient Disfluency Detection with Transition-based Parsing,"Shuangzhi Wu, Dongdong Zhang, Ming Zhou, Tiejun Zhao","Automatic speech recognition (ASR) out- puts often contain various disfluencies. It is necessary to remove these disfluencies before processing downstream tasks. In this paper, an efficient disfluency detection approach based on right-to-left transition- based parsing is proposed, which can effi- ciently identify disfluencies and keep ASR outputs grammatical. Our method exploits a global view to capture long-range de- pendencies for disfluency detection by in- tegrating a rich set of syntactic and dis- fluency features with linear complexity. The experimental results show that our method outperforms state-of-the-art work and achieves a 85.1% f-score on the com- monly used English Switchboard test set. We also apply our method to in-house an- notated Chinese data and achieve a sig- nificantly higher f-score compared to the baseline of CRF-based approach. ",,,,ACL
49,2015,S-MART: Novel Tree-based Structured Learning Algorithms Applied to Tweet Entity Linking,"Yi Yang, Ming-Wei Chang","Non-linear models recently receive a lot of attention as people are starting to dis- cover the power of statistical and em- bedding features. However, tree-based models are seldom studied in the con- text of structured learning despite their re- cent success on various classification and ranking tasks. In this paper, we propose S- MART , a tree-based structured learning framework based on multiple additive re- gression trees. S- MART is especially suit- able for handling tasks with dense fea- tures, and can be used to learn many dif- ferent structures under various loss func- tions. We apply S- MART to the task of tweet entity linking — a core component of tweet information extraction, which aims to identify and link name mentions to en- tities in a knowledge base. A novel infer- ence algorithm is proposed to handle the special structure of the task. The exper- imental results show that S- MART signif- icantly outperforms state-of-the-art tweet entity linking systems. ",,,,ACL
50,2015,Entity Retrieval via Entity Factoid Hierarchy,"Chunliang Lu, Wai Lam, Yi Liao","We propose that entity queries are gener- ated via a two-step process: users first se- lect entity facts that can distinguish tar- get entities from the others; and then choose words to describe each selected fact. Based on this query generation paradigm, we propose a new entity rep- resentation model named as entity fac- toid hierarchy. An entity factoid hierar- chy is a tree structure composed of fac- toid nodes. A factoid node describes one or more facts about the entity in different information granularities. The entity fac- toid hierarchy is constructed via a factor graph model, and the inference on the fac- tor graph is achieved by a modified variant of Multiple-try Metropolis algorithm. En- tity retrieval is performed by decompos- ing entity queries and computing the query likelihood on the entity factoid hierarchy. Using an array of benchmark datasets, we demonstrate that our proposed framework significantly improves the retrieval perfor- mance over existing models. ",,,,ACL
51,2015,Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment,"Muyu Zhang, Bing Qin, Mao Zheng, Graeme Hirst","Document enrichment focuses on retriev- ing relevant knowledge from external re- sources, which is essential because text is generally replete with gaps. Since conven- tional work primarily relies on special re- sources, we instead use triples of Subject, Predicate, Object as knowledge and in- corporate distributional semantics to rank them. Our model first extracts these triples automatically from raw text and converts them into real-valued vectors based on the word semantics captured by Latent Dirich- let Allocation. We then represent these triples, together with the source document that is to be enriched, as a graph of triples, and adopt a global iterative algorithm to propagate relevance weight from source document to these triples so as to select the most relevant ones. Evaluated as a rank- ing problem, our model significantly out- performs multiple strong baselines. More- over, we conduct a task-based evaluation by incorporating these triples as additional features into document classification and enhances the performance by 3.02%. ",,,,ACL
52,2015,A Strategic Reasoning Model for Generating Alternative Answers,"Jon Stevens, Anton Benz, Sebastian Reuße, Ralf Klabunde","We characterize a class of indirect an- swers to yes/no questions, alternative an- swers, where information is given that is not directly asked about, but which might nonetheless address the underlying moti- vation for the question. We develop a model rooted in game theory that gener- ates these answers via strategic reasoning about possible unobserved domain-level user requirements. We implement the model within an interactive question an- swering system simulating real estate dia- logue. The system learns a prior probabil- ity distribution over possible user require- ments by analyzing training dialogues, which it uses to make strategic deci- sions about answer selection. The system generates pragmatically natural and inter- pretable answers which make for more ef- ficient interactions compared to a baseline. ",,,,ACL
53,2015,Modeling Argument Strength in Student Essays,"Isaac Persing, Vincent Ng","While recent years have seen a surge of in- terest in automated essay grading, includ- ing work on grading essays with respect to particular dimensions such as prompt adherence, coherence, and technical qual- ity, there has been relatively little work on grading the essay dimension of argu- ment strength, which is arguably the most important aspect of argumentative essays. We introduce a new corpus of argumen- tative student essays annotated with argu- ment strength scores and propose a su- pervised, feature-rich approach to auto- matically scoring the essays along this dimension. Our approach significantly outperforms a baseline that relies solely on heuristically applied sentence argument function labels by up to 16.1%. ",,,,ACL
54,2015,Summarization of Multi-Document Topic Hierarchies using Submodular Mixtures,"Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan, Jeff Bilmes","We study the problem of summarizing DAG-structured topic hierarchies over a given set of documents. Example appli- cations include automatically generating Wikipedia disambiguation pages for a set of articles, and generating candidate multi-labels for preparing machine learn- ing datasets (e.g., for text classification, functional genomics, and image classi- fication). Unlike previous work, which focuses on clustering the set of documents using the topic hierarchy as features, we directly pose the problem as a submodular optimization problem on a topic hierarchy using the documents as features. Desirable properties of the chosen topics include document coverage, specificity, topic diversity, and topic homogeneity, each of which, we show, is naturally modeled by a submodular function. Other information, provided say by unsupervised approaches such as LDA and its variants, can also be utilized by defining a submodular function that expresses coherence between the chosen topics and this information. We use a large-margin framework to learn convex mixtures over the set of submodular components. We empirically evaluate our method on the problem of automatically generating Wikipedia disambiguation pages using human generated clusterings as ground truth. We find that our frame- work improves upon several baselines according to a variety of standard evalua- tion metrics including the Jaccard Index, F1 score and NMI, and moreover, can be scaled to extremely large scale problems. ",,,,ACL
55,2015,Learning to Explain Entity Relationships in Knowledge Graphs,"Nikos Voskarides, Edgar Meij, Manos Tsagkias, Maarten de Rijke","We study the problem of explaining re- lationships between pairs of knowledge graph entities with human-readable de- scriptions. Our method extracts and en- riches sentences that refer to an entity pair from a corpus and ranks the sentences ac- cording to how well they describe the re- lationship between the entities. We model this task as a learning to rank problem for sentences and employ a rich set of fea- tures. When evaluated on a large set of manually annotated sentences, we find that our method significantly improves over state-of-the-art baseline models. ",,,,ACL
56,2015,Bring you to the past: Automatic Generation of Topically Relevant Event Chronicles,"Tao Ge, Wenzhe Pei, Heng Ji, Sujian Li","An event chronicle provides people with an easy and fast access to learn the past. In this paper, we propose the first novel approach to automatically generate a top- ically relevant event chronicle during a certain period given a reference chronicle during another period. Our approach con- sists of two core components – a time- aware hierarchical Bayesian model for event detection, and a learning-to-rank model to select the salient events to con- struct the final chronicle. Experimental re- sults demonstrate our approach is promis- ing to tackle this new problem. ",,,,ACL
57,2015,Context-aware Entity Morph Decoding,"Boliang Zhang, Hongzhao Huang, Xiaoman Pan, Sujian Li","People create morphs, a special type of fake alternative names, to achieve certain communication goals such as expressing strong sentiment or evading censors. For example, “Black Mamba”, the name for a highly venomous snake, is a morph that Kobe Bryant created for himself due to his agility and aggressiveness in playing bas- ketball games. This paper presents the first end-to-end context-aware entity morph de- coding system that can automatically iden- tify, disambiguate, verify morph mentions based on specific contexts, and resolve them to target entities. Our approach is based on an absolute “cold-start” - it does not require any candidate morph or tar- get entity lists as input, nor any manually constructed morph-target pairs for train- ing. We design a semi-supervised collec- tive inference framework for morph men- tion extraction, and compare various deep learning based approaches for morph res- olution. Our approach achieved signifi- cant improvement over the state-of-the-art method (Huang et al., 2013), which used a large amount of training data. 1 ",,,,ACL
58,2015,Multi-Objective Optimization for the Joint Disambiguation of Nouns and Named Entities,"Dirk Weissenborn, Leonhard Hennig, Feiyu Xu, Hans Uszkoreit","In this paper, we present a novel approach to joint word sense disambiguation (WSD) and entity linking (EL) that combines a set of complementary objectives in an exten- sible multi-objective formalism. During disambiguation the system performs con- tinuous optimization to find optimal prob- ability distributions over candidate senses. The performance of our system on nomi- nal WSD as well as EL improves state-of- the-art results on several corpora. These improvements demonstrate the importance of combining complementary objectives in a joint model for robust disambiguation. ",,,,ACL
59,2015,Building a Scientific Concept Hierarchy Database (SCHBase),"Eytan Adar, Srayan Datta","Extracted keyphrases can enhance numer- ous applications ranging from search to tracking the evolution of scientific dis- course. We present SCHB ASE , a hier- archical database of keyphrases extracted from large collections of scientific liter- ature. SCHB ASE relies on a tendency of scientists to generate new abbrevia- tions that “extend” existing forms as a form of signaling novelty. We demon- strate how these keyphrases/concepts can be extracted, and their viability as a database in relation to existing collections. We further show how keyphrases can be placed into a semantically-meaningful “phylogenetic” structure and describe key features of this structure. The com- plete SCHB ASE dataset is available at: http://cond.org/schbase.html. ",,,,ACL
60,2015,Sentiment-Aspect Extraction based on Restricted Boltzmann Machines,"Linlin Wang, Kang Liu, Zhu Cao, Jun Zhao",Aspect extraction and sentiment analysis of reviews are both important tasks in opinion mining. We propose a novel senti- ment and aspect extraction model based on Restricted Boltzmann Machines to jointly address these two tasks in an unsupervised setting. This model reflects the gener- ation process of reviews by introducing a heterogeneous structure into the hidden layer and incorporating informative priors. Experiments show that our model outper- forms previous state-of-the-art methods. ,,,,ACL
61,2015,Classifying Relations by Ranking with Convolutional Neural Networks,"Cícero dos Santos, Bing Xiang, Bowen Zhou","Relation classification is an important se- mantic processing task for which state-of- the-art systems still rely on costly hand- crafted features. In this work we tackle the relation classification task using a convo- lutional neural network that performs clas- sification by ranking (CR-CNN). We pro- pose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experi- ments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CR- CNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted fea- tures. Additionally, our experimental re- sults show that: (1) our approach is more effective than CNN followed by a soft- max classifier; (2) omitting the representa- tion of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals. ",,,,ACL
62,2015,Semantic Representations for Domain Adaptation: A Case Study on the Tree Kernel-based Method for Relation Extraction,"Thien Huu Nguyen, Barbara Plank, Ralph Grishman","We study the application of word embed- dings to generate semantic representations for the domain adaptation problem of re- lation extraction (RE) in the tree kernel- based method. We systematically evaluate various techniques to generate the seman- tic representations and demonstrate that they are effective to improve the general- ization performance of a tree kernel-based relation extractor across domains (up to 7% relative improvement). In addition, we compare the tree kernel-based and the feature-based method for RE in a compat- ible way, on the same resources and set- tings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. ",,,,ACL
63,2015,"Omnia Mutantur, Nihil Interit: Connecting Past with Present by Finding Corresponding Terms across Time","Yating Zhang, Adam Jatowt, Sourav Bhowmick, Katsumi Tanaka","In the current fast-paced world, people tend to possess limited knowledge about things from the past. For example, some young users may not know that Walkman played similar func- tion as iPod does nowadays. In this paper, we approach the temporal correspondence prob- lem in which, given an input term (e.g., iPod) and the target time (e.g. 1980s), the task is to find the counterpart of the query that existed in the target time. We propose an approach that transforms word contexts across time based on their neural network representations. We then experimentally demonstrate the ef- fectiveness of our method on the New York Times Annotated Corpus. ",,,,ACL
64,2015,Negation and Speculation Identification in Chinese Language,"Bowei Zou, Qiaoming Zhu, Guodong Zhou","Identifying negative or speculative narra- tive fragments from fact is crucial for natural language processing (NLP) appli- cations. Previous studies on negation and speculation identification in Chinese lan- guage suffers much from two problems: corpus scarcity and the bottleneck in fun- damental Chinese information processing. To resolve these problems, this paper constructs a Chinese corpus which con- sists of three sub-corpora from different resources. In order to detect the negative and speculative cues, a sequence labeling model is proposed. Moreover, a bilingual cue expansion method is proposed to in- crease the coverage in cue detection. In addition, this paper presents a new syn- tactic structure-based framework to iden- tify the linguistic scope of a cue, instead of the traditional chunking-based frame- work. Experimental results justify the usefulness of our Chinese corpus and the appropriateness of our syntactic struc- ture-based framework which obtained significant improvement over the state- of-the-art on negation and speculation identification in Chinese language. * ",,,,ACL
65,2015,Learning Relational Features with Backward Random Walks,"Ni Lao, Einat Minkov, William Cohen","The path ranking algorithm (PRA) has been recently proposed to address relational classification and retrieval tasks at large scale. We describe Cor-PRA, an enhanced system that can model a larger space of relational rules, including longer relational rules and a class of first order rules with constants, while maintaining scalability. We describe and test faster algorithms for searching for these features. A key contribution is to leverage backward random walks to efficiently discover these types of rules. An empirical study is conducted on the tasks of graph-based knowledge base inference, and person named entity extraction from parsed text. Our results show that learning paths with constants improves performance on both tasks, and that modeling longer paths dramatically improves performance for the named entity extraction task. ",,,,ACL
66,2015,Learning the Semantics of Manipulation Action,"Yezhou Yang, Yiannis Aloimonos, Cornelia Fermüller, Eren Erdal Aksoy","In this paper we present a formal compu- tational framework for modeling manip- ulation actions. The introduced formal- ism leads to semantics of manipulation ac- tion and has applications to both observ- ing and understanding human manipula- tion actions as well as executing them with a robotic mechanism (e.g. a humanoid robot). It is based on a Combinatory Cat- egorial Grammar. The goal of the intro- duced framework is to: (1) represent ma- nipulation actions with both syntax and se- mantic parts, where the semantic part em- ploys λ-calculus; (2) enable a probabilis- tic semantic parsing schema to learn the lambda-calculus representation of manip- ulation action from an annotated action corpus of videos; (3) use (1) and (2) to de- velop a system that visually observes ma- nipulation actions and understands their meaning while it can reason beyond ob- servations using propositional logic and axiom schemata. The experiments con- ducted on a public available large manip- ulation action dataset validate the theoret- ical framework and our implementation. ",,,,ACL
67,2015,Knowledge Graph Embedding via Dynamic Mapping Matrix,"Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu","Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR re- gard a relation as translation from head en- tity to tail entity and the CTransR achieves state-of-the-art performance. In this pa- per, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named sym- bol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to con- struct mapping matrix dynamically. Com- pared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less param- eters and has no matrix-vector multipli- cation operations, which makes it can be applied on large scale graphs. In Experi- ments, we evaluate our model on two typ- ical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms state- of-the-art methods. ",,,,ACL
68,2015,How Far are We from Fully Automatic High Quality Grammatical Error Correction?,"Christopher Bryant, Hwee Tou Ng","In this paper, we first explore the role of inter-annotator agreement statistics in grammatical error correction and conclude that they are less informative in fields where there may be more than one correct answer. We next created a dataset of 50 student essays, each corrected by 10 dif- ferent annotators for all error types, and in- vestigated how both human and GEC sys- tem scores vary when different combina- tions of these annotations are used as the gold standard. Upon learning that even hu- mans are unable to score higher than 75% F 0.5 , we propose a new metric based on the ratio between human and system per- formance. We also use this method to in- vestigate the extent to which annotators agree on certain error categories, and find that similar results can be obtained from a smaller subset of just 10 essays. ",,,,ACL
69,2015,Knowledge Portability with Semantic Expansion of Ontology Labels,"Mihael Arcan, Marco Turchi, Paul Buitelaar","Our research focuses on the multilin- gual enhancement of ontologies that, of- ten represented only in English, need to be translated in different languages to en- able knowledge access across languages. Ontology translation is a rather different task then the classic document translation, because ontologies contain highly specific vocabulary and they lack contextual in- formation. For these reasons, to improve automatic ontology translations, we first focus on identifying relevant unambigu- ous and domain-specific sentences from a large set of generic parallel corpora. Then, we leverage Linked Open Data resources, such as DBPedia, to isolate ontology- specific bilingual lexical knowledge. In both cases, we take advantage of the se- mantic information of the labels to se- lect relevant bilingual data with the aim of building an ontology-specific statistical machine translation system. We evaluate our approach on the translation of a medi- cal ontology, translating from English into German. Our experiment shows a sig- nificant improvement of around 3 BLEU points compared to a generic as well as a domain-specific translation approach. ",,,,ACL
70,2015,Automatic disambiguation of English puns,"Tristan Miller, Iryna Gurevych","Traditional approaches to word sense dis- ambiguation (WSD) rest on the assump- tion that there exists a single, unambigu- ous communicative intention underlying every word in a document. However, writ- ers sometimes intend for a word to be in- terpreted as simultaneously carrying mul- tiple distinct meanings. This deliberate use of lexical ambiguity—i.e., punning— is a particularly common source of humour. In this paper we describe how traditional, language-agnostic WSD approaches can be adapted to “disambiguate” puns, or rather to identify their double meanings. We eval- uate several such approaches on a manually sense-annotated collection of English puns and observe performance exceeding that of some knowledge-based and supervised baselines. ",,,,ACL
71,2015,Unsupervised Cross-Domain Word Representation Learning,"Danushka Bollegala, Takanori Maehara, Ken-ichi Kawarabayashi","Meaning of a word varies from one do- main to another. Despite this impor- tant domain dependence in word seman- tics, existing word representation learning methods are bound to a single domain. Given a pair of source-target domains, we propose an unsupervised method for learning domain-specific word representa- tions that accurately capture the domain- specific aspects of word semantics. First, we select a subset of frequent words that occur in both domains as pivots. Next, we optimize an objective function that enforces two constraints: (a) for both source and target domain documents, piv- ots that appear in a document must accu- rately predict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two do- mains. Moreover, we propose a method to perform domain adaptation using the learnt word representations. Our proposed method significantly outperforms compet- itive baselines including the state-of-the- art domain-insensitive word representa- tions, and reports best sentiment classifi- cation accuracies for all domain-pairs in a benchmark dataset. ",,,,ACL
72,2015,A Unified Multilingual Semantic Representation of Concepts,"José Camacho-Collados, Mohammad Taher Pilehvar, Roberto Navigli","Semantic representation lies at the core of several applications in Natural Language Processing. However, most existing se- mantic representation techniques cannot be used effectively for the representation of individual word senses. We put for- ward a novel multilingual concept repre- sentation, called M UFFIN , which not only enables accurate representation of word senses in different languages, but also pro- vides multiple advantages over existing approaches. M UFFIN represents a given concept in a unified semantic space irre- spective of the language of interest, en- abling cross-lingual comparison of differ- ent concepts. We evaluate our approach in two different evaluation benchmarks, se- mantic similarity and Word Sense Disam- biguation, reporting state-of-the-art per- formance on several standard datasets. ",,,,ACL
73,2015,Demographic Factors Improve Classification Performance,Dirk Hovy,"Extra-linguistic factors influence language use, and are accounted for by speakers and listeners. Most natural language pro- cessing (NLP) tasks to date, however, treat language as uniform. This assump- tion can harm performance. We investi- gate the effect of including demographic information on performance in a variety of text-classification tasks. We find that by including age or gender information, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages. ",,,,ACL
74,2015,Vector-space calculation of semantic surprisal for predicting word pronunciation duration,"Asad Sayeed, Stefan Fischer, Vera Demberg","In order to build psycholinguistic mod- els of processing difficulty and evaluate these models against human data, we need highly accurate language models. Here we specifically consider surprisal, a word’s predictability in context. Existing ap- proaches have mostly used n-gram models or more sophisticated syntax-based pars- ing models; this largely does not account for effects specific to semantics. We build on the work by Mitchell et al. (2010) and show that the semantic prediction model suggested there can successfully predict spoken word durations in naturalistic con- versational data. An interesting finding is that the training data for the semantic model also plays a strong role: the model trained on in- domain data, even though a better lan- guage model for our data, is not able to predict word durations, while the out-of- domain trained language model does pre- dict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data. ",,,,ACL
75,2015,Efficient Methods for Inferring Large Sparse Topic Hierarchies,"Doug Downey, Chandra Bhagavatula, Yi Yang","Latent variable topic models such as La- tent Dirichlet Allocation (LDA) can dis- cover topics from text in an unsupervised fashion. However, scaling the models up to the many distinct topics exhibited in modern corpora is challenging. “Flat” topic models like LDA have difficulty modeling sparsely expressed topics, and richer hierarchical models become compu- tationally intractable as the number of top- ics increases. In this paper, we introduce efficient meth- ods for inferring large topic hierarchies. Our approach is built upon the Sparse Backoff Tree (SBT), a new prior for la- tent topic distributions that organizes the latent topics as leaves in a tree. We show how a document model based on SBTs can effectively infer accurate topic spaces of over a million topics. We introduce a collapsed sampler for the model that ex- ploits sparsity and the tree structure in or- der to make inference efficient. In exper- iments with multiple data sets, we show that scaling to large topic spaces results in much more accurate models, and that SBT document models make use of large topic spaces more effectively than flat LDA. ",,,,ACL
76,2015,Trans-dimensional Random Fields for Language Modeling,"Bin Wang, Zhijian Ou, Zhiqiang Tan","Language modeling (LM) involves determining the joint probability of words in a sentence. The conditional approach is dominant, representing the joint probability in terms of conditionals. Examples include n-gram LMs and neural network LMs. An alternative approach, called the random field (RF) approach, is used in whole-sentence maximum entropy (WSME) LMs. Although the RF approach has potential benefits, the empirical results of previous WSME models are not satisfactory. In this paper, we revisit the RF approach for language modeling, with a number of innovations. We propose a trans-dimensional RF (TDRF) model and develop a training algorithm using joint stochastic approximation and trans-dimensional mixture sampling. We perform speech recognition experiments on Wall Street Journal data, and find that our TDRF models lead to performances as good as the recurrent neural network LMs but are computationally more efficient in computing sentence probability. ",,,,ACL
77,2015,Gaussian LDA for Topic Models with Word Embeddings,"Rajarshi Das, Manzil Zaheer, Chris Dyer","Continuous space word embeddings learned from large, unstructured corpora have been shown to be effective at cap- turing semantic regularities in language. In this paper we replace LDA’s param- eterization of “topics” as categorical distributions over opaque word types with multivariate Gaussian distributions on the embedding space. This encourages the model to group words that are a priori known to be semantically related into topics. To perform inference, we introduce a fast collapsed Gibbs sampling algorithm based on Cholesky decom- positions of covariance matrices of the posterior predictive distributions. We fur- ther derive a scalable algorithm that draws samples from stale posterior predictive distributions and corrects them with a Metropolis–Hastings step. Using vectors learned from a domain-general corpus (English Wikipedia), we report results on two document collections (20-newsgroups and NIPS). Qualitatively, Gaussian LDA infers different (but still very sensible) topics relative to standard LDA. Quantita- tively, our technique outperforms existing models at dealing with OOV words in held-out documents. ",,,,ACL
78,2015,Pairwise Neural Machine Translation Evaluation,"Francisco Guzmán, Shafiq Joty, Lluís Màrquez, Preslav Nakov","We present a novel framework for ma- chine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypothe- ses and the reference, as well as between the two hypotheses. These compact repre- sentations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art. ",,,,ACL
79,2015,String-to-Tree Multi Bottom-up Tree Transducers,"Nina Seemann, Fabienne Braune, Andreas Maletti",We achieve significant improvements in several syntax-based machine translation experiments using a string-to-tree vari- ant of multi bottom-up tree transducers. Our new parameterized rule extraction al- gorithm extracts string-to-tree rules that can be discontiguous and non-minimal in contrast to existing algorithms for the tree-to-tree setting. The obtained models significantly outperform the string-to-tree component of the Moses framework in a large-scale empirical evaluation on several known translation tasks. Our linguistic analysis reveals the remarkable benefits of discontiguous and non-minimal rules. ,,,,ACL
80,2015,Non-linear Learning for Statistical Machine Translation,"Shujian Huang, Huadong Chen, Xin-Yu Dai, Jiajun Chen","Modern statistical machine translation (SMT) systems usually use a linear com- bination of features to model the quality of each translation hypothesis. The linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest fea- tures in an linear manner, which might limit the expressive power of the model and lead to a under-fit model on the cur- rent data. In this paper, we propose a non- linear modeling for the quality of transla- tion hypotheses based on neural networks, which allows more complex interaction between features. A learning framework is presented for training the non-linear mod- els. We also discuss possible heuristics in designing the network structure which may improve the non-linear learning per- formance. Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model. ",,,,ACL
81,2015,Unifying Bayesian Inference and Vector Space Models for Improved Decipherment,"Qing Dou, Ashish Vaswani, Kevin Knight, Chris Dyer","We introduce into Bayesian decipherment a base distribution derived from similari- ties of word embeddings. We use Dirich- let multinomial regression (Mimno and McCallum, 2012) to learn a mapping be- tween ciphertext and plaintext word em- beddings from non-parallel data. Exper- imental results show that the base dis- tribution is highly beneficial to decipher- ment, improving state-of-the-art decipher- ment accuracy from 45.8% to 67.4% for Spanish/English, and from 5.1% to 11.2% for Malagasy/English. ",,,,ACL
82,2015,Non-projective Dependency-based Pre-Reordering with Recurrent Neural Network for Machine Translation,"Antonio Valerio Miceli-Barone, Giuseppe Attardi","The quality of statistical machine translation performed with phrase based approaches can be increased by permuting the words in the source sentences in an order which resem- bles that of the target language. We propose a class of recurrent neu- ral models which exploit source-side dependency syntax features to re- order the words into a target-like or- der. We evaluate these models on the German-to-English and Italian-to- English language pairs, showing sig- nificant improvements over a phrase- based Moses baseline. We also com- pare with state of the art German-to- English pre-reordering rules, showing that our method obtains similar or bet- ter results. ",,,,ACL
83,2015,Detecting Deceptive Groups Using Conversations and Network Analysis,"Dian Yu, Yulia Tyshchuk, Heng Ji, William Wallace","Deception detection has been formulated as a supervised binary classification prob- lem on single documents. However, in daily life, millions of fraud cases involve detailed conversations between deceivers and victims. Deceivers may dynamically adjust their deceptive statements accord- ing to the reactions of victims. In addition, people may form groups and collaborate to deceive others. In this paper, we seek to identify deceptive groups from their con- versations. We propose a novel subgroup detection method that combines linguis- tic signals and signed network analysis for dynamic clustering. A social-elimination game called Killer Game is introduced as a case study 1 . Experimental results demon- strate that our approach significantly out- performs human voting and state-of-the- art subgroup detection methods at dynam- ically differentiating the deceptive groups from truth-tellers. ",,,,ACL
84,2015,WikiKreator: Improving Wikipedia Stubs Automatically,"Siddhartha Banerjee, Prasenjit Mitra","Stubs on Wikipedia often lack comprehen- sive information. The huge cost of edit- ing Wikipedia and the presence of only a limited number of active contributors curb the consistent growth of Wikipedia. In this work, we present WikiKreator, a system that is capable of generating content au- tomatically to improve existing stubs on Wikipedia. The system has two compo- nents. First, a text classifier built using topic distribution vectors is used to as- sign content from the web to various sec- tions on a Wikipedia article. Second, we propose a novel abstractive summariza- tion technique based on an optimization framework that generates section-specific summaries for Wikipedia stubs. Experi- ments show that WikiKreator is capable of generating well-formed informative con- tent. Further, automatically generated con- tent from our system have been appended to Wikipedia stubs and the content has been retained successfully proving the ef- fectiveness of our approach. ",,,,ACL
85,2015,Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes,"Chris Quirk, Raymond Mooney, Michel Galley","Using natural language to write programs is a touchstone problem for computational linguistics. We present an approach that learns to map natural-language descrip- tions of simple “if-then” rules to executable code. By training and testing on a large cor- pus of naturally-occurring programs (called “recipes”) and their natural language de- scriptions, we demonstrate the ability to effectively map language to code. We compare a number of semantic parsing ap- proaches on the highly noisy training data collected from ordinary users, and find that loosely synchronous systems perform best. ",,,,ACL
86,2015,Deep Questions without Deep Understanding,"Igor Labutov, Sumit Basu, Lucy Vanderwende","We develop an approach for generating deep (i.e, high-level) comprehension questions from novel text that bypasses the myriad challenges of creating a full se- mantic representation. We do this by de- composing the task into an ontology- crowd-relevance workflow, consisting of first representing the original text in a low-dimensional ontology, then crowd- sourcing candidate question templates aligned with that space, and finally rank- ing potentially relevant templates for a novel region of text. If ontological labels are not available, we infer them from the text. We demonstrate the effectiveness of this method on a corpus of articles from Wikipedia alongside human judgments, and find that we can generate relevant deep questions with a precision of over 85% while maintaining a recall of 70%. ",,,,ACL
87,2015,The NL2KR Platform for building Natural Language Translation Systems,"Nguyen Vo, Arindam Mitra, Chitta Baral","This paper presents the NL2KR platform to build systems that can translate text to different formal languages. It is freely- available 1 , customizable, and comes with an Interactive GUI support that is use- ful in the development of a translation system. Our key contribution is a user- friendly system based on an interactive multistage learning algorithm. This effec- tive algorithm employs Inverse-λ, Gener- alization and user provided dictionary to learn new meanings of words from sen- tences and their representations. Using the learned meanings, and the Generaliza- tion approach, it is able to translate new sentences. NL2KR is evaluated on two standard corpora, Jobs and GeoQuery and it exhibits state-of-the-art performance on both of them. ",,,,ACL
88,2015,Multiple Many-to-Many Sequence Alignment for Combining String-Valued Variables: A G2P Experiment,Steffen Eger,"We investigate multiple many-to-many alignments as a primary step in integrat- ing supplemental information strings in string transduction. Besides outlining DP based solutions to the multiple alignment problem, we detail an approximation of the problem in terms of multiple sequence segmentations satisfying a coupling con- straint. We apply our approach to boosting baseline G2P systems using homogeneous as well as heterogeneous sources of sup- plemental information. ",,,,ACL
89,2015,Tweet Normalization with Syllables,"Ke Xu, Yunqing Xia, Chin-Hui Lee","In this paper, we propose a syllable-based method for tweet normalization to study the cognitive process of non-standard word creation in social media. Assuming that syllable plays a fundamental role in forming the non-standard tweet words, we choose syllable as the basic unit and extend the conventional noisy channel model by incorporating the syllables to represent the word-to-word transitions at both word and syllable levels. The syllables are used in our method not only to suggest more candidates, but also to measure similarity between words. Novelty of this work is three-fold: First, to the best of our knowledge, this is an early attempt to explore syllables in tweet normalization. Second, our proposed normalization method relies on unlabeled samples, making it much easier to adapt our method to handle non-standard words in any period of history. And third, we conduct a series of experiments and prove that the proposed method is advantageous over the state-of-art solutions for tweet normalization. ",,,,ACL
90,2015,Improving Named Entity Recognition in Tweets via Detecting Non-Standard Words,"Chen Li, Yang Liu","Most previous work of text normalization on informal text made a strong assumption that the system has already known which tokens are non-standard words (NSW) and thus need normalization. However, this is not realistic. In this paper, we propose a method for NSW detection. In addi- tion to the information based on the dic- tionary, e.g., whether a word is out-of- vocabulary (OOV), we leverage novel in- formation derived from the normalization results for OOV words to help make deci- sions. Second, this paper investigates two methods using NSW detection results for named entity recognition (NER) in social media data. One adopts a pipeline strat- egy, and the other uses a joint decoding fashion. We also create a new data set with newly added normalization annota- tion beyond the existing named entity la- bels. This is the first data set with such annotation and we release it for research purpose. Our experiment results demon- strate the effectiveness of our NSW detec- tion method and the benefit of NSW detec- tion for NER. Our proposed methods per- form better than the state-of-the-art NER system. ",,,,ACL
91,2015,A Unified Kernel Approach for Learning Typed Sentence Rewritings,"Martin Gleize, Brigitte Grau","Many high level natural language process- ing problems can be framed as determin- ing if two given sentences are a rewrit- ing of each other. In this paper, we pro- pose a class of kernel functions, referred to as type-enriched string rewriting ker- nels, which, used in kernel-based machine learning algorithms, allow to learn sen- tence rewritings. Unlike previous work, this method can be fed external lexical se- mantic relations to capture a wider class of rewriting rules. It also does not assume preliminary syntactic parsing but is still able to provide a unified framework to cap- ture syntactic structure and alignments be- tween the two sentences. We experiment on three different natural sentence rewrit- ing tasks and obtain state-of-the-art results for all of them. ",,,,ACL
92,2015,Perceptually Grounded Selectional Preferences,"Ekaterina Shutova, Niket Tandon, Gerard de Melo","Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been tradi- tionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present the first SP learning method that si- multaneously draws knowledge from text, images and videos, using image and video descriptions to obtain visual features. Our results show that it outperforms linguistic and visual models in isolation, as well as the existing SP induction approaches. ",,,,ACL
93,2015,Joint Case Argument Identification for Japanese Predicate Argument Structure Analysis,"Hiroki Ouchi, Hiroyuki Shindo, Kevin Duh, Yuji Matsumoto","Existing methods for Japanese predicate argument structure (PAS) analysis identify case arguments of each predicate without considering interactions between the tar- get PAS and others in a sentence. How- ever, the argument structures of the pred- icates in a sentence are semantically re- lated to each other. This paper proposes new methods for Japanese PAS analysis to jointly identify case arguments of all predicates in a sentence by (1) modeling multiple PAS interactions with a bipar- tite graph and (2) approximately search- ing optimal PAS combinations. Perform- ing experiments on the NAIST Text Cor- pus, we demonstrate that our joint analysis methods substantially outperform a strong baseline and are comparable to previous work. ",,,,ACL
94,2015,Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model,"Nghia The Pham, Germán Kruszewski, Angeliki Lazaridou, Marco Baroni","We introduce C-PHRASE, a distributional semantic model that learns word repre- sentations by optimizing context predic- tion for phrases at all levels in a syntactic tree, from single words to full sentences. C-PHRASE outperforms the state-of-the- art C-BOW model on a variety of lexical tasks. Moreover, since C-PHRASE word vectors are induced through a composi- tional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sen- tence representations that rival those gen- erated by ad-hoc compositional models. ",,,,ACL
95,2015,Robust Subgraph Generation Improves Abstract Meaning Representation Parsing,"Keenon Werling, Gabor Angeli, Christopher D. Manning","Meaning Representation (AMR) is a representation for open- domain rich semantics, with potential use in fields like event extraction and machine translation. Node generation, typically done using a simple dictionary lookup, is currently an important limiting factor in AMR parsing. We propose a small set of actions that derive AMR subgraphs by transformations on spans of text, which allows for more robust learning of this stage. Our set of construction actions generalize better than the previous ap- proach, and can be learned with a sim- ple classifier. We improve on the previ- ous state-of-the-art result for AMR pars- ing, boosting end-to-end performance by 3 F 1 on both the LDC2013E117 and LDC2014T12 datasets. ",,,,ACL
96,2015,Environment-Driven Lexicon Induction for High-Level Instructions,"Dipendra Kumar Misra, Kejia Tao, Percy Liang, Ashutosh Saxena","We focus on the task of interpreting com- plex natural language instructions to a robot, in which we must ground high-level commands such as microwave the cup to low-level actions such as grasping. Pre- vious approaches that learn a lexicon dur- ing training have inadequate coverage at test time, and pure search strategies can- not handle the exponential search space. We propose a new hybrid approach that leverages the environment to induce new lexical entries at test time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to suc- cessfully ground new verbs such as dis- tribute, mix, arrange to complex logical forms, each containing up to four predi- cates. ",,,,ACL
97,2015,Structural Representations for Learning Relations between Pairs of Texts,"Simone Filice, Giovanni Da San Martino, Alessandro Moschitti","This paper studies the use of structural representations for learning relations be- tween pairs of short texts (e.g., sentences or paragraphs) of the kind: the second text answers to, or conveys exactly the same information of, or is implied by, the first text. Engineering effective features that can capture syntactic and semantic re- lations between the constituents compos- ing the target text pairs is rather complex. Thus, we define syntactic and semantic structures representing the text pairs and then apply graph and tree kernels to them for automatically engineering features in Support Vector Machines. We carry out an extensive comparative analysis of state- of-the-art models for this type of relational learning. Our findings allow for achiev- ing the highest accuracy in two differ- ent and important related tasks, i.e., Para- phrasing Identification and Textual Entail- ment Recognition. ",,,,ACL
98,2015,Learning Semantic Representations of Users and Products for Document Level Sentiment Classification,"Duyu Tang, Bing Qin, Ting Liu","Neural network methods have achieved promising results for sentiment classifica- tion of text. However, these models on- ly use semantics of texts, while ignoring users who express the sentiment and prod- ucts which are evaluated, both of which have great influences on interpreting the sentiment of text. In this paper, we ad- dress this issue by incorporating user- and product- level information into a neural network approach for document level sen- timent classification. Users and product- s are modeled using vector space mod- els, the representations of which capture important global clues such as individu- al preferences of users or overall quali- ties of products. Such global evidence in turn facilitates embedding learning pro- cedure at document level, yielding better text representations. By combining ev- idence at user-, product- and document- level in a unified neural framework, the proposed model achieves state-of-the-art performances on IMDB and Yelp dataset- s 1 . ",,,,ACL
99,2015,Towards Debugging Sentiment Lexicons,"Andrew Schneider, Eduard Dragut","Central to many sentiment analysis tasks are sentiment lexicons (SLs). SLs exhibit polarity inconsistencies. Previous work studied the problem of checking the con- sistency of an SL for the case when the en- tries have categorical labels (positive, neg- ative or neutral) and showed that it is NP- hard. In this paper, we address the more general problem, in which polarity tags take the form of a continuous distribution in the interval [0, 1]. We show that this problem is polynomial. We develop a gen- eral framework for addressing the consis- tency problem using linear programming (LP) theory. LP tools allow us to uncover inconsistencies efficiently, paving the way to building SL debugging tools. We show that previous work corresponds to 0-1 inte- ger programming, a particular case of LP. Our experimental studies show a strong correlation between polarity consistency in SLs and the accuracy of sentiment tag- ging in practice. ",,,,ACL
100,2015,"Sparse, Contextually Informed Models for Irony Detection: Exploiting User Communities, Entities and Sentiment","Byron C. Wallace, Do Kook Choe, Eugene Charniak","Automatically detecting verbal irony (roughly, sarcasm) in online content is important for many practical applications (e.g., sentiment detection), but it is dif- ficult. Previous approaches have relied predominantly on signal gleaned from word counts and grammatical cues. But such approaches fail to exploit the context in which comments are embedded. We thus propose a novel strategy for verbal irony classification that exploits contex- tual features, specifically by combining noun phrases and sentiment extracted from comments with the forum type (e.g., conservative or liberal) to which they were posted. We show that this approach improves verbal irony classifica- tion performance. Furthermore, because this method generates a very large feature space (and we expect predictive contextual features to be strong but few), we propose a mixed regularization strategy that places a sparsity-inducing 1 penalty on the contextual feature weights on top of the 2 penalty applied to all model coefficients. This increases model sparsity and reduces the variance of model performance. ",,,,ACL
101,2015,Sentence-level Emotion Classification with Label and Context Dependence,"Shoushan Li, Lei Huang, Rong Wang, Guodong Zhou","Predicting emotion categories, such as anger, joy, and anxiety, expressed by a sentence is challenging due to its inherent multi-label classification difficulty and data sparseness. In this paper, we address above two chal- lenges by incorporating the label dependence among the emotion labels and the context de- pendence among the contextual instances into a factor graph model. Specifically, we recast sentence-level emotion classification as a fac- tor graph inferring problem in which the label and context dependence are modeled as vari- ous factor functions. Empirical evaluation demonstrates the great potential and effective- ness of our proposed approach to sentence- level emotion classification. 1 ",,,,ACL
102,2015,Co-training for Semi-supervised Sentiment Classification Based on Dual-view Bags-of-words Representation,"Rui Xia, Cheng Wang, Xin-Yu Dai, Tao Li","A review text is normally represented as a bag-of-words (BOW) in sentiment clas- sification. Such a simplified BOW model has fundamental deficiencies in modeling some complex linguistic phenomena such as negation. In this work, we propose a dual-view co-training algorithm based on dual-view BOW representation for semi- supervised sentiment classification. In dual-view BOW, we automatically con- struct antonymous reviews and model a review text by a pair of bags-of-words with opposite views. We make use of the original and antonymous views in pairs, in the training, bootstrapping and test- ing process, all based on a joint observa- tion of two views. The experimental re- sults demonstrate the advantages of our ap- proach, in meeting the two co-training re- quirements, addressing the negation prob- lem, and enhancing the semi-supervised sentiment classification efficiency. ",,,,ACL
103,2015,Improving social relationships in face-to-face human-agent interactions: when the agent wants to know user’s likes and dislikes,"Caroline Langlet, Chloé Clavel","This paper tackles the issue of the detec- tion of user’s verbal expressions of likes and dislikes in a human-agent interaction. We present a system grounded on the theo- retical framework provided by (Martin and White, 2005) that integrates the interac- tion context by jointly processing agent’s and user’s utterances. It is designed as a rule-based and bottom-up process based on a symbolic representation of the struc- ture of the sentence. This article also describes the annotation campaign – car- ried out through Amazon Mechanical Turk – for the creation of the evaluation data- set. Finally, we present all measures for rating agreement between our system and the human reference and obtain agreement scores that are equal or higher than sub- stantial agreements. ",,,,ACL
104,2015,Learning Word Representations from Scarce and Noisy Data with Embedding Subspaces,"Ramon Astudillo, Silvio Amir, Wang Ling, Mário Silva","We investigate a technique to adapt unsu- pervised word embeddings to specific ap- plications, when only small and noisy la- beled datasets are available. Current meth- ods use pre-trained embeddings to initial- ize model parameters, and then use the la- beled data to tailor them for the intended task. However, this approach is prone to overfitting when the training is performed with scarce and noisy data. To overcome this issue, we use the supervised data to find an embedding subspace that fits the task complexity. All the word representa- tions are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset. This ap- proach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results. Here we show results improving those of the chal- lenge, as well as additional experiments in a Twitter Part-Of-Speech tagging task. ",,,,ACL
105,2015,Automatic Spontaneous Speech Grading: A Novel Feature Derivation Technique using the Crowd,"Vinay Shashidhar, Nishant Pandey, Varun Aggarwal","In this paper, we address the problem of evaluating spontaneous speech us- ing a combination of machine learning and crowdsourcing. Machine learning techniques inadequately solve the stated problem because automatic speaker- independent speech transcription is inaccurate. The features derived from it are also inaccurate and so is the machine learning model developed for speech evaluation. To address this, we post the task of speech transcription to a large community of online workers (crowd). We also get spoken English grades from the crowd. We achieve 95% transcription accuracy by combining transcriptions from multiple crowd workers. Speech and prosody features are derived by force aligning the speech samples on these highly accurate transcriptions. Addi- tionally, we derive surface and semantic level features directly from the transcrip- tion. To demonstrate the efficacy of our approach we performed experiments on an expert–graded speech sample of 319 adult non–native speakers. Using these features in a regression model, we are able achieve a Pearson correlation of 0.76 with expert grades, an accuracy much higher than any previously reported machine learning approach. Our approach has an accuracy that rivals that of expert agreement. This work is timely given the huge requirement of spoken English training and assessment. ",,,,ACL
106,2015,Driving ROVER with Segment-based ASR Quality Estimation,"Shahab Jalalvand, Matteo Negri, Daniele Falavigna, Marco Turchi","ROVER is a widely used method to combine the output of multiple auto- matic speech recognition (ASR) systems. Though effective, the basic approach and its variants suffer from potential draw- backs: i) their results depend on the order in which the hypotheses are used to feed the combination process, ii) when applied to combine long hypotheses, they disre- gard possible differences in transcription quality at local level, iii) they often rely on word confidence information. We address these issues by proposing a segment-based ROVER in which hypothesis ranking is obtained from a confidence-independent ASR quality estimation method. Our re- sults on English data from the IWSLT2012 and IWSLT2013 evaluation campaigns significantly outperform standard ROVER and approximate two strong oracles. ",,,,ACL
107,2015,A Hierarchical Neural Autoencoder for Paragraphs and Documents,"Jiwei Li, Thang Luong, Dan Jurafsky","Natural language generation of coherent long texts like paragraphs or longer doc- uments is a challenging problem for re- current networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long- short term memory) auto-encoder to pre- serve and reconstruct multi-sentence para- graphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph us- ing standard metrics like ROUGE and En- tity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coher- ence. While only a first step toward gener- ating coherent text units from neural mod- els, our work has the potential to signifi- cantly impact natural language generation and summarization 1 . ",,,,ACL
108,2015,Joint Dependency Parsing and Multiword Expression Tokenization,"Alexis Nasr, Carlos Ramisch, José Deulofeu, André Valli","Complex conjunctions and determiners are often considered as pretokenized units in parsing. This is not always realistic, since they can be ambiguous. We pro- pose a model for joint dependency parsing and multiword expressions identification, in which complex function words are rep- resented as individual tokens linked with morphological dependencies. Our graph- based parser includes standard second- order features and verbal subcategoriza- tion features derived from a syntactic lex- icon.We train it on a modified version of the French Treebank enriched with mor- phological dependencies. It recognizes 81.79% of ADV +que conjunctions with 91.57% precision, and 82.74% of de+ DET determiners with 86.70% precision. ",,,,ACL
109,2015,End-to-end learning of semantic role labeling using recurrent neural networks,"Jie Zhou, Wei Xu","Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems. To this date, most of the suc- cessful SRL systems were built on top of some form of parsing results (Koomen et al., 2005; Palmer et al., 2010; Pradhan et al., 2013), where pre-defined feature tem- plates over the syntactic structure are used. The attempts of building an end-to-end SRL learning system without using pars- ing were less successful (Collobert et al., 2011). In this work, we propose to use deep bi-directional recurrent network as an end-to-end system for SRL. We take on- ly original text information as input fea- ture, without using any syntactic knowl- edge. The proposed algorithm for seman- tic role labeling was mainly evaluated on CoNLL-2005 shared task and achieved F 1 score of 81.07. This result outperforms the previous state-of-the-art system from the combination of different parsing trees or models. We also obtained the same conclusion with F 1 = 81.27 on CoNLL- 2012 shared task. As a result of simplicity, our model is also computationally efficient that the parsing speed is 6.7k tokens per second. Our analysis shows that our model is better at handling longer sentences than traditional models. And the latent vari- ables of our model implicitly capture the syntactic structure of a sentence. ",,,,ACL
110,2015,Feature Optimization for Constituent Parsing via Neural Networks,"Zhiguo Wang, Haitao Mi, Nianwen Xue","The performance of discriminative con- stituent parsing relies crucially on feature engineering, and effective features usu- ally have to be carefully selected through a painful manual process. In this paper, we propose to automatically learn a set of effective features via neural networks. Specifically, we build a feedforward neu- ral network model, which takes as input a few primitive units (words, POS tags and certain contextual tokens) from the lo- cal context, induces the feature represen- tation in the hidden layer and makes pars- ing predictions in the output layer. The network simultaneously learns the feature representation and the prediction model parameters using a back propagation al- gorithm. By pre-training the model on a large amount of automatically parsed data, and then fine-tuning on the manually an- notated Treebank data, our parser achieves the highest F 1 score at 86.6% on Chi- nese Treebank 5.1, and a competitive F 1 score at 90.7% on English Treebank. More importantly, our parser generalizes well on cross-domain test sets, where we sig- nificantly outperform Berkeley parser by 3.4 points on average for Chinese and 2.5 points for English. ",,,,ACL
111,2015,Identifying Cascading Errors using Constraints in Dependency Parsing,"Dominick Ng, James R. Curran","Dependency parsers are usually evaluated on attachment accuracy. Whilst easily in- terpreted, the metric does not illustrate the cascading impact of errors, where the parser chooses an incorrect arc, and is sub- sequently forced to choose further incor- rect arcs elsewhere in the parse. We apply arc-level constraints to MST- parser and ZPar, enforcing the correct analysis of specific error classes, whilst otherwise continuing with decoding. We investigate the direct and indirect impact of applying constraints to the parser. Er- roneous NP and punctuation attachments cause the most cascading errors, while in- correct PP and coordination attachments are frequent but less influential. Punctu- ation is especially challenging, as it has long been ignored in parsing, and serves a variety of disparate syntactic roles. ",,,,ACL
112,2015,A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network,"Chenxi Zhu, Xipeng Qiu, Xinchi Chen, Xuanjing Huang","In this work, we address the prob- lem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntac- tic and compositional-semantic represen- tations of phrases and words in a depen- dency tree. Different with the original re- cursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most infor- mative compositions by the pooling lay- ers. Based on RCNN, we use a discrimina- tive model to re-rank a k-best list of can- didate dependency parsing trees. The ex- periments show that RCNN is very effec- tive to improve the state-of-the-art depen- dency parsing on both English and Chi- nese datasets. ",,,,ACL
113,2015,Transition-based Neural Constituent Parsing,"Taro Watanabe, Eiichiro Sumita","Constituent parsing is typically modeled by a chart-based algorithm under prob- abilistic context-free grammars or by a transition-based algorithm with rich fea- tures. Previous models rely heavily on richer syntactic information through lex- icalizing rules, splitting categories, or memorizing long histories. However en- riched models incur numerous parameters and sparsity issues, and are insufficient for capturing various syntactic phenomena. We propose a neural network structure that explicitly models the unbounded history of actions performed on the stack and queue employed in transition-based parsing, in addition to the representations of partially parsed tree structure. Our transition-based neural constituent parsing achieves perfor- mance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, with- out reranking, feature templates or addi- tional data to train model parameters. ",,,,ACL
114,2015,Feature Selection in Kernel Space: A Case Study on Dependency Parsing,"Xian Qian, Yang Liu","Given a set of basic binary features, we propose a new L 1 norm SVM based feature selection method that explicitly selects the features in their polynomial or tree kernel spaces. The efficiency comes from the anti-monotone property of the subgradients: the subgradient with respect to a combined feature can be bounded by the subgradient with respect to each of its component features, and a feature can be pruned safely without further consideration if its corresponding subgradient is not steep enough. We conduct experiments on the English dependency parsing task with a third order graph-based parser. Benefiting from the rich features selected in the tree kernel space, our model achieved the best reported unlabeled attachment score of 93.72 without using any additional resource. ",,,,ACL
115,2015,Semantic Role Labeling Improves Incremental Parsing,"Ioannis Konstas, Frank Keller","Incremental parsing is the task of assign- ing a syntactic structure to an input sen- tence as it unfolds word by word. Incre- mental parsing is more difficult than full- sentence parsing, as incomplete input in- creases ambiguity. Intuitively, an incre- mental parser that has access to seman- tic information should be able to reduce ambiguity by ruling out semantically im- plausible analyses, even for incomplete in- put. In this paper, we test this hypothesis by combining an incremental TAG parser with an incremental semantic role labeler in a discriminative framework. We show a substantial improvement in parsing per- formance compared to the baseline parser, both in full-sentence F-score and in incre- mental F-score. ",,,,ACL
116,2015,Discontinuous Incremental Shift-reduce Parsing,Wolfgang Maier,"We present an extension to incremental shift-reduce parsing that handles discon- tinuous constituents, using a linear clas- sifier and beam search. We achieve very high parsing speeds (up to 640 sent./sec.) and accurate results (up to 79.52 F 1 on TiGer). ",,,,ACL
117,2015,A Neural Probabilistic Structured-Prediction Model for Transition-Based Dependency Parsing,"Hao Zhou, Yue Zhang, Shujian Huang, Jiajun Chen","Neural probabilistic parsers are attrac- tive for their capability of automatic fea- ture combination and small data sizes. A transition-based greedy neural parser has given better accuracies over its lin- ear counterpart. We propose a neural probabilistic structured-prediction model for transition-based dependency parsing, which integrates search and learning. Beam search is used for decoding, and contrastive learning is performed for max- imizing the sentence-level log-likelihood. In standard Penn Treebank experiments, the structured neural parser achieves a 1.8% accuracy improvement upon a com- petitive greedy neural parser baseline, giv- ing performance comparable to the best linear parser. ",,,,ACL
118,2015,Parsing Paraphrases with Joint Inference,"Do Kook Choe, David McClosky","Treebanks are key resources for develop- ing accurate statistical parsers. However, building treebanks is expensive and time- consuming for humans. For domains re- quiring deep subject matter expertise such as law and medicine, treebanking is even more difficult. To reduce annotation costs for these domains, we develop methods to improve cross-domain parsing inference using paraphrases. Paraphrases are eas- ier to obtain than full syntactic analyses as they do not require deep linguistic knowl- edge, only linguistic fluency. A sentence and its paraphrase may have similar syn- tactic structures, allowing their parses to mutually inform each other. We present several methods to incorporate paraphrase information by jointly parsing a sentence with its paraphrase. These methods are ap- plied to state-of-the-art constituency and dependency parsers and provide signif- icant improvements across multiple do- mains. ",,,,ACL
119,2015,Cross-lingual Dependency Parsing Based on Distributed Representations,"Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng Wang","This paper investigates the problem of cross-lingual dependency parsing, aim- ing at inducing dependency parsers for low-resource languages while using only training data from a resource-rich lan- guage (e.g. English). Existing approaches typically don’t include lexical features, which are not transferable across lan- guages. In this paper, we bridge the lex- ical feature gap by using distributed fea- ture representations and their composition. We provide two algorithms for inducing cross-lingual distributed representations of words, which map vocabularies from two different languages into a common vector space. Consequently, both lexical features and non-lexical features can be used in our model for cross-lingual transfer. Furthermore, our framework is able to in- corporate additional useful features such as cross-lingual word clusters. Our com- bined contributions achieve an average rel- ative error reduction of 10.9% in labeled attachment score as compared with the delexicalized parser, trained on English universal treebank and transferred to three other languages. It also significantly out- performs McDonald et al. (2013) aug- mented with projected cluster features on identical data. ",,,,ACL
120,2015,Can Natural Language Processing Become Natural Language Coaching?,Marti A. Hearst,"How we teach and learn is undergoing a revolution, due to changes in technology and connectivity. Education may be one of the best application areas for advanced NLP techniques, and NLP researchers have much to contribute to this problem, especially in the areas of learning to write, mastery learning, and peer learning. In this paper I consider what happens when we convert natural language processors into natural language coaches. ",,,,ACL
121,2015,Machine Comprehension with Discourse Relations,"Karthik Narasimhan, Regina Barzilay","This paper proposes a novel approach for incorporating discourse information into machine comprehension applications. Traditionally, such information is com- puted using off-the-shelf discourse analyz- ers. This design provides limited oppor- tunities for guiding the discourse parser based on the requirements of the target task. In contrast, our model induces re- lations between sentences while optimiz- ing a task-specific objective. This ap- proach enables the model to benefit from discourse information without relying on explicit annotations of discourse structure during training. The model jointly iden- tifies relevant sentences, establishes rela- tions between them and predicts an an- swer. We implement this idea in a discrim- inative framework with hidden variables that capture relevant sentences and rela- tions unobserved during training. Our ex- periments demonstrate that the discourse aware model outperforms state-of-the-art machine comprehension systems. 1 ",,,,ACL
122,2015,Implicit Role Linking on Chinese Discourse: Exploiting Explicit Roles and Frame-to-Frame Relations,"Ru Li, Juan Wu, Zhiqiang Wang, Qinghua Chai","There is a growing interest in research- ing null instantiations, which are those implicit semantic arguments. Many of these implicit arguments can be linked to referents in context, and their discoveries are of great benefits to semantic process- ing. We address the issue of automat- ically identifying and resolving implicit arguments in Chinese discourse. For their resolutions, we present an approach that combines the information about overtly la- beled arguments and frame-to-frame rela- tions defined by FrameNet. Experimental results on our created corpus demonstrate the effectiveness of our approach. ",,,,ACL
123,2015,Discourse-sensitive Automatic Identification of Generic Expressions,"Annemarie Friedrich, Manfred Pinkal","This paper describes a novel sequence la- beling method for identifying generic ex- pressions, which refer to kinds or arbitrary members of a class, in discourse context. The automatic recognition of such expres- sions is important for any natural language processing task that requires text under- standing. Prior work has focused on iden- tifying generic noun phrases; we present a new corpus in which not only subjects but also clauses are annotated for generic- ity according to an annotation scheme mo- tivated by semantic theory. Our context- aware approach for automatically identi- fying generic expressions uses conditional random fields and outperforms previous work based on local decisions when evalu- ated on this corpus and on related data sets (ACE-2 and ACE-2005). ",,,,ACL
124,2015,Model-based Word Embeddings from Decompositions of Count Matrices,"Karl Stratos, Michael Collins, Daniel Hsu","This work develops a new statistical un- derstanding of word embeddings induced from transformed count data. Using the class of hidden Markov models (HMMs) underlying Brown clustering as a genera- tive model, we demonstrate how canoni- cal correlation analysis (CCA) and certain count transformations permit efficient and effective recovery of model parameters with lexical semantics. We further show in experiments that these techniques empir- ically outperform existing spectral meth- ods on word similarity and analogy tasks, and are also competitive with other pop- ular methods such as WORD2VEC and GLOVE. ",,,,ACL
125,2015,Entity Hierarchy Embedding,"Zhiting Hu, Poyao Huang, Yuntian Deng, Yingkai Gao","Existing distributed representations are limited in utilizing structured knowledge to improve semantic relatedness modeling. We propose a principled framework of em- bedding entities that integrates hierarchi- cal information from large-scale knowl- edge bases. The novel embedding model associates each category node of the hi- erarchy with a distance metric. To cap- ture structured semantics, the entity sim- ilarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and cat- egory distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. ",,,,ACL
126,2015,Orthogonality of Syntax and Semantics within Distributional Spaces,"Jeff Mitchell, Mark Steedman","A recent distributional approach to word- analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that per- formance on this task can be related to orthogonality within the space. Explic- itly designing such structure into a neu- ral network model results in represen- tations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within En- glish Wikipedia text to enable this de- composition can produce substantial im- provements on semantic-similarity, pos- induction and word-analogy tasks. ",,,,ACL
127,2015,Scalable Semantic Parsing with Partial Ontologies,"Eunsol Choi, Tom Kwiatkowski, Luke Zettlemoyer","We consider the problem of building scal- able semantic parsers for Freebase, and present a new approach for learning to do partial analyses that ground as much of the input text as possible without requiring that all content words be mapped to Freebase concepts. We study this problem on two newly introduced large-scale noun phrase datasets, and present a new semantic pars- ing model and semi-supervised learning approach for reasoning with partial onto- logical support. Experiments demonstrate strong performance on two tasks: refer- ring expression resolution and entity at- tribute extraction. In both cases, the par- tial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques. ",,,,ACL
128,2015,Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base,"Wen-tau Yih, Ming-Wei Chang, Xiaodong He, Jianfeng Gao","We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowl- edge base and can be directly mapped to a logical form. Semantic parsing is re- duced to query graph generation, formu- lated as a staged search problem. Unlike traditional approaches, our method lever- ages the knowledge base in an early stage to prune the search space and thus simpli- fies the semantic matching problem. By applying an advanced entity linking sys- tem and a deep convolutional neural net- work model that matches questions and predicate sequences, our system outper- forms previous methods substantially, and achieves an F 1 measure of 52.5% on the W EB Q UESTIONS dataset. ",,,,ACL
129,2015,Building a Semantic Parser Overnight,"Yushi Wang, Jonathan Berant, Percy Liang","How do we build a semantic parser in a new domain starting with zero training ex- amples? We introduce a new methodol- ogy for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canon- ical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We fur- ther study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate se- mantic parser in just a few hours. ",,,,ACL
130,2015,Predicting Polarities of Tweets by Composing Word Embeddings with Long Short-Term Memory,"Xin Wang, Yuanchao Liu, Chengjie Sun, Baoxun Wang","In this paper, we introduce Long Short- Term Memory (LSTM) recurrent network for twitter sentiment prediction. With the help of gates and constant error carousels in the memory block structure, the model could handle interactions between words through a flexible compositional function. Experiments on a public noisy labelled data show that our model outperforms sev- eral feature-engineering approaches, with the result comparable to the current best data-driven technique. According to the evaluation on a generated negation phrase test set, the proposed architecture dou- bles the performance of non-neural model based on bag-of-word features. Further- more, words with special functions (such as negation and transition) are distin- guished and the dissimilarities of words with opposite sentiment are magnified. An interesting case study on negation expres- sion processing shows a promising poten- tial of the architecture dealing with com- plex sentiment phrases. ",,,,ACL
131,2015,Topic Modeling based Sentiment Analysis on Social Media for Stock Market Prediction,"Thien Hai Nguyen, Kiyoaki Shirai","The goal of this research is to build a model to predict stock price movement us- ing sentiments on social media. A new feature which captures topics and their sentiments simultaneously is introduced in the prediction model. In addition, a new topic model TSLDA is proposed to obtain this feature. Our method outperformed a model using only historical prices by about 6.07% in accuracy. Furthermore, when comparing to other sentiment anal- ysis methods, the accuracy of our method was also better than LDA and JST based methods by 6.43% and 6.07%. The results show that incorporation of the sentiment information from social media can help to improve the stock prediction. ",,,,ACL
132,2015,Learning Tag Embeddings and Tag-specific Composition Functions in Recursive Neural Network,"Qiao Qian, Bo Tian, Minlie Huang, Yang Liu","Recursive neural network is one of the most successful deep learning models for natural language processing due to the compositional nature of text. The model recursively composes the vector of a parent phrase from those of child words or phrases, with a key compo- nent named composition function. Al- though a variety of composition func- tions have been proposed, the syntactic information has not been fully encoded in the composition process. We pro- pose two models, Tag Guided RNN (TG- RNN for short) which chooses a compo- sition function according to the part-of- speech tag of a phrase, and Tag Embedded RNN/RNTN (TE-RNN/RNTN for short) which learns tag embeddings and then combines tag and word embeddings to- gether. In the fine-grained sentiment classification, experiment results show the proposed models obtain remarkable improvement: TG-RNN/TE-RNN obtain remarkable improvement over baselines, TE-RNTN obtains the second best result among all the top performing models, and all the proposed models have much less parameters/complexity than their counter- parts. ",,,,ACL
133,2015,A convex and feature-rich discriminative approach to dependency grammar induction,"Édouard Grave, Noémie Elhadad","In this paper, we introduce a new method for the problem of unsupervised depen- dency parsing. Most current approaches are based on generative models. Learning the parameters of such models relies on solving a non-convex optimization prob- lem, thus making them sensitive to initial- ization. We propose a new convex formu- lation to the task of dependency grammar induction. Our approach is discriminative, allowing the use of different kinds of fea- tures. We describe an efficient optimiza- tion algorithm to learn the parameters of our model, based on the Frank-Wolfe algo- rithm. Our method can easily be general- ized to other unsupervised learning prob- lems. We evaluate our approach on ten languages belonging to four different fam- ilies, showing that our method is competi- tive with other state-of-the-art methods. ",,,,ACL
134,2015,Parse Imputation for Dependency Annotations,"Jason Mielens, Liang Sun, Jason Baldridge","Syntactic annotation is a hard task, but it can be made easier by allowing annotators flexibility to leave aspects of a sentence underspecified. Unfortunately, partial an- notations are not typically directly usable for training parsers. We describe a method for imputing missing dependencies from sentences that have been partially anno- tated using the Graph Fragment Language, such that a standard dependency parser can then be trained on all annotations. We show that this strategy improves perfor- mance over not using partial annotations for English, Chinese, Portuguese and Kin- yarwanda, and that performance competi- tive with state-of-the-art unsupervised and weakly-supervised parsers can be reached with just a few hours of annotation. ",,,,ACL
135,2015,Probing the Linguistic Strengths and Limitations of Unsupervised Grammar Induction,"Yonatan Bisk, Julia Hockenmaier","Work in grammar induction should help shed light on the amount of syntactic struc- ture that is discoverable from raw word or tag sequences. But since most cur- rent grammar induction algorithms pro- duce unlabeled dependencies, it is diffi- cult to analyze what types of constructions these algorithms can or cannot capture, and, therefore, to identify where additional supervision may be necessary. This pa- per provides an in-depth analysis of the errors made by unsupervised CCG parsers by evaluating them against the labeled de- pendencies in CCGbank, hinting at new research directions necessary for progress in grammar induction. ",,,,ACL
136,2015,Entity-Centric Coreference Resolution with Model Stacking,"Kevin Clark, Christopher D. Manning","Mention pair models that predict whether or not two mentions are coreferent have historically been very effective for coref- erence resolution, but do not make use of entity-level information. However, we show that the scores produced by such models can be aggregated to define pow- erful entity-level features between clusters of mentions. Using these features, we train an entity-centric coreference system that learns an effective policy for building up coreference chains incrementally. The mention pair scores are also used to prune the search space the system works in, al- lowing for efficient training with an exact loss function. We evaluate our system on the English portion of the 2012 CoNLL Shared Task dataset and show that it im- proves over the current state of the art. ",,,,ACL
137,2015,Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution,"Sam Wiseman, Alexander M. Rush, Stuart Shieber, Jason Weston","We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representa- tions, and we report the best overall score on the CoNLL 2012 English test set to date. ",,,,ACL
138,2015,Transferring Coreference Resolvers with Posterior Regularization,André F. T. Martins,"We propose a cross-lingual framework for learning coreference resolvers for resource-poor target languages, given a re- solver in a source language. Our method uses word-aligned bitext to project infor- mation from the source to the target. To handle task-specific costs, we propose a softmax-margin variant of posterior regu- larization, and we use it to achieve robust- ness to projection errors. We show empir- ically that this strategy outperforms com- petitive cross-lingual methods, such as delexicalized transfer with bilingual word embeddings, bitext direct projection, and vanilla posterior regularization. ",,,,ACL
139,2015,Tea Party in the House: A Hierarchical Ideal Point Topic Model and Its Application to Republican Legislators in the 112th Congress,"Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik, Kristina Miler","We introduce the Hierarchical Ideal Point Topic Model, which provides a rich picture of policy issues, framing, and voting behav- ior using a joint model of votes, bill text, and the language that legislators use when debating bills. We use this model to look at the relationship between Tea Party Repub- licans and “establishment” Republicans in the U.S. House of Representatives during the 112 th Congress. ",,,,ACL
140,2015,"KB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations, and Facts","Dana Movshovitz-Attias, William W. Cohen","Many existing knowledge bases (KBs), in- cluding Freebase, Yago, and NELL, rely on a fixed ontology, given as an input to the system, which defines the data to be cataloged in the KB, i.e., a hierar- chy of categories and relations between them. The system then extracts facts that match the predefined ontology. We pro- pose an unsupervised model that jointly learns a latent ontological structure of an input corpus, and identifies facts from the corpus that match the learned structure. Our approach combines mixed member- ship stochastic block models and topic models to infer a structure by jointly mod- eling text, a latent concept hierarchy, and latent semantic relationships among the entities mentioned in the text. As a case study, we apply the model to a corpus of Web documents from the software do- main, and evaluate the accuracy of the var- ious components of the learned ontology. ",,,,ACL
141,2015,A Computationally Efficient Algorithm for Learning Topical Collocation Models,"Zhendong Zhao, Lan Du, Benjamin Börschinger, John K Pate","Most existing topic models make the bag- of-words assumption that words are gener- ated independently, and so ignore poten- tially useful information about word or- der. Previous attempts to use collocations (short sequences of adjacent words) in topic models have either relied on a pipe- line approach, restricted attention to bi- grams, or resulted in models whose infer- ence does not scale to large corpora. This paper studies how to simultaneously learn both collocations and their topic assign- ments. We present an efficient reformula- tion of the Adaptor Grammar-based topi- cal collocation model (AG-colloc) (John- son, 2010), and develop a point-wise sam- pling algorithm for posterior inference in this new formulation. We further improve the efficiency of the sampling algorithm by exploiting sparsity and parallelising in- ference. Experimental results derived in text classification, information retrieval and human evaluation tasks across a range of datasets show that this reformulation scales to hundreds of thousands of docu- ments while maintaining the good perfor- mance of the AG-colloc model. ",,,,ACL
142,2015,Compositional Semantic Parsing on Semi-Structured Tables,"Panupong Pasupat, Percy Liang","Two important aspects of semantic pars- ing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured ta- bles using question-answer pairs as super- vision. The central challenge arises from two compounding factors: the broader do- main results in an open-ended set of re- lations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improve- ments over natural baselines. For evalua- tion, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available. ",,,,ACL
143,2015,Graph parsing with s-graph grammars,"Jonas Groschwitz, Alexander Koller, Christoph Teichmann","A key problem in semantic parsing with graph-based semantic representations is graph parsing, i.e. computing all pos- sible analyses of a given graph accord- ing to a grammar. This problem arises in training synchronous string-to-graph grammars, and when generating strings from them. We present two algorithms for graph parsing (bottom-up and top-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementa- tions outperform the best previous system by several orders of magnitude. ",,,,ACL
144,2015,Sparse Overcomplete Word Vector Representations,"Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer","Current distributed representations of words show little resemblance to theo- ries of lexical semantics. The former are dense and uninterpretable, the lat- ter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We pro- pose methods that transform word vec- tors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are dis- covered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we find that they out- perform the original vectors on benchmark tasks. ",,,,ACL
145,2015,Learning Semantic Word Embeddings based on Ordinal Knowledge Constraints,"Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling","In this paper, we propose a general frame- work to incorporate semantic knowledge into the popular data-driven learning pro- cess of word embeddings to improve the quality of them. Under this framework, we represent semantic knowledge as many ordinal ranking inequalities and formu- late the learning of semantic word embed- dings (SWE) as a constrained optimiza- tion problem, where the data-derived ob- jective function is optimized subject to all ordinal knowledge inequality constraints extracted from available knowledge re- sources such as Thesaurus and Word- Net. We have demonstrated that this con- strained optimization problem can be ef- ficiently solved by the stochastic gradient descent (SGD) algorithm, even for a large number of inequality constraints. Experi- mental results on four standard NLP tasks, including word similarity measure, sen- tence completion, name entity recogni- tion, and the TOEFL synonym selection, have all demonstrated that the quality of learned word vectors can be significantly improved after semantic knowledge is in- corporated as inequality constraints during the learning process of word embeddings. ",,,,ACL
146,2015,Adding Semantics to Data-Driven Paraphrasing,"Ellie Pavlick, Johan Bos, Malvina Nissim, Charley Beller","We add an interpretable semantics to the paraphrase database (PPDB). To date, the relationship between phrase pairs in the database has been weakly de- fined as approximately equivalent. We show that these pairs represent a vari- ety of relations, including directed entail- ment (little girl/girl) and exclusion (no- body/someone). We automatically assign semantic entailment relations to entries in PPDB using features derived from past work on discovering inference rules from text and semantic taxonomy induction. We demonstrate that our model assigns these relations with high accuracy. In a down- stream RTE task, our labels rival relations from WordNet and improve the coverage of a proof-based RTE system by 17%. ",,,,ACL
147,2015,Parsing as Reduction,"Daniel Fernández-González, André F. T. Martins","We reduce phrase-based parsing to depen- dency parsing. Our reduction is grounded on a new intermediate representation, “head-ordered dependency trees,” shown to be isomorphic to constituent trees. By encoding order information in the depen- dency labels, we show that any off-the- shelf, trainable dependency parser can be used to produce constituents. When this parser is non-projective, we can perform discontinuous parsing in a very natural manner. Despite the simplicity of our ap- proach, experiments show that the result- ing parsers are on par with strong base- lines, such as the Berkeley parser for En- glish and the best non-reranking system in the SPMRL-2014 shared task. Results are particularly striking for discontinuous parsing of German, where we surpass the current state of the art by a wide margin. ",,,,ACL
148,2015,Optimal Shift-Reduce Constituent Parsing with Structured Perceptron,"Le Quang Thang, Hiroshi Noji, Yusuke Miyao","We present a constituent shift-reduce parser with a structured perceptron that finds the optimal parse in a practical run- time. The key ideas are new feature tem- plates that facilitate state merging of dy- namic programming and A* search. Our system achieves 91.1 F1 on a standard English experiment, a level which cannot be reached by other beam-based systems even with large beam sizes. 1 ",,,,ACL
149,2015,"A Data-Driven, Factorization Parser for CCG Dependency Structures","Yantao Du, Weiwei Sun, Xiaojun Wan","This paper is concerned with building CCG-grounded, semantics-oriented deep dependency structures with a data-driven, factorization model. Three types of fac- torization together with different higher- order features are designed to capture different syntacto-semantic properties of functor-argument dependencies. Integrat- ing heterogeneous factorizations results in intractability in decoding. We pro- pose a principled method to obtain opti- mal graphs based on dual decomposition. Our parser obtains an unlabeled f-score of 93.23 on the CCGBank data, resulting in an error reduction of 6.5% over the best published result. which yields a signifi- cant improvement over the best published result in the literature. Our implementa- tion is available at http://www.icst. pku.edu.cn/lcwm/grass. ",,,,ACL
150,2015,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,"Kai Sheng Tai, Richard Socher, Christopher D. Manning","Because of their superior ability to pre- serve sequence information over time, Long Short-Term Memory (LSTM) net- works, a type of recurrent neural net- work with a more complex computational unit, have obtained strong results on a va- riety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syn- tactic properties that would naturally com- bine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree- LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Senti- ment Treebank). ",,,,ACL
151,2015,genCNN: A Convolutional Architecture for Word Sequence Prediction,"Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang","We propose a convolutional neural net- work, named gen CNN, for word se- quence prediction. Different from previous work on neural network- based language modeling and genera- tion (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. In- stead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feed- forward networks for language mod- eling, our model can effectively fuse the local correlation and global cor- relation in the word sequence, with a convolution-gating strategy specifi- cally designed for the task. We argue that our model can give adequate rep- resentation of the history, and there- fore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and read- ily parallelized. Our extensive exper- iments on text generation and n-best re-ranking in machine translation show that gen CNN outperforms the state-of- the-arts with big margins. ",,,,ACL
152,2015,Neural Responding Machine for Short-Text Conversation,"Lifeng Shang, Zhengdong Lu, Hang Li","We propose Neural Responding Ma- chine (NRM), a neural network-based re- sponse generator for Short-Text Conver- sation. NRM takes the general encoder- decoder framework: it formalizes the gen- eration of response as a decoding process based on the latent representation of the in- put text, while both encoding and decod- ing are realized with recurrent neural net- works (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging ser- vice. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state- of-the-arts in the same setting, including retrieval-based and SMT-based models. ",,,,ACL
153,2015,Abstractive Multi-Document Summarization via Phrase Selection and Merging,"Lidong Bing, Piji Li, Yi Liao, Wai Lam","We propose an abstraction-based multi- document summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sen- tences, namely, noun/verb phrases. Dif- ferent from existing abstraction-based ap- proaches, our method first constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to max- imize the salience of phrases and mean- while satisfy the sentence construction constraints. We employ integer linear op- timization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-of- the-art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation. ",,,,ACL
154,2015,Joint Graphical Models for Date Selection in Timeline Summarization,"Giang Tran, Eelco Herder, Katja Markert","Automatic timeline summarization (TLS) generates precise, dated overviews over (often prolonged) events, such as wars or economic crises. One subtask of TLS se- lects the most important dates for an event within a certain time frame. Date selec- tion has up to now been handled via su- pervised machine learning approaches that estimate the importance of each date sepa- rately, using features such as the frequency of date mentions in news corpora. This ap- proach neglects interactions between dif- ferent dates that occur due to connections between subevents. We therefore suggest a joint graphical model for date selection. Even unsupervised versions of this model perform as well as supervised state-of-the- art approaches. With parameter tuning on training data, it outperforms prior super- vised models by a considerable margin. ",,,,ACL
155,2015,Predicting Salient Updates for Disaster Summarization,"Chris Kedzie, Kathleen McKeown, Fernando Diaz","During crises such as natural disasters or other human tragedies, information needs of both civilians and responders often re- quire urgent, specialized treatment. Moni- toring and summarizing a text stream dur- ing such an event remains a difficult prob- lem. We present a system for update sum- marization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence se- lection, increasing the quality of the up- dates. We use novel, disaster-specific features for salience prediction, including geo-locations and language models repre- senting the language of disaster. Our eval- uation on a standard set of retrospective events using ROUGE shows that salience prediction provides a significant improve- ment over other approaches. ",,,,ACL
156,2015,Unsupervised Prediction of Acceptability Judgements,"Jey Han Lau, Alexander Clark, Shalom Lappin","In this paper we present the task of un- supervised prediction of speakers’ accept- ability judgements. We use a test set generated from the British National Cor- pus ( BNC ) containing both grammatical sentences and sentences containing a va- riety of syntactic infelicities introduced by round trip machine translation. This set was annotated for acceptability judge- ments through crowd sourcing. We trained a variety of unsupervised language mod- els on the original BNC , and tested them to see the extent to which they could pre- dict mean speakers’ judgements on the test set. To map probability to acceptability, we experimented with several normalisa- tion functions to neutralise the effects of sentence length and word frequencies. We found encouraging results with the unsu- pervised models predicting acceptability across two different datasets. Our method- ology is highly portable to other domains and languages, and the approach has po- tential implications for the representation and the acquisition of linguistic knowl- edge. ",,,,ACL
157,2015,A Frame of Mind: Using Statistical Models for Detection of Framing and Agenda Setting Campaigns,"Oren Tsur, Dan Calacci, David Lazer","Framing is a sophisticated form of dis- course in which the speaker tries to in- duce a cognitive bias through consis- tent linkage between a topic and a spe- cific context (frame). We build on po- litical science and communication theory and use probabilistic topic models com- bined with time series regression analy- sis (autoregressive distributed-lag models) to gain insights about the language dy- namics in the political processes. Pro- cessing four years of public statements is- sued by members of the U.S. Congress, our results provide a glimpse into the com- plex dynamic processes of framing, atten- tion shifts and agenda setting, commonly known as ‘spin’. We further provide new evidence for the divergence in party disci- pline in U.S. politics. ",,,,ACL
158,2015,Why discourse affects speakers’ choice of referring expressions,"Naho Orita, Eliana Vornov, Naomi Feldman, Hal Daumé III","We propose a language production model that uses dynamic discourse information to account for speakers’ choices of refer- ring expressions. Our model extends pre- vious rational speech act models (Frank and Goodman, 2012) to more naturally dis- tributed linguistic data, instead of assuming a controlled experimental setting. Simula- tions show a close match between speakers’ utterances and model predictions, indicat- ing that speakers’ behavior can be modeled in a principled way by considering the prob- abilities of referents in the discourse and the information conveyed by each word. ",,,,ACL
159,2015,Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy Game,"Vlad Niculae, Srijan Kumar, Jordan Boyd-Graber, Cristian Danescu-Niculescu-Mizil","Interpersonal relations are fickle, with close friendships often dissolving into en- mity. In this work, we explore linguis- tic cues that presage such transitions by studying dyadic interactions in an on- line strategy game where players form al- liances and break those alliances through betrayal. We characterize friendships that are unlikely to last and examine temporal patterns that foretell betrayal. We reveal that subtle signs of imminent betrayal are encoded in the conversational patterns of the dyad, even if the victim is not aware of the relationship’s fate. In particular, we find that lasting friend- ships exhibit a form of balance that man- ifests itself through language. In contrast, sudden changes in the balance of certain conversational attributes—such as positive sentiment, politeness, or focus on future planning—signal impending betrayal. ",,,,ACL
160,2015,Who caught a cold ? - Identifying the subject of a symptom,"Shin Kanouchi, Mamoru Komachi, Naoaki Okazaki, Eiji Aramaki","The development and proliferation of so- cial media services has led to the emer- gence of new approaches for surveying the population and addressing social issues. One popular application of social media data is health surveillance, e.g., predicting the outbreak of an epidemic by recogniz- ing diseases and symptoms from text mes- sages posted on social media platforms. In this paper, we propose a novel task that is crucial and generic from the viewpoint of health surveillance: estimating a sub- ject (carrier) of a disease or symptom men- tioned in a Japanese tweet. By designing an annotation guideline for labeling the subject of a disease/symptom in a tweet, we perform annotations on an existing cor- pus for public surveillance. In addition, we present a supervised approach for pre- dicting the subject of a disease/symptom. The results of our experiments demon- strate the impact of subject identification on the effective detection of an episode of a disease/symptom. Moreover, the results suggest that our task is independent of the type of disease/symptom. ",,,,ACL
161,2015,Weakly Supervised Role Identification in Teamwork Interactions,"Diyi Yang, Miaomiao Wen, Carolyn Rosé","In this paper, we model conversational roles in terms of distributions of turn level behaviors, including conversation acts and stylistic markers, as they occur over the whole interaction. This work presents a lightly supervised approach to inducing role definitions over sets of contributions within an extended interaction, where the supervision comes in the form of an out- come measure from the interaction. The identified role definitions enable a map- ping from behavior profiles of each par- ticipant in an interaction to limited sized feature vectors that can be used effectively to predict the teamwork outcome. An em- pirical evaluation applied to two Massive Open Online Course (MOOCs) datasets demonstrates that this approach yields su- perior performance in learning representa- tions for predicting the teamwork outcome over several baselines. ",,,,ACL
162,2015,Deep Unordered Composition Rivals Syntactic Methods for Text Classification,"Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, Hal Daumé III","Many existing deep learning models for natural language processing tasks focus on learning the compositionality of their in- puts, which requires many expensive com- putations. We present a simple deep neural network that competes with and, in some cases, outperforms such models on sen- timent analysis and factoid question an- swering tasks while taking only a fraction of the training time. While our model is syntactically-ignorant, we show significant improvements over previous bag-of-words models by deepening our network and ap- plying a novel variant of dropout. More- over, our model performs better than syn- tactic models on datasets with high syn- tactic variance. We show that our model makes similar errors to syntactically-aware models, indicating that for the tasks we con- sider, nonlinearly transforming the input is more important than tailoring a network to incorporate word order and syntax. ",,,,ACL
163,2015,SOLAR: Scalable Online Learning Algorithms for Ranking,"Jialei Wang, Ji Wan, Yongdong Zhang, Steven Hoi","Traditional learning to rank methods learn ranking models from training data in a batch and offline learning mode, which suffers from some critical limitations, e.g., poor scalability as the model has to be re- trained from scratch whenever new train- ing data arrives. This is clearly non- scalable for many real applications in practice where training data often arrives sequentially and frequently. To overcome the limitations, this paper presents SO- LAR — a new framework of Scalable On- line Learning Algorithms for Ranking, to tackle the challenge of scalable learning to rank. Specifically, we propose two novel SOLAR algorithms and analyze their IR measure bounds theoretically. We conduct extensive empirical studies by comparing our SOLAR algorithms with conventional learning to rank algorithms on benchmark testbeds, in which promising results vali- date the efficacy and scalability of the pro- posed novel SOLAR algorithms. ",,,,ACL
164,2015,Text Categorization as a Graph Classification Problem,"François Rousseau, Emmanouil Kiagias, Michalis Vazirgiannis","In this paper, we consider the task of text categorization as a graph classifica- tion problem. By representing textual doc- uments as graph-of-words instead of his- torical n-gram bag-of-words, we extract more discriminative features that corre- spond to long-distance n-grams through frequent subgraph mining. Moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its dens- est part – its main core – speeding up the feature extraction step for little to no cost in prediction performances. Experiments on four standard text classification datasets show statistically significant higher accu- racy and macro-averaged F1-score com- pared to baseline approaches. ",,,,ACL
165,2015,Inverted indexing for cross-lingual NLP,"Anders Søgaard, Željko Agić, Héctor Martínez Alonso, Barbara Plank","We present a novel, count-based approach to obtaining inter-lingual word represen- tations based on inverted indexing of Wikipedia. We present experiments ap- plying these representations to 17 datasets in document classification, POS tagging, dependency parsing, and word alignment. Our approach has the advantage that it is simple, computationally efficient and almost parameter-free, and, more im- portantly, it enables multi-source cross- lingual learning. In 14/17 cases, we im- prove over using state-of-the-art bilingual embeddings. ",,,,ACL
166,2015,Multi-Task Learning for Multiple Language Translation,"Daxiang Dong, Hua Wu, Wei He, Dianhai Yu","In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available. ",,,,ACL
167,2015,Accurate Linear-Time Chinese Word Segmentation via Embedding Matching,"Jianqiang Ma, Erhard Hinrichs","This paper proposes an embedding match- ing approach to Chinese word segmenta- tion, which generalizes the traditional se- quence labeling framework and takes ad- vantage of distributed representations. The training and prediction algorithms have linear-time complexity. Based on the pro- posed model, a greedy segmenter is de- veloped and evaluated on benchmark cor- pora. Experiments show that our greedy segmenter achieves improved results over previous neural network-based word seg- menters, and its performance is competi- tive with state-of-the-art methods, despite its simple feature set and the absence of ex- ternal resources for training. ",,,,ACL
168,2015,Gated Recursive Neural Network for Chinese Word Segmentation,"Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Xuanjing Huang","Recently, neural network models for natu- ral language processing tasks have been in- creasingly focused on for their ability of al- leviating the burden of manual feature en- gineering. However, the previous neural models cannot extract the complicated fea- ture compositions as the traditional meth- ods with discrete features. In this paper, we propose a gated recursive neural net- work (GRNN) for Chinese word segmen- tation, which contains reset and update gates to incorporate the complicated com- binations of the context characters. Since GRNN is relative deep, we also use a supervised layer-wise training method to avoid the problem of gradient diffusion. Experiments on the benchmark datasets show that our model outperforms the pre- vious neural network models as well as the state-of-the-art methods. ",,,,ACL
169,2015,An analysis of the user occupational class through Twitter content,"Daniel Preoţiuc-Pietro, Vasileios Lampos, Nikolaos Aletras","Social media content can be used as a complementary source to the traditional methods for extracting and studying col- lective social attributes. This study focuses on the prediction of the occupational class for a public user profile. Our analysis is conducted on a new annotated corpus of Twitter users, their respective job titles, posted textual content and platform-related attributes. We frame our task as classifi- cation using latent feature representations such as word clusters and embeddings. The employed linear and, especially, non-linear methods can predict a user’s occupational class with strong accuracy for the coars- est level of a standard occupation taxon- omy which includes nine classes. Com- bined with a qualitative assessment, the derived results confirm the feasibility of our approach in inferring a new user at- tribute that can be embedded in a multitude of downstream applications. ",,,,ACL
170,2015,Tracking unbounded Topic Streams,"Dominik Wurzer, Victor Lavrenko, Miles Osborne","Tracking topics on social media streams is non-trivial as the number of topics men- tioned grows without bound. This com- plexity is compounded when we want to track such topics against other fast mov- ing streams. We go beyond traditional small scale topic tracking and consider a stream of topics against another document stream. We introduce two tracking ap- proaches which are fully applicable to true streaming environments. When tracking 4.4 million topics against 52 million doc- uments in constant time and space, we demonstrate that counter to expectations, simple single-pass clustering can outper- form locality sensitive hashing for nearest neighbour search on streams. ",,,,ACL
171,2015,Inducing Word and Part-of-Speech with Pitman-Yor Hidden Semi-Markov Models,"Kei Uchiumi, Hiroshi Tsukahara, Daichi Mochihashi","We propose a nonparametric Bayesian model for joint unsupervised word seg- mentation and part-of-speech tagging from raw strings. Extending a previous model for word segmentation, our model is called a Pitman-Yor Hidden Semi- Markov Model (PYHSMM) and consid- ered as a method to build a class n-gram language model directly from strings, while integrating character and word level information. Experimental results on stan- dard datasets on Japanese, Chinese and Thai revealed it outperforms previous re- sults to yield the state-of-the-art accura- cies. This model will also serve to analyze a structure of a language whose words are not identified a priori. ",,,,ACL
172,2015,Coupled Sequence Labeling on Heterogeneous Annotations: POS Tagging as a Case Study,"Zhenghua Li, Jiayuan Chao, Min Zhang, Wenliang Chen","In order to effectively utilize multiple datasets with heterogeneous annotations, this paper proposes a coupled sequence labeling model that can directly learn and infer two heterogeneous annotations simultaneously, and to facilitate discussion we use Chinese part-of- speech (POS) tagging as our case study. The key idea is to bundle two sets of POS tags together (e.g. “ [NN, n ]”), and build a conditional random field (CRF) based tagging model in the enlarged space of bundled tags with the help of ambiguous labelings. To train our model on two non-overlapping datasets that each has only one-side tags, we transform a one-side tag into a set of bundled tags by considering all possible mappings at the missing side and derive an objective function based on ambiguous labelings. The key advantage of our coupled model is to provide us with the flexibility of 1) incorporating joint features on the bundled tags to implicitly learn the loose mapping between heterogeneous annotations, and 2) exploring separate features on one-side tags to overcome the data sparseness problem of using only bundled tags. Experiments on benchmark datasets show that our coupled model significantly outperforms the state-of- the-art baselines on both one-side POS tagging and annotation conversion tasks. The codes and newly annotated data are released for non-commercial usage. 1 ? Correspondence author. 1 http://hlt.suda.edu.cn/ ? zhli ",,,,ACL
173,2015,AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes,"Sascha Rothe, Hinrich Schütze","We present AutoExtend, a system to learn embeddings for synsets and lexemes. It is flexible in that it can take any word embed- dings as input and does not need an addi- tional training corpus. The synset/lexeme embeddings obtained live in the same vec- tor space as the word embeddings. A sparse tensor formalization guarantees ef- ficiency and parallelizability. We use WordNet as a lexical resource, but Auto- Extend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disam- biguation tasks. ",,,,ACL
174,2015,Improving Evaluation of Machine Translation Quality Estimation,Yvette Graham,"Quality estimation evaluation commonly takes the form of measurement of the error that exists between predictions and gold standard labels for a particular test set of translations. Issues can arise during com- parison of quality estimation prediction score distributions and gold label distribu- tions, however. In this paper, we provide an analysis of methods of comparison and identify areas of concern with respect to widely used measures, such as the ability to gain by prediction of aggregate statistics specific to gold label distributions or by optimally conservative variance in predic- tion score distributions. As an alternative, we propose the use of the unit-free Pear- son correlation, in addition to providing an appropriate method of significance testing improvements over a baseline. Compo- nents of W MT -13 and W MT -14 quality es- timation shared tasks are replicated to re- veal substantially increased conclusivity in system rankings, including identification of outright winners of tasks. ",,,,ACL
1,2016,Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing,"James Goodman, Andreas Vlachos, Jason Naradowsky","Semantic parsers map natural language statements into meaning representations, and must abstract over syntactic phenom- ena, resolve anaphora, and identify word senses to eliminate ambiguous interpre- tations. Abstract meaning representation (AMR) is a recent example of one such semantic formalism which, similar to a de- pendency parse, utilizes a graph to repre- sent relationships between concepts (Ba- narescu et al., 2013). As with dependency parsing, transition-based approaches are a common approach to this problem. How- ever, when trained in the traditional man- ner these systems are susceptible to the ac- cumulation of errors when they find un- desirable states during greedy decoding. Imitation learning algorithms have been shown to help these systems recover from such errors. To effectively use these meth- ods for AMR parsing we find it highly beneficial to introduce two novel exten- sions: noise reduction and targeted explo- ration. The former mitigates the noise in the feature representation, a result of the complexity of the task. The latter targets the exploration steps of imitation learning towards areas which are likely to provide the most information in the context of a large action-space. We achieve state-of- the art results, and improve upon standard transition-based parsing by 4.7 F 1 points. ",,,,ACL
2,2016,Data Recombination for Neural Semantic Parsing,"Robin Jia, Percy Liang","Modeling crisp logical regularities is cru- cial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recom- bination, a novel framework for inject- ing such prior knowledge into a model. From the training data, we induce a high- precision synchronous context-free gram- mar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sam- pled from this grammar, thereby teaching the model about these structural proper- ties. Data recombination improves the ac- curacy of our RNN model on three se- mantic parsing datasets, leading to new state-of-the-art performance on the stan- dard GeoQuery dataset for models with comparable supervision. ",,,,ACL
3,2016,Inferring Logical Forms From Denotations,"Panupong Pasupat, Percy Liang","A core problem in learning semantic parsers from denotations is picking out consistent logical forms—those that yield the correct denotation—from a combina- torially large space. To control the search space, previous work relied on restricted set of rules, which limits expressivity. In this paper, we consider a much more ex- pressive class of logical forms, and show how to use dynamic programming to effi- ciently represent the complete set of con- sistent logical forms. Expressivity also introduces many more spurious logical forms which are consistent with the cor- rect denotation but do not represent the meaning of the utterance. To address this, we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms. On the W IKI T ABLE Q UESTIONS dataset, we increase the coverage of answerable ques- tions from 53.5% to 76%, and the ad- ditional crowdsourced supervision lets us rule out 92.1% of spurious logical forms. ",,,,ACL
4,2016,Language to Logical Form with Neural Attention,"Li Dong, Mirella Lapata","Semantic parsing aims at mapping nat- ural language to machine interpretable meaning representations. Traditional ap- proaches rely on high-quality lexicons, manually-built templates, and linguis- tic features which are either domain- or representation-specific. In this pa- per we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vec- tors. Experimental results on four datasets show that our approach performs compet- itively without using hand-engineered fea- tures and is easy to adapt across domains and meaning representations. ",,,,ACL
5,2016,Unsupervised Person Slot Filling based on Graph Mining,"Dian Yu, Heng Ji","Slot filling aims to extract the values (slot fillers) of specific attributes (slots types) for a given entity (query) from a large- scale corpus. Slot filling remains very challenging over the past seven years. We propose a simple yet effective unsuper- vised approach to extract slot fillers based on the following two observations: (1) a trigger is usually a salient node relative to the query and filler nodes in the depen- dency graph of a context sentence; (2) a relation is likely to exist if the query and candidate filler nodes are strongly con- nected by a relation-specific trigger. Thus we design a graph-based algorithm to au- tomatically identify triggers based on per- sonalized PageRank and Affinity Prop- agation for a given (query, filler) pair and then label the slot type based on the identified triggers. Our approach achieves 11.6%-25% higher F-score over state-of- the-art English slot filling methods. Our experiments also demonstrate that as long as a few trigger seeds, name tagging and dependency parsing capabilities exist, this approach can be quickly adapted to any language and new slot types. Our promis- ing results on Chinese slot filling can serve as a new benchmark. ",,,,ACL
6,2016,A Multi-media Approach to Cross-lingual Entity Knowledge Transfer,"Di Lu, Xiaoman Pan, Nima Pourdamghani, Shih-Fu Chang","When a large-scale incident or disaster oc- curs, there is often a great demand for rapidly developing a system to extract detailed and new information from low- resource languages (LLs). We propose a novel approach to discover compara- ble documents in high-resource languages (HLs), and project Entity Discovery and Linking results from HLs documents back to LLs. We leverage a wide variety of language-independent forms from multi- ple data modalities, including image pro- cessing (image-to-image retrieval, visual similarity and face recognition) and sound matching. We also propose novel meth- ods to learn entity priors from a large-scale HL corpus and knowledge base. Using Hausa and Chinese as the LLs and En- glish as the HL, experiments show that our approach achieves 36.1% higher Hausa name tagging F-score over a costly super- vised model, and 9.4% higher Chinese- to-English Entity Linking accuracy over state-of-the-art. ",,,,ACL
7,2016,Models and Inference for Prefix-Constrained Machine Translation,"Joern Wuebker, Spence Green, John DeNero, Saša Hasan","We apply phrase-based and neural models to a core task in interactive machine trans- lation: suggesting how to complete a par- tial translation. For the phrase-based sys- tem, we demonstrate improvements in sug- gestion quality using novel objective func- tions, learning techniques, and inference algorithms tailored to this task. Our con- tributions include new tunable metrics, an improved beam search strategy, an n-best extraction method that increases sugges- tion diversity, and a tuning procedure for a hierarchical joint model of alignment and translation. The combination of these tech- niques improves next-word suggestion accu- racy dramatically from 28.5% to 41.2% in a large-scale English-German experiment. Our recurrent neural translation system in- creases accuracy yet further to 53.0%, but inference is two orders of magnitude slower. Manual error analysis shows the strengths and weaknesses of both approaches. ",,,,ACL
8,2016,Modeling Coverage for Neural Machine Translation,"Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu","Attention mechanism has enhanced state- of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vec- tor is fed to the attention model to help ad- just future attention, which lets NMT sys- tem to consider more about untranslated source words. Experiments show that the proposed approach significantly im- proves both translation quality and align- ment quality over standard attention-based NMT. 1 ",,,,ACL
9,2016,Improving Neural Machine Translation Models with Monolingual Data,"Rico Sennrich, Barry Haddow, Alexandra Birch","Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only us- ing parallel data for training. Target- side monolingual data plays an impor- tant role in boosting fluency for phrase- based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with sep- arately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolin- gual data without changing the neural net- work architecture. By pairing monolin- gual training data with an automatic back- translation, we can treat it as additional parallel training data, and we obtain sub- stantial improvements on the WMT 15 task English?German (+2.8–3.7 B LEU ), and for the low-resourced IWSLT 14 task Turkish→English (+2.1–3.4 B LEU ), ob- taining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives sub- stantial improvements for the IWSLT 15 task English→German. ",,,,ACL
10,2016,Graph-Based Translation Via Graph Segmentation,"Liangyou Li, Andy Way, Qun Liu","One major drawback of phrase-based translation is that it segments an input sen- tence into continuous phrases. To sup- port linguistically informed source discon- tinuity, in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based trans- lation model. The model segments an input graph into connected subgraphs, each of which may cover a discontinuous phrase. We use beam search to combine translations of each subgraph left-to-right to produce a complete translation. Experi- ments on Chinese–English and German– English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores. By explicitly modeling the graph segmen- tation, our system obtains further improve- ment, especially on German–English. ",,,,ACL
11,2016,Incremental Acquisition of Verb Hypothesis Space towards Physical World Interaction,"Lanbo She, Joyce Chai","As a new generation of cognitive robots start to enter our lives, it is important to enable robots to follow human commands and to learn new actions from human lan- guage instructions. To address this issue, this paper presents an approach that ex- plicitly represents verb semantics through hypothesis spaces of fluents and automat- ically acquires these hypothesis spaces by interacting with humans. The learned hy- pothesis spaces can be used to automati- cally plan for lower-level primitive actions towards physical world interaction. Our empirical results have shown that the rep- resentation of a hypothesis space of flu- ents, combined with the learned hypothe- sis selection algorithm, outperforms a pre- vious baseline. In addition, our approach applies incremental learning, which can contribute to life-long learning from hu- mans in the future. ",,,,ACL
12,2016,Language Transfer Learning for Supervised Lexical Substitution,"Gerold Hintz, Chris Biemann","We propose a framework for lexical sub- stitution that is able to perform transfer learning across languages. Datasets for this task are available in at least three languages (English, Italian, and German). Previous work has addressed each of these tasks in isolation. In contrast, we regard the union of three shared tasks as a com- bined multilingual dataset. We show that a supervised system can be trained effec- tively, even if training and evaluation data are from different languages. Successful transfer learning between languages sug- gests that the learned model is in fact in- dependent of the underlying language. We combine state-of-the-art unsupervised fea- tures obtained from syntactic word em- beddings and distributional thesauri in a supervised delexicalized ranking system. Our system improves over state of the art in the full lexical substitution task in all three languages. ",,,,ACL
13,2016,Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning,"Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Brian MacWhinney","We use Bayesian optimization to learn curricula for word representation learning, optimizing performance on downstream tasks that depend on the learned represen- tations as features. The curricula are mod- eled by a linear ranking function which is the scalar product of a learned weight vec- tor and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus. We show that learning the curricu- lum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order. ",,,,ACL
14,2016,Pointing the Unknown Words,"Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou","The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP sys- tems, including traditional count-based and deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models us- ing attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sen- tence, and the other predicts a word in the shortlist vocabulary. At each timestep, the decision of which softmax layer to use is adaptively made by an MLP which is con- ditioned on the context. We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the envi- ronment when the name of an object is not known. Using our proposed model, we ob- serve improvements on two tasks, neural machine translation on the Europarl En- glish to French parallel corpora and text summarization on the Gigaword dataset. ",,,,ACL
15,2016,Generalized Transition-based Dependency Parsing via Control Parameters,"Bernd Bohnet, Ryan McDonald, Emily Pitler, Ji Ma","In this paper, we present a generalized transition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain tran- sitions between parser states. This gener- alization provides a unified framework to describe and compare various transition- based parsing approaches from both a the- oretical and empirical perspective. This includes well-known transition systems, but also previously unstudied systems. ",,,,ACL
16,2016,A Transition-Based System for Joint Lexical and Syntactic Analysis,"Matthieu Constant, Joakim Nivre","We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words: a syntactic dependency tree and a forest of lexical units including multiword expres- sions (MWEs). This combined represen- tation allows us to capture both the syn- tactic and semantic structure of MWEs, which in turn enables deeper downstream semantic analysis, especially for semi- compositional MWEs. The proposed sys- tem extends the arc-standard transition system for dependency parsing with tran- sitions for building complex lexical units. Experiments on two different data sets show that the approach significantly im- proves MWE identification accuracy (and sometimes syntactic accuracy) compared to existing joint approaches. ",,,,ACL
17,2016,Neural Greedy Constituent Parsing with Dynamic Oracles,"Maximin Coavoux, Benoît Crabbé","Dynamic oracle training has shown sub- stantial improvements for dependency parsing in various settings, but has not been explored for constituent parsing. The present article introduces a dynamic ora- cle for transition-based constituent pars- ing. Experiments on the 9 languages of the SPMRL dataset show that a neu- ral greedy parser with morphological fea- tures, trained with a dynamic oracle, leads to accuracies comparable with the best non-reranking and non-ensemble parsers. ",,,,ACL
18,2016,Literal and Metaphorical Senses in Compositional Distributional Semantic Models,"E.Dario Gutiérrez, Ekaterina Shutova, Tyler Marghetis, Benjamin Bergen","Metaphorical expressions are pervasive in natural language and pose a substan- tial challenge for computational seman- tics. The inherent compositionality of metaphor makes it an important test case for compositional distributional semantic models (CDSMs). This paper is the first to investigate whether metaphorical compo- sition warrants a distinct treatment in the CDSM framework. We propose a method to learn metaphors as linear transforma- tions in a vector space and find that, across a variety of semantic domains, explicitly modeling metaphor improves the result- ing semantic representations. We then use these representations in a metaphor iden- tification task, achieving a high perfor- mance of 0.82 in terms of F-score. ",,,,ACL
19,2016,Idiom Token Classification using Sentential Distributed Semantics,"Giancarlo Salton, Robert Ross, John Kelleher","Idiom token classification is the task of deciding for a set of potentially idiomatic phrases whether each occurrence of a phrase is a literal or idiomatic usage of the phrase. In this work we explore the use of Skip-Thought Vectors to create dis- tributed representations that encode fea- tures that are predictive with respect to id- iom token classification. We show that classifiers using these representations have competitive performance compared with the state of the art in idiom token classifi- cation. Importantly, however, our models use only the sentence containing the tar- get phrase as input and are thus less de- pendent on a potentially inaccurate or in- complete model of discourse context. We further demonstrate the feasibility of using these representations to train a competitive general idiom token classifier. ",,,,ACL
20,2016,Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings,"Kazuma Hashimoto, Yoshimasa Tsuruoka","We present a novel method for jointly learning compositional and non- compositional phrase embeddings by adaptively weighting both types of em- beddings using a compositionality scoring function. The scoring function is used to quantify the level of compositionality of each phrase, and the parameters of the function are jointly optimized with the ob- jective for learning phrase embeddings. In experiments, we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases, and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality, substantially outperforming the previous state of the art. Moreover, our embeddings improve upon the previous best model on a transitive verb disambiguation task. We also show that a simple ensemble technique further improves the results for both tasks. ",,,,ACL
21,2016,"Metaphor Detection with Topic Transition, Emotion and Cognition in Context","Hyeju Jang, Yohan Jo, Qinlan Shen, Michael Miller","Metaphor is a common linguistic tool in communication, making its detection in discourse a crucial task for natural lan- guage understanding. One popular ap- proach to this challenge is to capture se- mantic incohesion between a metaphor and the dominant topic of the surrounding text. While these methods are effective, they tend to overclassify target words as metaphorical when they deviate in mean- ing from its context. We present a new approach that (1) distinguishes literal and non-literal use of target words by exam- ining sentence-level topic transitions and (2) captures the motivation of speakers to express emotions and abstract concepts metaphorically. Experiments on an on- line breast cancer discussion forum dataset demonstrate a significant improvement in metaphor detection over the state-of-the- art. These experimental results also re- veal a tendency toward metaphor usage in personal topics and certain emotional con- texts. ",,,,ACL
22,2016,Compressing Neural Language Models by Sparse Word Representations,"Yunchuan Chen, Lili Mou, Yan Xu, Ge Li","Neural networks are among the state-of- the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such ap- proaches are time- and memory-intensive because of the large numbers of parame- ters for word embeddings and the output layer. In this paper, we propose to com- press neural language models by sparse word representations. In the experiments, the number of parameters in our model in- creases very slowly with the growth of the vocabulary size, which is almost imper- ceptible. Moreover, our approach not only reduces the parameter space to a large ex- tent, but also improves the performance in terms of the perplexity measure. 1 ",,,,ACL
23,2016,Intrinsic Subspace Evaluation of Word Embedding Representations,"Yadollah Yaghoobzadeh, Hinrich Schütze","We introduce a new methodology for in- trinsic evaluation of word representations. Specifically, we identify four fundamen- tal criteria based on the characteristics of natural language that pose difficulties to NLP systems; and develop tests that di- rectly show whether or not representations contain the subspaces necessary to satisfy these criteria. Current intrinsic evalua- tions are mostly based on the overall simi- larity or full-space similarity of words and thus view vector representations as points. We show the limits of these point-based intrinsic evaluations. We apply our evalu- ation methodology to the comparison of a count vector model and several neural net- work models and demonstrate important properties of these models. ",,,,ACL
24,2016,On the Role of Seed Lexicons in Learning Bilingual Word Embeddings,"Ivan Vulić, Anna Korhonen","A shared bilingual word embedding space (SBWES) is an indispensable resource in a variety of cross-language NLP and IR tasks. A common approach to the SB- WES induction is to learn a mapping func- tion between monolingual semantic spaces, where the mapping critically relies on a seed word lexicon used in the learning pro- cess. In this work, we analyze the impor- tance and properties of seed lexicons for the SBWES induction across different di- mensions (i.e., lexicon source, lexicon size, translation method, translation pair relia- bility). On the basis of our analysis, we propose a simple but effective hybrid bilin- gual word embedding (BWE) model. This model (HYBWE) learns the mapping be- tween two monolingual embedding spaces using only highly reliable symmetric trans- lation pairs from a seed document-level embedding space. We perform bilingual lexicon learning (BLL) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models, all of which use more expensive bilingual signals. Effectively, we demonstrate that a SBWES may be in- duced by leveraging only a very weak bilin- gual signal (document alignments) along with monolingual data. ",,,,ACL
25,2016,Liberal Event Extraction and Event Schema Induction,"Lifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng Ji","Meaning Repre- sentation) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simulta- neously extract event types and argument roles and discover an event schema. Ex- periments on general and specific domains demonstrate that this framework can con- struct high-quality schemas with many event and argument role types, covering a high proportion of event types and argu- ment roles in manually defined schemas. We show that extraction performance us- ing discovered schemas is comparable to supervised models trained from a large amount of data labeled according to pre- defined event types. The extraction quality of new event types is also promising. ",,,,ACL
26,2016,Jointly Event Extraction and Visualization on Twitter via Probabilistic Modelling,"Deyu Zhou, Tianmeng Gao, Yulan He","Event extraction from texts aims to de- tect structured information such as what has happened, to whom, where and when. Event extraction and visualization are typ- ically considered as two different tasks. In this paper, we propose a novel approach based on probabilistic modelling to joint- ly extract and visualize events from tweet- s where both tasks benefit from each oth- er. We model each event as a joint dis- tribution over named entities, a date, a lo- cation and event-related keywords. More- over, both tweets and event instances are associated with coordinates in the visual- ization space. The manifold assumption that the intrinsic geometry of tweets is a low-rank, non-linear manifold within the high-dimensional space is incorporated in- to the learning framework using a regu- larization. Experimental results show that the proposed approach can effectively deal with both event extraction and visualiza- tion and performs remarkably better than both the state-of-the-art event extraction method and a pipeline approach for event extraction and visualization. ",,,,ACL
27,2016,Using Sentence-Level LSTM Language Models for Script Inference,"Karl Pichotta, Raymond J. Mooney","There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from docu- ments. These systems operate on struc- tured verb-argument events produced by an NLP pipeline. We compare these sys- tems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents. ",,,,ACL
28,2016,Two Discourse Driven Language Models for Semantics,"Haoruo Peng, Dan Roth","Natural language understanding often re- quires deep semantic knowledge. Ex- panding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of ab- straction. We develop two distinct mod- els that capture semantic frame chains and discourse information while abstract- ing over the specific mentions of predi- cates and entities. For each model, we in- vestigate four implementations: a “stan- dard” N-gram language model and three discriminatively trained “neural” language models that generate embeddings for se- mantic frames. The quality of the se- mantic language models (SemLM) is eval- uated both intrinsically, using perplexity and a narrative cloze test and extrinsically – we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference res- olution and discourse parsing. ",,,,ACL
29,2016,Sentiment Domain Adaptation with Multiple Sources,"Fangzhao Wu, Yongfeng Huang","Domain adaptation is an important re- search topic in sentiment analysis area. Existing domain adaptation methods usu- ally transfer sentiment knowledge from only one source domain to target do- main. In this paper, we propose a new domain adaptation approach which can exploit sentiment knowledge from mul- tiple source domains. We first extrac- t both global and domain-specific senti- ment knowledge from the data of multi- ple source domains using multi-task learn- ing. Then we transfer them to target do- main with the help of words’ sentimen- t polarity relations extracted from the un- labeled target domain data. The similar- ities between target domain and different source domains are also incorporated into the adaptation process. Experimental re- sults on benchmark dataset show the ef- fectiveness of our approach in improving cross-domain sentiment classification per- formance. ",,,,ACL
30,2016,Connotation Frames: A Data-Driven Investigation,"Hannah Rashkin, Sameer Singh, Yejin Choi","Through a particular choice of a predicate (e.g., “x violated y”), a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y: (1) writer’s perspective: projecting x as an “antagonist” and y as a “victim”, (2) entities’ perspective: y probably dislikes x, (3) effect: something bad happened to y, (4) value: y is something valuable, and (5) mental state: y is distressed by the event. We introduce connotation frames as a rep- resentation formalism to organize these rich dimensions of connotation using typed relations. First, we investigate the fea- sibility of obtaining connotative labels through crowdsourcing experiments. We then present models for predicting the con- notation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations. Empirical results confirm that connotation frames can be in- duced from various data sources that reflect how language is used in context. We con- clude with analytical results that show the potential use of connotation frames for an- alyzing subtle biases in online news media. ",,,,ACL
31,2016,Bi-Transferring Deep Neural Networks for Domain Adaptation,"Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang, Tingting He","Sentiment classification aims to automati- cally predict sentiment polarity (e.g., pos- itive or negative) of user generated sen- timent data (e.g., reviews, blogs). Due to the mismatch among different domains, a sentiment classifier trained in one do- main may not work well when directly applied to other domains. Thus, domain adaptation for sentiment classification al- gorithms are highly desirable to reduce the domain discrepancy and manual labeling costs. To address the above challenge, we propose a novel domain adaptation method, called Bi-Transferring Deep Neu- ral Networks (BTDNNs). The proposed BTDNNs attempts to transfer the source domain examples to the target domain, and also transfer the target domain examples to the source domain. The linear transfor- mation of BTDNNs ensures the feasibility of transferring between domains, and the distribution consistency between the trans- ferred domain and the desirable domain is constrained with a linear data reconstruc- tion manner. As a result, the transferred source domain is supervised and follows similar distribution as the target domain. Therefore, any supervised method can be used on the transferred source domain to train a classifier for sentiment classifica- tion in a target domain. We conduct ex- periments on a benchmark composed of reviews of 4 types of Amazon products. Experimental results show that our pro- posed approach significantly outperforms the several baseline methods, and achieves an accuracy which is competitive with the state-of-the-art method for domain adapta- tion. ",,,,ACL
32,2016,"Document-level Sentiment Inference with Social, Faction, and Discourse Context","Eunsol Choi, Hannah Rashkin, Luke Zettlemoyer, Yejin Choi","We present a new approach for document- level sentiment inference, where the goal is to predict directed opinions (who feels positively or negatively towards whom) for all entities mentioned in a text. To encour- age more complete and consistent predic- tions, we introduce an ILP that jointly models (1) sentence- and discourse-level sentiment cues, (2) factual evidence about entity factions, and (3) global constraints based on social science theories such as homophily, social balance, and reci- procity. Together, these cues allow for rich inference across groups of entities, includ- ing for example that CEOs and the com- panies they lead are likely to have simi- lar sentiment towards others. We evalu- ate performance on new, densely labeled data that provides supervision for all pairs, complementing previous work that only labeled pairs mentioned in the same sen- tence. Experiments demonstrate that the global model outperforms sentence-level baselines, by providing more coherent pre- dictions across sets of related entities. ",,,,ACL
33,2016,Active Learning for Dependency Parsing with Partial Annotation,"Zhenghua Li, Min Zhang, Yue Zhang, Zhanyi Liu","Different from traditional active learning based on sentence-wise full annotation (FA), this paper proposes active learning with dependency-wise partial annotation (PA) as a finer-grained unit for dependency parsing. At each iteration, we select a few most uncertain words from an unlabeled data pool, manually annotate their syntactic heads, and add the partial trees into labeled data for parser retraining. Compared with sentence-wise FA, dependency-wise PA gives us more flexibility in task selection and avoids wasting time on annotating trivial tasks in a sentence. Our work makes the following contributions. First, we are the first to apply a probabilistic model to active learning for dependency parsing, which can 1) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics, and 2) directly learn parameters from PA based on a forest-based training objective. Second, we propose and compare several uncertainty metrics through simulation experiments on both Chinese and English. Finally, we conduct human annotation experiments to compare FA and PA on real annotation time and quality. ",,,,ACL
34,2016,Dependency Parsing with Bounded Block Degree and Well-nestedness via Lagrangian Relaxation and Branch-and-Bound,"Caio Corro, Joseph Le Roux, Mathieu Lacroix, Antoine Rozenknop","We present a novel dependency pars- ing method which enforces two structural properties on dependency trees: bounded block degree and well-nestedness. These properties are useful to better represent the set of admissible dependency structures in treebanks and connect dependency pars- ing to context-sensitive grammatical for- malisms. We cast this problem as an Inte- ger Linear Program that we solve with La- grangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search. Experimen- tally, we see that these methods are effi- cient and competitive compared to a base- line unconstrained parser, while enforcing structural properties in all cases. ",,,,ACL
35,2016,Query Expansion with Locally-Trained Word Embeddings,"Fernando Diaz, Bhaskar Mitra, Nick Craswell","Continuous space word embeddings have received a great deal of atten- tion in the natural language processing and machine learning communities for their ability to model term similarity and other relationships. We study the use of term relatedness in the context of query expansion for ad hoc informa- tion retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when trained globally, under- perform corpus and query specific em- beddings for retrieval tasks. These re- sults suggest that other tasks benefit- ing from global embeddings may also benefit from local embeddings. ",,,,ACL
36,2016,Together we stand: Siamese Networks for Similar Question Retrieval,"Arpita Das, Harish Yenala, Manoj Chinnakotla, Manish Shrivastava","Community Question Answering (cQA) services like Yahoo! Answers 1 , Baidu Zhidao 2 , Quora 3 , StackOverflow 4 etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions. The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives. The main challenge in this task is the “lexico- syntactic” gap between the current and the previous questions. In this paper, we pro- pose a novel approach called “Siamese Convolutional Neural Network for cQA (SCQA)” to find the semantic similarity between the current and the archived ques- tions. SCQA consist of twin convolu- tional neural networks with shared param- eters and a contrastive loss function join- ing them. SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA fo- rum archives. The model projects semanti- cally similar question pairs nearer to each other and dissimilar question pairs far- ther away from each other in the seman- tic space. Experiments on large scale real- life “Yahoo! Answers” dataset reveals that SCQA outperforms current state-of-the- art approaches based on translation mod- els, topic models and deep neural network 1 https://answers.yahoo.com/ 2 http://zhidao.baidu.com/ 3 http://www.quora.com/ 4 http://stackoverflow.com/ based models which use non-shared pa- rameters. ",,,,ACL
37,2016,News Citation Recommendation with Implicit and Explicit Semantics,"Hao Peng, Jing Liu, Chin-Yew Lin","In this work, we focus on the problem of news citation recommendation. The task aims to recommend news citations for both authors and readers to create and search news references. Due to the sparsity issue of news citations and the engineering difficulty in obtaining infor- mation on authors, we focus on content similarity-based methods instead of col- laborative filtering-based approaches. In this paper, we explore word embedding (i.e., implicit semantics) and grounded en- tities (i.e., explicit semantics) to address the variety and ambiguity issues of lan- guage. We formulate the problem as a re- ranking task and integrate different simi- larity measures under the learning to rank framework. We evaluate our approach on a real-world dataset. The experimental re- sults show the efficacy of our method. ",,,,ACL
38,2016,Grapheme-to-Phoneme Models for (Almost) Any Language,"Aliya Deri, Kevin Knight","Grapheme-to-phoneme (g2p) models are rarely available in low-resource languages, as the creation of training and evaluation data is expensive and time-consuming. We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages. We then develop phoneme and language distance metrics based on phono- logical and linguistic knowledge; apply- ing those, we adapt g2p models for high- resource languages to create models for related low-resource languages. We pro- vide results for models for 229 adapted lan- guages. ",,,,ACL
39,2016,Neural Word Segmentation Learning for Chinese,"Deng Cai, Hai Zhao","Most previous approaches to Chinese word segmentation formalize this prob- lem as a character-based sequence label- ing task so that only contextual informa- tion within fixed sized local windows and simple interactions between adjacent tags can be captured. In this paper, we pro- pose a novel neural framework which thor- oughly eliminates context windows and can utilize complete segmentation history. Our model employs a gated combination neural network over characters to produce distributed representations of word candi- dates, which are then given to a long short- term memory (LSTM) language scoring model. Experiments on the benchmark datasets show that without the help of feature engineering as most existing ap- proaches, our models achieve competitive or better performances with previous state- of-the-art methods. ",,,,ACL
40,2016,Transition-Based Neural Word Segmentation,"Meishan Zhang, Yue Zhang, Guohong Fu","Character-based and word-based methods are two main types of statistical models for Chinese word segmentation, the for- mer exploiting sequence labeling models over characters and the latter typically ex- ploiting a transition-based model, with the advantages that word-level features can be easily utilized. Neural models have been exploited for character-based Chi- nese word segmentation, giving high accu- racies by making use of external character embeddings, yet requiring less feature en- gineering. In this paper, we study a neu- ral model for word-based Chinese word segmentation, by replacing the manually- designed discrete features with neural fea- tures in a word-based segmentation frame- work. Experimental results demonstrate that word features lead to comparable per- formances to the best systems in the litera- ture, and a further combination of discrete and neural features gives top accuracies. ",,,,ACL
41,2016,A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data,"Adam Trischler, Zheng Ye, Xingdi Yuan, Jing He","Understanding unstructured text is a ma- jor goal within natural language process- ing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we in- vestigate machine comprehension on the challenging MCTest benchmark. Partly because of its limited size, prior work on MCTest has focused mainly on engi- neering better features. We tackle the dataset with a neural approach, harness- ing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the pas- sage, question, and answer from a vari- ety of trainable perspectives, as opposed to using a manually designed, rigid fea- ture set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology de- signed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for MCTest, outper- forming previous feature-engineered ap- proaches slightly and previous neural ap- proaches by a significant margin (over 15 percentage points). ",,,,ACL
42,2016,Combining Natural Logic and Shallow Reasoning for Question Answering,"Gabor Angeli, Neha Nayak, Christopher D. Manning","Broad domain question answering is of- ten difficult in the absence of structured knowledge bases, and can benefit from shallow lexical methods (broad coverage) and logical reasoning (high precision). We propose an approach for incorporating both of these signals in a unified frame- work based on natural logic. We extend the breadth of inferences afforded by nat- ural logic to include relational entailment (e.g., buy → own) and meronymy (e.g., a person born in a city is born the city’s country). Furthermore, we train an eval- uation function – akin to gameplaying – to evaluate the expected truth of candidate premises on the fly. We evaluate our ap- proach on answering multiple choice sci- ence questions, achieving strong results on the dataset. ",,,,ACL
43,2016,Easy Questions First? A Case Study on Curriculum Learning for Question Answering,"Mrinmaya Sachan, Eric Xing","Cognitive science researchers have em- phasized the importance of ordering a complex task into a sequence of easy to hard problems. Such an ordering provides an easier path to learning and increases the speed of acquisition of the task com- pared to conventional learning. Recent works in machine learning have explored a curriculum learning approach called self- paced learning which orders data samples on the easiness scale so that easy sam- ples can be introduced to the learning algo- rithm first and harder samples can be intro- duced successively. We introduce a num- ber of heuristics that improve upon self- paced learning. Then, we argue that incor- porating easy, yet, a diverse set of samples can further improve learning. We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improve- ments in each of them. ",,,,ACL
44,2016,Improved Representation Learning for Question Answer Matching,"Ming Tan, Cicero dos Santos, Bing Xiang, Bowen Zhou","Passage-level question answer matching is a challenging task since it requires effec- tive representations that capture the com- plex semantic relations between questions and answers. In this work, we propose a series of deep learning models to address passage answer selection. To match pas- sage answers to questions accommodat- ing their complex semantic relations, un- like most previous work that utilizes a sin- gle deep learning structure, we develop hybrid models that process the text us- ing both convolutional and recurrent neu- ral networks, combining the merits on ex- tracting linguistic information from both structures. Additionally, we also develop a simple but effective attention mechanism for the purpose of constructing better an- swer representations according to the in- put question, which is imperative for bet- ter modeling long answer sequences. The results on two public benchmark datasets, InsuranceQA and TREC-QA, show that our proposed models outperform a variety of strong baselines. ",,,,ACL
45,2016,Tables as Semi-structured Knowledge for Question Answering,"Sujay Kumar Jauhar, Peter Turney, Eduard Hovy","Question answering requires access to a knowledge base to check facts and rea- son about information. Knowledge in the form of natural language text is easy to ac- quire, but difficult for automated reason- ing. Highly-structured knowledge bases can facilitate reasoning, but are difficult to acquire. In this paper we explore tables as a semi-structured formalism that pro- vides a balanced compromise to this trade- off. We first use the structure of tables to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations, easily and ef- ficiently via crowd-sourcing. We then use this annotated data to train a semi- structured feature-driven model for ques- tion answering that uses tables as a knowl- edge base. In benchmark evaluations, we significantly outperform both a strong un- structured retrieval baseline and a highly- structured Markov Logic Network model. ",,,,ACL
46,2016,Neural Summarization by Extracting Sentences and Words,"Jianpeng Cheng, Mirella Lapata",Traditional approaches to extractive summarization rely heavily on human- engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general frame- work for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs 1 . Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation. ,,,,ACL
47,2016,Neural Networks For Negation Scope Detection,"Federico Fancellu, Adam Lopez, Bonnie Webber","Automatic negation scope detection is a task that has been tackled using differ- ent classifiers and heuristics. Most sys- tems are however 1) highly-engineered, 2) English-specific, and 3) only tested on the same genre they were trained on. We start by addressing 1) and 2) using a neural network architecture. Results obtained on data from the *SEM2012 shared task on negation scope detection show that even a simple feed-forward neural network us- ing word-embedding features alone, per- forms on par with earlier classifiers, with a bi-directional LSTM outperforming all of them. We then address 3) by means of a specially-designed synthetic test set; in doing so, we explore the problem of de- tecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered. ",,,,ACL
48,2016,CSE: Conceptual Sentence Embeddings based on Attention Model,"Yashen Wang, Heyan Huang, Chong Feng, Qiang Zhou","Most sentence embedding models typical- ly represent each sentence only using word surface, which makes these models indis- criminative for ubiquitous homonymy and polysemy. In order to enhance represen- tation capability of sentence, we employ conceptualization model to assign associ- ated concepts for each sentence in the tex- t corpus, and then learn conceptual sen- tence embedding (CSE). Hence, this se- mantic representation is more expressive than some widely-used text representation models such as latent topic model, espe- cially for short-text. Moreover, we fur- ther extend CSE models by utilizing a lo- cal attention-based model that select rel- evant words within the context to make more efficient prediction. In the experi- ments, we evaluate the CSE models on two tasks, text classification and information retrieval. The experimental results show that the proposed models outperform typi- cal sentence embed-ding models. ",,,,ACL
49,2016,DocChat: An Information Retrieval Approach for Chatbot Engines Using Unstructured Documents,"Zhao Yan, Nan Duan, Junwei Bao, Peng Chen","Most current chatbot engines are designed to reply to user utterances based on exist- ing utterance-response (or Q-R) 1 pairs. In this paper, we present DocChat, a novel information retrieval approach for chat- bot engines that can leverage unstructured documents, instead of Q-R pairs, to re- spond to utterances. A learning to rank model with features designed at different levels of granularity is proposed to mea- sure the relevance between utterances and responses directly. We evaluate our pro- posed approach in both English and Chi- nese: (i) For English, we evaluate Doc- Chat on WikiQA and QASent, two answer sentence selection tasks, and compare it with state-of-the-art methods. Reasonable improvements and good adaptability are observed. (ii) For Chinese, we compare DocChat with XiaoIce 2 , a famous chitchat engine in China, and side-by-side evalua- tion shows that DocChat is a perfect com- plement for chatbot engines using Q-R pairs as main source of responses. ",,,,ACL
50,2016,Investigating the Sources of Linguistic Alignment in Conversation,"Gabriel Doyle, Michael C. Frank","In conversation, speakers tend to “ac- commodate” or “align” to their partners, changing the style and substance of their communications to be more similar to their partners’ utterances. We focus here on “linguistic alignment,” changes in word choice based on others’ choices. Although linguistic alignment is observed across many different contexts and its degree cor- relates with important social factors such as power and likability, its sources are still uncertain. We build on a recent probabilis- tic model of alignment, using it to separate out alignment attributable to words ver- sus word categories. We model alignment in two contexts: telephone conversations and microblog replies. Our results show evidence of alignment, but it is primarily lexical rather than categorical. Further- more, we find that discourse acts modu- late alignment substantially. This evidence supports the view that alignment is shaped by strategic communicative processes re- lated to the ongoing discourse. ",,,,ACL
51,2016,Entropy Converges Between Dialogue Participants: Explanations from an Information-Theoretic Perspective,"Yang Xu, David Reitter","The applicability of entropy rate constancy to dialogue is examined on two spoken di- alogue corpora. The principle is found to hold; however, new entropy change pat- terns within the topic episodes of dialogue are described, which are different from written text. Speaker’s dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy, respectively, which results in lo- cal convergence between these speakers in each topic episode. This implies that the sentence entropy in dialogue is con- ditioned on different contexts determined by the speaker’s roles. Explanations from the perspectives of grounding theory and interactive alignment are discussed, re- sulting in a novel, unified information- theoretic approach of dialogue. ",,,,ACL
52,2016,Finding the Middle Ground - A Model for Planning Satisficing Answers,"Sabine Janzen, Wolfgang Maaß, Tobias Kowatsch","To establish sophisticated dialogue sys- tems, text planning needs to cope with congruent as well as incongruent inter- locutor interests as given in everyday di- alogues. Little attention has been given to this topic in text planning in contrast to di- alogues that are fully aligned with antic- ipated user interests. When considering dialogues with congruent and incongru- ent interlocutor interests, dialogue part- ners are facing the constant challenge of finding a balance between cooperation and competition. We introduce the concept of fairness that operationalize an equal and adequate, i.e. equitable satisfaction of all interlocutors’ interests. Focusing on Question-Answering (QA) settings, we describe an answer planning approach that support fair dialogues under congruent and incongruent interlocutor interests. Due to the fact that fairness is subjective per se, we present promising results from an empirical study (N=107) in which human subjects interacted with a QA system im- plementing the proposed approach. ",,,,ACL
53,2016,A Sentence Interaction Network for Modeling Dependence between Sentences,"Biao Liu, Minlie Huang","Modeling interactions between two sen- tences is crucial for a number of natu- ral language processing tasks including Answer Selection, Dialogue Act Analy- sis, etc. While deep learning methods like Recurrent Neural Network or Convo- lutional Neural Network have been proved to be powerful for sentence modeling, prior studies paid less attention on inter- actions between sentences. In this work, we propose a Sentence Interaction Net- work (SIN) for modeling the complex in- teractions between two sentences. By in- troducing “interaction states” for word and phrase pairs, SIN is powerful and flexi- ble in capturing sentence interactions for different tasks. We obtain significant im- provements on Answer Selection and Dia- logue Act Analysis without any feature en- gineering. ",,,,ACL
54,2016,Towards more variation in text generation: Developing and evaluating variation models for choice of referential form,"Thiago Castro Ferreira, Emiel Krahmer, Sander Wubben","In this study, we introduce a non- deterministic method for referring expres- sion generation. We describe two models that account for individual variation in the choice of referential form in automatically generated text: a Naive Bayes model and a Recurrent Neural Network. Both are eval- uated using the VaREG corpus. Then we select the best performing model to gen- erate referential forms in texts from the GREC-2.0 corpus and conduct an evalu- ation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original references and those pro- duced by a random baseline model. ",,,,ACL
55,2016,How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions,"Arun Chaganty, Percy Liang","How much is 131 million US dollars? To help readers put such numbers in con- text, we propose a new task of automati- cally generating short descriptions known as perspectives, e.g. “$131 million is about the cost to employ everyone in Texas over a lunch period”. First, we collect a dataset of numeric mentions in news arti- cles, where each mention is labeled with a set of rated perspectives. We then pro- pose a system to generate these descrip- tions consisting of two steps: formula con- struction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on fa- miliarity, numeric proximity and seman- tic compatibility. In generation, we con- vert a formula into natural language us- ing a sequence-to-sequence recurrent neu- ral network. Our system obtains a 15.2% F 1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation. ",,,,ACL
56,2016,Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus,"Iulian Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn","Over the past decade, large-scale super- vised learning corpora have enabled ma- chine learning researchers to make sub- stantial advances. However, to this date, there are no large-scale question- answer corpora available. In this paper we present the 30M Factoid Question- Answer Corpus, an enormous question- answer pair corpus produced by apply- ing a novel neural network architecture on the knowledge base Freebase to trans- duce facts into natural language ques- tions. The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine trans- lation and sentence similarity metrics. Across all evaluation criteria the question- generation model outperforms the compet- ing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear to be compa- rable in quality to real human-generated questions. * First authors. ? Email: {iulian.vlad.serban,caglar.gulcehre, sungjin.ahn,sarath.chandar.anbil.parthipan, aaron.courville,yoshua.bengio}@umontreal.ca Email: alberto.garcia-duran@utc.fr ? CIFAR Senior Fellow ",,,,ACL
57,2016,Latent Predictor Networks for Code Generation,"Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz Hermann","Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architec- ture which generates an output sequence conditioned on an arbitrary number of in- put functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating pro- gramming code from a mixed natural lan- guage and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearth- stone. On these, and a third preexisting corpus, we demonstrate that marginalis- ing multiple predictors allows our model to outperform strong benchmarks. ",,,,ACL
58,2016,Easy Things First: Installments Improve Referring Expression Generation for Objects in Photographs,"Sina Zarrieß, David Schlangen","Research on generating referring expres- sions has so far mostly focussed on “one- shot reference”, where the aim is to gener- ate a single, discriminating expression. In interactive settings, however, it is not un- common for reference to be established in “installments”, where referring informa- tion is offered piecewise until success has been confirmed. We show that this strat- egy can also be advantageous in technical systems that only have uncertain access to object attributes and categories. We train a recently introduced model of grounded word meaning on a data set of REs for objects in images and learn to predict se- mantically appropriate expressions. In a human evaluation, we observe that users are sensitive to inadequate object names - which unfortunately are not unlikely to be generated from low-level visual input. We propose a solution inspired from hu- man task-oriented interaction and imple- ment strategies for avoiding and repair- ing semantically inaccurate words. We enhance a word-based REG with context- aware, referential installments and find that they substantially improve the refer- ential success of the system. ",,,,ACL
59,2016,Collective Entity Resolution with Multi-Focal Attention,"Amir Globerson, Nevena Lazic, Soumen Chakrabarti, Amarnag Subramanya","Entity resolution is the task of linking each mention of an entity in text to the cor- responding record in a knowledge base (KB). Coherence models for entity resolu- tion encourage all referring expressions in a document to resolve to entities that are related in the KB. We explore attention- like mechanisms for coherence, where the evidence for each candidate is based on a small set of strong relations, rather than relations to all other entities in the doc- ument. The rationale is that document- wide support may simply not exist for non-salient entities, or entities not densely connected in the KB. Our proposed sys- tem outperforms state-of-the-art systems on the CoNLL 2003, TAC KBP 2010, 2011 and 2012 tasks. ",,,,ACL
60,2016,Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric,"Nafise Sadat Moosavi, Michael Strube","Interpretability and discriminative power are the two most basic requirements for an evaluation metric. In this paper, we re- port the mention identification effect in the B 3 , CEAF, and BLANC coreference eval- uation metrics that makes it impossible to interpret their results properly. The only metric which is insensitive to this flaw is MUC, which, however, is known to be the least discriminative metric. It is a known fact that none of the current metrics are reliable. The common practice for rank- ing coreference resolvers is to use the av- erage of three different metrics. However, one cannot expect to obtain a reliable score by averaging three unreliable metrics. We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to over- come the shortcomings of the current eval- uation metrics. LEA is available as branch LEA-scorer in the reference implemen- tation of the official CoNLL scorer. ",,,,ACL
61,2016,Improving Coreference Resolution by Learning Entity-Level Distributed Representations,"Kevin Clark, Christopher D. Manning","A long-standing challenge in coreference resolution has been the incorporation of entity-level information – features defined over clusters of mentions instead of men- tion pairs. We present a neural net- work based coreference system that pro- duces high-dimensional vector represen- tations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is de- sirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final corefer- ence partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features. ",,,,ACL
62,2016,Effects of Creativity and Cluster Tightness on Short Text Clustering Performance,"Catherine Finegan-Dollak, Reed Coke, Rui Zhang, Xiangyi Ye","Properties of corpora, such as the diver- sity of vocabulary and how tightly related texts cluster together, impact the best way to cluster short texts. We examine several such properties in a variety of corpora and track their effects on various combinations of similarity metrics and clustering algo- rithms. We show that semantic similar- ity metrics outperform traditional n-gram and dependency similarity metrics for k- means clustering of a linguistically cre- ative dataset, but do not help with less creative texts. Yet the choice of simi- larity metric interacts with the choice of clustering method. We find that graph- based clustering methods perform well on tightly clustered data but poorly on loosely clustered data. Semantic similarity met- rics generate loosely clustered output even when applied to a tightly clustered dataset. Thus, the best performing clustering sys- tems could not use semantic metrics. ",,,,ACL
63,2016,Generative Topic Embedding: a Continuous Representation of Documents,"Shaohua Li, Tat-Seng Chua, Jun Zhu, Chunyan Miao","Word embedding maps words into a low- dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps docu- ments onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The proba- bility of each word is influenced by both its local context and its topic. A variational inference method yields the topic embed- dings as well as the topic mixing propor- tions for each document. Jointly they rep- resent the document in a low-dimensional continuous space. In two document clas- sification tasks, our method performs bet- ter than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate co- herent topics even based on only one doc- ument. ",,,,ACL
64,2016,Detecting Common Discussion Topics Across Culture From News Reader Comments,"Bei Shi, Wai Lam, Lidong Bing, Yinqing Xu","News reader comments found in many on-line news websites are typically massive in amount. We investigate the task of Cultural-common Topic Detection (CTD), which is aimed at discovering common discussion topics from news reader comments written in different languages. We propose a new probabilistic graphical model called MCTA which can cope with the language gap and capture the common semantics in different languages. We also develop a partially collapsed Gibbs sampler which effectively incorporates the term trans- lation relationship into the detection of cultural-common topics for model param- eter learning. Experimental results show improvements over the state-of-the-art model. ",,,,ACL
65,2016,A Discriminative Topic Model using Document Network Structure,"Weiwei Yang, Jordan Boyd-Graber, Philip Resnik","Document collections often have links be- tween documents—citations, hyperlinks, or revisions—and which links are added is often based on topical similarity. To model these intuitions, we introduce a new topic model for documents situated within a net- work structure, integrating latent blocks of documents with a max-margin learning criterion for link prediction using topic- and word-level features. Experiments on a scientific paper dataset and collection of webpages show that, by more robustly exploiting the rich link structure within a document network, our model improves link prediction, topic quality, and block distributions. ",,,,ACL
66,2016,AraSenTi: Large-Scale Twitter-Specific Arabic Sentiment Lexicons,"Nora Al-Twairesh, Hend Al-Khalifa, Abdulmalik Al-Salman","Sentiment Analysis (SA) is an active research area nowadays due to the tremendous interest in aggregating and evaluating opinions being disseminated by users on the Web. SA of English has been thoroughly researched; however research on SA of Arabic has just flourished. Twitter is considered a powerful tool for disseminating information and a rich resource for opinionated text containing views on many different topics. In this paper we attempt to bridge a gap in Arabic SA of Twitter which is the lack of sentiment lexi- cons that are tailored for the informal lan- guage of Twitter. We generate two lexicons extracted from a large dataset of tweets using two approaches and evaluate their use in a simple lexicon based method. The evaluation is performed on internal and external da- tasets. The performance of these automatical- ly generated lexicons was very promising, al- beit the simple method used for classification. The best F-score obtained was 89.58% on the internal dataset and 63.1-64.7% on the exter- nal datasets. ",,,,ACL
67,2016,Unsupervised Multi-Author Document Decomposition Based on Hidden Markov Model,"Khaled Aldebei, Xiangjian He, Wenjing Jia, Jie Yang","This paper proposes an unsupervised approach for segmenting a multi- author document into authorial com- ponents. The key novelty is that we utilize the sequential patterns hid- den among document elements when determining their authorships. For this purpose, we adopt Hidden Markov Model (HMM) and construct a sequen- tial probabilistic model to capture the dependencies of sequential sentences and their authorships. An unsuper- vised learning method is developed to initialize the HMM parameters. Exper- imental results on benchmark datasets have demonstrated the significant ben- efit of our idea and our approach has outperformed the state-of-the-arts on all tests. As an example of its applica- tions, the proposed approach is applied for attributing authorship of a docu- ment and has also shown promising re- sults. ",,,,ACL
68,2016,Automatic Text Scoring Using Neural Networks,"Dimitrios Alikaniotis, Helen Yannakoudakis, Marek Rei","Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the pre- dictive features of the system need to be manually engineered by human ex- perts. We introduce a model that forms word representations by learning the ex- tent to which specific words contribute to the text’s score. Using Long-Short Term Memory networks to represent the mean- ing of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent ad- vances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative. ",,,,ACL
69,2016,Improved Semantic Parsers For If-Then Statements,"I. Beltagy, Chris Quirk","Digital personal assistants are becoming both more common and more useful. The major NLP challenge for personal assis- tants is machine understanding: translat- ing natural language user commands into an executable representation. This paper focuses on understanding rules written as If-Then statements, though the techniques should be portable to other semantic pars- ing tasks. We view understanding as struc- ture prediction and show improved mod- els using both conventional techniques and neural network models. We also discuss various ways to improve generalization and reduce overfitting: synthetic training data from paraphrase, grammar combina- tions, feature selection and ensembles of multiple systems. An ensemble of these techniques achieves a new state of the art result with 8% accuracy improvement. ",,,,ACL
70,2016,Universal Dependencies for Learner English,"Yevgeni Berzak, Jessica Kenney, Carolyn Spadine, Jing Xian Wang","We introduce the Treebank of Learner En- glish (TLE), the first publicly available syntactic treebank for English as a Sec- ond Language (ESL). The TLE provides manually annotated POS tags and Univer- sal Dependency (UD) trees for 5,124 sen- tences from the Cambridge First Certifi- cate in English (FCE) corpus. The UD annotations are tied to a pre-existing er- ror annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungram- matical English. Finally, we benchmark POS tagging and dependency parsing per- formance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to sup- port a wide range of linguistic and compu- tational research on second language ac- quisition as well as automatic processing of ungrammatical language 1 . ",,,,ACL
71,2016,Extracting token-level signals of syntactic processing from fMRI - with an application to PoS induction,"Joachim Bingel, Maria Barrett, Anders Søgaard","Neuro-imaging studies on reading differ- ent parts of speech (PoS) report somewhat mixed results, yet some of them indicate different activations with different PoS. This paper addresses the difficulty of using fMRI to discriminate between linguistic tokens in reading of running text because of low temporal resolution. We show that once we solve this problem, fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4%. ",,,,ACL
72,2016,Bidirectional Recurrent Convolutional Neural Network for Relation Classification,"Rui Cai, Xiaodong Zhang, Houfeng Wang","Relation classification is an important se- mantic processing task in the field of natu- ral language processing (NLP). In this pa- per, we present a novel model BRCNN to classify the relation of two entities in a sentence. Some state-of-the-art systems concentrate on modeling the shortest de- pendency path (SDP) between two entities leveraging convolutional or recurrent neu- ral networks. We further explore how to make full use of the dependency relations information in the SDP, by combining convolutional neural networks and two- channel recurrent neural networks with long short term memory (LSTM) units. We propose a bidirectional architecture to learn relation representations with di- rectional information along the SDP for- wards and backwards at the same time, which benefits classifying the direction of relations. Experimental results show that our method outperforms the state-of-the- art approaches on the SemEval-2010 Task 8 dataset. ",,,,ACL
73,2016,Sentence Rewriting for Semantic Parsing,"Bo Chen, Le Sun, Xianpei Han, Bo An","A major challenge of semantic parsing is the vocabulary mismatch problem be- tween natural language and target ontol- ogy. In this paper, we propose a sen- tence rewriting based semantic parsing method, which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same struc- ture with its target logical form. Specifi- cally, we propose two sentence-rewriting methods for two common types of mis- match: a dictionary-based method for 1- N mismatch and a template-based method for N-1 mismatch. We evaluate our sen- tence rewriting based semantic parser on the benchmark semantic parsing dataset – WEBQUESTIONS. Experimental results show that our system outperforms the base system with a 3.4% gain in F1, and gen- erates logical forms more accurately and parses sentences more robustly. ",,,,ACL
74,2016,Chinese Zero Pronoun Resolution with Deep Neural Networks,"Chen Chen, Vincent Ng","While unsupervised anaphoric zero pro- noun (AZP) resolvers have recently been shown to rival their supervised counter- parts in performance, it is relatively diffi- cult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features. To address these weaknesses, we propose a supervised approach to AZP resolution based on deep neural networks, taking advantage of their ability to learn useful task-specific representations and ef- fectively exploit lexical features via word embeddings. Our approach achieves state- of-the-art performance when resolving the Chinese AZPs in the OntoNotes corpus. ",,,,ACL
75,2016,Constrained Multi-Task Learning for Automated Essay Scoring,"Ronan Cummins, Meng Zhang, Ted Briscoe","Supervised machine learning models for automated essay scoring (AES) usually re- quire substantial task-specific training data in order to make accurate predictions for a particular writing task. This limita- tion hinders their utility, and consequently their deployment in real-world settings. In this paper, we overcome this shortcoming using a constrained multi-task pairwise- preference learning approach that enables the data from multiple tasks to be com- bined effectively. Furthermore, contrary to some recent re- search, we show that high performance AES systems can be built with little or no task-specific training data. We perform a detailed study of our approach on a pub- licly available dataset in scenarios where we have varying amounts of task-specific training data and in scenarios where the number of tasks increases. ",,,,ACL
76,2016,CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases,"Zihang Dai, Lei Li, Wei Xu","How can we enable computers to automat- ically answer questions like “Who created the character Harry Potter”? Carefully built knowledge bases provide rich sources of facts. However, it remains a chal- lenge to answer factoid questions raised in natural language due to numerous ex- pressions of one question. In particular, we focus on the most common questions — ones that can be answered with a sin- gle fact in the knowledge base. We pro- pose CFO, a Conditional Focused neural- network-based approach to answering fac- toid questions with knowledge bases. Our approach first zooms in a question to find more probable candidate subject men- tions, and infers the final answers with a unified conditional probabilistic frame- work. Powered by deep recurrent neural networks and neural embeddings, our pro- posed CFO achieves an accuracy of 75.7% on a dataset of 108k questions – the largest public one to date. It outperforms the cur- rent state of the art by an absolute margin of 11.8%. ",,,,ACL
77,2016,Verbs Taking Clausal and Non-Finite Arguments as Signals of Modality – Revisiting the Issue of Meaning Grounded in Syntax,Judith Eckle-Kohler,"We revisit Levin’s theory about the correspondence of verb meaning and syntax and infer semantic classes from a large syntactic classification of more than 600 German verbs taking clausal and non-finite arguments. Grasping the meaning components of Levin-classes is known to be hard. We address this chal- lenge by setting up a multi-perspective semantic characterization of the inferred classes. To this end, we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons – the German wordnet GermaNet, VerbNet and FrameNet – and perform a detailed analysis and evaluation of the resulting German–English classification (avail- able at www.ukp.tu-darmstadt. de/modality-verbclasses/). ",,,,ACL
78,2016,Tree-to-Sequence Attentional Neural Machine Translation,"Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka","Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntac- tic NMT model, extending a sequence- to-sequence model with the source-side phrase structure. Our model has an at- tention mechanism that enables the de- coder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experi- mental results on the WAT’15 English- to-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system. ",,,,ACL
79,2016,Coordination Annotation Extension in the Penn Tree Bank,"Jessica Ficler, Yoav Goldberg","Coordination is an important and common syntactic construction which is not han- dled well by state of the art parsers. Co- ordinations in the Penn Treebank are miss- ing internal structure in many cases, do not include explicit marking of the conjuncts and contain various errors and inconsisten- cies. In this work, we initiated manual an- notation process for solving these issues. We identify the different elements in a co- ordination phrase and label each element with its function. We add phrase bound- aries when these are missing, unify incon- sistencies, and fix errors. The outcome is an extension of the PTB that includes con- sistent and detailed structures for coordi- nations. We make the coordination anno- tation publicly available, in hope that they will facilitate further research into coordi- nation disambiguation. 1 ",,,,ACL
80,2016,Analyzing Biases in Human Perception of User Age and Gender from Text,"Lucie Flekova, Jordan Carpenter, Salvatore Giorgi, Lyle Ungar","User traits disclosed through written text, such as age and gender, can be used to per- sonalize applications such as recommender systems or conversational agents. However, human perception of these traits is not per- fectly aligned with reality. In this paper, we conduct a large-scale crowdsourcing ex- periment on guessing age and gender from tweets. We systematically analyze the qual- ity and possible biases of these predictions. We identify the textual cues which lead to miss-assessments of traits or make annota- tors more or less confident in their choice. Our study demonstrates that differences be- tween real and perceived traits are notewor- thy and elucidates inaccurately used stereo- types in human perception. ",,,,ACL
81,2016,Modeling Social Norms Evolution for Personalized Sentiment Classification,"Lin Gong, Mohammad Al Boni, Hongning Wang","Motivated by the findings in social sci- ence that people’s opinions are diverse and variable while together they are shaped by evolving social norms, we perform person- alized sentiment classification via shared model adaptation over time. In our pro- posed solution, a global sentiment model is constantly updated to capture the ho- mogeneity in which users express opin- ions, while personalized models are simul- taneously adapted from the global model to recognize the heterogeneity of opin- ions from individuals. Global model shar- ing alleviates data sparsity issue, and in- dividualized model adaptation enables ef- ficient online model learning. Extensive experimentations are performed on two large review collections from Amazon and Yelp, and encouraging performance gain is achieved against several state-of-the-art transfer learning and multi-task learning based sentiment classification solutions. ",,,,ACL
82,2016,Modeling Concept Dependencies in a Scientific Corpus,"Jonathan Gordon, Linhong Zhu, Aram Galstyan, Prem Natarajan","Our goal is to generate reading lists for stu- dents that help them optimally learn techni- cal material. Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query. This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered. Here we formulate an information-theoretic view of concept de- pendency and present methods to construct a “concept graph” automatically from a text corpus. We perform the first human evalu- ation of concept dependency edges (to be published as open data), and the results ver- ify the feasibility of automatic approaches for inferring concepts and their dependency relations. This result can support search ca- pabilities that may be tuned to help users learn a subject rather than retrieve docu- ments based on a single query. ",,,,ACL
83,2016,Normalized Log-Linear Interpolation of Backoff Language Models is Efficient,"Kenneth Heafield, Chase Geigle, Sean Massung, Lane Schwartz","We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single nor- malized backoff model, contradicting Hsu (2007). While prior work reported that log-linear interpolation yields lower per- plexity than linear interpolation, normaliz- ing at query time was impractical. We nor- malize the model offline in advance, which is efficient due to a recurrence relationship between the normalizing factors. To tune interpolation weights, we apply Newton’s method to this convex problem and show that the derivatives can be computed ef- ficiently in a batch process. These find- ings are combined in new open-source in- terpolation tool, which is distributed with KenLM. With 21 out-of-domain corpora, log-linear interpolation yields 72.58 per- plexity on TED talks, compared to 75.91 for linear interpolation. ",,,,ACL
84,2016,How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation,"Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin","Recently a few systems for automatically solving math word problems have reported promising results. However, the datasets used for evaluation have limitations in both scale and diversity. In this paper, we build a large-scale dataset which is more than 9 times the size of previous ones, and contains many more problem types. Problems in the dataset are semi- automatically obtained from community question-answering (CQA) web pages. A ranking SVM model is trained to auto- matically extract problem answers from the answer text provided by CQA users, which significantly reduces human anno- tation cost. Experiments conducted on the new dataset lead to interesting and surpris- ing results. ",,,,ACL
85,2016,Embeddings for Word Sense Disambiguation: An Evaluation Study,"Ignacio Iacobacci, Mohammad Taher Pilehvar, Roberto Navigli","Recent years have seen a dramatic growth in the popularity of word embeddings mainly owing to their ability to capture se- mantic information from massive amounts of textual content. As a result, many tasks in Natural Language Processing have tried to take advantage of the potential of these distributional models. In this work, we study how word embeddings can be used in Word Sense Disambiguation, one of the oldest tasks in Natural Language Processing and Artificial Intelligence. We propose different methods through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture, and perform a deep analysis of how different parameters affect perfor- mance. We show how a WSD system that makes use of word embeddings alone, if designed properly, can provide significant performance improvement over a state-of- the-art WSD system that incorporates sev- eral standard WSD features. ",,,,ACL
86,2016,Text Understanding with the Attention Sum Reader Network,"Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, Jan Kleindienst","Several large cloze-style context-question- answer datasets have been introduced re- cently: the CNN and Daily Mail news data and the Children’s Book Test. Thanks to the size of these datasets, the asso- ciated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alterna- tive approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended rep- resentation of words in the document as is usual in similar models. This makes the model particularly suitable for question- answering problems where the answer is a single word from the document. Ensem- ble of our models sets new state of the art on all evaluated datasets. ",,,,ACL
87,2016,Investigating LSTMs for Joint Extraction of Opinion Entities and Relations,"Arzoo Katiyar, Claire Cardie","We investigate the use of deep bi- directional LSTMs for joint extraction of opinion entities and the IS - FROM and IS - ABOUT relations that connect them — the first such attempt using a deep learning approach. Perhaps surprisingly, we find that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint in- ference approach (Yang and Cardie, 2013) to opinion entities extraction, perform- ing below even the standalone sequence- tagging CRF. Incorporating sentence-level and a novel relation-level optimization, however, allows the LSTM to identify opinion relations and to perform within 1– 3% of the state-of-the-art joint model for opinion entities and the IS - FROM relation; and to perform as well as the state-of-the- art for the IS - ABOUT relation — all with- out access to opinion lexicons, parsers and other preprocessing components required for the feature-rich CRF+ILP approach. ",,,,ACL
88,2016,Transition-Based Left-Corner Parsing for Identifying PTB-Style Nonlocal Dependencies,"Yoshihide Kato, Shigeki Matsubara",This paper proposes a left-corner parser which can identify nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based sys- tem. We use a structured perceptron which enables our parser to utilize global features captured by nonlocal dependencies. An experimental result demonstrates that our parser achieves a good balance between constituent parsing and nonlocal depen- dency identification. ,,,,ACL
89,2016,Siamese CBOW: Optimizing Word Embeddings for Sentence Representations,"Tom Kenter, Alexey Borisov, Maarten de Rijke","We present the Siamese Continuous Bag of Words (Siamese CBOW) model, a neural network for efficient estimation of high- quality sentence embeddings. Averaging the embeddings of words in a sentence has proven to be a surprisingly success- ful and efficient way of obtaining sen- tence embeddings. However, word em- beddings trained with the methods cur- rently available are not optimized for the task of sentence representation, and, thus, likely to be suboptimal. Siamese CBOW handles this problem by training word em- beddings directly for the purpose of be- ing averaged. The underlying neural net- work learns word embeddings by predict- ing, from a sentence representation, its surrounding sentences. We show the ro- bustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources. ",,,,ACL
90,2016,Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings,"Fereshte Khani, Martin Rinard, Percy Liang","Can we train a system that, on any new input, either says “don’t know” or makes a prediction that is guaranteed to be cor- rect? We answer the question in the affir- mative provided our model family is well- specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operational- ize this principle for semantic parsing, the task of mapping utterances to logi- cal forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only check- ing two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empiri- cally, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset. ",,,,ACL
91,2016,Exploring Convolutional and Recurrent Neural Networks in Sequential Labelling for Dialogue Topic Tracking,"Seokhwan Kim, Rafael Banchs, Haizhou Li","Dialogue topic tracking is a sequential la- belling problem of recognizing the topic state at each time step in given dialogue sequences. This paper presents various ar- tificial neural network models for dialogue topic tracking, including convolutional neural networks to account for semantics at each individual utterance, and recurrent neural networks to account for conversa- tional contexts along multiple turns in the dialogue history. The experimental results demonstrate that our proposed models can significantly improve the tracking perfor- mances in human-human conversations. ",,,,ACL
92,2016,Cross-Lingual Lexico-Semantic Transfer in Language Learning,"Ekaterina Kochmar, Ekaterina Shutova","Lexico-semantic knowledge of our native language provides an initial foundation for second language learning. In this paper, we investigate whether and to what extent the lexico-semantic models of the native language (L1) are transferred to the sec- ond language (L2). Specifically, we focus on the problem of lexical choice and in- vestigate it in the context of three typolog- ically diverse languages: Russian, Span- ish and English. We show that a statistical semantic model learned from L1 data im- proves automatic error detection in L2 for the speakers of the respective L1. Finally, we investigate whether the semantic model learned from a particular L1 is portable to other, typologically related languages. ",,,,ACL
93,2016,A CALL System for Learning Preposition Usage,"John Lee, Donald Sturgeon, Mengqi Luo","Fill-in-the-blank items are commonly fea- tured in computer-assisted language learn- ing (CALL) systems. An item displays a sentence with a blank, and often proposes a number of choices for filling it. These choices should include one correct answer and several plausible distractors. We de- scribe a system that, given an English cor- pus, automatically generates distractors to produce items for preposition usage. We report a comprehensive evaluation on this system, involving both experts and learners. First, we analyze the diffi- culty levels of machine-generated carrier sentences and distractors, comparing sev- eral methods that exploit learner error and learner revision patterns. We show that the quality of machine-generated items ap- proaches that of human-crafted ones. Fur- ther, we investigate the extent to which mismatched L1 between the user and the learner corpora affects the quality of dis- tractors. Finally, we measure the system’s impact on the user’s language proficiency in both the short and the long term. ",,,,ACL
94,2016,A Persona-Based Neural Conversation Model,"Jiwei Li, Michel Galley, Chris Brockett, Georgios Spithourakis","We present persona-based models for han- dling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed em- beddings that capture individual charac- teristics such as background information and speaking style. A dyadic speaker- addressee model captures properties of in- teractions between two interlocutors. Our models yield qualitative performance im- provements in both perplexity and B LEU scores over baseline sequence-to-sequence models, with similar gains in speaker con- sistency as measured by human judges. ",,,,ACL
95,2016,Discriminative Deep Random Walk for Network Classification,"Juzheng Li, Jun Zhu, Bo Zhang","Deep Random Walk (DeepWalk) can learn a latent space representation for describ- ing the topological structure of a network. However, for relational network classifi- cation, DeepWalk can be suboptimal as it lacks a mechanism to optimize the ob- jective of the target task. In this paper, we present Discriminative Deep Random Walk (DDRW), a novel method for re- lational network classification. By solv- ing a joint optimization problem, DDRW can learn the latent space representations that well capture the topological struc- ture and meanwhile are discriminative for the network classification task. Our ex- perimental results on several real social networks demonstrate that DDRW signif- icantly outperforms DeepWalk on multi- label network classification tasks, while retaining the topological structure in the latent space. DDRW is stable and con- sistently outperforms the baseline meth- ods by various percentages of labeled data. DDRW is also an online method that is scalable and can be naturally parallelized. ",,,,ACL
96,2016,Normalising Medical Concepts in Social Media Texts by Learning Semantic Representation,"Nut Limsopatham, Nigel Collier","Automatically recognising medical con- cepts mentioned in social media messages (e.g. tweets) enables several applications for enhancing health quality of people in a community, e.g. real-time monitoring of infectious diseases in population. How- ever, the discrepancy between the type of language used in social media and med- ical ontologies poses a major challenge. Existing studies deal with this challenge by employing techniques, such as lexi- cal term matching and statistical machine translation. In this work, we handle the medical concept normalisation at the se- mantic level. We investigate the use of neural networks to learn the transition be- tween layman’s language used in social media messages and formal medical lan- guage used in the descriptions of medi- cal concepts in a standard ontology. We evaluate our approaches using three differ- ent datasets, where social media texts are extracted from Twitter messages and blog posts. Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines, which achieved state-of-the-art performance on several medical concept normalisation tasks, by up to 44%. ",,,,ACL
97,2016,Agreement-based Learning of Parallel Lexicons and Phrases from Non-Parallel Corpora,"Chunyang Liu, Yang Liu, Maosong Sun, Huanbo Luan","We introduce an agreement-based ap- proach to learning parallel lexicons and phrases from non-parallel corpora. The basic idea is to encourage two asym- metric latent-variable translation models (i.e., source-to-target and target-to-source) to agree on identifying latent phrase and word alignments. The agreement is de- fined at both word and phrase levels. We develop a Viterbi EM algorithm for jointly training the two unidirectional models ef- ficiently. Experiments on the Chinese- English dataset show that agreement- based learning significantly improves both alignment and translation performance. ",,,,ACL
98,2016,Deep Fusion LSTMs for Text Semantic Matching,"Pengfei Liu, Xipeng Qiu, Jifan Chen, Xuanjing Huang","Recently, there is rising interest in mod- elling the interactions of text pair with deep neural networks. In this paper, we propose a model of deep fusion LSTMs (DF-LSTMs) to model the strong inter- action of text pair in a recursive match- ing way. Specifically, DF-LSTMs con- sist of two interdependent LSTMs, each of which models a sequence under the in- fluence of another. We also use exter- nal memory to increase the capacity of LSTMs, thereby possibly capturing more complicated matching patterns. Experi- ments on two very large datasets demon- strate the efficacy of our proposed archi- tecture. Furthermore, we present an elab- orate qualitative analysis of our models, giving an intuitive understanding how our model worked. ",,,,ACL
99,2016,Understanding Discourse on Work and Job-Related Well-Being in Public Social Media,"Tong Liu, Christopher Homan, Cecilia Ovesdotter Alm, Megan Lytle","We construct a humans-in-the-loop super- vised learning framework that integrates crowdsourcing feedback and local knowl- edge to detect job-related tweets from in- dividual and business accounts. Using data-driven ethnography, we examine dis- course about work by fusing language- based analysis with temporal, geospa- tional, and labor statistics information. ",,,,ACL
100,2016,Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models,"Minh-Thang Luong, Christopher D. Manning","Nearly all previous work on neural ma- chine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word- character solution to achieving open vo- cabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recur- rent neural networks compute source word representations and recover unknown tar- get words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT’15 English to Czech translation task, this hybrid approach offers an ad- dition boost of +2.1?11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words. ",,,,ACL
101,2016,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,"Xuezhe Ma, Eduard Hovy","State-of-the-art sequence labeling systems traditionally require large amounts of task- specific knowledge in the form of hand- crafted features and data pre-processing. In this paper, we introduce a novel neu- tral network architecture that benefits from both word- and character-level representa- tions automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requir- ing no feature engineering or data pre- processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks — Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 cor- pus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets — 97.55% accuracy for POS tagging and 91.21% F1 for NER. ",,,,ACL
102,2016,Off-topic Response Detection for Spontaneous Spoken English Assessment,"Andrey Malinin, Rogier Van Dalen, Kate Knill, Yu Wang","Automatic spoken language assessment systems are becoming increasingly impor- tant to meet the demand for English sec- ond language learning. This is a challeng- ing task due to the high error rates of, even state-of-the-art, non-native speech recog- nition. Consequently current systems pri- marily assess fluency and pronunciation. However, content assessment is essential for full automation. As a first stage it is important to judge whether the speaker re- sponds on topic to test questions designed to elicit spontaneous speech. Standard ap- proaches to off-topic response detection assess similarity between the response and question based on bag-of-words represen- tations. An alternative framework based on Recurrent Neural Network Language Models (RNNLM) is proposed in this pa- per. The RNNLM is adapted to the topic of each test question. It learns to asso- ciate example responses to questions with points in a topic space constructed using these example responses. Classification is done by ranking the topic-conditional posterior probabilities of a response. The RNNLMs associate a broad range of re- sponses with each topic, incorporate se- quence information and scale better with additional training data, unlike standard methods. On experiments conducted on data from the Business Language Testing Service (BULATS) this approach outper- forms standard approaches. ",,,,ACL
103,2016,Synthesizing Compound Words for Machine Translation,"Austin Matthews, Eva Schlinger, Alon Lavie, Chris Dyer","Most machine translation systems con- struct translations from a closed vocabu- lary of target word forms, posing problems for translating into languages that have productive compounding processes. We present a simple and effective approach that deals with this problem in two phases. First, we build a classifier that identifies spans of the input text that can be trans- lated into a single compound word in the target language. Then, for each identi- fied span, we generate a pool of possible compounds which are added to the trans- lation model as “synthetic” phrase trans- lations. Experiments reveal that (i) we can effectively predict what spans can be compounded; (ii) our compound gener- ation model produces good compounds; and (iii) modest improvements are pos- sible in end-to-end English–German and English–Finnish translation tasks. We ad- ditionally introduce KomposEval, a new multi-reference dataset of English phrases and their translations into German com- pounds. ",,,,ACL
104,2016,Harnessing Cognitive Features for Sarcasm Detection,"Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal Dey","In this paper, we propose a novel mecha- nism for enriching the feature vector, for the task of sarcasm detection, with cogni- tive features extracted from eye-movement patterns of human readers. Sarcasm detec- tion has been a challenging research prob- lem, and its importance for NLP applica- tions such as review summarization, dia- log systems and sentiment analysis is well recognized. Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds. This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text. We observe the difference in the be- haviour of the eye, while reading sarcastic and non sarcastic sentences. Motivated by this observation, we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features ob- tained from readers eye movement data. We perform statistical classification using the enhanced feature set so obtained. The augmented cognitive features improve sar- casm detection by 3.7% (in terms of F- score), over the performance of the best reported system. ",,,,ACL
105,2016,End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures,"Makoto Miwa, Mohit Bansal","We present a novel end-to-end neural model to extract entities and relations be- tween them. Our recurrent neural net- work based model captures both word se- quence and dependency tree substructure information by stacking bidirectional tree- structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a sin- gle model. We further encourage detec- tion of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sam- pling. Our model improves over the state- of-the-art feature-based model on end-to- end relation extraction, achieving 12.1% and 5.7% relative error reductions in F1- score on ACE2005 and ACE2004, respec- tively. We also show that our LSTM- RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classifica- tion (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components. ",,,,ACL
106,2016,A short proof that O_2 is an MCFL,Mark-Jan Nederhof,"We present a new proof that O 2 is a mul- tiple context-free language. It contrasts with a recent proof by Salvati (2015) in its avoidance of concepts that seem specific to two-dimensional geometry, such as the complex exponential function. Our simple proof creates realistic prospects of widen- ing the results to higher dimensions. This finding is of central importance to the rela- tion between extreme free word order and classes of grammars used to describe the syntax of natural language. ",,,,ACL
107,2016,Context-aware Argumentative Relation Mining,"Huy Nguyen, Diane Litman","Context is crucial for identifying argumen- tative relations in text, but many argument mining methods make little use of contex- tual features. This paper presents context- aware argumentative relation mining that uses features extracted from writing top- ics as well as from windows of context sentences. Experiments on student essays demonstrate that the proposed features im- prove predictive performance in two argu- mentative relation classification tasks. ",,,,ACL
108,2016,Leveraging Inflection Tables for Stemming and Lemmatization.,"Garrett Nicolai, Grzegorz Kondrak","We present several methods for stemming and lemmatization based on discrimina- tive string transduction. We exploit the paradigmatic regularity of semi-structured inflection tables to identify stems in an un- supervised manner with over 85% accu- racy. Experiments on English, Dutch and German show that our stemmers substan- tially outperform Snowball and Morfes- sor, and approach the accuracy of a super- vised model. Furthermore, the generated stems are more consistent than those an- notated by experts. Our direct lemmatiza- tion model is more accurate than Morfette and Lemming on most datasets. Finally, we test our methods on the data from the shared task on morphological reinflection. ",,,,ACL
109,2016,Scaling a Natural Language Generation System,"Jonathan Pfeil, Soumya Ray","A key goal in natural language genera- tion (NLG) is to enable fast generation even with large vocabularies, grammars and worlds. In this work, we build upon a recently proposed NLG system, Sentence Tree Realization with UCT (STRUCT). We describe four enhancements to this system: (i) pruning the grammar based on the world and the communicative goal, (ii) intelligently caching and pruning the com- binatorial space of semantic bindings, (iii) reusing the lookahead search tree at differ- ent search depths, and (iv) learning and us- ing a search control heuristic. We evaluate the resulting system on three datasets of increasing size and complexity, the largest of which has a vocabulary of about 10K words, a grammar of about 32K lexical- ized trees and a world with about 11K enti- ties and 23K relations between them. Our results show that the system has a median generation time of 8.5s and finds the best sentence on average within 25s. These re- sults are based on a sequential, interpreted implementation and are significantly bet- ter than the state of the art for planning- based NLG systems. ",,,,ACL
110,2016,ALTO: Active Learning with Topic Overviews for Speeding Label Induction and Document Labeling,"Forough Poursabzi-Sangdeh, Jordan Boyd-Graber, Leah Findlater, Kevin Seppi","Effective text classification requires experts to annotate data with labels; these training data are time-consuming and expensive to obtain. If you know what labels you want, active learning can reduce the number of labeled documents needed. However, estab- lishing the label set remains difficult. An- notators often lack the global knowledge needed to induce a label set. We intro- duce ALTO : Active Learning with Topic Overviews, an interactive system to help humans annotate documents: topic mod- els provide a global overview of what la- bels to create and active learning directs them to the right documents to label. Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions, topic models (even by themselves) lead to better label sets, and ALTO ’s combination is best over- all. ",,,,ACL
111,2016,Predicting the Rise and Fall of Scientific Topics from Trends in their Rhetorical Framing,"Vinodkumar Prabhakaran, William L. Hamilton, Dan McFarland, Dan Jurafsky","Computationally modeling the evolution of science by tracking how scientific top- ics rise and fall over time has important implications for research funding and pub- lic policy. However, little is known about the mechanisms underlying topic growth and decline. We investigate the role of rhetorical framing: whether the rhetori- cal role or function that authors ascribe to topics (as methods, as goals, as results, etc.) relates to the historical trajectory of the topics. We train topic models and a rhetorical function classifier to map topic models onto their rhetorical roles in 2.4 million abstracts from the Web of Science from 1991-2010. We find that a topic’s rhetorical function is highly predictive of its eventual growth or decline. For exam- ple, topics that are rhetorically described as results tend to be in decline, while top- ics that function as methods tend to be in early phases of growth. ",,,,ACL
112,2016,Compositional Sequence Labeling Models for Error Detection in Learner Writing,"Marek Rei, Helen Yannakoudakis","In this paper, we present the first exper- iments using neural network models for the task of error detection in learner writ- ing. We perform a systematic comparison of alternative compositional architectures and propose a framework for error detec- tion based on bidirectional LSTMs. Ex- periments on the CoNLL-14 shared task dataset show the model is able to outper- form other participants on detecting er- rors in learner writing. Finally, the model is integrated with a publicly deployed self-assessment system, leading to perfor- mance comparable to human annotators. ",,,,ACL
113,2016,Neural Semantic Role Labeling with Dependency Path Embeddings,"Michael Roth, Mirella Lapata","This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the obser- vation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as sub- sequences of lexicalized dependency paths and learns suitable embedding representa- tions. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role la- belers, and showcase qualitative improve- ments obtained by our method. ",,,,ACL
114,2016,Prediction of Prospective User Engagement with Intelligent Assistants,"Shumpei Sano, Nobuhiro Kaji, Manabu Sassano","Intelligent assistants on mobile devices, such as Siri, have recently gained con- siderable attention as novel applications of dialogue technologies. A tremendous amount of real users of intelligent assis- tants provide us with an opportunity to ex- plore a novel task of predicting whether users will continually use their intelligent assistants in the future. We developed pre- diction models of prospective user engage- ment by using large-scale user logs ob- tained from a commercial intelligent as- sistant. Experiments demonstrated that our models can predict prospective user engagement reasonably well, and outper- forms a strong baseline that makes predic- tion based past utterance frequency. ",,,,ACL
115,2016,Resolving References to Objects in Photographs using the Words-As-Classifiers Model,"David Schlangen, Sina Zarrieß, Casey Kennington","A common use of language is to refer to visually present objects. Modelling it in computers requires modelling the link between language and perception. The “words as classifiers” model of grounded semantics views words as classifiers of perceptual contexts, and composes the meaning of a phrase through composition of the denotations of its component words. It was recently shown to perform well in a game-playing scenario with a small num- ber of object types. We apply it to two large sets of real-world photographs that contain a much larger variety of object types and for which referring expressions are available. Using a pre-trained convolu- tional neural network to extract image re- gion features, and augmenting these with positional information, we show that the model achieves performance competitive with the state of the art in a reference res- olution task (given expression, find bound- ing box of its referent), while, as we argue, being conceptually simpler and more flex- ible. ",,,,ACL
116,2016,RBPB: Regularization-Based Pattern Balancing Method for Event Extraction,"Lei Sha, Jing Liu, Chin-Yew Lin, Sujian Li","Event extraction is a particularly chal- lenging information extraction task, which intends to identify and classify event triggers and arguments from raw text. In recent works, when determining event types (trigger classification), most of the works are either pattern-only or feature-only. However, although patterns cannot cover all representations of an event, it is still a very important feature. In addition, when identifying and classifying arguments, previous works consider each candidate argument separately while ignoring the relationship between arguments. This paper proposes a Regularization-Based Pattern Balancing Method (RBPB). Inspired by the progress in representation learning, we use trigger embedding, sentence-level embedding and pattern features together as our features for trigger classification so that the effect of patterns and other useful features can be balanced. In addition, RBPB uses a regularization method to take advantage of the relationship between arguments. Experiments show that we achieve results better than current state-of-art equivalents. ",,,,ACL
117,2016,Neural Network-Based Model for Japanese Predicate Argument Structure Analysis,"Tomohide Shibata, Daisuke Kawahara, Sadao Kurohashi","This paper presents a novel model for Japanese predicate argument structure (PAS) analysis based on a neural network framework. Japanese PAS analysis is chal- lenging due to the tangled characteristics of the Japanese language, such as case dis- appearance and argument omission. To unravel this problem, we learn selectional preferences from a large raw corpus, and incorporate them into a SOTA PAS anal- ysis model, which considers the consis- tency of all PASs in a given sentence. We demonstrate that the proposed PAS anal- ysis model significantly outperforms the base SOTA system. ",,,,ACL
118,2016,Addressing Limited Data for Textual Entailment Across Domains,"Chaitanya Shivade, Preethi Raghavan, Siddharth Patwardhan","We seek to address the lack of labeled data (and high cost of annotation) for textual entailment in some domains. To that end, we first create (for experimental purposes) an entailment dataset for the clinical do- main, and a highly competitive supervised entailment system, E NT , that is effective (out of the box) on two domains. We then explore self-training and active learn- ing strategies to address the lack of la- beled data. With self-training, we success- fully exploit unlabeled data to improve over E NT by 15% F-score on the newswire domain, and 13% F-score on clinical data. On the other hand, our active learning ex- periments demonstrate that we can match (and even beat) E NT using only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain. ",,,,ACL
119,2016,Annotating and Predicting Non-Restrictive Noun Phrase Modifications,"Gabriel Stanovsky, Ido Dagan","The distinction between restrictive and non-restrictive modification in noun phrases is a well studied subject in linguistics. Automatically identifying non-restrictive modifiers can provide NLP applications with shorter, more salient arguments, which were found beneficial by several recent works. While previous work showed that restrictiveness can be annotated with high agreement, no large scale corpus was created, hindering the development of suitable classification algorithms. In this work we devise a novel crowdsourcing annotation methodology, and an accompanying large scale corpus. Then, we present a robust automated system which identifies non-restrictive modifiers, notably improving over prior methods. ",,,,ACL
120,2016,Bilingual Segmented Topic Model,"Akihiro Tamura, Eiichiro Sumita","This study proposes the bilingual seg- mented topic model (BiSTM), which hi- erarchically models documents by treat- ing each document as a set of segments, e.g., sections. While previous bilingual topic models, such as bilingual latent Dirichlet allocation (BiLDA) (Mimno et al., 2009; Ni et al., 2009), consider only cross-lingual alignments between entire documents, the proposed model consid- ers cross-lingual alignments between seg- ments in addition to document-level align- ments and assigns the same topic distri- bution to aligned segments. This study also presents a method for simultane- ously inferring latent topics and segmen- tation boundaries, incorporating unsuper- vised topic segmentation (Du et al., 2013) into BiSTM. Experimental results show that the proposed model significantly out- performs BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction (up to +0.083 extraction accuracy). ",,,,ACL
121,2016,Learning Semantically and Additively Compositional Distributional Representations,"Ran Tian, Naoaki Okazaki, Kentaro Inui","This paper connects a vector-based com- position model to a formal semantics, the Dependency-based Compositional Se- mantics (DCS). We show theoretical evi- dence that the vector compositions in our model conform to the logic of DCS. Ex- perimentally, we show that vector-based composition brings a strong ability to calculate similar phrases as similar vec- tors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence comple- tion task and report a new state-of-the-art. ",,,,ACL
122,2016,Inner Attention based Recurrent Neural Networks for Answer Selection,"Bingning Wang, Kang Liu, Jun Zhao","Attention based recurrent neural networks have shown advantages in representing natural language sentences (Hermann et al., 2015; Rockt¨aschel et al., 2015; Tan et al., 2015). Based on recurrent neural networks (RNN), external attention infor- mation was added to hidden representa- tions to get an attentive sentence represen- tation. Despite the improvement over non- attentive models, the attention mechanism under RNN is not well studied. In this work, we analyze the deficiency of tradi- tional attention based RNN models quanti- tatively and qualitatively. Then we present three new RNN models that add attention information before RNN hidden represen- tation, which shows advantage in repre- senting sentence and achieves new state- of-art results in answer selection task. ",,,,ACL
123,2016,Relation Classification via Multi-Level Attention CNNs,"Linlin Wang, Zhu Cao, Gerard de Melo, Zhiyuan Liu","Relation classification is a crucial ingredi- ent in numerous information extraction sys- tems seeking to mine structured facts from text. We propose a novel convolutional neural network architecture for this task, relying on two levels of attention in order to better discern patterns in heterogeneous contexts. This architecture enables end- to-end learning from task-specific labeled data, forgoing the need for external knowl- edge such as explicit dependency structures. Experiments show that our model outper- forms previous state-of-the-art methods, in- cluding those relying on much richer forms of prior knowledge. ",,,,ACL
124,2016,Knowledge Base Completion via Coupled Path Ranking,"Quan Wang, Jing Liu, Yuanfei Luo, Bin Wang","Knowledge bases (KBs) are often greatly incomplete, necessitating a demand for K- B completion. The path ranking algorith- m (PRA) is one of the most promising ap- proaches to this task. Previous work on PRA usually follows a single-task learn- ing paradigm, building a prediction mod- el for each relation independently with its own training data. It ignores meaningful associations among certain relations, and might not get enough training data for less frequent relations. This paper proposes a novel multi-task learning framework for PRA, referred to as coupled PRA (CPRA). It first devises an agglomerative clustering strategy to automatically discover relation- s that are highly correlated to each other, and then employs a multi-task learning s- trategy to effectively couple the prediction of such relations. As such, CPRA takes in- to account relation association and enables implicit data sharing among them. We empirically evaluate CPRA on benchmark data created from Freebase. Experimen- tal results show that CPRA can effective- ly identify coherent clusters in which rela- tions are highly correlated. By further cou- pling such relations, CPRA significantly outperforms PRA, in terms of both predic- tive accuracy and model interpretability. ",,,,ACL
125,2016,Larger-Context Language Modelling with Recurrent Neural Network,"Tian Wang, Kyunghyun Cho","In this work, we propose a novel method to incorporate corpus-level discourse infor- mation into language modelling. We call this larger-context language model. We in- troduce a late fusion approach to a recur- rent language model based on long short- term memory units (LSTM), which helps the LSTM unit keep intra-sentence depen- dencies and inter-sentence dependencies separate from each other. Through the evaluation on four corpora (IMDB, BBC, Penn TreeBank, and Fil9), we demonstrate that the proposed model improves per- plexity significantly. In the experiments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporat- ing additional inputs to the LSTM. By an- alyzing the trained larger-context language model, we discover that content words, in- cluding nouns, adjectives and verbs, bene- fit most from an increasing number of con- text sentences. This analysis suggests that larger-context language model improves the unconditional language model by cap- turing the theme of a document better and more easily. ",,,,ACL
126,2016,The Creation and Analysis of a Website Privacy Policy Corpus,"Shomir Wilson, Florian Schaub, Aswarth Abhilash Dara, Frederick Liu","Website privacy policies are often ignored by Internet users, because these docu- ments tend to be long and difficult to un- derstand. However, the significance of pri- vacy policies greatly exceeds the attention paid to them: these documents are binding legal agreements between website opera- tors and their users, and their opaqueness is a challenge not only to Internet users but also to policy regulators. One proposed al- ternative to the status quo is to automate or semi-automate the extraction of salient de- tails from privacy policy text, using a com- bination of crowdsourcing, natural lan- guage processing, and machine learning. However, there has been a relative dearth of datasets appropriate for identifying data practices in privacy policies. To remedy this problem, we introduce a corpus of 115 privacy policies (267K words) with man- ual annotations for 23K fine-grained data practices. We describe the process of us- ing skilled annotators and a purpose-built annotation tool to produce the data. We provide findings based on a census of the annotations and show results toward au- tomating the annotation procedure. Fi- nally, we describe challenges and oppor- tunities for the research community to use this corpus to advance research in both pri- vacy and language technologies. ",,,,ACL
127,2016,Sequence-based Structured Prediction for Semantic Parsing,"Chunyang Xiao, Marc Dymetman, Claire Gardent","We propose an approach for semantic parsing that uses a recurrent neural net- work to map a natural language question into a logical form representation of a KB query. Building on recent work by (Wang et al., 2015), the interpretable log- ical forms, which are structured objects obeying certain constraints, are enumer- ated by an underlying grammar and are paired with their canonical realizations. In order to use sequence prediction, we need to sequentialize these logical forms. We compare three sequentializations: a direct linearization of the logical form, a linearization of the associated canonical realization, and a sequence consisting of derivation steps relative to the underlying grammar. We also show how grammati- cal constraints on the derivation sequence can easily be integrated inside the RNN- based sequential predictor. Our experi- ments show important improvements over previous results for the same dataset, and also demonstrate the advantage of incor- porating the grammatical constraints. ",,,,ACL
128,2016,Learning Word Meta-Embeddings,"Wenpeng Yin, Hinrich Schütze","Word embeddings – distributed represen- tations of words – in deep learning are beneficial for many tasks in NLP. How- ever, different embedding sets vary greatly in quality and characteristics of the cap- tured information. Instead of relying on a more advanced algorithm for embed- ding learning, this paper proposes an en- semble approach of combining different public embedding sets with the aim of learning metaembeddings. Experiments on word similarity and analogy tasks and on part-of-speech tagging show better per- formance of metaembeddings compared to individual embedding sets. One advan- tage of metaembeddings is the increased vocabulary coverage. We release our metaembeddings publicly at http:// cistern.cis.lmu.de/meta-emb. ",,,,ACL
129,2016,Towards Constructing Sports News from Live Text Commentary,"Jianmin Zhang, Jin-ge Yao, Xiaojun Wan","In this paper, we investigate the possibil- ity to automatically generate sports news from live text commentary scripts. As a preliminary study, we treat this task as a special kind of document summarization based on sentence extraction. We for- mulate the task in a supervised learning to rank framework, utilizing both tradi- tional sentence features for generic docu- ment summarization and novelly designed task-specific features. To tackle the prob- lem of local redundancy, we also propose a probabilistic sentence selection algorithm. Experiments on our collected data from football live commentary scripts and cor- responding sports news demonstrate the feasibility of this task. Evaluation results show that our methods are indeed appro- priate for this task, outperforming several baseline methods in different aspects. ",,,,ACL
130,2016,A Continuous Space Rule Selection Model for Syntax-based Statistical Machine Translation,"Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Graham Neubig","One of the major challenges for statisti- cal machine translation (SMT) is to choose the appropriate translation rules based on the sentence context. This paper pro- poses a continuous space rule selection (CSRS) model for syntax-based SMT to perform this context-dependent rule selec- tion. In contrast to existing maximum en- tropy based rule selection (MERS) mod- els, which use discrete representations of words as features, the CSRS model is learned by a feed-forward neural network and uses real-valued vector representa- tions of words, allowing for better gen- eralization. In addition, we propose a method to train the rule selection models only on minimal rules, which are more fre- quent and have richer training data com- pared to non-minimal rules. We tested our model on different translation tasks and the CSRS model outperformed a base- line without rule selection and the previ- ous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. ",,,,ACL
131,2016,Probabilistic Graph-based Dependency Parsing with Convolutional Neural Network,"Zhisong Zhang, Hai Zhao, Lianhui Qin","This paper presents neural probabilistic parsing models which explore up to third- order graph-based parsing with maximum likelihood training criteria. Two neural network extensions are exploited for per- formance improvement. Firstly, a convo- lutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effec- tively captured. Secondly, a linear layer is added to integrate different order neu- ral models and trained with perceptron method. The proposed parsers are evalu- ated on English and Chinese Penn Tree- banks and obtain competitive accuracies. ",,,,ACL
132,2016,A Search-Based Dynamic Reranking Model for Dependency Parsing,"Hao Zhou, Yue Zhang, Shujian Huang, Junsheng Zhou","We propose a novel reranking method to extend a deterministic neural dependency parser. Different to conventional k-best reranking, the proposed model integrates search and learning by utilizing a dynamic action revising process, using the rerank- ing model to guide modification for the base outputs and to rerank the candidates. The dynamic reranking model achieves an absolute 1.78% accuracy improvement over the deterministic baseline parser on PTB, which is the highest improvement by neural rerankers in the literature. ",,,,ACL
133,2016,Cross-Lingual Sentiment Classification with Bilingual Document Representation Learning,"Xinjie Zhou, Xiaojun Wan, Jianguo Xiao","Cross-lingual sentiment classification aims to adapt the sentiment resource in a resource-rich language to a resource-poor language. In this study, we propose a representation learning approach which simultaneously learns vector representa- tions for the texts in both the source and the target languages. Different from pre- vious research which only gets bilingual word embedding, our Bilingual Document Representation Learning model BiDRL directly learns document representations. Both semantic and sentiment correlations are utilized to map the bilingual texts into the same embedding space. The experiments are based on the multilingual multi-domain Amazon review dataset. We use English as the source language and use Japanese, German and French as the target languages. The experimental results show that BiDRL outperforms the state-of-the-art methods for all the target languages. ",,,,ACL
134,2016,Segment-Level Sequence Modeling using Gated Recursive Semi-Markov Conditional Random Fields,"Jingwei Zhuo, Yong Cao, Jun Zhu, Bo Zhang","Most of the sequence tagging tasks in nat- ural language processing require to recog- nize segments with certain syntactic role or semantic meaning in a sentence. They are usually tackled with Conditional Ran- dom Fields (CRFs), which do indirect word-level modeling over word-level fea- tures and thus cannot make full use of segment-level information. Semi-Markov Conditional Random Fields (Semi-CRFs) model segments directly but extracting segment-level features for Semi-CRFs is still a very challenging problem. This pa- per presents Gated Recursive Semi-CRFs (grSemi-CRFs), which model segments directly and automatically learn segment- level features through a gated recursive convolutional neural network. Our exper- iments on text chunking and named en- tity recognition (NER) demonstrate that grSemi-CRFs generally outperform other neural models. ",,,,ACL
135,2016,Identifying Causal Relations Using Parallel Wikipedia Articles,"Christopher Hidey, Kathy McKeown","The automatic detection of causal relation- ships in text is important for natural lan- guage understanding. This task has proven to be difficult, however, due to the need for world knowledge and inference. We fo- cus on a sub-task of this problem where an open class set of linguistic markers can provide clues towards understanding causality. Unlike the explicit markers, a closed class, these markers vary signifi- cantly in their linguistic forms. We lever- age parallel Wikipedia corpora to identify new markers that are variations on known causal phrases, creating a training set via distant supervision. We also train a causal classifier using features from the open class markers and semantic features pro- viding contextual information. The results show that our features provide an 11.05 point absolute increase over the baseline on the task of identifying causality in text. ",,,,ACL
136,2016,Compositional Learning of Embeddings for Relation Paths in Knowledge Base and Text,"Kristina Toutanova, Victoria Lin, Wen-tau Yih, Hoifung Poon","Modeling relation paths has offered sig- nificant gains in embedding models for knowledge base (KB) completion. How- ever, enumerating paths between two en- tities is very expensive, and existing ap- proaches typically resort to approxima- tion with a sampled subset. This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts men- tioned in it. In this paper, we propose the first exact dynamic programming al- gorithm which enables efficient incorpo- ration of all relation paths of bounded length, while modeling both relation types and intermediate nodes in the composi- tional path representations. We conduct a theoretical analysis of the efficiency gain from the approach. Experiments on two datasets show that it addresses representa- tional limitations in prior approaches and improves accuracy in KB completion. ",,,,ACL
137,2016,Commonsense Knowledge Base Completion,"Xiang Li, Aynaz Taheri, Lifu Tu, Kevin Gimpel","We enrich a curated resource of common- sense knowledge by formulating the prob- lem as one of knowledge base comple- tion (KBC). Most work in KBC focuses on knowledge bases like Freebase that re- late entities drawn from a fixed set. How- ever, the tuples in ConceptNet (Speer and Havasi, 2012) define relations between an unbounded set of phrases. We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones. We find strong performance from a bilinear model using a simple additive architecture to model phrases. We manually evaluate our trained model’s ability to assign quality scores to novel tuples, finding that it can propose tu- ples at the same quality level as medium- confidence tuples from ConceptNet. ",,,,ACL
138,2016,Simpler Context-Dependent Logical Forms via Model Projections,"Reginald Long, Panupong Pasupat, Percy Liang","We consider the task of learning a context- dependent mapping from utterances to de- notations. With only denotations at train- ing time, we must search over a combina- torially large space of logical forms, which is even larger with context-dependent ut- terances. To cope with this challenge, we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms. Though less expressive, we find that these simpler models are much faster and can be surprisingly effective. Moreover, they can be used to bootstrap the full model. Finally, we collected three new context- dependent semantic parsing datasets, and develop a new left-to-right parser. ",,,,ACL
139,2016,A Fast Unified Model for Parsing and Sentence Understanding,"Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta","Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they su?er from two key techni- cal problems that make them slow and un- wieldy for large-scale NLP tasks: they usu- ally operate on parsed sentences and they do not directly support batched computa- tion. We address these issues by introduc- ing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift- reduce parser. Our model supports batched computation for a speedup of up to 25× over other tree-structured models, and its integrated parser can operate on unparsed data with little loss in accuracy. We evalu- ate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models. ",,,,ACL
140,2016,Investigating Language Universal and Specific Properties in Word Embeddings,"Peng Qian, Xipeng Qiu, Xuanjing Huang","Recently, many NLP tasks have benefit- ed from distributed word representation. However, it remains unknown whether embedding models are really immune to the typological diversity of languages, despite the language-independent archi- tecture. Here we investigate three repre- sentative models on a large set of language samples by mapping dense embedding to sparse linguistic property space. Experi- ment results reveal the language universal and specific properties encoded in various word representation. Additionally, strong evidence supports the utility of word form, especially for inflectional languages. ",,,,ACL
141,2016,Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change,"William L. Hamilton, Jure Leskovec, Dan Jurafsky","Understanding how words change their meanings over time is key to models of language and cultural evolution, but his- torical data on meaning is scarce, mak- ing theories hard to develop and test. Word embeddings show promise as a di- achronic tool, but have not been carefully evaluated. We develop a robust method- ology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evo- lution. Using six historical corpora span- ning four languages and two centuries, we propose two quantitative laws of seman- tic change: (i) the law of conformity—the rate of semantic change scales with an in- verse power-law of word frequency; (ii) the law of innovation—independent of fre- quency, words that are more polysemous have higher rates of semantic change. ",,,,ACL
142,2016,"Beyond Plain Spatial Knowledge: Determining Where Entities Are and Are Not Located, and For How Long","Alakananda Vempala, Eduardo Blanco","This paper complements semantic role representations with spatial knowledge be- yond indicating plain locations. Namely, we extract where entities are (and are not) located, and for how long (seconds, hours, days, etc.). Crowdsourced annotations show that this additional knowledge is in- tuitive to humans and can be annotated by non-experts. Experimental results show that the task can be automated. ",,,,ACL
143,2016,LexSemTm: A Semantic Dataset Based on All-words Unsupervised Sense Distribution Learning,"Andrew Bennett, Timothy Baldwin, Jey Han Lau, Diana McCarthy","There has recently been a lot of interest in unsupervised methods for learning sense distributions, particularly in applications where sense distinctions are needed. This paper analyses a state-of-the-art method for sense distribution learning, and op- timises it for application to the entire vocabulary of a given language. The optimised method is then used to pro- duce L EX S EM TM: a sense frequency and semantic dataset of unprecedented size, spanning approximately 88% of polyse- mous, English simplex lemmas, which is released as a public resource to the com- munity. Finally, the quality of this data is investigated, and the L EX S EM TM sense distributions are shown to be superior to those based on the W ORD N ET first sense for lemmas missing from S EM C OR , and at least on par with S EM C OR -based distribu- tions otherwise. ",,,,ACL
144,2016,The LAMBADA dataset: Word prediction requiring a broad discourse context,"Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham","We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages shar- ing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preced- ing the target word. To succeed on LAM- BADA, computational models cannot sim- ply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA ex- emplifies a wide range of linguistic phe- nomena, and that none of several state-of- the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the develop- ment of new models capable of genuine understanding of broad context in natural language text. ",,,,ACL
145,2016,WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia,"Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin","We present W IKI R EADING , a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the cor- responding Wikipedia articles. The task contains a rich variety of challenging clas- sification and extraction sub-tasks, mak- ing it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNN- based architectures for document classifi- cation, information extraction, and ques- tion answering. We find that models sup- porting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level se- quence to sequence model with a mecha- nism to copy out-of-vocabulary words, ob- tains an accuracy of 71.8%. ",,,,ACL
146,2016,Optimizing Spectral Learning for Parsing,"Shashi Narayan, Shay B. Cohen","We describe a search algorithm for opti- mizing the number of latent states when estimating latent-variable PCFGs with spectral methods. Our results show that contrary to the common belief that the number of latent states for each nontermi- nal in an L-PCFG can be decided in isola- tion with spectral methods, parsing results significantly improve if the number of la- tent states for each nonterminal is globally optimized, while taking into account in- teractions between the different nontermi- nals. In addition, we contribute an empiri- cal analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Ko- rean, Polish and Swedish. Our results show that our estimation consistently per- forms better or close to coarse-to-fine expectation-maximization techniques for these languages. ",,,,ACL
147,2016,Stack-propagation: Improved Representation Learning for Syntax,"Yuan Zhang, David Weiss","Traditional syntax models typically lever- age part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a reg- ularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call “stack-propagation”. We apply this to de- pendency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 lan- guages from the Universal Dependencies, our method is 1.3% (absolute) more accu- rate than a state-of-the-art graph-based ap- proach and 2.7% more accurate than the most comparable greedy model. ",,,,ACL
148,2016,Inferring Perceived Demographics from User Emotional Tone and User-Environment Emotional Contrast,"Svitlana Volkova, Yoram Bachrach","We examine communications in a social network to study user emotional contrast – the propensity of users to express dif- ferent emotions than those expressed by their neighbors. Our analysis is based on a large Twitter dataset, consisting of the tweets of 123,513 users from the USA and Canada. Focusing on Ekman’s basic emo- tions, we analyze differences between the emotional tone expressed by these users and their neighbors of different types, and correlate these differences with perceived user demographics. We demonstrate that many perceived demographic traits corre- late with the emotional contrast between users and their neighbors. Unlike other ap- proaches on inferring user attributes that rely solely on user communications, we explore the network structure and show that it is possible to accurately predict a range of perceived demographic traits based solely on the emotions emanating from users and their neighbors. ",,,,ACL
149,2016,Prototype Synthesis for Model Laws,"Matthew Burgess, Eugenia Giraudy, Eytan Adar","State legislatures often rely on existing text when drafting new bills. Resource and expertise constraints, which often drive this copying behavior, can be taken ad- vantage of by lobbyists and special inter- est groups. These groups provide model bills, which encode policy agendas, with the intent that the models become actual law. Unfortunately, model legislation is often opaque to the public–both in source and content. In this paper we present L OBBY B ACK , a system that reverse en- gineers model legislation from observed text. L OBBY B ACK identifies clusters of bills which have text reuse and gener- ates “prototypes” that represent a canon- ical version of the text shared between the documents. We demonstrate that L OBBY - B ACK accurately reconstructs model leg- islation and apply it to a dataset of over 550k bills. ",,,,ACL
150,2016,Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM,"Ivan Habernal, Iryna Gurevych","We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments, namely their convincingness. We cast the problem as relation classifica- tion, where a pair of arguments having the same stance to the same prompt is judged. We annotate a large datasets of 16k pairs of arguments over 32 topics and investi- gate whether the relation “A is more con- vincing than B” exhibits properties of total ordering; these findings are used as global constraints for cleaning the crowdsourced data. We propose two tasks: (1) predicting which argument from an argument pair is more convincing and (2) ranking all argu- ments to the topic based on their convinc- ingness. We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spear- man’s correlation in a cross-topic evalua- tion. We release the newly created corpus UKPConvArg1 and the experimental soft- ware under open licenses. ",,,,ACL
151,2016,Discovery of Treatments from Text Corpora,"Christian Fong, Justin Grimmer","An extensive literature in computational social science examines how features of messages, advertisements, and other cor- pora affect individuals’ decisions, but these analyses must specify the relevant features of the text before the experiment. Automated text analysis methods are able to discover features of text, but these meth- ods cannot be used to obtain the estimates of causal effects—the quantity of inter- est for applied researchers. We introduce a new experimental design and statistical model to simultaneously discover treat- ments in a corpora and estimate causal ef- fects for these discovered treatments. We prove the conditions to identify the treat- ment effects of texts and introduce the su- pervised Indian Buffet process to discover those treatments. Our method enables us to discover treatments in a training set us- ing a collection of texts and individuals’ responses to those texts, and then esti- mate the effects of these interventions in a test set of new texts and survey respon- dents. We apply the model to an exper- iment about candidate biographies, recov- ering intuitive features of voters’ decisions and revealing a penalty for lawyers and a bonus for military service. ",,,,ACL
152,2016,Learning Structured Predictors from Bandit Feedback for Interactive NLP,"Artem Sokolov, Julia Kreutzer, Christopher Lo, Stefan Riezler","Structured prediction from bandit feed- back describes a learning scenario where instead of having access to a gold standard structure, a learner only receives partial feedback in form of the loss value of a pre- dicted structure. We present new learning objectives and algorithms for this inter- active scenario, focusing on convergence speed and ease of elicitability of feed- back. We present supervised-to-bandit simulation experiments for several NLP tasks (machine translation, sequence la- beling, text classification), showing that bandit learning from relative preferences eases feedback strength and yields im- proved empirical convergence. ",,,,ACL
153,2016,Deep Reinforcement Learning with a Natural Language Action Space,"Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao","This paper introduces a novel architec- ture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture repre- sents action and state spaces with sepa- rate embedding vectors, which are com- bined with an interaction function to ap- proximate the Q-function in reinforce- ment learning. We evaluate the DRRN on two popular text games, showing su- perior performance over other deep Q- learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text. ",,,,ACL
154,2016,Incorporating Copying Mechanism in Sequence-to-Sequence Learning,"Jiatao Gu, Zhengdong Lu, Hang Li, Victor O.K. Li","We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which cer- tain segments in the input sequence are selectively replicated in the output se- quence. A similar phenomenon is ob- servable in human language communica- tion. For example, humans tend to re- peat entity names or even long phrases in conversation. The challenge with re- gard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network- based Seq2Seq learning and propose a new model called C OPY N ET with encoder- decoder structure. C OPY N ET can nicely integrate the regular way of word gener- ation in the decoder with the new copy- ing mechanism which can choose sub- sequences in the input sequence and put them at proper places in the output se- quence. Our empirical study on both syn- thetic data sets and real world data sets demonstrates the efficacy of C OPY N ET . For example, C OPY N ET can outperform regular RNN-based model with remark- able margins on text summarization tasks. ",,,,ACL
155,2016,Cross-domain Text Classification with Multiple Domains and Disparate Label Sets,"Himanshu Sharad Bhatt, Manjira Sinha, Shourya Roy","Advances in transfer learning have let go the limitations of traditional supervised machine learning algorithms for being de- pendent on annotated training data for training new models for every new do- main. However, several applications en- counter scenarios where models need to transfer/adapt across domains when the la- bel sets vary both in terms of count of la- bels as well as their connotations. This pa- per presents first-of-its-kind transfer learn- ing algorithm for cross-domain classifica- tion with multiple source domains and dis- parate label sets. It starts with identifying transferable knowledge from across multi- ple domains that can be useful for learning the target domain task. This knowledge in the form of selective labeled instances from different domains is congregated to form an auxiliary training set which is used for learning the target domain task. Experimental results validate the efficacy of the proposed algorithm against strong baselines on a real world social media and the 20 Newsgroups datasets. ",,,,ACL
156,2016,Morphological Smoothing and Extrapolation of Word Embeddings,"Ryan Cotterell, Hinrich Schütze, Jason Eisner","Languages with rich inflectional morphol- ogy exhibit lexical data sparsity, since the word used to express a given concept will vary with the syntactic context. For in- stance, each count noun in Czech has 12 forms (where English uses only singular and plural). Even in large corpora, we are un- likely to observe all inflections of a given lemma. This reduces the vocabulary cover- age of methods that induce continuous rep- resentations for words from distributional corpus information. We solve this prob- lem by exploiting existing morphological resources that can enumerate a word’s com- ponent morphemes. We present a latent- variable Gaussian graphical model that al- lows us to extrapolate continuous represen- tations for words not observed in the train- ing corpus, as well as smoothing the repre- sentations provided for the observed words. The latent variables represent embeddings of morphemes, which combine to create em- beddings of words. Over several languages and training sizes, our model improves the embeddings for words, when evaluated on an analogy task, skip-gram predictive accu- racy, and word similarity. ",,,,ACL
157,2016,Cross-lingual Models of Word Embeddings: An Empirical Comparison,"Shyam Upadhyay, Manaal Faruqui, Chris Dyer, Dan Roth","Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive eval- uation of four popular approaches of in- ducing cross-lingual embeddings, each re- quiring a different form of supervision, on four typologically different language pairs. Our evaluation setup spans four dif- ferent tasks, including intrinsic evaluation on mono-lingual and cross-lingual simi- larity, and extrinsic evaluation on down- stream semantic and syntactic applica- tions. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply super- vised models often prove competitive on certain tasks. ",,,,ACL
158,2016,"Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning","Ekaterina Vylomova, Laura Rimell, Trevor Cohn, Timothy Baldwin","Recent work has shown that simple vector subtraction over word embeddings is surpris- ingly effective at capturing different lexical relations, despite lacking explicit supervision. Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated. In this paper, we carry out such an evaluation in two learning settings: (1) spectral clustering to induce word rela- tions, and (2) supervised learning to classify vector differences into relation types. We find that word embeddings capture a surprising amount of information, and that, under suit- able supervised training, vector subtraction generalises well to a broad range of relations, including over unseen lexical items. ",,,,ACL
159,2016,Minimum Risk Training for Neural Machine Translation,"Shiqi Shen, Yong Cheng, Zhongjun He, Wei He","We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is ca- pable of optimizing model parameters di- rectly with respect to arbitrary evaluation metrics, which are not necessarily differ- entiable. Experiments show that our ap- proach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks. ",,,,ACL
160,2016,A Character-level Decoder without Explicit Segmentation for Neural Machine Translation,"Junyoung Chung, Kyunghyun Cho, Yoshua Bengio","The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder– decoder with a subword-level encoder and a character-level decoder on four language pairs–En-Cs, En-De, En-Ru and En-Fi– using the parallel corpora from WMT’15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Further- more, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine trans- lation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru. ",,,,ACL
161,2016,Target-Side Context for Discriminative Models in Statistical Machine Translation,"Aleš Tamchyna, Alexander Fraser, Ondřej Bojar, Marcin Junczys-Dowmunt","Discriminative translation models utiliz- ing source context have been shown to help statistical machine translation perfor- mance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the de- coding process. Our approach scales to large training data sizes and results in con- sistent improvements in translation qual- ity on four language pairs. We also pro- vide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target- context model and we show that our ex- tension allows us to better capture mor- phological coherence. Our work is freely available as part of Moses. ",,,,ACL
162,2016,Neural Machine Translation of Rare Words with Subword Units,"Rico Sennrich, Barry Haddow, Alexandra Birch","Neural machine translation (NMT) mod- els typically operate with a fixed vocabu- lary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this pa- per, we introduce a simpler and more ef- fective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as se- quences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via com- positional translation), and cognates and loanwords (via phonological and morpho- logical transformations). We discuss the suitability of different word segmentation techniques, including simple character n- gram models and a segmentation based on the byte pair encoding compression algo- rithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 B LEU , respectively. ",,,,ACL
163,2016,Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network,"Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu","Word pairs, which are one of the most eas- ily accessible features between two text segments, have been proven to be very useful for detecting the discourse relations held between text segments. However, be- cause of the data sparsity problem, the performance achieved by using word pair features is limited. In this paper, in or- der to overcome the data sparsity problem, we propose the use of word embeddings to replace the original words. Moreover, we adopt a gated relevance network to capture the semantic interaction between word pairs, and then aggregate those se- mantic interactions using a pooling layer to select the most informative interactions. Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recog- nizing the discourse level relations in all of the relations. ",,,,ACL
164,2016,Model Architectures for Quotation Detection,"Christian Scheible, Roman Klinger, Sebastian Padó","Quotation detection is the task of locating spans of quoted speech in text. The state of the art treats this problem as a sequence labeling task and employs linear-chain con- ditional random fields. We question the ef- ficacy of this choice: The Markov assump- tion in the model prohibits it from making joint decisions about the begin, end, and internal context of a quotation. We per- form an extensive analysis with two new model architectures. We find that (a), sim- ple boundary classification combined with a greedy prediction strategy is competitive with the state of the art; (b), a semi-Markov model significantly outperforms all others, by relaxing the Markov assumption. ",,,,ACL
165,2016,Speech Act Modeling of Written Asynchronous Conversations with Task-Specific Embeddings and Conditional Structured Models,"Shafiq Joty, Enamul Hoque","This paper addresses the problem of speech act recognition in written asyn- chronous conversations (e.g., fora, emails). We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversa- tional dependencies between sentences. Our models use sentence representations encoded by a long short term memory (LSTM) recurrent neural model. Empir- ical evaluation shows the effectiveness of our approach over existing ones: (i) LSTMs provide better task-specific representations, and (ii) the global joint model improves over local models. ",,,,ACL
166,2016,Situation entity types: automatic classification of clause-level aspect,"Annemarie Friedrich, Alexis Palmer, Manfred Pinkal","This paper describes the first robust ap- proach to automatically labeling clauses with their situation entity type (Smith, 2003), capturing aspectual phenomena at the clause level which are relevant for interpreting both semantics at the clause level and discourse structure. Previous work on this task used a small data set from a limited domain, and relied mainly on words as features, an approach which is impractical in larger settings. We pro- vide a new corpus of texts from 13 genres (40,000 clauses) annotated with situation entity types. We show that our sequence labeling approach using distributional in- formation in the form of Brown clusters, as well as syntactic-semantic features tar- geted to the task, is robust across genres, reaching accuracies of up to 76%. ",,,,ACL
167,2016,Learning Prototypical Event Structure from Photo Albums,"Antoine Bosselut, Jianfu Chen, David Warren, Hannaneh Hajishirzi","Activities and events in our lives are struc- tural, be it a vacation, a camping trip, or a wedding. While individual details vary, there are characteristic patterns that are specific to each of these scenarios. For ex- ample, a wedding typically consists of a sequence of events such as walking down the aisle, exchanging vows, and dancing. In this paper, we present a data-driven ap- proach to learning event knowledge from a large collection of photo albums. We for- mulate the task as constrained optimiza- tion to induce the prototypical temporal structure of an event, integrating both vi- sual and textual cues. Comprehensive evaluation demonstrates that it is possible to learn multimodal knowledge of event structure from noisy web content. ",,,,ACL
168,2016,Cross-Lingual Image Caption Generation,"Takashi Miyazaki, Nobuyuki Shimizu","Automatically generating a natural lan- guage description of an image is a fun- damental problem in artificial intelligence. This task involves both computer vision and natural language processing and is called “image caption generation.” Re- search on image caption generation has typically focused on taking in an image and generating a caption in English as ex- isting image caption corpora are mostly in English. The lack of corpora in languages other than English is an issue, especially for morphologically rich languages such as Japanese. There is thus a need for cor- pora sufficiently large for image caption- ing in other languages. We have developed a Japanese version of the MS COCO cap- tion dataset and a generative model based on a deep recurrent architecture that takes in an image and uses this Japanese ver- sion of the dataset to generate a caption in Japanese. As the Japanese portion of the corpus is small, our model was de- signed to transfer the knowledge represen- tation obtained from the English portion into the Japanese portion. Experiments showed that the resulting bilingual compa- rable corpus has better performance than a monolingual corpus, indicating that image understanding using a resource-rich lan- guage benefits a resource-poor language. ",,,,ACL
169,2016,Learning Concept Taxonomies from Multi-modal Data,"Hao Zhang, Zhiting Hu, Yuntian Deng, Mrinmaya Sachan","We study the problem of automatically building hypernym taxonomies from tex- tual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leverag- ing text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representa- tions of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evalu- ate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap. ",,,,ACL
170,2016,Generating Natural Questions About an Image,"Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell","There has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription, and answering ques- tions about images. These tasks have fo- cused on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image. In this paper, we introduce the novel task of Visual Question Gener- ation (VQG), where the system is tasked with asking a natural and engaging ques- tion when shown an image. We provide three datasets which cover a variety of im- ages from object-centric to event-centric, with considerably more abstract training data than provided to state-of-the-art cap- tioning systems thus far. We train and test several generative and retrieval mod- els to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions for a variety of im- ages, there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the commu- nity which we hope furthers interest in ex- ploring deeper connections between vision & language. ",,,,ACL
171,2016,Physical Causality of Action Verbs in Grounded Language Understanding,"Qiaozi Gao, Malcolm Doering, Shaohua Yang, Joyce Chai","Linguistics studies have shown that action verbs often denote some Change of State (CoS) as the result of an action. However, the causality of action verbs and its poten- tial connection with the physical world has not been systematically explored. To ad- dress this limitation, this paper presents a study on physical causality of action verbs and their implied changes in the physi- cal world. We first conducted a crowd- sourcing experiment and identified eigh- teen categories of physical causality for action verbs. For a subset of these cat- egories, we then defined a set of detec- tors that detect the corresponding change from visual perception of the physical en- vironment. We further incorporated phys- ical causality modeling and state detec- tion in grounded language understanding. Our empirical studies have demonstrated the effectiveness of causality modeling in grounding language to perception. ",,,,ACL
172,2016,Optimizing an Approximation of ROUGE - a Problem-Reduction Approach to Extractive Multi-Document Summarization,"Maxime Peyrard, Judith Eckle-Kohler","This paper presents a problem-reduction approach to extractive multi-document summarization: we propose a reduction to the problem of scoring individual sen- tences with their ROUGE scores based on supervised learning. For the summariza- tion, we solve an optimization problem where the ROUGE score of the selected summary sentences is maximized. To this end, we derive an approximation of the ROUGE-N score of a set of sentences, and define a principled discrete optimization problem for sentence selection. Mathe- matical and empirical evidence suggests that the sentence selection step is solved almost exactly, thus reducing the problem to the sentence scoring task. We perform a detailed experimental evaluation on two DUC datasets to demonstrate the validity of our approach. ",,,,ACL
173,2016,Phrase Structure Annotation and Parsing for Learner English,"Ryo Nagata, Keisuke Sakaguchi","There has been almost no work on phrase structure annotation and parsing specially designed for learner English despite the fact that they are useful for representing the structural characteristics of learner En- glish. To address this problem, in this pa- per, we first propose a phrase structure an- notation scheme for learner English and annotate two different learner corpora us- ing it. Second, we show their usefulness, reporting on (a) inter-annotator agreement rate, (b) characteristic CFG rules in the corpora, and (c) parsing performance on them. In addition, we explore methods to improve phrase structure parsing for learner English (achieving an F -measure of 0.878). Finally, we release the full annotation guidelines, the annotated data, and the improved parser model for learner English to the public. ",,,,ACL
174,2016,A Trainable Spaced Repetition Model for Language Learning,"Burr Settles, Brendan Meeder","We present half-life regression (HLR), a novel model for spaced repetition practice with applications to second language ac- quisition. HLR combines psycholinguis- tic theory with modern machine learning techniques, indirectly estimating the “half- life” of a word or concept in a student’s long-term memory. We use data from Duolingo — a popular online language learning application — to fit HLR models, reducing error by 45%+ compared to sev- eral baselines at predicting student recall rates. HLR model weights also shed light on which linguistic concepts are system- atically challenging for second language learners. Finally, HLR was able to im- prove Duolingo daily student engagement by 12% in an operational user study. ",,,,ACL
175,2016,User Modeling in Language Learning with Macaronic Texts,"Adithya Renduchintala, Rebecca Knowles, Philipp Koehn, Jason Eisner","Foreign language learners can acquire new vocabulary by using cognate and con- text clues when reading. To measure such incidental comprehension, we devise an experimental framework that involves reading mixed-language “macaronic” sen- tences. Using data collected via Ama- zon Mechanical Turk, we train a graphi- cal model to simulate a human subject’s comprehension of foreign words, based on cognate clues (edit distance to an English word), context clues (pointwise mutual in- formation), and prior exposure. Our model does a reasonable job at predicting which words a user will be able to understand, which should facilitate the automatic con- struction of comprehensible text for per- sonalized foreign language education. ",,,,ACL
176,2016,"On the Similarities Between Native, Non-native and Translated Texts","Ella Rabinovich, Sergiu Nisioi, Noam Ordan, Shuly Wintner","We present a computational analysis of three language varieties: native, advanced non-native, and translation. Our goal is to investigate the similarities and differ- ences between non-native language pro- ductions and translations, contrasting both with native language. Using a collec- tion of computational methods we estab- lish three main results: (1) the three types of texts are easily distinguishable; (2) non- native language and translations are closer to each other than each of them is to native language; and (3) some of these character- istics depend on the source or native lan- guage, while others do not, reflecting, per- haps, unified principles that similarly af- fect translations and non-native language. ",,,,ACL
177,2016,Learning Text Pair Similarity with Context-sensitive Autoencoders,"Hadi Amiri, Philip Resnik, Jordan Boyd-Graber, Hal Daumé III","We present a pairwise context-sensitive Autoencoder for computing text pair sim- ilarity. Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs. Our model outperforms the state-of-the-art models in two semantic re- trieval tasks and a contextual word simi- larity task. For retrieval, our unsupervised approach that merely ranks inputs with re- spect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art su- pervised models and in some cases outper- forms them. ",,,,ACL
178,2016,Linguistic Benchmarks of Online News Article Quality,"Ioannis Arapakis, Filipa Peleja, Barla Berkant, Joao Magalhaes","Online news editors ask themselves the same question many times: what is miss- ing in this news article to go online? This is not an easy question to be answered by computational linguistic methods. In this work, we address this important question and characterise the constituents of news article editorial quality. More specifically, we identify 14 aspects related to the con- tent of news articles. Through a correla- tion analysis, we quantify their indepen- dence and relation to assessing an article’s editorial quality. We also demonstrate that the identified aspects, when combined to- gether, can be used effectively in quality control methods for online news. ",,,,ACL
179,2016,Alleviating Poor Context with Background Knowledge for Named Entity Disambiguation,"Ander Barrena, Aitor Soroa, Eneko Agirre","Named Entity Disambiguation (NED) al- gorithms disambiguate mentions of named entities with respect to a knowledge-base, but sometimes the context might be poor or misleading. In this paper we introduce the acquisition of two kinds of background information to alleviate that problem: en- tity similarity and selectional preferences for syntactic positions. We show, using a generative N¨aive Bayes model for NED, that the additional sources of context are complementary, and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets, yielding the third best and the best results, respectively. We provide ex- amples and analysis which show the value of the acquired background information. ",,,,ACL
180,2016,Mining Paraphrasal Typed Templates from a Plain Text Corpus,"Or Biran, Terra Blevins, Kathleen McKeown","Finding paraphrases in text is an impor- tant task with implications for genera- tion, summarization and question answer- ing, among other applications. Of par- ticular interest to those applications is the specific formulation of the task where the paraphrases are templated, which provides an easy way to lexicalize one message in multiple ways by simply plugging in the relevant entities. Previous work has fo- cused on mining paraphrases from parallel and comparable corpora, or mining very short sub-sentence synonyms and para- phrases. In this paper we present an ap- proach which combines distributional and KB-driven methods to allow robust mining of sentence-level paraphrasal templates, utilizing a rich type system for the slots, from a plain text corpus. ",,,,ACL
181,2016,How to Train Dependency Parsers with Inexact Search for Joint Sentence Boundary Detection and Parsing of Entire Documents,"Anders Björkelund, Agnieszka Faleńska, Wolfgang Seeker, Jonas Kuhn","We cast sentence boundary detection and syntactic parsing as a joint problem, so an entire text document forms a training instance for transition-based dependency parsing. When trained with an early up- date or max-violation strategy for inexact search, we observe that only a tiny part of these very long training instances is ever exploited. We demonstrate this effect by extending the ArcStandard transition sys- tem with swap for the joint prediction task. When we use an alternative update strat- egy, our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation. A comparison between a standard pipeline and our joint model furthermore empirically shows the usefulness of syntactic information on the task of sentence boundary detection. ",,,,ACL
182,2016,MUTT: Metric Unit TesTing for Language Generation Tasks,"William Boag, Renan Campos, Kate Saenko, Anna Rumshisky","Precise evaluation metrics are important for assessing progress in high-level lan- guage generation tasks such as machine translation or image captioning. Histor- ically, these metrics have been evaluated using correlation with human judgment. However, human-derived scores are often alarmingly inconsistent and are also limited in their ability to identify precise areas of weakness. In this paper, we perform a case study for metric evaluation by measuring the effect that systematic sentence trans- formations (e.g. active to passive voice) have on the automatic metric scores. These sentence “corruptions” serve as unit tests for precisely measuring the strengths and weaknesses of a given metric. We find that not only are human annotations heavily in- consistent in this study, but that the Met- ric Unit TesT analysis is able to capture precise shortcomings of particular metrics (e.g. comparing passive and active sen- tences) better than a simple correlation with human judgment can. ",,,,ACL
183,2016,N-gram language models for massively parallel devices,"Nikolay Bogoychev, Adam Lopez","For many applications, the query speed of N -gram language models is a computa- tional bottleneck. Although massively par- allel hardware like GPUs offer a poten- tial solution to this bottleneck, exploiting this hardware requires a careful rethink- ing of basic algorithms and data structures. We present the first language model de- signed for such hardware, using B-trees to maximize data parallelism and minimize memory footprint and latency. Compared with a single-threaded instance of KenLM (Heafield, 2011), a highly optimized CPU- based language model, our GPU imple- mentation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task. When we saturate both devices, the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures. Our implementation is freely available at https://github.com/XapaJIaMnu/gLM ",,,,ACL
184,2016,Cross-Lingual Morphological Tagging for Low-Resource Languages,"Jan Buys, Jan A. Botha","Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language pro- cessing tools. We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision. Our approach extends existing approaches of projecting part-of-speech tags across lan- guages, using bitext to infer constraints on the possible tags for a given word type or token. We propose a tagging model us- ing Wsabie, a discriminative embedding- based model with rank-based learning. In our evaluation on 11 languages, on av- erage this model performs on par with a baseline weakly-supervised HMM, while being more scalable. Multilingual experi- ments show that the method performs best when projecting between related language pairs. Despite the inherently lossy pro- jection, we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average. ",,,,ACL
185,2016,Semi-Supervised Learning for Neural Machine Translation,"Yong Cheng, Wei Xu, Zhongjun He, Wei He","While end-to-end neural machine transla- tion (NMT) has made remarkable progress recently, NMT systems only rely on par- allel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, espe- cially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi- supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (mono- lingual corpora) data. The central idea is to reconstruct the monolingual corpora us- ing an autoencoder, in which the source- to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese- English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. ",,,,ACL
186,2016,Strategies for Training Large Vocabulary Neural Language Models,"Wenlin Chen, David Grangier, Michael Auli","Training neural network language mod- els over large vocabularies is computa- tionally costly compared to count-based models such as Kneser-Ney. We present a systematic comparison of neural strate- gies to represent and train large vocabular- ies, including softmax, hierarchical soft- max, target sampling, noise contrastive es- timation and self normalization. We ex- tend self normalization to be a proper esti- mator of likelihood and introduce an effi- cient variant of softmax. We evaluate each method on three popular benchmarks, ex- amining performance on rare words, the speed/accuracy trade-off and complemen- tarity to Kneser-Ney. ",,,,ACL
187,2016,Predicting the Compositionality of Nominal Compounds: Giving Word Embeddings a Hard Time,"Silvio Cordeiro, Carlos Ramisch, Marco Idiart, Aline Villavicencio","Distributional semantic models (DSMs) are often evaluated on artificial simi- larity datasets containing single words or fully compositional phrases. We present a large-scale multilingual eval- uation of DSMs for predicting the de- gree of semantic compositionality of nom- inal compounds on 4 datasets for En- glish and French. We build a total of 816 DSMs and perform 2,856 evaluations using word2vec, GloVe, and PPMI-based models. In addition to the DSMs, we com- pare the impact of different parameters, such as level of corpus preprocessing, con- text window size and number of dimen- sions. The results obtained have a high correlation with human judgments, being comparable to or outperforming the state of the art for some datasets (Spearman’s ρ=.82 for the Reddy dataset). ",,,,ACL
188,2016,Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints,"Greg Durrett, Taylor Berg-Kirkpatrick, Dan Klein","We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun’s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final sys- tem 1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality. ",,,,ACL
189,2016,Set-Theoretic Alignment for Comparable Corpora,"Thierry Etchegoyhen, Andoni Azpeitia","We describe and evaluate a simple method to extract parallel sentences from com- parable corpora. The approach, termed STACC , is based on expanded lexical sets and the Jaccard similarity coefficient. We evaluate our system against state-of-the- art methods on a large range of datasets in different domains, for ten language pairs, showing that it either matches or outper- forms current methods across the board and gives significantly better results on the noisiest datasets. STACC is a portable method, requiring no particular adaptation for new domains or language pairs, thus enabling the efficient mining of parallel sentences in comparable corpora. ",,,,ACL
190,2016,Jointly Learning to Embed and Predict with Multiple Languages,"Daniel C. Ferreira, André F. T. Martins, Mariana S. C. Almeida","We propose a joint formulation for learn- ing task-specific cross-lingual word em- beddings, along with classifiers for that task. Unlike prior work, which first learns the embeddings from parallel data and then plugs them in a supervised learning problem, our approach is one- shot: a single optimization problem com- bines a co-regularizer for the multilin- gual embeddings with a task-specific loss. We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension, a limitation which does not exist for other co-regularizers (such as the 1 - distance). Despite its simplicity, our method achieves state-of-the-art accura- cies on the RCV1/RCV2 dataset when transferring from English to German, with training times below 1 minute. On the TED Corpus, we obtain the highest re- ported scores on 10 out of 11 languages. ",,,,ACL
191,2016,"Supersense Embeddings: A Unified Model for Supersense Interpretation, Prediction, and Utilization","Lucie Flekova, Iryna Gurevych","Coarse-grained semantic categories such as supersenses have proven useful for a range of downstream tasks such as question an- swering or machine translation. To date, no effort has been put into integrating the supersenses into distributional word rep- resentations. We present a novel joint em- bedding model of words and supersenses, providing insights into the relationship be- tween words and supersenses in the same vector space. Using these embeddings in a deep neural network model, we demon- strate that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks. ",,,,ACL
192,2016,Efficient techniques for parsing with tree automata,"Jonas Groschwitz, Alexander Koller, Mark Johnson","Parsing for a wide variety of grammar for- malisms can be performed by intersecting finite tree automata. However, naive im- plementations of parsing by intersection are very inefficient. We present techniques that speed up tree-automata-based pars- ing, to the point that it becomes practically feasible on realistic data when applied to context-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature. ",,,,ACL
193,2016,A Vector Space for Distributional Semantics for Entailment,"James Henderson, Diana Popa","Distributional semantics creates vector- space representations that capture many forms of semantic similarity, but their re- lation to semantic entailment has been less clear. We propose a vector-space model which provides a formal foundation for a distributional semantics of entailment. Us- ing a mean-field approximation, we de- velop approximate inference procedures and entailment operators over vectors of probabilities of features being known (ver- sus unknown). We use this framework to reinterpret an existing distributional- semantic model (Word2Vec) as approxi- mating an entailment-based model of the distributions of words in contexts, thereby predicting lexical entailment relations. In both unsupervised and semi-supervised experiments on hyponymy detection, we get substantial improvements over previ- ous results. ",,,,ACL
194,2016,Hidden Softmax Sequence Model for Dialogue Structure Analysis,"Zhiyang He, Xien Liu, Ping Lv, Ji Wu","We propose a new unsupervised learning model, hidden softmax sequence model (HSSM), based on Boltzmann machine for dialogue structure analysis. The model employs three types of units in the hidden layer to discovery dialogue latent struc- tures: softmax units which represent latent states of utterances; binary units which represent latent topics specified by dia- logues; and a binary unit that represents the global general topic shared across the whole dialogue corpus. In addition, the model contains extra connections between adjacent hidden softmax units to formu- late the dependency between latent states. Two different kinds of real world dialogue corpora, Twitter-Post and AirTicketBook- ing, are utilized for extensive comparing experiments, and the results illustrate that the proposed model outperforms sate-of- the-art popular approaches. ",,,,ACL
195,2016,Summarizing Source Code using a Neural Attention Model,"Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer","High quality source code is often paired with high level summaries of the compu- tation it performs, for example in code documentation or in descriptions posted in online forums. Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done for a small frac- tion of all code that is produced. In this paper, we present the first completely data- driven approach for generating high level summaries of source code. Our model, CODE-NN , uses Long Short Term Mem- ory (LSTM) networks with attention to produce sentences that describe C# code snippets and SQL queries. CODE-NN is trained on a new corpus that is auto- matically collected from StackOverflow, which we release. Experiments demon- strate strong performance on two tasks: (1) code summarization, where we estab- lish the first end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C# benchmark by a large mar- gin. ",,,,ACL
196,2016,Continuous Profile Models in ASL Syntactic Facial Expression Synthesis,"Hernisa Kacorri, Matt Huenerfauth","To create accessible content for deaf users, we investigate automatically synthesizing animations of American Sign Language (ASL), including grammatically important facial expressions and head movements. Based on recordings of humans perform- ing various types of syntactic face and head movements (which include idiosyn- cratic variation), we evaluate the efficacy of Continuous Profile Models (CPMs) at identifying an essential “latent trace” of the performance, for use in producing ASL animations. A metric-based evalua- tion and a study with deaf users indicated that this approach was more effective than a prior method for producing animations. ",,,,ACL
197,2016,Evaluating Sentiment Analysis in the Context of Securities Trading,"Siavash Kazemian, Shunan Zhao, Gerald Penn","There are numerous studies suggesting that published news stories have an im- portant effect on the direction of the stock market, its volatility, the volume of trades, and the value of individual stocks men- tioned in the news. There is even some published research suggesting that auto- mated sentiment analysis of news docu- ments, quarterly reports, blogs and/or twit- ter data can be productively used as part of a trading strategy. This paper presents just such a family of trading strategies, and then uses this application to re-examine some of the tacit assumptions behind how sentiment analyzers are generally evalu- ated, in spite of the contexts of their appli- cation. This discrepancy comes at a cost. ",,,,ACL
198,2016,Edge-Linear First-Order Dependency Parsing with Undirected Minimum Spanning Tree Inference,"Effi Levi, Roi Reichart, Ari Rappoport","The run time complexity of state-of-the- art inference algorithms in graph-based dependency parsing is super-linear in the number of input words (n). Recently, pruning algorithms for these models have shown to cut a large portion of the graph edges, with minimal damage to the re- sulting parse trees. Solving the infer- ence problem in run time complexity de- termined solely by the number of edges (m) is hence of obvious importance. We propose such an inference algorithm for first-order models, which encodes the problem as a minimum spanning tree (MST) problem in an undirected graph. This allows us to utilize state-of-the-art undirected MST algorithms whose run time is O(m) at expectation and with a very high probability. A directed parse tree is then inferred from the undirected MST and is subsequently improved with respect to the directed parsing model through local greedy updates, both steps running in O(n) time. In experiments with 18 languages, a variant of the first-order MSTParser (McDonald et al., 2005b) that employs our algorithm performs very sim- ilarly to the original parser that runs an O(n 2 ) directed MST inference. ",,,,ACL
199,2016,Topic Extraction from Microblog Posts Using Conversation Structures,"Jing Li, Ming Liao, Wei Gao, Yulan He","Conventional topic models are ineffec- tive for topic extraction from microblog messages since the lack of structure and context among the posts renders poor message-level word co-occurrence pat- terns. In this work, we organize microblog posts as conversation trees based on re- posting and replying relations, which en- rich context information to alleviate data sparseness. Our model generates words according to topic dependencies derived from the conversation structures. In spe- cific, we differentiate messages as leader messages, which initiate key aspects of previously focused topics or shift the focus to different topics, and follower messages that do not introduce any new information but simply echo topics from the messages that they repost or reply. Our model cap- tures the different extents that leader and follower messages may contain the key topical words, thus further enhances the quality of the induced topics. The results of thorough experiments demonstrate the effectiveness of our proposed model. ",,,,ACL
200,2016,Neural Relation Extraction with Selective Attention over Instances,"Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan","Distant supervised relation extraction has been widely used to find novel relational facts from text. However, distant su- pervision inevitably accompanies with the wrong labelling problem, and these noisy data will substantially hurt the perfor- mance of relation extraction. To allevi- ate this issue, we propose a sentence-level attention-based model for relation extrac- tion. In this model, we employ convolu- tional neural networks to embed the se- mantics of sentences. Afterwards, we build sentence-level attention over multi- ple instances, which is expected to dy- namically reduce the weights of those noisy instances. Experimental results on real-world datasets show that, our model can make full use of all informative sen- tences and effectively reduce the influence of wrong labelled instances. Our model achieves significant and consistent im- provements on relation extraction as com- pared with baselines. The source code of this paper can be obtained from https: //github.com/thunlp/NRE. ",,,,ACL
201,2016,Leveraging FrameNet to Improve Automatic Event Detection,"Shulin Liu, Yubo Chen, Shizhu He, Kang Liu","Frames defined in FrameNet (FN) share highly similar structures with events in ACE event extraction program. An even- t in ACE is composed of an event trig- ger and a set of arguments. Analogously, a frame in FN is composed of a lexical u- nit and a set of frame elements, which play similar roles as triggers and arguments of ACE events respectively. Besides having similar structures, many frames in FN ac- tually express certain types of events. The above observations motivate us to explore whether there exists a good mapping from frames to event-types and if it is possible to improve event detection by using FN. In this paper, we propose a global infer- ence approach to detect events in FN. Fur- ther, based on the detected results, we an- alyze possible mappings from frames to event-types. Finally, we improve the per- formance of event detection and achieve a new state-of-the-art result by using the events automatically detected from FN. ",,,,ACL
202,2016,Learning To Use Formulas To Solve Simple Arithmetic Problems,"Arindam Mitra, Chitta Baral","Solving simple arithmetic word problems is one of the challenges in Natural Lan- guage Understanding. This paper presents a novel method to learn to use formulas to solve simple arithmetic word problems. Our system, analyzes each of the sen- tences to identify the variables and their attributes; and automatically maps this in- formation into a higher level representa- tion. It then uses that representation to recognize the presence of a formula along with its associated variables. An equa- tion is then generated from the formal de- scription of the formula. In the training phase, it learns to score the <formula, variables> pair from the systematically generated higher level representation. It is able to solve 86.07% of the problems in a corpus of standard primary school test questions and beats the state-of-the-art by a margin of 8.07%. ",,,,ACL
203,2016,Unravelling Names of Fictional Characters,"Katerina Papantoniou, Stasinos Konstantopoulos","In this paper we explore the correlation be- tween the sound of words and their mean- ing, by testing if the polarity (‘good guy’ or ‘bad guy’) of a character’s role in a work of fiction can be predicted by the name of the character in the absence of any other context. Our approach is based on phonological and other features pro- posed in prior theoretical studies of fic- tional names. These features are used to construct a predictive model over a man- ually annotated corpus of characters from motion pictures. By experimenting with different mixtures of features, we identify phonological features as being the most discriminative by comparison to social and other types of features, and we delve into a discussion of specific phonological and phonotactic indicators of a character’s role’s polarity. ",,,,ACL
204,2016,Most “babies” are “little” and most “problems” are “huge”: Compositional Entailment in Adjective-Nouns,"Ellie Pavlick, Chris Callison-Burch","We examine adjective-noun (AN) composi- tion in the task of recognizing textual entail- ment (RTE). We analyze behavior of ANs in large corpora and show that, despite conven- tional wisdom, adjectives do not always re- strict the denotation of the nouns they mod- ify. We use natural logic to characterize the variety of entailment relations that can result from AN composition. Predicting these re- lations depends on context and on common- sense knowledge, making AN composition es- pecially challenging for current RTE systems. We demonstrate the inability of current state- of-the-art systems to handle AN composition in a simplified RTE task which involves the in- sertion of only a single word. ",,,,ACL
205,2016,Modeling Stance in Student Essays,"Isaac Persing, Vincent Ng","Essay stance classification, the task of de- termining how much an essay’s author agrees with a given proposition, is an important yet under-investigated subtask in understanding an argumentative essay’s overall content. We introduce a new cor- pus of argumentative student essays an- notated with stance information and pro- pose a computational model for automati- cally predicting essay stance. In an evalu- ation on 826 essays, our approach signif- icantly outperforms four baselines, one of which relies on features previously devel- oped specifically for stance classification in student essays, yielding relative error reductions of at least 11.3% and 5.3%, in micro and macro F-score, respectively. ",,,,ACL
206,2016,A New Psychometric-inspired Evaluation Metric for Chinese Word Segmentation,"Peng Qian, Xipeng Qiu, Xuanjing Huang","Word segmentation is a fundamental task for Chinese language processing. Howev- er, with the successive improvements, the standard metric is becoming hard to distin- guish state-of-the-art word segmentation systems. In this paper, we propose a new psychometric-inspired evaluation metric for Chinese word segmentation, which addresses to balance the very skewed word distribution at different levels of difficulty 1 . The performance on a real evaluation shows that the proposed metric gives more reasonable and distinguishable scores and correlates well with human judgement. In addition, the proposed metric can be easily extended to evaluate other sequence labelling based NLP tasks. ",,,,ACL
207,2016,Temporal Anchoring of Events for the TimeBank Corpus,"Nils Reimers, Nazanin Dehghani, Iryna Gurevych","Today’s extraction of temporal informa- tion for events heavily depends on an- notated temporal links. These so called TLINKs capture the relation between pairs of event mentions and time expressions. One problem is that the number of possible TLINKs grows quadratic with the num- ber of event mentions, therefore most an- notation studies concentrate on links for mentions in the same or in adjacent sen- tences. However, as our annotation study shows, this restriction results for 58% of the event mentions in a less precise infor- mation when the event took place. This paper proposes a new annotation scheme to anchor events in time. Not only is the annotation effort much lower as it scales linear with the number of events, it also gives a more precise anchoring when the events have happened as the complete document can be taken into account. Us- ing this scheme, we annotated a subset of the TimeBank Corpus and compare our re- sults to other annotation schemes. Addi- tionally, we present some baseline exper- iments to automatically anchor events in time. Our annotation scheme, the auto- mated system and the annotated corpus are publicly available. 1 ",,,,ACL
208,2016,Grammatical Error Correction: Machine Translation and Classifiers,"Alla Rozovskaya, Dan Roth","We focus on two leading state-of-the-art approaches to grammatical error correc- tion – machine learning classification and machine translation. Based on the com- parative study of the two learning frame- works and through error analysis of the output of the state-of-the-art systems, we identify key strengths and weaknesses of each of these approaches and demonstrate their complementarity. In particular, the machine translation method learns from parallel data without requiring further lin- guistic input and is better at correcting complex mistakes. The classification ap- proach possesses other desirable charac- teristics, such as the ability to easily gener- alize beyond what was seen in training, the ability to train without human-annotated data, and the flexibility to adjust knowl- edge sources for individual error types. Based on this analysis, we develop an algorithmic approach that combines the strengths of both methods. We present several systems based on resources used in previous work with a relative improve- ment of over 20% (and 7.4 F score points) over the previous state-of-the-art. ",,,,ACL
209,2016,Recurrent neural network models for disease name recognition using domain invariant features,"Sunil Sahu, Ashish Anand","Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and pro- pose a domain-invariant framework for the disease name recognition task. In particu- lar, we propose various end-to-end recur- rent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined cate- gories. We also utilize convolution neu- ral network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded fea- tures in our model. We compare our mod- els with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recogni- tion task indicate that state-of-the-art per- formance can be obtained without relying on feature engineering. Further the pro- posed models obtained improved perfor- mance on the classification task of disease names. ",,,,ACL
210,2016,Domain Adaptation for Authorship Attribution: Improved Structural Correspondence Learning,"Upendra Sapkota, Thamar Solorio, Manuel Montes, Steven Bethard","We present the first domain adaptation model for authorship attribution to leverage unlabeled data. The model includes exten- sions to structural correspondence learning needed to make it appropriate for the task. For example, we propose a median-based classification instead of the standard binary classification used in previous work. Our results show that punctuation-based charac- ter n-grams form excellent pivot features. We also show how singular value decom- position plays a critical role in achieving domain adaptation, and that replacing (in- stead of concatenating) non-pivot features with correspondence features yields better performance. ",,,,ACL
211,2016,A Corpus-Based Analysis of Canonical Word Order of Japanese Double Object Constructions,"Ryohei Sasano, Manabu Okumura","The canonical word order of Japanese double object constructions has attracted considerable attention among linguists and has been a topic of many studies. How- ever, most of these studies require either manual analyses or measurements of hu- man characteristics such as brain activities or reading times for each example. Thus, while these analyses are reliable for the ex- amples they focus on, they cannot be gen- eralized to other examples. On the other hand, the trend of actual usage can be col- lected automatically from a large corpus. Thus, in this paper, we assume that there is a relationship between the canonical word order and the proportion of each word or- der in a large corpus and present a corpus- based analysis of canonical word order of Japanese double object constructions. ",,,,ACL
212,2016,Knowledge-Based Semantic Embedding for Machine Translation,"Chen Shi, Shujie Liu, Shuo Ren, Shi Feng","In this paper, with the help of knowl- edge base, we build and formulate a se- mantic space to connect the source and target languages, and apply it to the sequence-to-sequence framework to pro- pose a Knowledge-Based Semantic Em- bedding (KBSE) method. In our KB- SE method, the source sentence is firstly mapped into a knowledge based seman- tic space, and the target sentence is gen- erated using a recurrent neural network with the internal meaning preserved. Ex- periments are conducted on two transla- tion tasks, the electric business data and movie data, and the results show that our proposed method can achieve outstanding performance, compared with both the tra- ditional SMT methods and the existing encoder-decoder models. ",,,,ACL
213,2016,One for All: Towards Language Independent Named Entity Linking,"Avirup Sil, Radu Florian","Entity linking (EL) is the task of dis- ambiguating mentions in text by associ- ating them with entries in a predefined database of mentions (persons, organiza- tions, etc). Most previous EL research has focused mainly on one language, English, with less attention being paid to other lan- guages, such as Spanish or Chinese. In this paper, we introduce L IE L, a Lan- guage Independent Entity Linking system, which provides an EL framework which, once trained on one language, works re- markably well on a number of different languages without change. L IE L makes a joint global prediction over the entire document, employing a discriminative re- ranking framework with many domain and language-independent feature func- tions. Experiments on numerous bench- mark datasets, show that the proposed sys- tem, once trained on one language, En- glish, outperforms several state-of-the-art systems in English (by 4 points) and the trained model also works very well on Spanish (14 points better than a competi- tor system), demonstrating the viability of the approach. ",,,,ACL
214,2016,On Approximately Searching for Similar Word Embeddings,"Kohei Sugawara, Hayato Kobayashi, Masajiro Iwasaki","We discuss an approximate similarity search for word embeddings, which is an operation to approximately find em- beddings close to a given vector. We compared several metric-based search al- gorithms with hash-, tree-, and graph- based indexing from different aspects. Our experimental results showed that a graph-based indexing exhibits robust per- formance and additionally provided use- ful information, e.g., vector normalization achieves an efficient search with cosine similarity. ",,,,ACL
215,2016,Composing Distributed Representations of Relational Patterns,"Sho Takase, Naoaki Okazaki, Kentaro Inui","Learning distributed representations for relation instances is a central technique in downstream NLP applications. In or- der to address semantic modeling of rela- tional patterns, this paper constructs a new dataset that provides multiple similarity ratings for every pair of relational patterns on the existing dataset (Zeichner et al., 2012). In addition, we conduct a compar- ative study of different encoders includ- ing additive composition, RNN, LSTM, and GRU for composing distributed rep- resentations of relational patterns. We also present Gated Additive Composition, which is an enhancement of additive com- position with the gating mechanism. Ex- periments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in the relation classification task. ",,,,ACL
216,2016,"The More Antecedents, the Merrier: Resolving Multi-Antecedent Anaphors","Hardik Vala, Andrew Piper, Derek Ruths","Anaphor resolution is an important task in NLP with many applications. De- spite much research effort, it remains an open problem. The difficulty of the prob- lem varies substantially across different sub-problems. One sub-problem, in par- ticular, has been largely untouched by prior work despite occurring frequently throughout corpora: the anaphor that has multiple antecedents, which here we call multi-antecedent anaphors or m- anaphors. Current coreference resolvers restrict anaphors to at most a single an- tecedent. As we show in this paper, re- laxing this constraint poses serious prob- lems in coreference chain-building, where each chain is intended to refer to a single entity. This work provides a formaliza- tion of the new task with preliminary in- sights into multi-antecedent noun-phrase anaphors, and offers a method for resolv- ing such cases that outperforms a number of baseline methods by a significant mar- gin. Our system uses local agglomerative clustering on candidate antecedents and an existing coreference system to score clus- ters to determine which cluster of men- tions is antecedent for a given anaphor. When we augment an existing coreference system with our proposed method, we ob- serve a substantial increase in performance (0.6 absolute CoNLL F1) on an annotated corpus. ",,,,ACL
217,2016,Automatic Labeling of Topic Models Using Text Summaries,"Xiaojun Wan, Tianming Wang","Labeling topics learned by topic models is a challenging problem. Previous studies have used words, phrases and images to label topics. In this paper, we propose to use text summaries for topic labeling. Several sentences are extracted from the most related documents to form the sum- mary for each topic. In order to obtain summaries with both high relevance, cov- erage and discrimination for all the topics, we propose an algorithm based on sub- modular optimization. Both automatic and manual analysis have been conducted on two real document collections, and we find 1) the summaries extracted by our proposed algorithm are superior over the summaries extracted by existing popular summarization methods; 2) the use of summaries as labels has obvious ad- vantages over the use of words and phrases. ",,,,ACL
218,2016,Graph-based Dependency Parsing with Bidirectional LSTM,"Wenhui Wang, Baobao Chang","In this paper, we propose a neural network model for graph-based dependency pars- ing which utilizes Bidirectional LSTM (BLSTM) to capture richer contextual in- formation instead of using high-order fac- torization, and enable our model to use much fewer features than previous work. In addition, we propose an effective way to learn sentence segment embedding on sentence-level based on an extra forward LSTM network. Although our model uses only first-order factorization, experiments on English Peen Treebank and Chinese Penn Treebank show that our model could be competitive with previous higher-order graph-based dependency parsing models and state-of-the-art models. ",,,,ACL
219,2016,TransG : A Generative Model for Knowledge Graph Embedding,"Han Xiao, Minlie Huang, Xiaoyan Zhu","Recently, knowledge graph embedding, which projects symbolic entities and rela- tions into continuous vector space, has be- come a new, hot topic in artificial intelli- gence. This paper proposes a novel gen- erative model (TransG) to address the is- sue of multiple relation semantics that a relation may have multiple meanings re- vealed by the entity pairs associated with the corresponding triples. The new model can discover latent semantics for a rela- tion and leverage a mixture of relation- specific component vectors to embed a fact triple. To the best of our knowledge, this is the first generative model for knowl- edge graph embedding, and at the first time, the issue of multiple relation seman- tics is formally discussed. Extensive ex- periments show that the proposed model achieves substantial improvements against the state-of-the-art baselines. ",,,,ACL
220,2016,Question Answering on Freebase via Relation Extraction and Textual Evidence,"Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang","Existing knowledge-based question an- swering systems often rely on small an- notated training data. While shallow meth- ods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at an- swering questions involving multiple con- straints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a neural network based re- lation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers. Ex- periments on the WebQuestions question answering dataset show that our method achieves an F 1 of 53.3%, a substantial im- provement over the state-of-the-art. ",,,,ACL
221,2016,Vector-space topic models for detecting Alzheimer’s disease,"Maria Yancheva, Frank Rudzicz","Semantic deficit is a symptom of language impairment in Alzheimer’s disease (AD). We present a generalizable method for au- tomatic generation of information content units (ICUs) for a picture used in a stan- dard clinical task, achieving high recall, 96.8%, of human-supplied ICUs. We use the automatically generated topic model to extract semantic features, and train a ran- dom forest classifier to achieve an F-score of 0.74 in binary classification of controls versus people with AD using a set of only 12 features. This is comparable to re- sults (0.72 F-score) with a set of 85 man- ual features. Adding semantic informa- tion to a set of standard lexicosyntactic and acoustic features improves F-score to 0.80. While control and dementia subjects dis- cuss the same topics in the same contexts, controls are more informative per second of speech. ",,,,ACL
222,2016,Chinese Couplet Generation with Neural Network Structures,"Rui Yan, Cheng-Te Li, Xiaohua Hu, Ming Zhang","Part of the unique cultural heritage of Chi- na is the Chinese couplet. Given a sen- tence (namely an antecedent clause), peo- ple reply with another sentence (namely a subsequent clause) equal in length. More- over, a special phenomenon is that corre- sponding characters from the same posi- tion in the two clauses match each oth- er by following certain constraints on se- mantic and/or syntactic relatedness. Au- tomatic couplet generation by computer is viewed as a difficult problem and has not been fully explored. In this paper, we formulate the task as a natural language generation problem using neural network structures. Given the issued anteceden- t clause, the system generates the subse- quent clause via sequential language mod- eling. To satisfy special characteristics of couplets, we incorporate the attention mechanism and polishing schema into the encoding-decoding process. The couplet is generated incrementally and iterative- ly. A comprehensive evaluation, using per- plexity and BLEU measurements as well as human judgments, has demonstrated the effectiveness of our proposed approach. ",,,,ACL
223,2016,A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task,"Danqi Chen, Jason Bolton, Christopher D. Manning","Enabling a computer to understand a docu- ment so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solu- tion by machine learned systems is the lim- ited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language un- derstanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding cur- rent state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.1 ",,,,ACL
224,2016,Learning Language Games through Interaction,"Sida I. Wang, Percy Liang, Christopher D. Manning","We introduce a new language learning setting relevant to building adaptive nat- ural language interfaces. It is inspired by Wittgenstein’s language games: a hu- man wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The com- puter initially knows nothing about lan- guage and therefore must learn it from scratch through interaction, while the hu- man adapts to the computer’s capabilities. We created a game called SHRDLURN in a blocks world and collected interactions from 100 people playing it. First, we an- alyze the humans’ strategies, showing that using compositionality and avoiding syn- onyms correlates positively with task per- formance. Second, we compare computer strategies, showing that modeling prag- matics on a semantic parsing model accel- erates learning for more strategic players. ",,,,ACL
225,2016,Finding Non-Arbitrary Form-Meaning Systematicity Using String-Metric Learning for Kernel Regression,"E.Dario Gutiérrez, Roger Levy, Benjamin Bergen","Arbitrariness of the sign—the notion that the forms of words are unrelated to their meanings—is an underlying assumption of many linguistic theories. Two lines of research have recently challenged this as- sumption, but they produce differing char- acterizations of non-arbitrariness in lan- guage. Behavioral and corpus studies have confirmed the validity of localized form-meaning patterns manifested in lim- ited subsets of the lexicon. Meanwhile, global (lexicon-wide) statistical analyses instead find diffuse form-meaning system- aticity across the lexicon as a whole. We bridge the gap with an approach that can detect both local and global form- meaning systematicity in language. In the kernel regression formulation we in- troduce, form-meaning relationships can be used to predict words’ distributional semantic vectors from their forms. Fur- thermore, we introduce a novel metric learning algorithm that can learn weighted edit distances that minimize kernel regres- sion error. Our results suggest that the English lexicon exhibits far more global form-meaning systematicity than previ- ously discovered, and that much of this systematicity is focused in localized form- meaning patterns. ",,,,ACL
226,2016,Improving Hypernymy Detection with an Integrated Path-based and Distributional Method,"Vered Shwartz, Yoav Goldberg, Ido Dagan","Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complemen- tary approaches. Distributional methods, whose supervised variants are the cur- rent best performers, and path-based meth- ods, which received less research atten- tion. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural net- work, that achieves results comparable to distributional methods. We then ex- tend the approach to integrate both path- based and distributional signals, signifi- cantly improving upon the state-of-the-art on this task. ",,,,ACL
227,2016,Multimodal Pivots for Image Caption Translation,"Julian Hitschler, Shigehiko Schamoni, Stefan Riezler","We present an approach to improve sta- tistical machine translation of image de- scriptions by multimodal pivots defined in visual space. The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of trans- lation outputs. Our approach does not de- pend on the availability of large amounts of in-domain parallel data, but only re- lies on available large datasets of monolin- gually captioned images, and on state-of- the-art convolutional neural networks to compute image similarities. Our experi- mental evaluation shows improvements of ",,,,ACL
228,2016,Harnessing Deep Neural Networks with Logic Rules,"Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy","Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the struc- tured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment anal- ysis, and an RNN for named entity recog- nition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable re- sults to previous best-performing systems. ",,,,ACL
229,2016,Case and Cause in Icelandic: Reconstructing Causal Networks of Cascaded Language Changes,"Fermín Moscoso del Prado Martín, Christian Brendel","Linguistic drift is a process that produces slow irreversible changes in the grammar and function of a language’s construc- tions. Importantly, changes in a part of a language can have trickle down effects, triggering changes elsewhere in that lan- guage. Although such causally triggered chains of changes have long been hypoth- esized by historical linguists, no explicit demonstration of the actual causality has been provided. In this study, we use co- occurrence statistics and machine learning to demonstrate that the functions of mor- phological cases experience a slow, irre- versible drift along history, even in a lan- guage as conservative as is Icelandic. Cru- cially, we then move on to demonstrate –using the notion of Granger-causality– that there are explicit causal connections between the changes in the functions of the different cases, which are consistent with documented processes in the history of Icelandic. Our technique provides a means for the quantitative reconstruction of connected networks of subtle linguistic changes. ",,,,ACL
230,2016,On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems,"Pei-Hao Su, Milica Gašić, Nikola Mrkšić, Lina M. Rojas-Barahona","The ability to compute an accurate re- ward function is essential for optimising a dialogue policy via reinforcement learn- ing. In real-world applications, using ex- plicit user feedback as the reward sig- nal is often unreliable and costly to col- lect. This problem can be mitigated if the user’s intent is known in advance or data is available to pre-train a task suc- cess predictor off-line. In practice neither of these apply for most real world applica- tions. Here we propose an on-line learn- ing framework whereby the dialogue pol- icy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process op- erates on a continuous space dialogue rep- resentation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annota- tion costs and mitigate noisy user feedback in dialogue policy learning. ",,,,ACL
231,2016,Globally Normalized Transition-Based Neural Networks,"Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn","We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of- speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We dis- cuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models. ",,,,ACL
1,2017,Adversarial Multi-task Learning for Text Classification,"Pengfei Liu, Xipeng Qiu, Xuanjing Huang","Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at http://nlp.fudan.edu.cn/data/.",,,,ACL
2,2017,Neural End-to-End Learning for Computational Argumentation Mining,"Steffen Eger, Johannes Daxenberger, Iryna Gurevych","We investigate neural techniques for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning ‘natural’ subtasks, in a multi-task learning setup, improves performance.",,,,ACL
3,2017,Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision,"Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus","Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural “programmer”, i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic “computer”, i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.",,,,ACL
4,2017,Neural Relation Extraction with Multi-lingual Attention,"Yankai Lin, Zhiyuan Liu, Maosong Sun","Relation extraction has been widely used for finding unknown relational facts from plain text. Most existing methods focus on exploiting mono-lingual data for relation extraction, ignoring massive information from the texts in various languages. To address this issue, we introduce a multi-lingual neural relation extraction framework, which employs mono-lingual attention to utilize the information within mono-lingual texts and further proposes cross-lingual attention to consider the information consistency and complementarity among cross-lingual texts. Experimental results on real-world datasets show that, our model can take advantage of multi-lingual texts and consistently achieve significant improvements on relation extraction as compared with baselines.",,,,ACL
5,2017,Learning Structured Natural Language Representations for Semantic Parsing,"Jianpeng Cheng, Siva Reddy, Vijay Saraswat, Mirella Lapata","We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEOQUERY and WEBQUESTIONS. The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones.",,,,ACL
6,2017,Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules,"Ivan Vulić, Nikola Mrkšić, Roi Reichart, Diarmuid Ó Séaghdha","Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for low-frequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for language understanding systems, which may infer that ‘inexpensive’ is a rephrasing for ‘expensive’ or may not associate ‘acquire’ with ‘acquires’. In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces. Instead, our method injects morphological constraints generated using simple language-specific rules, pulling inflectional forms of the same word close together and pushing derivational antonyms far apart. In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection. Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks.",,,,ACL
7,2017,Skip-Gram − Zipf + Uniform = Vector Additivity,"Alex Gittens, Dimitris Achlioptas, Michael W. Mahoney","In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected “side-effect” of such models is that their vectors often exhibit compositionality, i.e., addingtwo word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., “man” + “royal” = “king”. This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.",,,,ACL
8,2017,The State of the Art in Semantic Representation,"Omri Abend, Ari Rappoport","Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.",,,,ACL
9,2017,Joint Learning for Event Coreference Resolution,"Jing Lu, Vincent Ng","While joint models have been developed for many NLP tasks, the vast majority of event coreference resolvers, including the top-performing resolvers competing in the recent TAC KBP 2016 Event Nugget Detection and Coreference task, are pipeline-based, where the propagation of errors from the trigger detection component to the event coreference component is a major performance limiting factor. To address this problem, we propose a model for jointly learning event coreference, trigger detection, and event anaphoricity. Our joint model is novel in its choice of tasks and its features for capturing cross-task interactions. To our knowledge, this is the first attempt to train a mention-ranking model and employ event anaphoricity for event coreference. Our model achieves the best results to date on the KBP 2016 English and Chinese datasets.",,,,ACL
10,2017,Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution,"Ting Liu, Yiming Cui, Qingyu Yin, Wei-Nan Zhang","Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.",,,,ACL
11,2017,Discourse Mode Identification in Essays,"Wei Song, Dong Wang, Ruiji Fu, Lizhen Liu","Discourse modes play an important role in writing composition and evaluation. This paper presents a study on the manual and automatic identification of narration,exposition, description, argument and emotion expressing sentences in narrative essays. We annotate a corpus to study the characteristics of discourse modes and describe a neural sequence labeling model for identification. Evaluation results show that discourse modes can be identified automatically with an average F1-score of 0.7. We further demonstrate that discourse modes can be used as features that improve automatic essay scoring (AES). The impacts of discourse modes for AES are also discussed.",,,,ACL
12,2017,A Convolutional Encoder Model for Neural Machine Translation,"Jonas Gehring, Michael Auli, David Grangier, Yann Dauphin",The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. We present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT’16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and on WMT’15 English-German we outperform several recently published results. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT’14 English-French translation. We speed up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM.,,,,ACL
13,2017,Deep Neural Machine Translation with Linear Associative Unit,"Mingxuan Wang, Zhengdong Lu, Jie Zhou, Qun Liu","Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with its capability in modeling complex functions and capturing complex linguistic structures. However NMT with deep architecture in its encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often makes the optimization much more difficult. To address this problem we propose a novel linear associative units (LAU) to reduce the gradient propagation path inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs uses linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported on results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art.",,,,ACL
14,2017,Neural AMR: Sequence-to-Sequence Models for Parsing and Generation,"Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi","Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.",,,,ACL
15,2017,Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems,"Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom","Solving algebraic word problems requires executing a series of arithmetic operations—a program—to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.",,,,ACL
16,2017,Automatically Generating Rhythmic Verse with Neural Networks,"Jack Hopkins, Douwe Kiela","We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machine-generated poem to be the best amongst all evaluated.",,,,ACL
17,2017,Creating Training Corpora for NLG Micro-Planners,"Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini","In this paper, we present a novel framework for semi-automatically creating linguistically challenging micro-planning data-to-text corpora from existing Knowledge Bases. Because our method pairs data of varying size and shape with texts ranging from simple clauses to short texts, a dataset created using this framework provides a challenging benchmark for microplanning. Another feature of this framework is that it can be applied to any large scale knowledge base and can therefore be used to train and learn KB verbalisers. We apply our framework to DBpedia data and compare the resulting dataset with Wen et al. 2016’s. We show that while Wen et al.’s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of handling the complex interactions occurring during in micro-planning between lexicalisation, aggregation, surface realisation, referring expression generation and sentence segmentation. To encourage researchers to take up this challenge, we made available a dataset of 21,855 data/text pairs created using this framework in the context of the WebNLG shared task.",,,,ACL
18,2017,Gated Self-Matching Networks for Reading Comprehension and Question Answering,"Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang","In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.",,,,ACL
19,2017,Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning,"Shizhu He, Cao Liu, Kang Liu, Jun Zhao","Generating answer with natural language sentence is very important in real-world question answering systems, which needs to obtain a right answer as well as a coherent natural response. In this paper, we propose an end-to-end question answering system called COREQA in sequence-to-sequence learning, which incorporates copying and retrieving mechanisms to generate natural answers within an encoder-decoder framework. Specifically, in COREQA, the semantic units (words, phrases and entities) in a natural answer are dynamically predicted from the vocabulary, copied from the given question and/or retrieved from the corresponding knowledge base jointly. Our empirical study on both synthetic and real-world datasets demonstrates the efficiency of COREQA, which is able to generate correct, coherent and natural answers for knowledge inquired questions.",,,,ACL
20,2017,Coarse-to-Fine Question Answering for Long Documents,"Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin","We present a framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance of state-of-the-art models. While most successful approaches for reading comprehension rely on recurrent neural networks (RNNs), running them over long documents is prohibitively slow because it is difficult to parallelize over sequences. Inspired by how people first skim the document, identify relevant parts, and carefully read these parts to produce an answer, we combine a coarse, fast model for selecting relevant sentences and a more expensive RNN for producing the answer from those sentences. We treat sentence selection as a latent variable trained jointly from the answer only using reinforcement learning. Experiments demonstrate state-of-the-art performance on a challenging subset of the WikiReading dataset and on a new dataset, while speeding up the model by 3.5x-6.7x.",,,,ACL
21,2017,An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge,"Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He","With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Question answering over knowledge base (KB-QA) is one of the promising approaches to access the substantial knowledge. Meanwhile, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put more emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is not easy to express the proper information in the question. Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. As a result, it could alleviates the out-of-vocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely. The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach.",,,,ACL
22,2017,Translating Neuralese,"Jacob Andreas, Anca Dragan, Dan Klein","Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents’ messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.",,,,ACL
23,2017,Obtaining referential word meanings from visual and distributional information: Experiments on object naming,"Sina Zarrieß, David Schlangen","We investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings. We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.",,,,ACL
24,2017,FOIL it! Find One mismatch between Image and Language caption,"Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurélie Herbelot","In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and ‘foil’ captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake (‘foil word’). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.",,,,ACL
25,2017,Verb Physics: Relative Physical Knowledge of Actions and Objects,"Maxwell Forbes, Yejin Choi","Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., “My house is bigger than me.” However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, “Tyler entered his house” implies that his house is bigger than Tyler. In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance.",,,,ACL
26,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,"Masashi Yoshikawa, Hiroshi Noji, Yuji Matsumoto","We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies. Our model achieves the state-of-the-art results on English and Japanese CCG parsing.",,,,ACL
27,2017,A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing,"Daniel Fernández-González, Carlos Gómez-Rodríguez","Restricted non-monotonicity has been shown beneficial for the projective arc-eager dependency parser in previous research, as posterior decisions can repair mistakes made in previous states due to the lack of information. In this paper, we propose a novel, fully non-monotonic transition system based on the non-projective Covington algorithm. As a non-monotonic system requires exploration of erroneous actions during the training process, we develop several non-monotonic variants of the recently defined dynamic oracle for the Covington parser, based on tight approximations of the loss. Experiments on datasets from the CoNLL-X and CoNLL-XI shared tasks show that a non-monotonic dynamic oracle outperforms the monotonic version in the majority of languages.",,,,ACL
28,2017,Aggregating and Predicting Sequence Labels from Crowd Annotations,"An Thanh Nguyen, Byron Wallace, Junyi Jessy Li, Ani Nenkova","Despite sequences being core to NLP, scant work has considered how to handle noisy sequence labels from multiple annotators for the same text. Given such annotations, we consider two complementary tasks: (1) aggregating sequential crowd labels to infer a best single set of consensus annotations; and (2) using crowd annotations as training data for a model that can predict sequences in unannotated text. For aggregation, we propose a novel Hidden Markov Model variant. To predict sequences in unannotated text, we propose a neural approach using Long Short Term Memory. We evaluate a suite of methods across two different applications and text genres: Named-Entity Recognition in news articles and Information Extraction from biomedical abstracts. Results show improvement over strong baselines. Our source code and data are available online.",,,,ACL
29,2017,Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction,"Chunting Zhou, Graham Neubig","Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.",,,,ACL
30,2017,Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling,"Zhe Gan, Chunyuan Li, Changyou Chen, Yunchen Pu","Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.",,,,ACL
31,2017,Learning attention for historical text normalization by learning to pronounce,"Marcel Bollmann, Joachim Bingel, Anders Søgaard","Automated processing of historical texts often relies on pre-normalization to modern word forms. Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task. We address this problem by using several novel encoder-decoder architectures, including a multi-task learning (MTL) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-the-art by an absolute 2% increase in performance. We analyze the induced models across 44 different texts from Early New High German. Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms. This, we believe, is an important step toward understanding how MTL works.",,,,ACL
32,2017,Deep Learning in Semantic Kernel Spaces,"Danilo Croce, Simone Filice, Giuseppe Castellucci, Roberto Basili","Kernel methods enable the direct usage of structured representations of textual data during language learning and inference tasks. Expressive kernels, such as Tree Kernels, achieve excellent performance in NLP. On the other side, deep neural networks have been demonstrated effective in automatically learning feature representations during training. However, their input is tensor data, i.e., they can not manage rich structured information. In this paper, we show that expressive kernels and deep neural networks can be combined in a common framework in order to (i) explicitly model structured information and (ii) learn non-linear decision functions. We show that the input layer of a deep architecture can be pre-trained through the application of the Nystrom low-rank approximation of kernel spaces. The resulting “kernelized” neural network achieves state-of-the-art accuracy in three different tasks.",,,,ACL
33,2017,Topically Driven Neural Language Model,"Jey Han Lau, Timothy Baldwin, Trevor Cohn","Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.",,,,ACL
34,2017,Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors,"Xuepeng Wang, Kang Liu, Jun Zhao","Solving cold-start problem in review spam detection is an urgent and significant task. It can help the on-line review websites to relieve the damage of spammers in time, but has never been investigated by previous work. This paper proposes a novel neural network model to detect review spam for cold-start problem, by learning to represent the new reviewers’ review with jointly embedded textual and behavioral information. Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability. It is also applicable to a large scale dataset in an unsupervised way.",,,,ACL
35,2017,Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network,"Abhijit Mishra, Kuntal Dey, Pushpak Bhattacharyya","Cognitive NLP systems- i.e., NLP systems that make use of behavioral data - augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc. Such extraction of features is typically manual. We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like Sentiment Analysis and Sarcasm Detection, and that even the extraction and choice of features should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement/gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",,,,ACL
36,2017,An Unsupervised Neural Attention Model for Aspect Extraction,"Ruidan He, Wee Sun Lee, Hwee Tou Ng, Daniel Dahlmeier","Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.",,,,ACL
37,2017,Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization,"Akira Sasaki, Kazuaki Hanawa, Naoaki Okazaki, Kentaro Inui","We presents in this paper our approach for modeling inter-topic preferences of Twitter users: for example, “those who agree with the Trans-Pacific Partnership (TPP) also agree with free trade”. This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion survey, electoral prediction, electoral campaigns, and online debates. In order to extract users’ preferences on Twitter, we design linguistic patterns in which people agree and disagree about specific topics (e.g., “A is completely wrong”). By applying these linguistic patterns to a collection of tweets, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization: representing users’ preference as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our presented approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences.",,,,ACL
38,2017,Automatically Labeled Data Generation for Large Scale Event Extraction,"Yubo Chen, Shulin Liu, Xiang Zhang, Kang Liu","Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",,,,ACL
39,2017,Time Expression Analysis and Recognition Using Syntactic Token Types and General Heuristic Rules,"Xiaoshi Zhong, Aixin Sun, Erik Cambria","Extracting time expressions from free text is a fundamental task for many applications. We analyze the time expressions from four datasets and find that only a small group of words are used to express time information, and the words in time expressions demonstrate similar syntactic behaviour. Based on the findings, we propose a type-based approach, named SynTime, to recognize time expressions. Specifically, we define three main syntactic token types, namely time token, modifier, and numeral, to group time-related regular expressions over tokens. On the types we design general heuristic rules to recognize time expressions. In recognition, SynTime first identifies the time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions. As a light-weight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text of different types and of different domains. Experiment on benchmark datasets and tweets data shows that SynTime outperforms state-of-the-art methods.",,,,ACL
40,2017,Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix,"Bingfeng Luo, Yansong Feng, Zheng Wang, Zhanxing Zhu","Distant supervision significantly reduces human efforts in building training data for many classification tasks. While promising, this technique often introduces noise to the generated training data, which can severely affect the model performance. In this paper, we take a deep look at the application of distant supervision in relation extraction. We show that the dynamic transition matrix can effectively characterize the noise in the training data built by distant supervision. The transition matrix can be effectively trained using a novel curriculum learning based method without any direct supervision about the noise. We thoroughly evaluate our approach under a wide range of extraction scenarios. Experimental results show that our approach consistently improves the extraction results and outperforms the state-of-the-art in various evaluation scenarios.",,,,ACL
41,2017,A Syntactic Neural Model for General-Purpose Code Generation,"Pengcheng Yin, Graham Neubig","We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.",,,,ACL
42,2017,Learning bilingual word embeddings with (almost) no bilingual data,"Mikel Artetxe, Gorka Labaka, Eneko Agirre","Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs. This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead. In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.",,,,ACL
43,2017,Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks,"William Foland, James H. Martin","We present a system which parses sentences into Abstract Meaning Representations, improving state-of-the-art results for this task by more than 5%. AMR graphs represent semantic content using linguistic properties such as semantic roles, coreference, negation, and more. The AMR parser does not rely on a syntactic pre-parse, or heavily engineered features, and uses five recurrent neural networks as the key architectural components for inferring AMR graphs.",,,,ACL
44,2017,Deep Semantic Role Labeling: What Works and What’s Next,"Luheng He, Kenton Lee, Mike Lewis, Luke Zettlemoyer","We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10% relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.",,,,ACL
45,2017,Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access,"Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao","This paper proposes KB-InfoBot - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced “soft” posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents.",,,,ACL
46,2017,Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots,"Yu Wu, Wei Wu, Chen Xing, Ming Zhou","We study response selection for multi-turn conversation in retrieval based chatbots. Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among the utterances or important information in the context. We propose a sequential matching network (SMN) to address both problems. SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among the utterances. The final matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.",,,,ACL
47,2017,Learning Word-Like Units from Joint Audio-Visual Analysis,"David Harwath, James Glass","Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the word ‘lighthouse’ within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.",,,,ACL
48,2017,Joint CTC/attention decoding for end-to-end speech recognition,"Takaaki Hori, Shinji Watanabe, John Hershey","End-to-end automatic speech recognition (ASR) has become a popular alternative to conventional DNN/HMM systems because it avoids the need for linguistic resources such as pronunciation dictionary, tokenization, and context-dependency trees, leading to a greatly simplified model-building process. There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes joint decoding algorithm for end-to-end ASR with a hybrid CTC/attention architecture, which effectively utilizes both advantages in decoding. We have applied the proposed method to two ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and showing the comparable performance to conventional state-of-the-art DNN/HMM ASR systems without linguistic resources.",,,,ACL
49,2017,Found in Translation: Reconstructing Phylogenetic Language Trees from Translations,"Ella Rabinovich, Noam Ordan, Shuly Wintner","Translation has played an important role in trade, law, commerce, politics, and literature for thousands of years. Translators have always tried to be invisible; ideal translations should look as if they were written originally in the target language. We show that traces of the source language remain in the translation product to the extent that it is possible to uncover the history of the source language by looking only at the translation. Specifically, we automatically reconstruct phylogenetic language trees from monolingual texts (translated from several source languages). The signal of the source language is so powerful that it is retained even after two phases of translation. This strongly indicates that source language interference is the most dominant characteristic of translated texts, overshadowing the more subtle signals of universal properties of translation.",,,,ACL
50,2017,Predicting Native Language from Gaze,"Yevgeni Berzak, Chie Nakamura, Suzanne Flynn, Boris Katz","A fundamental question in language learning concerns the role of a speaker’s first language in second language acquisition. We present a novel methodology for studying this question: analysis of eye-movement patterns in second language reading of free-form text. Using this methodology, we demonstrate for the first time that the native language of English learners can be predicted from their gaze fixations when reading English. We provide analysis of classifier uncertainty and learned features, which indicates that differences in English reading are likely to be rooted in linguistic divergences across native languages. The presented framework complements production studies and offers new ground for advancing research on multilingualism.",,,,ACL
51,2017,MORSE: Semantic-ally Drive-n MORpheme SEgment-er,"Tarek Sakakini, Suma Bhat, Pramod Viswanath","We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic information for this task. We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality. We validate our algorithm across datasets and present state-of-the-art results.",,,,ACL
52,2017,Deep Pyramid Convolutional Neural Networks for Text Categorization,"Rie Johnson, Tong Zhang","This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text categorization that can efficiently represent long-range associations in text. In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data. However, the associated computational complexity increases as the networks go deeper, which poses serious challenges in practical applications. Moreover, it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data. Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much. We call it deep pyramid CNN. The proposed model with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization.",,,,ACL
53,2017,Improved Neural Relation Detection for Knowledge Base Question Answering,"Mo Yu, Wenpeng Yin, Kazi Saidul Hasan, Cicero dos Santos","Relation detection is a core component of many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning which detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different levels of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.",,,,ACL
54,2017,Deep Keyphrase Generation,"Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He","Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at https://github.com/memray/seq2seq-keyphrase.",,,,ACL
55,2017,Attention-over-Attention Neural Networks for Reading Comprehension,"Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang","Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. The proposed model aims to place another attention mechanism over the document-level attention and induces “attended attention” for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children’s Book Test.",,,,ACL
56,2017,Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations,"Gabriel Doyle, Amir Goldberg, Sameer Srivastava, Michael Frank","Cultural fit is widely believed to affect the success of individuals and the groups to which they belong. Yet it remains an elusive, poorly measured construct. Recent research draws on computational linguistics to measure cultural fit but overlooks asymmetries in cultural adaptation. By contrast, we develop a directed, dynamic measure of cultural fit based on linguistic alignment, which estimates the influence of one person’s word use on another’s and distinguishes between two enculturation mechanisms: internalization and self-regulation. We use this measure to trace employees’ enculturation trajectories over a large, multi-year corpus of corporate emails and find that patterns of alignment in the first six months of employment are predictive of individuals’ downstream outcomes, especially involuntary exit. Further predictive analyses suggest referential alignment plays an overlooked role in linguistic alignment.",,,,ACL
57,2017,Representations of language in a model of visually grounded speech signal,"Grzegorz Chrupała, Lieke Gelderloos, Afra Alishahi","We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.",,,,ACL
58,2017,Spectral Analysis of Information Density in Dialogue Predicts Collaborative Task Performance,"Yang Xu, David Reitter","We propose a perspective on dialogue that focuses on relative information contributions of conversation partners as a key to successful communication. We predict the success of collaborative task in English and Danish corpora of task-oriented dialogue. Two features are extracted from the frequency domain representations of the lexical entropy series of each interlocutor, power spectrum overlap (PSO) and relative phase (RP). We find that PSO is a negative predictor of task success, while RP is a positive one. An SVM with these features significantly improved on previous task success prediction models. Our findings suggest that the strategic distribution of information density between interlocutors is relevant to task success.",,,,ACL
59,2017,Affect-LM: A Neural Language Model for Customizable Affective Text Generation,"Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency","Human verbal communication includes affective messages which are conveyed through use of emotionally colored words. There has been a lot of research effort in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generation of conversational text, conditioned on affect categories. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using Amazon Mechanical Turk show that Affect-LM can generate naturally looking emotional sentences without sacrificing grammatical correctness. Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction.",,,,ACL
60,2017,Domain Attention with an Ensemble of Experts,"Young-Bum Kim, Karl Stratos, Dongchan Kim","An important problem in domain adaptation is to quickly generalize to a new domain with limited supervision given K existing domains. One approach is to retrain a global model across all K + 1 domains using standard techniques, for instance Daumé III (2009). However, it is desirable to adapt without having to re-estimate a global model from scratch each time a new domain with potentially new intents and slots is added. We describe a solution based on attending an ensemble of domain experts. We assume K domain specific intent and slot models trained on respective domains. When given domain K + 1, our model uses a weighted combination of the K domain experts’ feedback along with its own opinion to make predictions on the new domain. In experiments, the model significantly outperforms baselines that do not use domain adaptation and also performs better than the full retraining approach.",,,,ACL
61,2017,Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,"Tiancheng Zhao, Ran Zhao, Maxine Eskenazi","While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder from word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that capture the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved through introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence of discourse-level decision-making.",,,,ACL
62,2017,Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning,"Jason D. Williams, Kavosh Asadi, Geoffrey Zweig","End-to-end learning of recurrent neural networks (RNNs) is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors. We introduce Hybrid Code Networks (HCNs), which combine an RNN with domain-specific knowledge encoded as software and system action templates. Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state. In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both. HCNs attain state-of-the-art performance on the bAbI dialog dataset (Bordes and Weston, 2016), and outperform two commercially deployed customer-facing dialog systems at our company.",,,,ACL
63,2017,Generating Contrastive Referring Expressions,"Martín Villalba, Christoph Teichmann, Alexander Koller","The referring expressions (REs) produced by a natural language generation (NLG) system can be misunderstood by the hearer, even when they are semantically correct. In an interactive setting, the NLG system can try to recognize such misunderstandings and correct them. We present an algorithm for generating corrective REs that use contrastive focus (“no, the BLUE button”) to emphasize the information the hearer most likely misunderstood. We show empirically that these contrastive REs are preferred over REs without contrast marking.",,,,ACL
64,2017,Modeling Source Syntax for Neural Machine Translation,"Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu","Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize parse trees of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.",,,,ACL
65,2017,Sequence-to-Dependency Neural Machine Translation,"Shuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li","Nowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned. Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and Japanese-English translation tasks.",,,,ACL
66,2017,Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning,"Jing Ma, Wei Gao, Kam-Fai Wong","How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-of-the-art rumor detection models.",,,,ACL
67,2017,EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks,"Muhammad Abdul-Mageed, Lyle Ungar","Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on emotion detection has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it. We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58%). We also extend the task beyond emotion types to model Robert Plutick’s 8 primary emotion dimensions, acquiring a superior accuracy of 95.68%.",,,,ACL
68,2017,Beyond Binary Labels: Political Ideology Prediction of Twitter Users,"Daniel Preoţiuc-Pietro, Ye Liu, Daniel Hopkins, Lyle Ungar","Automatic political orientation prediction from social media posts has to date proven successful only in distinguishing between publicly declared liberals and conservatives in the US. This study examines users’ political ideology using a seven-point scale which enables us to identify politically moderate and neutral users – groups which are of particular interest to political scientists and pollsters. Using a novel data set with political ideology labels self-reported through surveys, our goal is two-fold: a) to characterize the groups of politically engaged users through language use on Twitter; b) to build a fine-grained model that predicts political ideology of unseen users. Our results identify differences in both political leaning and engagement and the extent to which each group tweets using political keywords. Finally, we demonstrate how to improve ideology prediction accuracy by exploiting the relationships between the user groups.",,,,ACL
69,2017,Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter,"Kristen Johnson, Di Jin, Dan Goldwasser","Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring political framing typically analyze frame usage in longer texts, such as congressional speeches. We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter. Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.",,,,ACL
70,2017,A Nested Attention Neural Hybrid Model for Grammatical Error Correction,"Jianshu Ji, Qinlong Wang, Kristina Toutanova, Yongen Gong","Grammatical error correction (GEC) systems strive to correct both global errors inword order and usage, and local errors inspelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC.Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information, and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset.Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective incorrecting local errors that involve small edits in orthography.",,,,ACL
71,2017,TextFlow: A Text Similarity Measure based on Continuous Sequences,"Yassine Mrabet, Halil Kilicoglu, Dina Demner-Fushman","Text similarity measures are used in multiple tasks such as plagiarism detection, information ranking and recognition of paraphrases and textual entailment. While recent advances in deep learning highlighted the relevance of sequential models in natural language generation, existing similarity measures do not fully exploit the sequential nature of language. Examples of such similarity measures include n-grams and skip-grams overlap which rely on distinct slices of the input texts. In this paper we present a novel text similarity measure inspired from a common representation in DNA sequence alignment algorithms. The new measure, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the similarity value. Our experiments on 8 different datasets show very encouraging results in paraphrase detection, textual entailment recognition and ranking relevance.",,,,ACL
72,2017,"Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts","Chenhao Tan, Dallas Card, Noah A. Smith","Understanding how ideas relate to each other is a fundamental question in many domains, ranging from intellectual history to public communication. Because ideas are naturally embedded in texts, we propose the first framework to systematically characterize the relations between ideas based on their occurrence in a corpus of documents, independent of how these ideas are represented. Combining two statistics—cooccurrence within documents and prevalence correlation over time—our approach reveals a number of different ways in which ideas can cooperate and compete. For instance, two ideas can closely track each other’s prevalence over time, and yet rarely cooccur, almost like a “cold war” scenario. We observe that pairwise cooccurrence and prevalence correlation exhibit different distributions. We further demonstrate that our approach is able to uncover intriguing relations between ideas through in-depth case studies on news articles and research papers.",,,,ACL
73,2017,Polish evaluation dataset for compositional distributional semantics models,"Alina Wróblewska, Katarzyna Krasnowska-Kieraś","The paper presents a procedure of building an evaluation dataset. for the validation of compositional distributional semantics models estimated for languages other than English. The procedure generally builds on steps designed to assemble the SICK corpus, which contains pairs of English sentences annotated for semantic relatedness and entailment, because we aim at building a comparable dataset. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the need for language-specific transformation rules. The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset. The resource consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish.",,,,ACL
74,2017,Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction,"Christopher Bryant, Mariano Felice, Ted Briscoe","Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as “Good” or “Acceptable” in at least 95% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.",,,,ACL
75,2017,Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability,"Saku Sugawara, Yusuke Kido, Hikaru Yokono, Akiko Aizawa","Knowing the quality of reading comprehension (RC) datasets is important for the development of natural-language understanding systems. In this study, two classes of metrics were adopted for evaluating RC datasets: prerequisite skills and readability. We applied these classes to six existing datasets, including MCTest and SQuAD, and highlighted the characteristics of the datasets according to each metric and the correlation between the two classes. Our dataset analysis suggests that the readability of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy to read but difficult to answer.",,,,ACL
76,2017,A Minimal Span-Based Neural Constituency Parser,"Mitchell Stern, Jacob Andreas, Dan Klein","In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).",,,,ACL
77,2017,Semantic Dependency Parsing via Book Embedding,"Weiwei Sun, Junjie Cao, Xiaojun Wan","We model a dependency graph as a book, a particular kind of topological space, for semantic dependency parsing. The spine of the book is made up of a sequence of words, and each page contains a subset of noncrossing arcs. To build a semantic graph for a given sentence, we design new Maximum Subgraph algorithms to generate noncrossing graphs on each page, and a Lagrangian Relaxation-based algorithm tocombine pages into a book. Experiments demonstrate the effectiveness of the bookembedding framework across a wide range of conditions. Our parser obtains comparable results with a state-of-the-art transition-based parser.",,,,ACL
78,2017,Neural Word Segmentation with Rich Pretraining,"Jie Yang, Yue Zhang, Fei Dong","Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.",,,,ACL
79,2017,Neural Machine Translation via Binary Code Prediction,"Yusuke Oda, Philip Arthur, Graham Neubig, Koichiro Yoshino","In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.",,,,ACL
80,2017,What do Neural Machine Translation Models Learn about Morphology?,"Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad","Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.",,,,ACL
81,2017,Context-Dependent Sentiment Analysis in User-Generated Videos,"Soujanya Poria, Erik Cambria, Devamanyu Hazarika, Navonil Majumder","Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the interdependencies and relations among the utterances of a video. In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process. Our method shows 5-10% performance improvement over the state of the art and high robustness to generalizability.",,,,ACL
82,2017,A Multidimensional Lexicon for Interpersonal Stancetaking,"Umashanthi Pavalanathan, Jim Fitzpatrick, Scott Kiesling, Jacob Eisenstein","The sociolinguistic construct of stancetaking describes the activities through which discourse participants create and signal relationships to their interlocutors, to the topic of discussion, and to the talk itself. Stancetaking underlies a wide range of interactional phenomena, relating to formality, politeness, affect, and subjectivity. We present a computational approach to stancetaking, in which we build a theoretically-motivated lexicon of stance markers, and then use multidimensional analysis to identify a set of underlying stance dimensions. We validate these dimensions intrinscially and extrinsically, showing that they are internally coherent, match pre-registered hypotheses, and correlate with social phenomena.",,,,ACL
83,2017,Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,"Jeffrey Lund, Connor Cook, Kevin Seppi, Jordan Boyd-Graber","Interactive topic models are powerful tools for those seeking to understand large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, going beyond existing single word anchor algorithms—an approach we call “Tandem Anchors”. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and non-interactive approaches. Tandem anchors are faster and more intuitive than existing interactive approaches.",,,,ACL
84,2017,Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task,"Omid Bakhshandeh, James Allen","Understanding common entities and their attributes is a primary requirement for any system that comprehends natural language. In order to enable learning about common entities, we introduce a novel machine comprehension task, GuessTwo: given a short paragraph comparing different aspects of two real-world semantically-similar entities, a system should guess what those entities are. Accomplishing this task requires deep language understanding which enables inference, connecting each comparison paragraph to different levels of knowledge about world entities and their attributes. So far we have crowdsourced a dataset of more than 14K comparison paragraphs comparing entities from a variety of categories such as fruits and animals. We have designed two schemes for evaluation: open-ended, and binary-choice prediction. For benchmarking further progress in the task, we have collected a set of paragraphs as the test set on which human can accomplish the task with an accuracy of 94.2% on open-ended prediction. We have implemented various models for tackling the task, ranging from semantic-driven to neural models. The semantic-driven approach outperforms the neural models, however, the results indicate that the task is very challenging across the models.",,,,ACL
85,2017,Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees,"Arzoo Katiyar, Claire Cardie","We present a novel attention-based recurrent neural network for joint extraction of entity mentions and relations. We show that attention along with long short term memory (LSTM) network can extract semantic relations between entity mentions without having access to dependency trees. Experiments on Automatic Content Extraction (ACE) corpora show that our model significantly outperforms feature-based joint model by Li and Ji (2014). We also compare our model with an end-to-end tree-based LSTM model (SPTree) by Miwa and Bansal (2016) and show that our model performs within 1% on entity mentions and 2% on relations. Our fine-grained analysis also shows that our model performs significantly better on Agent-Artifact relations, while SPTree performs better on Physical and Part-Whole relations.",,,,ACL
86,2017,Naturalizing a Programming Language via Interactive Learning,"Sida I. Wang, Samuel Ginn, Percy Liang, Christopher D. Manning","Our goal is to create a convenient natural language interface for performing well-specified but complex actions such as analyzing data, manipulating text, and querying databases. However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language. To bridge this gap, we start with a core programming language and allow users to “naturalize” the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9% of the last 10K utterances.",,,,ACL
87,2017,Semantic Word Clusters Using Signed Spectral Clustering,"João Sedoc, Jean Gallier, Dean Foster, Lyle Ungar","Vector space representations of words capture many aspects of word similarity, but such methods tend to produce vector spaces in which antonyms (as well as synonyms) are close to each other. For spectral clustering using such word embeddings, words are points in a vector space where synonyms are linked with positive weights, while antonyms are linked with negative weights. We present a new signed spectral normalized graph cut algorithm, signed clustering, that overlays existing thesauri upon distributionally derived vector representations of words, so that antonym relationships between word pairs are represented by negative weights. Our signed clustering algorithm produces clusters of words that simultaneously capture distributional and synonym relations. By using randomized spectral decomposition (Halko et al., 2011) and sparse matrices, our method is both fast and scalable. We validate our clusters using datasets containing human judgments of word pair similarities and show the benefit of using our word clusters for sentiment prediction.",,,,ACL
88,2017,An Interpretable Knowledge Transfer Model for Knowledge Base Completion,"Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy","Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets—WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.",,,,ACL
89,2017,Learning a Neural Semantic Parser from User Feedback,"Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy","We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.",,,,ACL
90,2017,Joint Modeling of Content and Discourse Relations in Dialogues,"Kechen Qin, Lu Wang, Joseph Kim",We present a joint modeling approach to identify salient discussion points in spoken meetings as well as to label the discourse relations between speaker turns. A variation of our model is also discussed when discourse relations are treated as latent variables. Experimental results on two popular meeting corpora show that our joint model can outperform state-of-the-art approaches for both phrase-based content selection and discourse relation prediction tasks. We also evaluate our model on predicting the consistency among team members’ understanding of their group decisions. Classifiers trained with features constructed from our model achieve significant better predictive performance than the state-of-the-art.,,,,ACL
91,2017,Argument Mining with Structured SVMs and RNNs,"Vlad Niculae, Joonsuk Park, Claire Cardie","We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure. (This is the case in over 20% of the web comments dataset we release.) Our model jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.",,,,ACL
92,2017,Neural Discourse Structure for Text Categorization,"Yangfeng Ji, Noah A. Smith","We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.",,,,ACL
93,2017,Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification,"Lianhui Qin, Zhisong Zhang, Hai Zhao, Zhiting Hu","Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.",,,,ACL
94,2017,Don’t understand a measure? Learn it: Structured Prediction for Coreference Resolution optimizing its measures,"Iryna Haponchyk, Alessandro Moschitti","An interesting aspect of structured prediction is the evaluation of an output structure against the gold standard. Especially in the loss-augmented setting, the need of finding the max-violating constraint has severely limited the expressivity of effective loss functions. In this paper, we trade off exact computation for enabling the use and study of more complex loss functions for coreference resolution. Most interestingly, we show that such functions can be (i) automatically learned also from controversial but commonly accepted coreference measures, e.g., MELA, and (ii) successfully used in learning algorithms. The accurate model comparison on the standard CoNLL-2012 setting shows the benefit of more expressive loss functions.",,,,ACL
95,2017,Bayesian Modeling of Lexical Resources for Low-Resource Settings,"Nicholas Andrews, Mark Dredze, Benjamin Van Durme, Jason Eisner","Lexical resources such as dictionaries and gazetteers are often used as auxiliary data for tasks such as part-of-speech induction and named-entity recognition. However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of features which generalize better. In this paper, we investigate a more robust approach: we stipulate that the lexicon is the result of an assumed generative process. Practically, this means that we may treat the lexical resources as observations under the proposed generative model. The lexical resources provide training data for the generative model without requiring separate data to estimate lexical feature weights. We evaluate the proposed approach in two settings: part-of-speech induction and low-resource named-entity recognition.",,,,ACL
96,2017,Semi-Supervised QA with Generative Domain-Adaptive Nets,"Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, William Cohen","We study the problem of semi-supervised question answering—utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.",,,,ACL
97,2017,From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood,"Kelvin Guu, Panupong Pasupat, Evan Liu, Percy Liang","Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.",,,,ACL
98,2017,Diversity driven attention model for query-based abstractive summarization,"Preksha Nema, Mitesh M. Khapra, Anirban Laha, Balaraman Ravindran","Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.",,,,ACL
99,2017,Get To The Point: Summarization with Pointer-Generator Networks,"Abigail See, Peter J. Liu, Christopher D. Manning","Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",,,,ACL
100,2017,Supervised Learning of Automatic Pyramid for Optimization-Based Multi-Document Summarization,"Maxime Peyrard, Judith Eckle-Kohler","We present a new supervised framework that learns to estimate automatic Pyramid scores and uses them for optimization-based extractive multi-document summarization. For learning automatic Pyramid scores, we developed a method for automatic training data generation which is based on a genetic algorithm using automatic Pyramid as the fitness function. Our experimental evaluation shows that our new framework significantly outperforms strong baselines regarding automatic Pyramid, and that there is much room for improvement in comparison with the upper-bound for automatic Pyramid.",,,,ACL
101,2017,Selective Encoding for Abstractive Sentence Summarization,"Qingyu Zhou, Nan Yang, Furu Wei, Ming Zhou","We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.",,,,ACL
102,2017,PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents,"Corina Florescu, Cornelia Caragea","The large and growing amounts of online scholarly data present both challenges and opportunities to enhance knowledge discovery. One such challenge is to automatically extract a small set of keyphrases from a document that can accurately describe the document’s content and can facilitate fast information processing. In this paper, we propose PositionRank, an unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a word’s occurrences into a biased PageRank. Our model obtains remarkable improvements in performance over PageRank models that do not take into account word positions as well as over strong baselines for this task. Specifically, on several datasets of research papers, PositionRank achieves improvements as high as 29.09%.",,,,ACL
103,2017,Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses,"Ryan Lowe, Michael Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier","Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem.We present an evaluation model (ADEM)that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model’s predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue mod-els unseen during training, an important step for automatic dialogue evaluation.",,,,ACL
104,2017,A Transition-Based Directed Acyclic Graph Parser for UCCA,"Daniel Hershcovich, Omri Abend, Ari Rappoport","We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.",,,,ACL
105,2017,Abstract Syntax Networks for Code Generation and Semantic Parsing,"Maxim Rabinovich, Mitchell Stern, Dan Klein","Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.",,,,ACL
106,2017,Visualizing and Understanding Neural Machine Translation,"Yanzhuo Ding, Yang Liu, Huanbo Luan, Maosong Sun","While neural machine translation (NMT) has made remarkable progress in recent years, it is hard to interpret its internal workings due to the continuous representations and non-linearity of neural networks. In this work, we propose to use layer-wise relevance propagation (LRP) to compute the contribution of each contextual word to arbitrary hidden states in the attention-based encoder-decoder framework. We show that visualization with LRP helps to interpret the internal workings of NMT and analyze translation errors.",,,,ACL
107,2017,Detecting annotation noise in automatically labelled data,"Ines Rehbein, Josef Ruppenhofer","We introduce a method for error detection in automatically annotated text, aimed at supporting the creation of high-quality language resources at affordable cost. Our method combines an unsupervised generative model with human supervision from active learning. We test our approach on in-domain and out-of-domain data in two languages, in AL simulations and in a real world setting. For all settings, the results show that our method is able to detect annotation errors with high precision and high recall.",,,,ACL
108,2017,Abstractive Document Summarization with a Graph-Based Attentional Neural Model,"Jiwei Tan, Xiaojun Wan, Jianguo Xiao","Abstractive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization, which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.",,,,ACL
109,2017,Probabilistic Typology: Deep Generative Models of Vowel Inventories,"Ryan Cotterell, Jason Eisner","Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most—but not all—languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.",,,,ACL
110,2017,Adversarial Multi-Criteria Learning for Chinese Word Segmentation,"Xinchi Chen, Zhan Shi, Xipeng Qiu, Xuanjing Huang","Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github.",,,,ACL
111,2017,Neural Joint Model for Transition-based Chinese Syntactic Analysis,"Shuhei Kurita, Daisuke Kawahara, Sadao Kurohashi","We present neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing. Our models are the first neural approaches for fully joint Chinese analysis that is known to prevent the error propagation problem of pipeline models. Although word embeddings play a key role in dependency parsing, they cannot be applied directly to the joint task in the previous work. To address this problem, we propose embeddings of character strings, in addition to words. Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing. We also explore bi-LSTM models with fewer features.",,,,ACL
112,2017,Robust Incremental Neural Semantic Graph Parsing,"Jan Buys, Phil Blunsom","Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.",,,,ACL
113,2017,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,"Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao","Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What’s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.",,,,ACL
114,2017,A Local Detection Approach for Named Entity Recognition and Mention Detection,"Mingbin Xu, Hui Jiang, Sedtawut Watcharawittayakul","In this paper, we study a novel approach for named entity recognition (NER) and mention detection (MD) in natural language processing. Instead of treating NER as a sequence labeling problem, we propose a new local detection approach, which relies on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixed-size representation. Subsequently, a simple feedforward neural network (FFNN) is learned to either reject or predict entity label for each individual text fragment. The proposed method has been evaluated in several popular NER and MD tasks, including CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks. Our method has yielded pretty strong performance in all of these examined tasks. This local detection approach has shown many advantages over the traditional sequence labeling methods.",,,,ACL
115,2017,Vancouver Welcomes You! Minimalist Location Metonymy Resolution,"Milan Gritta, Mohammad Taher Pilehvar, Nut Limsopatham, Nigel Collier","Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.",,,,ACL
116,2017,"Unifying Text, Metadata, and User Network Representations with a Neural Network for Geolocation Prediction","Yasuhide Miura, Motoki Taniguchi, Tomoki Taniguchi, Tomoko Ohkuma","We propose a novel geolocation prediction model using a complex neural network. Geolocation prediction in social media has attracted many researchers to use information of various types. Our model unifies text, metadata, and user network representations with an attention mechanism to overcome previous ensemble approaches. In an evaluation using two open datasets, the proposed model exhibited a maximum 3.8% increase in accuracy and a maximum of 6.6% increase in accuracy@161 against previous models. We further analyzed several intermediate layers of our model, which revealed that their states capture some statistical characteristics of the datasets.",,,,ACL
117,2017,Multi-Task Video Captioning with Video and Entailment Generation,"Ramakanth Pasunuru, Mohit Bansal","Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailing caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.",,,,ACL
118,2017,Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts,"Leandro Santos, Edilson Anselmo Corrêa Júnior, Osvaldo Oliveira Jr, Diego Amancio","Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose. Linguistic features, mainly from parsers, have been used to detect MCI, but this is not suitable for large-scale assessments. MCI disfluencies produce non-grammatical speech that requires manual or high precision automatic correction of transcripts. In this paper, we modeled transcripts into complex networks and enriched them with word embedding (CNE) to better represent short texts produced in neuropsychological assessments. The network measurements were applied with well-known classifiers to automatically identify MCI in transcripts, in a binary classification task. A comparison was made with the performance of traditional approaches using Bag of Words (BoW) and linguistic features for three datasets: DementiaBank in English, and Cinderella and Arizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using only complex networks, while Support Vector Machine was superior to other classifiers. CNE provided the highest accuracies for DementiaBank and Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably owing to its short narratives. The approach using linguistic features yielded higher accuracy if the transcriptions of the Cinderella dataset were manually revised. Taken together, the results indicate that complex networks enriched with embedding is promising for detecting MCI in large-scale assessments.",,,,ACL
119,2017,Adversarial Adaptation of Synthetic or Stale Data,"Young-Bum Kim, Karl Stratos, Dongchan Kim","Two types of data shift common in practice are 1. transferring from synthetic data to live user data (a deployment shift), and 2. transferring from stale data to current data (a temporal shift). Both cause a distribution mismatch between training and evaluation, leading to a model that overfits the flawed training data and performs poorly on the test data. We propose a solution to this mismatch problem by framing it as domain adaptation, treating the flawed training dataset as a source domain and the evaluation dataset as a target domain. To this end, we use and build on several recent advances in neural domain adaptation such as adversarial training (Ganinet al., 2016) and domain separation network (Bousmalis et al., 2016), proposing a new effective adversarial training scheme. In both supervised and unsupervised adaptation scenarios, our approach yields clear improvement over strong baselines.",,,,ACL
120,2017,Chat Detection in an Intelligent Assistant: Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems,"Satoshi Akasaki, Nobuhiro Kaji","Recently emerged intelligent assistants on smartphones and home electronics (e.g., Siri and Alexa) can be seen as novel hybrids of domain-specific task-oriented spoken dialogue systems and open-domain non-task-oriented ones. To realize such hybrid dialogue systems, this paper investigates determining whether or not a user is going to have a chat with the system. To address the lack of benchmark datasets for this task, we construct a new dataset consisting of 15,160 utterances collected from the real log data of a commercial intelligent assistant (and will release the dataset to facilitate future research activity). In addition, we investigate using tweets and Web search queries for handling open-domain user utterances, which characterize the task of chat detection. Experimental experiments demonstrated that, while simple supervised methods are effective, the use of the tweets and search queries further improves the F1-score from 86.21 to 87.53.",,,,ACL
121,2017,A Neural Local Coherence Model,"Dat Tien Nguyen, Shafiq Joty","We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.",,,,ACL
122,2017,Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation (ONLG),"Tomer Cagan, Stefan L. Frank, Reut Tsarfaty","Opinionated Natural Language Generation (ONLG) is a new, challenging, task that aims to automatically generate human-like, subjective, responses to opinionated articles online. We present a data-driven architecture for ONLG that generates subjective responses triggered by users’ agendas, consisting of topics and sentiments, and based on wide-coverage automatically-acquired generative grammars. We compare three types of grammatical representations that we design for ONLG, which interleave different layers of linguistic information and are induced from a new, enriched dataset we developed. Our evaluation shows that generation with Relational-Realizational (Tsarfaty and Sima’an, 2008) inspired grammar gets better language model scores than lexicalized grammars ‘a la Collins (2003), and that the latter gets better human-evaluation scores. We also show that conditioning the generation on topic models makes generated responses more relevant to the document content.",,,,ACL
123,2017,Learning to Ask: Neural Question Generation for Reading Comprehension,"Xinya Du, Junru Shao, Claire Cardie","We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e.,, grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).",,,,ACL
124,2017,Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback,"Avinesh P.V.S, Christian M. Meyer","In this paper, we propose an extractive multi-document summarization (MDS) system using joint optimization and active learning for content selection grounded in user feedback. Our method interactively obtains user feedback to gradually improve the results of a state-of-the-art integer linear programming (ILP) framework for MDS. Our methods complement fully automatic methods in producing high-quality summaries with a minimum number of iterations and feedbacks. We conduct multiple simulation-based experiments and analyze the effect of feedback-based concept selection in the ILP setup in order to maximize the user-desired content in the summary.",,,,ACL
125,2017,Flexible and Creative Chinese Poetry Generation Using Neural Memory,"Jiyuan Zhang, Yang Feng, Dong Wang, Yang Wang","It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.",,,,ACL
126,2017,Learning to Generate Market Comments from Stock Prices,"Soichiro Murakami, Akihiko Watanabe, Akira Miyazawa, Keiichi Goshima","This paper presents a novel encoder-decoder model for automatically generating market comments from stock prices. The model first encodes both short- and long-term series of stock prices so that it can mention short- and long-term changes in stock prices. In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic operation such as subtraction or rounding, and applying it to the input stock prices. Empirical experiments show that our best model generates market comments at the fluency and the informativeness approaching human-generated reference texts.",,,,ACL
127,2017,Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains,"Liangguo Wang, Jing Jiang, Hai Leong Chieu, Chen Hui Ong","In this paper, we study how to improve the domain adaptability of a deletion-based Long Short-Term Memory (LSTM) neural network model for sentence compression. We hypothesize that syntactic information helps in making such models more robust across domains. We propose two major changes to the model: using explicit syntactic features and introducing syntactic constraints through Integer Linear Programming (ILP). Our evaluation shows that the proposed model works better than the original model as well as a traditional non-neural-network-based model in a cross-domain setting.",,,,ACL
128,2017,Transductive Non-linear Learning for Chinese Hypernym Prediction,"Chengyu Wang, Junchi Yan, Aoying Zhou, Xiaofeng He","Finding the correct hypernyms for entities is essential for taxonomy learning, fine-grained entity categorization, query understanding, etc. Due to the flexibility of the Chinese language, it is challenging to identify hypernyms in Chinese accurately. Rather than extracting hypernyms from texts, in this paper, we present a transductive learning approach to establish mappings from entities to hypernyms in the embedding space directly. It combines linear and non-linear embedding projection models, with the capacity of encoding arbitrary language-specific rules. Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction.",,,,ACL
129,2017,A Constituent-Centric Neural Architecture for Reading Comprehension,"Pengtao Xie, Eric Xing","Reading comprehension (RC), aiming to understand natural texts and answer questions therein, is a challenging task. In this paper, we study the RC problem on the Stanford Question Answering Dataset (SQuAD). Observing from the training set that most correct answers are centered around constituents in the parse tree, we design a constituent-centric neural architecture where the generation of candidate answers and their representation learning are both based on constituents and guided by the parse tree. Under this architecture, the search space of candidate answers can be greatly reduced without sacrificing the coverage of correct answers and the syntactic, hierarchical and compositional structure among constituents can be well captured, which contributes to better representation learning of the candidate answers. On SQuAD, our method achieves the state of the art performance and the ablation study corroborates the effectiveness of individual modules.",,,,ACL
130,2017,Cross-lingual Distillation for Text Classification,"Ruochen Xu, Yiming Yang","Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.",,,,ACL
131,2017,Understanding and Predicting Empathic Behavior in Counseling Therapy,"Verónica Pérez-Rosas, Rada Mihalcea, Kenneth Resnicow, Satinder Singh","Counselor empathy is associated with better outcomes in psychology and behavioral counseling. In this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to counselor empathy during motivational interviewing encounters. Particularly, we analyze aspects such as participants’ engagement, participants’ verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80%.",,,,ACL
132,2017,Leveraging Knowledge Bases in LSTMs for Improving Machine Reading,"Bishan Yang, Tom Mitchell","This paper focuses on how to take advantage of external knowledge bases (KBs) to improve recurrent neural networks for machine reading. Traditional methods that exploit knowledge from KBs encode knowledge as discrete indicator features. Not only do these features generalize poorly, but they require task-specific feature engineering to achieve good performance. We propose KBLSTM, a novel neural model that leverages continuous representations of KBs to enhance the learning of recurrent neural networks for machine reading. To effectively integrate background knowledge with information from the currently processed text, our model employs an attention mechanism with a sentinel to adaptively decide whether to attend to background knowledge and which information from KBs is useful. Experimental results show that our model achieves accuracies that surpass the previous state-of-the-art results for both entity extraction and event extraction on the widely used ACE2005 dataset.",,,,ACL
133,2017,Prerequisite Relation Learning for Concepts in MOOCs,"Liangming Pan, Chengjiang Li, Juanzi Li, Jie Tang","What prerequisite knowledge should students achieve a level of mastery before moving forward to learn subsequent coursewares? We study the extent to which the prerequisite relation between knowledge concepts in Massive Open Online Courses (MOOCs) can be inferred automatically. In particular, what kinds of information can be leverage to uncover the potential prerequisite relation between knowledge concepts. We first propose a representation learning-based method for learning latent representations of course concepts, and then investigate how different features capture the prerequisite relations between concepts. Our experiments on three datasets form Coursera show that the proposed method achieves significant improvements (+5.9-48.0% by F1-score) comparing with existing methods.",,,,ACL
134,2017,Unsupervised Text Segmentation Based on Native Language Characteristics,"Shervin Malmasi, Mark Dras, Mark Johnson, Lan Du","Most work on segmenting text does so on the basis of topic changes, but it can be of interest to segment by other, stylistically expressed characteristics such as change of authorship or native language. We propose a Bayesian unsupervised text segmentation approach to the latter. While baseline models achieve essentially random segmentation on our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation.",,,,ACL
135,2017,Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection,"Jian Ni, Georgiana Dinu, Radu Florian","The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of manually annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper, we present two weakly supervised approaches for cross-lingual NER with no human annotation in a target language. The first approach is to create automatically labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuristic scheme that effectively selects good-quality projection-labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without re-training. We also design two co-decoding schemes that effectively combine the outputs of the two projection-based approaches. We evaluate the performance of the proposed approaches on both in-house and open NER data for several target languages. The results show that the combined systems outperform three other weakly supervised approaches on the CoNLL data.",,,,ACL
136,2017,Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks,"Abhisek Chakrabarty, Onkar Arun Pandit, Utpal Garain","We introduce a composite deep neural network architecture for supervised and language independent context sensitive lemmatization. The proposed method considers the task as to identify the correct edit tree representing the transformation between a word-lemma pair. To find the lemma of a surface word, we exploit two successive bidirectional gated recurrent structures - the first one is used to extract the character level dependencies and the next one captures the contextual information of the given word. The key advantages of our model compared to the state-of-the-art lemmatizers such as Lemming and Morfette are - (i) it is independent of human decided features (ii) except the gold lemma, no other expensive morphological attribute is required for joint learning. We evaluate the lemmatizer on nine languages - Bengali, Catalan, Dutch, Hindi, Hungarian, Italian, Latin, Romanian and Spanish. It is found that except Bengali, the proposed method outperforms Lemming and Morfette on the other languages. To train the model on Bengali, we develop a gold lemma annotated dataset (having 1,702 sentences with a total of 20,257 word tokens), which is an additional contribution of this work.",,,,ACL
137,2017,Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling,"Kazuya Kawakami, Chris Dyer, Phil Blunsom","Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the “bursty” distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.",,,,ACL
138,2017,Bandit Structured Prediction for Neural Sequence-to-Sequence Learning,"Julia Kreutzer, Artem Sokolov, Stefan Riezler","Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.",,,,ACL
139,2017,Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization,"Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu","Although neural machine translation has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge. In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into neural machine translation. We represent prior knowledge sources as features in a log-linear model, which guides the learning processing of the neural translation model. Experiments on Chinese-English dataset show that our approach leads to significant improvements.",,,,ACL
140,2017,Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation,"Jinchao Zhang, Mingxuan Wang, Qun Liu, Jie Zhou","This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance. Our proposed models enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality.",,,,ACL
141,2017,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,"Chris Hokamp, Qun Liu","We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model’s output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.",,,,ACL
142,2017,Combating Human Trafficking with Multimodal Deep Models,"Edmund Tong, Amir Zadeh, Cara Jones, Louis-Philippe Morency","Human trafficking is a global epidemic affecting millions of people across the planet. Sex trafficking, the dominant form of human trafficking, has seen a significant rise mostly due to the abundance of escort websites, where human traffickers can openly advertise among at-will escort advertisements. In this paper, we take a major step in the automatic detection of advertisements suspected to pertain to human trafficking. We present a novel dataset called Trafficking-10k, with more than 10,000 advertisements annotated for this task. The dataset contains two sources of information per advertisement: text and images. For the accurate detection of trafficking advertisements, we designed and trained a deep multimodal model called the Human Trafficking Deep Network (HTDN).",,,,ACL
143,2017,MalwareTextDB: A Database for Annotated Malware Articles,"Swee Kiat Lim, Aldrian Obaja Muis, Wei Lu, Chen Hui Ong","Cybersecurity risks and malware threats are becoming increasingly dangerous and common. Despite the severity of the problem, there has been few NLP efforts focused on tackling cybersecurity. In this paper, we discuss the construction of a new database for annotated malware texts. An annotation framework is introduced based on the MAEC vocabulary for defining malware characteristics, along with a database consisting of 39 annotated APT reports with a total of 6,819 sentences. We also use the database to construct models that can potentially help cybersecurity researchers in their data collection and analytics efforts.",,,,ACL
144,2017,A Corpus of Annotated Revisions for Studying Argumentative Writing,"Fan Zhang, Homa B. Hashemi, Rebecca Hwa, Diane Litman","This paper presents ArgRewrite, a corpus of between-draft revisions of argumentative essays. Drafts are manually aligned at the sentence level, and the writer’s purpose for each revision is annotated with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction.",,,,ACL
145,2017,Watset: Automatic Induction of Synsets from a Graph of Synonyms,"Dmitry Ustalov, Alexander Panchenko, Chris Biemann","This paper presents a new graph-based approach that induces synsets using synonymy dictionaries and word embeddings. First, we build a weighted graph of synonyms extracted from commonly available resources, such as Wiktionary. Second, we apply word sense induction to deal with ambiguous words. Finally, we cluster the disambiguated version of the ambiguous input graph into synsets. Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform a fuzzy clustering of the graph. Despite its simplicity, our approach shows excellent results, outperforming five competitive state-of-the-art methods in terms of F-score on three gold standard datasets for English and Russian derived from large-scale manually constructed lexical resources.",,,,ACL
146,2017,Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis,"Hiroki Ouchi, Hiroyuki Shindo, Yuji Matsumoto","The performance of Japanese predicate argument structure (PAS) analysis has improved in recent years thanks to the joint modeling of interactions between multiple predicates. However, this approach relies heavily on syntactic information predicted by parsers, and suffers from errorpropagation. To remedy this problem, we introduce a model that uses grid-type recurrent neural networks. The proposed model automatically induces features sensitive to multi-predicate interactions from the word sequence information of a sentence. Experiments on the NAIST Text Corpus demonstrate that without syntactic information, our model outperforms previous syntax-dependent models.",,,,ACL
147,2017,TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension,"Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer","We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.",,,,ACL
148,2017,Learning Semantic Correspondences in Technical Documentation,"Kyle Richardson, Jonas Kuhn","We consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation. We focus specifically on the problem of learning translational correspondences between text descriptions and grounded representations in the target documentation, such as formal representation of functions or code templates. Our approach exploits the parallel nature of such documentation, or the tight coupling between high-level text and the low-level representations we aim to learn. Data is collected by mining technical documents for such parallel text-representation pairs, which we use to train a simple semantic parsing model. We report new baseline results on sixteen novel datasets, including the standard library documentation for nine popular programming languages across seven natural languages, and a small collection of Unix utility manuals.",,,,ACL
149,2017,Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding,"Yixin Cao, Lifu Huang, Heng Ji, Xu Chen","Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense. In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance.",,,,ACL
150,2017,Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication,"Lanbo She, Joyce Chai","To enable human-robot communication and collaboration, previous works represent grounded verb semantics as the potential change of state to the physical world caused by these verbs. Grounded verb semantics are acquired mainly based on the parallel data of the use of a verb phrase and its corresponding sequences of primitive actions demonstrated by humans. The rich interaction between teachers and students that is considered important in learning new skills has not yet been explored. To address this limitation, this paper presents a new interactive learning approach that allows robots to proactively engage in interaction with human partners by asking good questions to learn models for grounded verb semantics. The proposed approach uses reinforcement learning to allow the robot to acquire an optimal policy for its question-asking behaviors by maximizing the long-term reward. Our empirical results have shown that the interactive learning approach leads to more reliable models for grounded verb semantics, especially in the noisy environment which is full of uncertainties. Compared to previous work, the models acquired from interactive learning result in a 48% to 145% performance gain when applied in new situations.",,,,ACL
151,2017,Multimodal Word Distributions,"Ben Athiwaratkun, Andrew Wilson","Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.",,,,ACL
152,2017,Enhanced LSTM for Natural Language Inference,"Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei","Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result—it further improves the performance even when added to the already very strong model.",,,,ACL
153,2017,Linguistic analysis of differences in portrayal of movie characters,"Anil Ramakrishna, Victor R. Martínez, Nikolaos Malandrakis, Karan Singla","We examine differences in portrayal of characters in movies using psycholinguistic and graph theoretic measures computed directly from screenplays. Differences are examined with respect to characters’ gender, race, age and other metadata. Psycholinguistic metrics are extrapolated to dialogues in movies using a linear regression model built on a set of manually annotated seed words. Interesting patterns are revealed about relationships between genders of production team and the gender ratio of characters. Several correlations are noted between gender, race, age of characters and the linguistic metrics.",,,,ACL
154,2017,Linguistically Regularized LSTM for Sentiment Classification,"Qiao Qian, Minlie Huang, Jinhao Lei, Xiaoyan Zhu","This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words). In this paper, we propose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression.",,,,ACL
155,2017,Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation,"Lotem Peled, Roi Reichart","Sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment. In other words, “Sarcasm is the giant chasm between what I say, and the person who doesn’t get it.”. In this paper we present the novel task of sarcasm interpretation, defined as the generation of a non-sarcastic utterance conveying the same message as the original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets, each interpreted by five human judges. Addressing the task as monolingual machine translation (MT), we experiment with MT algorithms and evaluation measures. We then present SIGN: an MT based sarcasm interpretation algorithm that targets sentiment words, a defining element of textual sarcasm. We show that while the scores of n-gram based automatic measures are similar for all interpretation models, SIGN’s interpretations are scored higher by humans for adequacy and sentiment polarity. We conclude with a discussion on future research directions for our new task.",,,,ACL
156,2017,Active Sentiment Domain Adaptation,"Fangzhao Wu, Yongfeng Huang, Jun Yan","Domain adaptation is an important technology to handle domain dependence problem in sentiment analysis field. Existing methods usually rely on sentiment classifiers trained in source domains. However, their performance may heavily decline if the distributions of sentiment features in source and target domains have significant difference. In this paper, we propose an active sentiment domain adaptation approach to handle this problem. Instead of the source domain sentiment classifiers, our approach adapts the general-purpose sentiment lexicons to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode, as well as the domain-specific sentiment similarities among words mined from unlabeled samples of target domain. A unified model is proposed to fuse different types of sentiment information and train sentiment classifier for target domain. Extensive experiments on benchmark datasets show that our approach can train accurate sentiment classifier with less labeled samples.",,,,ACL
157,2017,Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models,"Navid Rekabsaz, Mihai Lupu, Artem Baklanov, Alexander Dür","Volatility prediction—an essential concept in financial markets—has recently been addressed using sentiment analysis methods. We investigate the sentiment of annual disclosures of companies in stock markets to forecast volatility. We specifically explore the use of recent Information Retrieval (IR) term weighting models that are effectively extended by related terms using word embeddings. In parallel to textual information, factual market data have been widely used as the mainstream approach to forecast market risk. We therefore study different fusion methods to combine text and market data resources. Our word embedding-based approach significantly outperforms state-of-the-art methods. In addition, we investigate the characteristics of the reports of the companies in different financial sectors.",,,,ACL
158,2017,CANE: Context-Aware Network Embedding for Relation Modeling,"Cunchao Tu, Han Liu, Zhiyuan Liu, Maosong Sun","Network embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively. Therefore, we present Context-Aware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our model with existing NE models on three real-world datasets. Experimental results show that CANE achieves significant improvement than state-of-the-art methods on link prediction and comparable performance on vertex classification. The source code and datasets can be obtained from https://github.com/thunlp/CANE.",,,,ACL
159,2017,Universal Dependencies Parsing for Colloquial Singaporean English,"Hongmin Wang, Yue Zhang, GuangYong Leonard Chan, Jie Yang","Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-of-the-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies. To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages. We make both our annotation and parser available for further research.",,,,ACL
160,2017,Generic Axiomatization of Families of Noncrossing Graphs in Dependency Parsing,"Anssi Yli-Jyrä, Carlos Gómez-Rodríguez","We present a simple encoding for unlabeled noncrossing graphs and show how its latent counterpart helps us to represent several families of directed and undirected graphs used in syntactic and semantic parsing of natural language as context-free languages. The families are separated purely on the basis of forbidden patterns in latent encoding, eliminating the need to differentiate the families of non-crossing graphs in inference algorithms: one algorithm works for all when the search space can be controlled in parser input.",,,,ACL
161,2017,Semi-supervised sequence tagging with bidirectional language models,"Matthew Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power","Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.",,,,ACL
162,2017,Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings,"He He, Anusha Balakrishnan, Mihail Eric, Percy Liang","We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.",,,,ACL
163,2017,Neural Belief Tracker: Data-Driven Dialogue State Tracking,"Nikola Mrkšić, Diarmuid Ó Séaghdha, Tsung-Hsien Wen, Blaise Thomson","One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user’s goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users’ language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.",,,,ACL
164,2017,Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms,"Shulin Liu, Yubo Chen, Kang Liu, Jun Zhao","This paper tackles the task of event detection (ED), which involves identifying and categorizing events. We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms. In specific, we systematically investigate the proposed model under the supervision of different attention strategies. Experimental results show that our approach advances state-of-the-arts and achieves the best F1 score on ACE 2005 dataset.",,,,ACL
165,2017,Topical Coherence in LDA-based Models through Induced Segmentation,"Hesam Amoualian, Wei Lu, Eric Gaussier, Georgios Balikas","This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.",,,,ACL
166,2017,Jointly Extracting Relations with Class Ties via Effective Deep Ranking,"Hai Ye, Wenhan Chao, Zhunchen Luo, Zhoujun Li","Connections between relations in relation extraction, which we call class ties, are common. In distantly supervised scenario, one entity tuple may have multiple relation facts. Exploiting class ties between relations of one entity tuple will be promising for distantly supervised relation extraction. However, previous models are not effective or ignore to model this property. In this work, to effectively leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network (CNN) with a general pairwise ranking framework, in which three novel ranking loss functions are introduced. Additionally, an effective method is presented to relieve the severe class imbalance problem from NR (not relation) for model training. Experiments on a widely used dataset show that leveraging class ties will enhance extraction and demonstrate the effectiveness of our model to learn class ties. Our model outperforms the baselines significantly, achieving state-of-the-art performance.",,,,ACL
167,2017,Search-based Neural Structured Learning for Sequential Question Answering,"Mohit Iyyer, Wen-tau Yih, Ming-Wei Chang","Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but inter-related questions. We collect a dataset of 6,066 question sequences that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs in total. To solve this sequential question answering task, we propose a novel dynamic neural semantic parsing framework trained using a weakly supervised reward-guided search. Our model effectively leverages the sequential context to outperform state-of-the-art QA systems that are designed to answer highly complex questions.",,,,ACL
168,2017,Gated-Attention Readers for Text Comprehension,"Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William Cohen","In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task–the CNN & Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention.",,,,ACL
169,2017,Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering,"Jianbo Ye, Yanran Li, Zhaohui Wu, James Z. Wang","Word embeddings have become widely-used in document analysis. While a large number of models for mapping words to vector spaces have been developed, it remains undetermined how much net gain can be achieved over traditional approaches based on bag-of-words. In this paper, we propose a new document clustering approach by combining any word embedding with a state-of-the-art algorithm for clustering empirical distributions. By using the Wasserstein distance between distributions, the word-to-word semantic relationship is taken into account in a principled way. The new clustering method is easy to use and consistently outperforms other methods on a variety of data sets. More importantly, the method provides an effective framework for determining when and how much word embeddings contribute to document analysis. Experimental results with multiple embedding models are reported.",,,,ACL
170,2017,Towards a Seamless Integration of Word Senses into Downstream NLP Applications,"Mohammad Taher Pilehvar, Jose Camacho-Collados, Roberto Navigli, Nigel Collier","Lexical ambiguity can impede NLP systems from accurate understanding of semantics. Despite its potential benefits, the integration of sense-level information into NLP systems has remained understudied. By incorporating a novel disambiguation algorithm into a state-of-the-art classification model, we create a pipeline to integrate sense-level information into downstream NLP applications. We show that a simple disambiguation of the input text can lead to consistent performance improvement on multiple topic categorization and polarity detection datasets, particularly when the fine granularity of the underlying sense inventory is reduced and the document is sufficiently large. Our results also point to the need for sense representation research to focus more on in vivo evaluations which target the performance in downstream NLP applications rather than artificial benchmarks.",,,,ACL
171,2017,Reading Wikipedia to Answer Open-Domain Questions,"Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes",This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.,,,,ACL
172,2017,Learning to Skim Text,"Adams Wei Yu, Hongrae Lee, Quoc Le","Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.",,,,ACL
173,2017,An Algebra for Feature Extraction,Vivek Srikumar,"Though feature extraction is a necessary first step in statistical NLP, it is often seen as a mere preprocessing step. Yet, it can dominate computation time, both during training, and especially at deployment. In this paper, we formalize feature extraction from an algebraic perspective. Our formalization allows us to define a message passing algorithm that can restructure feature templates to be more computationally efficient. We show via experiments on text chunking and relation extraction that this restructuring does indeed speed up feature extraction in practice by reducing redundant computation.",,,,ACL
174,2017,Chunk-based Decoder for Neural Machine Translation,"Shonosuke Ishiwatari, Jingtao Yao, Shujie Liu, Mu Li","Chunks (or phrases) once played a pivotal role in machine translation. By using a chunk rather than a word as the basic translation unit, local (intra-chunk) and global (inter-chunk) word orders and dependencies can be easily modeled. The chunk structure, despite its importance, has not been considered in the decoders used for neural machine translation (NMT). In this paper, we propose chunk-based decoders for (NMT), each of which consists of a chunk-level decoder and a word-level decoder. The chunk-level decoder models global dependencies while the word-level decoder decides the local word order in a chunk. To output a target sentence, the chunk-level decoder generates a chunk representation containing global information, which the word-level decoder then uses as a basis to predict the words inside the chunk. Experimental results show that our proposed decoders can significantly improve translation performance in a WAT ‘16 English-to-Japanese translation task.",,,,ACL
175,2017,Doubly-Attentive Decoder for Multi-modal Neural Machine Translation,"Iacer Calixto, Qun Liu, Nick Campbell","We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.",,,,ACL
176,2017,A Teacher-Student Framework for Zero-Resource Neural Machine Translation,"Yun Chen, Yang Liu, Yong Cheng, Victor O.K. Li","While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on the assumption, our method is able to train a source-to-target NMT model (“student”) without parallel corpora available guided by an existing pivot-to-target NMT model (“teacher”) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.",,,,ACL
177,2017,Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder,"Huadong Chen, Shujian Huang, David Chiang, Jiajun Chen","Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.",,,,ACL
178,2017,Cross-lingual Name Tagging and Linking for 282 Languages,"Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman","The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating “silver-standard” annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.",,,,ACL
179,2017,Adversarial Training for Unsupervised Bilingual Lexicon Induction,"Meng Zhang, Yang Liu, Huanbo Luan, Maosong Sun","Word embeddings are well known to capture linguistic regularities of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previous endeavors to connect separate monolingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lexicon. In this work, we show that such cross-lingual connection can actually be established without any form of supervision. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues.",,,,ACL
180,2017,Estimating Code-Switching on Twitter with a Novel Generalized Word-Level Language Detection Technique,"Shruti Rijhwani, Royal Sequiera, Monojit Choudhury, Kalika Bali","Word-level language detection is necessary for analyzing code-switched text, where multiple languages could be mixed within a sentence. Existing models are restricted to code-switching between two specific languages and fail in real-world scenarios as text input rarely has a priori information on the languages used. We present a novel unsupervised word-level language detection technique for code-switched text for an arbitrarily large number of languages, which does not require any manually annotated training data. Our experiments with tweets in seven languages show a 74% relative error reduction in word-level labeling with respect to competitive baselines. We then use this system to conduct a large-scale quantitative analysis of code-switching patterns on Twitter, both global as well as region-specific, with 58M tweets.",,,,ACL
181,2017,Using Global Constraints and Reranking to Improve Cognates Detection,"Michael Bloodgood, Benjamin Strauss","Global constraints and reranking have not been used in cognates detection research to date. We propose methods for using global constraints by performing rescoring of the score matrices produced by state of the art cognates detection systems. Using global constraints to perform rescoring is complementary to state of the art methods for performing cognates detection and results in significant performance improvements beyond current state of the art performance on publicly available datasets with different language pairs and various conditions such as different levels of baseline state of the art performance and different data size conditions, including with more realistic large data size conditions than have been evaluated with in the past.",,,,ACL
182,2017,One-Shot Neural Cross-Lingual Transfer for Paradigm Completion,"Katharina Kann, Ryan Cotterell, Hinrich Schütze","We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.",,,,ACL
183,2017,Morphological Inflection Generation with Hard Monotonic Attention,"Roee Aharoni, Yoav Goldberg","We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft (Bahdanau, 2014) attention models for the task, shedding some light on the features such models extract.",,,,ACL
184,2017,From Characters to Words to in Between: Do We Capture Morphology?,"Clara Vania, Adam Lopez","Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.",,,,ACL
185,2017,Riemannian Optimization for Skip-Gram Negative Sampling,"Alexander Fonarev, Oleksii Grinchuk, Gleb Gusev, Pavel Serdyukov","Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in “word2vec” software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.",,,,ACL
186,2017,Deep Multitask Learning for Semantic Dependency Parsing,"Hao Peng, Sam Thomson, Noah A. Smith","We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches—one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at https://github.com/Noahs-ARK/NeurboParser.",,,,ACL
187,2017,Improved Word Representation Learning with Sememes,"Yilin Niu, Ruobing Xie, Zhiyuan Liu, Maosong Sun","Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes. Since sememes are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases. In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks. The key idea is to utilize word sememes to capture exact meanings of a word within specific contexts accurately. More specifically, we follow the framework of Skip-gram and present three sememe-encoded models to learn representations of sememes, senses and words, where we apply the attention scheme to detect word senses in various contexts. We conduct experiments on two tasks including word similarity and word analogy, and our models significantly outperform baselines. The results indicate that WRL can benefit from sememes via the attention scheme, and also confirm our models being capable of correctly modeling sememe information.",,,,ACL
188,2017,Learning Character-level Compositionality with Visual Features,"Frederick Liu, Han Lu, Chieh Lo, Graham Neubig","Previous work has modeled the compositionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry topical content which resulting in embeddings that are coherent in visual space.",,,,ACL
189,2017,A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data,"Qiaolin Xia, Lei Sha, Baobao Chang, Zhifang Sui","Previous studies on Chinese semantic role labeling (SRL) have concentrated on a single semantically annotated corpus. But the training data of single corpus is often limited. Whereas the other existing semantically annotated corpora for Chinese SRL are scattered across different annotation frameworks. But still, Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In this paper, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that our model outperforms state-of-the-art methods.",,,,ACL
190,2017,Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings,"John Wieting, Kevin Gimpel","We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the Gated Recurrent Averaging Network, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.",,,,ACL
191,2017,Ontology-Aware Token Embeddings for Prepositional Phrase Attachment,"Pradeep Dasigi, Waleed Ammar, Chris Dyer, Eduard Hovy","Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.",,,,ACL
192,2017,Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition,"Ellie Pavlick, Marius Paşca","We present a method for populating fine-grained classes (e.g., “1950s American jazz musicians”) with instances (e.g., Charles Mingus ). While state-of-the-art methods tend to treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a >10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns.",,,,ACL
193,2017,"Parsing to 1-Endpoint-Crossing, Pagenumber-2 Graphs","Junjie Cao, Sheng Huang, Weiwei Sun, Xiaojun Wan","We study the Maximum Subgraph problem in deep dependency parsing. We consider two restrictions to deep dependency graphs: (a) 1-endpoint-crossing and (b) pagenumber-2. Our main contribution is an exact algorithm that obtains maximum subgraphs satisfying both restrictions simultaneously in time O(n5). Moreover, ignoring one linguistically-rare structure descreases the complexity to O(n4). We also extend our quartic-time algorithm into a practical parser with a discriminative disambiguation model and evaluate its performance on four linguistic data sets used in semantic dependency parsing.",,,,ACL
194,2017,Semi-supervised Multitask Learning for Sequence Labeling,Marek Rei,"We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.",,,,ACL
195,2017,Semantic Parsing of Pre-university Math Problems,"Takuya Matsuzaki, Takumi Ito, Hidenao Iwane, Hirokazu Anai",We have been developing an end-to-end math problem solving system that accepts natural language input. The current paper focuses on how we analyze the problem sentences to produce logical forms. We chose a hybrid approach combining a shallow syntactic analyzer and a manually-developed lexicalized grammar. A feature of the grammar is that it is extensively typed on the basis of a formal ontology for pre-university math. These types are helpful in semantic disambiguation inside and across sentences. Experimental results show that the hybrid system produces a well-formed logical form with 88% precision and 56% recall.,,,,ACL
1,2018,Probabilistic FastText for Multi-Sense Word Embeddings,"Ben Athiwaratkun, Andrew Wilson, Anima Anandkumar","We introduce Probabilistic FastText, a new model for word embeddings that can capture multiple word senses, sub-word structure, and uncertainty information. In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams. This representation allows the model to share the “strength” across sub-word structures (e.g. Latin roots), producing accurate representations of rare, misspelt, or even unseen words. Moreover, each component of the mixture can capture a different word sense. Probabilistic FastText outperforms both FastText, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets. We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings. Thus, our model is the first to achieve best of both the worlds: multi-sense representations while having enriched semantics on rare words.",,,,ACL
2,2018,A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors,"Mikhail Khodak, Nikunj Saunshi, Yingyu Liang, Tengyu Ma","Motivations like domain adaptation, transfer learning, and feature learning have fueled interest in inducing embeddings for rare or unseen words, n-grams, synsets, and other textual features. This paper introduces a la carte embedding, a simple and general alternative to the usual word2vec-based approaches for building such representations that is based upon recent theoretical results for GloVe-like embeddings. Our method relies mainly on a linear transformation that is efficiently learnable using pretrained word vectors and linear regression. This transform is applicable on the fly in the future when a new text feature or rare word is encountered, even if only a single usage example is available. We introduce a new dataset showing how the a la carte method requires fewer examples of words in context to learn high-quality embeddings and we obtain state-of-the-art results on a nonce task and some unsupervised document classification tasks.",,,,ACL
3,2018,Unsupervised Learning of Distributional Relation Vectors,"Shoaib Jameel, Zied Bouraoui, Steven Schockaert","Word embedding models such as GloVe rely on co-occurrence statistics to learn vector representations of word meaning. While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words, existing approaches for modeling such relationships are based on manipulating pre-trained word vectors. In this paper, we introduce a novel method which directly learns relation vectors from co-occurrence statistics. To this end, we first introduce a variant of GloVe, in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors. We then show how relation vectors can be naturally embedded into the resulting vector space.",,,,ACL
4,2018,Explicit Retrofitting of Distributional Word Vectors,"Goran Glavaš, Ivan Vulić","Semantic specialization of distributional word vectors, referred to as retrofitting, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation. Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints. In this work, in contrast, we transform external lexico-semantic relations into training examples which we use to learn an explicit retrofitting model (ER). The ER model allows us to learn a global specialization function and specialize the vectors of words unobserved in the training data as well. We report large gains over original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks − lexical simplification and dialog state tracking. Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces.",,,,ACL
5,2018,Unsupervised Neural Machine Translation with Weight Sharing,"Zhen Yang, Wei Chen, Feng Wang, Bo Xu","Unsupervised neural machine translation (NMT) is a recently proposed approach for machine translation which aims to train the model without using any labeled data. The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared-latent space, which is weak in keeping the unique and internal characteristics of each language, such as the style, terminology, and sentence structure. To address this issue, we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high-level representations of the input sentences. Besides, two different generative adversarial networks (GANs), namely the local GAN and global GAN, are proposed to enhance the cross-language translation. With this new approach, we achieve significant improvements on English-German, English-French and Chinese-to-English translation tasks.",,,,ACL
6,2018,Triangular Architecture for Rare Language Translation,"Shuo Ren, Wenhu Chen, Shujie Liu, Mu Li","Neural Machine Translation (NMT) performs poor on the low-resource language pair (X,Z), especially when Z is a rare language. By introducing another rich language Y, we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data (Y,Z) (may be small) and (X,Y) (can be rich) to improve the translation performance of low-resource pairs. In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with an unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X,Y). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.",,,,ACL
7,2018,Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates,Taku Kudo,"Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.",,,,ACL
8,2018,The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,"Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson","The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT’14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",,,,ACL
9,2018,Ultra-Fine Entity Typing,"Eunsol Choi, Omer Levy, Yejin Choi, Luke Zettlemoyer","We introduce a new entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity. This formulation allows us to use a new type of distant supervision at large scale: head words, which indicate the type of the noun phrases they appear in. We show that these ultra-fine types can be crowd-sourced, and introduce new evaluation sets that are much more diverse and fine-grained than existing benchmarks. We present a model that can predict ultra-fine types, and is trained using a multitask objective that pools our new head-word supervision with prior supervision from entity linking. Experimental results demonstrate that our model is effective in predicting entity types at varying granularity; it achieves state of the art performance on an existing fine-grained entity typing benchmark, and sets baselines for our newly-introduced datasets.",,,,ACL
10,2018,Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking,"Shikhar Murty, Patrick Verga, Luke Vilnis, Irena Radovanovic","Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies. Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies. This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset. We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMentions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and TypeNet, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types. In experiments on all three datasets we show substantial gains from hierarchy-aware training.",,,,ACL
11,2018,Improving Knowledge Graph Embedding Using Simple Constraints,"Boyang Ding, Quan Wang, Bin Wang, Li Guo","Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/iieir-km/ComplEx-NNE_AER.",,,,ACL
12,2018,Towards Understanding the Geometry of Knowledge Graph Embeddings,"Chandrahas, Aditya Sharma, Partha Talukdar","Knowledge Graph (KG) embedding has emerged as a very active area of research over the last few years, resulting in the development of several embedding methods. These KG embedding methods represent KG entities and relations as vectors in a high-dimensional space. Despite this popularity and effectiveness of KG embeddings in various tasks (e.g., link prediction), geometric understanding of such embeddings (i.e., arrangement of entity and relation vectors in vector space) is unexplored – we fill this gap in the paper. We initiate a study to analyze the geometry of KG embeddings and correlate it with task performance and other hyperparameters. To the best of our knowledge, this is the first study of its kind. Through extensive experiments on real-world datasets, we discover several insights. For example, we find that there are sharp differences between the geometry of embeddings learnt by different classes of KG embeddings methods. We hope that this initial study will inspire other follow-up research on this important but unexplored problem.",,,,ACL
13,2018,A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss,"Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min","We propose a unified model combining the strength of extractive and abstractive summarization. On the one hand, a simple extractive model can obtain sentence-level attention with high ROUGE scores but less readable. On the other hand, a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph. In our model, sentence-level attention is used to modulate the word-level attention such that words in less attended sentences are less likely to be generated. Moreover, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions. By end-to-end training our model with the inconsistency loss and original losses of extractive and abstractive models, we achieve state-of-the-art ROUGE scores while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation.",,,,ACL
14,2018,Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks,"Aishwarya Jadhav, Vaibhav Rajan","We present a new neural sequence-to-sequence model for extractive summarization called SWAP-NET (Sentences and Words from Alternating Pointer Networks). Extractive summaries comprising a salient subset of input sentences, often also contain important key words. Guided by this principle, we design SWAP-NET that models the interaction of key words and salient sentences using a new two-level pointer network based architecture. SWAP-NET identifies both salient sentences and key words in an input document, and then combines them to form the extractive summary. Experiments on large scale benchmark corpora demonstrate the efficacy of SWAP-NET that outperforms state-of-the-art extractive summarizers.",,,,ACL
15,2018,"Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization","Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei","Most previous seq2seq summarization systems purely depend on the source text to generate summaries, which tends to work unstably. Inspired by the traditional template-based summarization approaches, this paper proposes to use existing summaries as soft templates to guide the seq2seq model. To this end, we use a popular IR platform to Retrieve proper summaries as candidate templates. Then, we extend the seq2seq framework to jointly conduct template Reranking and template-aware summary generation (Rewriting). Experiments show that, in terms of informativeness, our model significantly outperforms the state-of-the-art methods, and even soft templates themselves demonstrate high competitiveness. In addition, the import of high-quality external summaries improves the stability and readability of generated summaries.",,,,ACL
16,2018,Simple and Effective Text Simplification Using Semantic and Neural Methods,"Elior Sulem, Omri Abend, Ari Rappoport","Sentence splitting is a major simplification operator. Here we present a simple and efficient splitting algorithm based on an automatic semantic parser. After splitting, the text is amenable for further fine-tuned simplification operations. In particular, we show that neural Machine Translation can be effectively used in this situation. Previous application of Machine Translation for simplification suffers from a considerable disadvantage in that they are over-conservative, often failing to modify the source in any way. Splitting based on semantic parsing, as proposed here, alleviates this issue. Extensive automatic and human evaluation shows that the proposed method compares favorably to the state-of-the-art in combined lexical and structural simplification.",,,,ACL
17,2018,"Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20,000 English Words",Saif Mohammad,"Words play a central role in language and thought. Factor analysis studies have shown that the primary dimensions of meaning are valence, arousal, and dominance (VAD). We present the NRC VAD Lexicon, which has human ratings of valence, arousal, and dominance for more than 20,000 English words. We use Best–Worst Scaling to obtain fine-grained scores and address issues of annotation consistency that plague traditional rating scale methods of annotation. We show that the ratings obtained are vastly more reliable than those in existing lexicons. We also show that there exist statistically significant differences in the shared understanding of valence, arousal, and dominance across demographic variables such as age, gender, and personality.",,,,ACL
18,2018,Comprehensive Supersense Disambiguation of English Prepositions and Possessives,"Nathan Schneider, Jena D. Hwang, Vivek Srikumar, Jakob Prange","Semantic relations are often signaled with prepositional or possessive marking—but extreme polysemy bedevils their analysis and automatic interpretation. We introduce a new annotation scheme, corpus, and task for the disambiguation of prepositions and possessives in English. Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; use broadly applicable supersense classes rather than fine-grained dictionary definitions; unite prepositions and possessives under the same class inventory; and distinguish between a marker’s lexical contribution and the role it marks in the context of a predicate or scene. Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task.",,,,ACL
19,2018,"A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature","Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang","We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the ‘PICO’ elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.",,,,ACL
20,2018,Efficient Online Scalar Annotation with Bounded Support,"Keisuke Sakaguchi, Benjamin Van Durme","We describe a novel method for efficiently eliciting scalar annotations for dataset construction and system quality estimation by human judgments. We contrast direct assessment (annotators assign scores to items directly), online pairwise ranking aggregation (scores derive from annotator comparison of items), and a hybrid approach (EASL: Efficient Annotation of Scalar Labels) proposed here. Our proposal leads to increased correlation with ground truth, at far greater annotator efficiency, suggesting this strategy as an improved mechanism for dataset creation and manual system evaluation.",,,,ACL
21,2018,Neural Argument Generation Augmented with Externally Retrieved Evidence,"Xinyu Hua, Lu Wang","High quality arguments are essential elements for human reasoning and decision-making processes. However, effective argument construction is a challenging task for both human and machines. In this work, we study a novel task on automatically generating arguments of a different stance for a given statement. We propose an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia. Our model first generates a set of talking point phrases as intermediate representation, followed by a separate decoder producing the final argument based on both input and the keyphrases. Experiments on a large-scale dataset collected from Reddit show that our model constructs arguments with more topic-relevant content than popular sequence-to-sequence generation models according to automatic evaluation and human assessments.",,,,ACL
22,2018,A Stylometric Inquiry into Hyperpartisan and Fake News,"Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",,,,ACL
23,2018,Retrieval of the Best Counterargument without Prior Topic Knowledge,"Henning Wachsmuth, Shahbaz Syed, Benno Stein","Given any argument on any controversial topic, how to counter it? This question implies the challenging retrieval task of finding the best counterargument. Since prior knowledge of a topic cannot be expected in general, we hypothesize the best counterargument to invoke the same aspects as the argument while having the opposite stance. To operationalize our hypothesis, we simultaneously model the similarity and dissimilarity of pairs of arguments, based on the words and embeddings of the arguments’ premises and conclusions. A salient property of our model is its independence from the topic at hand, i.e., it applies to arbitrary arguments. We evaluate different model variations on millions of argument pairs derived from the web portal idebate.org. Systematic ranking experiments suggest that our hypothesis is true for many arguments: For 7.6 candidates with opposing stance on average, we rank the best counterargument highest with 60% accuracy. Even among all 2801 test set pairs as candidates, we still find the best one about every third time.",,,,ACL
24,2018,LinkNBed: Multi-Graph Representation Learning with Entity Linkage,"Rakshit Trivedi, Bunyamin Sisman, Xin Luna Dong, Christos Faloutsos","Knowledge graphs have emerged as an important model for studying complex multi-relational data. This has given rise to the construction of numerous large scale but incomplete knowledge graphs encoding information extracted from various resources. An effective and scalable approach to jointly learn over multiple graphs and eventually construct a unified graph is a crucial next step for the success of knowledge-based inference for many downstream applications. To this end, we propose LinkNBed, a deep relational learning framework that learns entity and relationship representations across multiple graphs. We identify entity linkage across graphs as a vital component to achieve our goal. We design a novel objective that leverage entity linkage and build an efficient multi-task training procedure. Experiments on link prediction and entity linkage demonstrate substantial improvements over the state-of-the-art relational learning approaches.",,,,ACL
25,2018,Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures,"Luke Vilnis, Xiang Li, Shikhar Murty, Andrew McCallum","Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations). Probabilistic extensions of OE have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anti-correlation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts, and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs, and investigate the power of the model.",,,,ACL
26,2018,Graph-to-Sequence Learning using Gated Graph Neural Networks,"Daniel Beck, Gholamreza Haffari, Trevor Cohn","Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architectures on graph-to-sequence obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work. Experimental results shows that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation.",,,,ACL
27,2018,"Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context","Urvashi Khandelwal, He He, Peng Qi, Dan Jurafsky","We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.",,,,ACL
28,2018,"Bridging CNNs, RNNs, and Weighted Finite-State Machines","Roy Schwartz, Sam Thomson, Noah A. Smith","Recurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a new model that aims to bridge these two approaches. SoPa combines neural representation learning with weighted finite-state automata (WFSAs) to learn a soft version of traditional surface patterns. We show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly, to a restricted form of WFSA. Empirically, on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.",,,,ACL
29,2018,Zero-shot Learning of Classifiers from Natural Language Quantification,"Shashank Srivastava, Igor Labutov, Tom Mitchell","Humans can efficiently learn new concepts using language. We present a framework through which a set of explanations of a concept can be used to learn a classifier without access to any labeled examples. We use semantic parsing to map explanations to probabilistic assertions grounded in latent class labels and observed attributes of unlabeled data, and leverage the differential semantics of linguistic quantifiers (e.g., ‘usually’ vs ‘always’) to drive model training. Experiments on three domains show that the learned classifiers outperform previous approaches for learning with limited data, and are comparable with fully supervised classifiers trained from a small number of labeled examples.",,,,ACL
30,2018,Sentence-State LSTM for Text Representation,"Yue Zhang, Qi Liu, Linfeng Song","Bi-directional LSTMs are a powerful tool for text representation. On the other hand, they have been shown to suffer various limitations due to their sequential nature. We investigate an alternative LSTM structure for encoding text, which consists of a parallel state for each word. Recurrent steps are used to perform local and global information exchange between words simultaneously, rather than incremental reading of a sequence of words. Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers.",,,,ACL
31,2018,Universal Language Model Fine-tuning for Text Classification,"Jeremy Howard, Sebastian Ruder","Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",,,,ACL
32,2018,Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement,"Nina Poerner, Hinrich Schütze, Benjamin Roth","The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.",,,,ACL
33,2018,Improving Text-to-SQL Evaluation Methodology,"Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",,,,ACL
34,2018,Semantic Parsing with Syntax- and Table-Aware SQL Generation,"Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji","We present a generative model to map natural language questions into SQL queries. Existing neural network based approaches typically generate a SQL query word-by-word, however, a large portion of the generated results is incorrect or not executable due to the mismatch between question words and table contents. Our approach addresses this problem by considering the structure of table and the syntax of SQL language. The quality of the generated SQL query is significantly improved through (1) learning to replicate content from column names, cells or SQL keywords; and (2) improving the generation of WHERE clause by leveraging the column-cell relation. Experiments are conducted on WikiSQL, a recently released dataset with the largest question- SQL pairs. Our approach significantly improves the state-of-the-art execution accuracy from 69.0% to 74.4%.",,,,ACL
35,2018,Multitask Parsing Across Semantic Representations,"Daniel Hershcovich, Omri Abend, Ari Rappoport","The ability to consolidate information of different types is at the core of intelligence, and has tremendous practical value in allowing learning for one task to benefit from generalizations learned for others. In this paper we tackle the challenging task of improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings.",,,,ACL
36,2018,Character-Level Models versus Morphology in Semantic Role Labeling,"Gözde Gül Şahin, Mark Steedman","Character-level models have become a popular approach specially for their accessibility and ability to handle unseen data. However, little is known on their ability to reveal the underlying morphological structure of a word, which is a crucial skill for high-level semantic analysis tasks, such as semantic role labeling (SRL). In this work, we train various types of SRL models that use word, character and morphology level information and analyze how performance of characters compare to words and morphology for several languages. We conduct an in-depth error analysis for each morphological typology and analyze the strengths and limitations of character-level models that relate to out-of-domain data, training data size, long range dependencies and model complexity. Our exhaustive analyses shed light on important characteristics of character-level models and their semantic capability.",,,,ACL
37,2018,AMR Parsing as Graph Prediction with Latent Alignment,"Chunchuan Lyu, Ivan Titov","Abstract meaning representations (AMRs) are broad-coverage sentence-level semantic representations. AMRs represent sentences as rooted labeled directed acyclic graphs. AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the graphs and words in the corresponding sentences. We introduce a neural parser which treats alignments as latent variables within a joint probabilistic model of concepts, relations and alignments. As exact inference requires marginalizing over alignments and is infeasible, we use the variational autoencoding framework and a continuous relaxation of the discrete alignments. We show that joint modeling is preferable to using a pipeline of align and parse. The parser achieves the best reported results on the standard benchmark (74.4% on LDC2016E25).",,,,ACL
38,2018,Accurate SHRG-Based Semantic Parsing,"Yufei Chen, Weiwei Sun, Xiaojun Wan","We demonstrate that an SHRG-based parser can produce semantic graphs much more accurately than previously shown, by relating synchronous production rules to the syntacto-semantic composition process. Our parser achieves an accuracy of 90.35 for EDS (89.51 for DMRS) in terms of elementary dependency match, which is a 4.87 (5.45) point improvement over the best existing data-driven model, indicating, in our view, the importance of linguistically-informed derivation for data-driven semantic parsing. This accuracy is equivalent to that of English Resource Grammar guided models, suggesting that (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations.",,,,ACL
39,2018,Using Intermediate Representations to Solve Math Word Problems,"Danqing Huang, Jin-Ge Yao, Chin-Yew Lin, Qingyu Zhou","To solve math word problems, previous statistical approaches attempt at learning a direct mapping from a problem description to its corresponding equation system. However, such mappings do not include the information of a few higher-order operations that cannot be explicitly represented in equations but are required to solve the problem. The gap between natural language and equations makes it difficult for a learned model to generalize from limited data. In this work we present an intermediate meaning representation scheme that tries to reduce this gap. We use a sequence-to-sequence model with a novel attention regularization term to generate the intermediate forms, then execute them to obtain the final answers. Since the intermediate forms are latent, we propose an iterative labeling framework for learning by leveraging supervision signals from both equations and answers. Our experiments show using intermediate forms outperforms directly predicting equations.",,,,ACL
40,2018,Discourse Representation Structure Parsing,"Jiangming Liu, Shay B. Cohen, Mirella Lapata","We introduce an open-domain neural semantic parser which generates formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). We propose a method which transforms Discourse Representation Structures (DRSs) to trees and develop a structure-aware model which decomposes the decoding process into three stages: basic DRS structure prediction, condition prediction (i.e., predicates and relations), and referent prediction (i.e., variables). Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin.",,,,ACL
41,2018,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,"Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min","Many deep learning architectures have been proposed to model the compositionality in text sequences, requiring substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (i) a max-pooling operation for improved interpretability; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (i) (long) document classification; (ii) text sequence matching; and (iii) short text tasks, including classification and tagging.",,,,ACL
42,2018,ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations,"John Wieting, Kevin Gimpel","We describe ParaNMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following Wieting et al. (2017). Our hope is that ParaNMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use ParaNMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.",,,,ACL
43,2018,"Event2Mind: Commonsense Inference on Events, Intents, and Reactions","Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A. Smith","We investigate a new commonsense inference task: given an event described in a short free-form text (“X drinks coffee in the morning”), a system reasons about the likely intents (“X wants to stay awake”) and reactions (“X feels alert”) of the event’s participants. To support this study, we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. We report baseline performance on this task, demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants. In addition, we demonstrate how commonsense inference on people’s intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts.",,,,ACL
44,2018,Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis,"Shuhei Kurita, Daisuke Kawahara, Sadao Kurohashi","Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult. To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with PAS. However, since it is prohibitively expensive, it is promising to take advantage of a large amount of raw corpora. In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a raw corpus. In our experiments, our model outperforms existing state-of-the-art models for Japanese PAS analysis.",,,,ACL
45,2018,Improving Event Coreference Resolution by Modeling Correlations between Event Coreference Chains and Document Topic Structures,"Prafulla Kumar Choubey, Ruihong Huang","This paper proposes a novel approach for event coreference resolution that models correlations between event coreference chains and document topical structures through an Integer Linear Programming formulation. We explicitly model correlations between the main event chains of a document with topic transition sentences, inter-coreference chain correlations, event mention distributional characteristics and sub-event structure, and use them with scores obtained from a local coreference relation classifier for jointly resolving multiple event chains in a document. Our experiments across KBP 2016 and 2017 datasets suggest that each of the structures contribute to improving event coreference resolution performance.",,,,ACL
46,2018,DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction,"Pengda Qin, Weiran Xu, William Yang Wang","Distant supervision can effectively label data for relation extraction, but suffers from the noise labeling problem. Recent works mainly perform soft bag-level noise reduction strategies to find the relatively better samples in a sentence bag, which is suboptimal compared with making a hard decision of false positive samples in sentence level. In this paper, we introduce an adversarial learning framework, which we named DSGAN, to learn a sentence-level true-positive generator. Inspired by Generative Adversarial Networks, we regard the positive samples generated by the generator as the negative samples to train the discriminator. The optimal generator is obtained until the discrimination ability of the discriminator has the greatest decline. We adopt the generator to filter distant supervision training dataset and redistribute the false positive instances into the negative set, in which way to provide a cleaned dataset for relation classification. The experimental results show that the proposed strategy significantly improves the performance of distant supervision relation extraction comparing to state-of-the-art systems.",,,,ACL
47,2018,Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism,"Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu","The relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. We divided the sentences into three types according to triplet overlap degree, including Normal, EntityPairOverlap and SingleEntiyOverlap. Existing methods mainly focus on Normal class and fail to extract relational triplets precisely. In this paper, we propose an end-to-end model based on sequence-to-sequence learning with copy mechanism, which can jointly extract relational facts from sentences of any of these classes. We adopt two different strategies in decoding process: employing only one united decoder or applying multiple separated decoders. We test our models in two public datasets and our model outperform the baseline method significantly.",,,,ACL
48,2018,Self-regulation: Employing a Generative Adversarial Network to Improve Event Detection,"Yu Hong, Wenxuan Zhou, Jingli Zhang, Guodong Zhou","Due to the ability of encoding and mapping semantic information into a high-dimensional latent feature space, neural networks have been successfully used for detecting events to a certain extent. However, such a feature space can be easily contaminated by spurious features inherent in event detection. In this paper, we propose a self-regulated learning approach by utilizing a generative adversarial network to generate spurious features. On the basis, we employ a recurrent network to eliminate the fakes. Detailed experiments on the ACE 2005 and TAC-KBP 2015 corpora show that our proposed method is highly effective and adaptable.",,,,ACL
49,2018,Context-Aware Neural Model for Temporal Information Extraction,"Yuanliang Meng, Anna Rumshisky","We propose a context-aware neural network model for temporal information extraction. This model has a uniform architecture for event-event, event-timex and timex-timex pairs. A Global Context Layer (GCL), inspired by Neural Turing Machine (NTM), stores processed temporal relations in narrative order, and retrieves them for use when relevant entities come in. Relations are then classified in context. The GCL model has long-term memory and attention mechanisms to resolve irregular long-distance dependencies that regular RNNs such as LSTM cannot recognize. It does not require any new input features, while outperforming the existing models in literature. To our knowledge it is also the first model to use NTM-like architecture to process the information from global context in discourse-scale natural text processing. We are going to release the source code in the future.",,,,ACL
50,2018,Temporal Event Knowledge Acquisition via Identifying Narratives,"Wenlin Yao, Ruihong Huang","Inspired by the double temporality characteristic of narrative texts, we propose a novel approach for acquiring rich temporal “before/after” event knowledge across sentences in narrative stories. The double temporality states that a narrative story often describes a sequence of events following the chronological order and therefore, the temporal order of events matches with their textual order. We explored narratology principles and built a weakly supervised approach that identifies 287k narrative paragraphs from three large corpora. We then extracted rich temporal event knowledge from these narrative paragraphs. Such event knowledge is shown useful to improve temporal relation classification and outperforms several recent neural network models on the narrative cloze task.",,,,ACL
51,2018,Textual Deconvolution Saliency (TDS) : a deep tool box for linguistic analysis,"Laurent Vanni, Melanie Ducoffe, Carlos Aguilar, Frederic Precioso","In this paper, we propose a new strategy, called Text Deconvolution Saliency (TDS), to visualize linguistic information detected by a CNN for text classification. We extend Deconvolution Networks to text in order to present a new perspective on text analysis to the linguistic community. We empirically demonstrated the efficiency of our Text Deconvolution Saliency on corpora from three different languages: English, French, and Latin. For every tested dataset, our Text Deconvolution Saliency automatically encodes complex linguistic patterns based on co-occurrences and possibly on grammatical and syntax analysis.",,,,ACL
52,2018,Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach,"Shafiq Joty, Muhammad Tasnim Mohiuddin, Dat Tien Nguyen","We propose a novel coherence model for written asynchronous conversations (e.g., forums, emails), and show its applications in coherence assessment and thread reconstruction tasks. We conduct our research in two steps. First, we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions. Then, we extend the model to asynchronous conversations by incorporating the underlying conversational structure in the entity grid representation and feature computation. Our model achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing models. We also demonstrate its effectiveness in reconstructing thread structures.",,,,ACL
53,2018,Deep Reinforcement Learning for Chinese Zero Pronoun Resolution,"Qingyu Yin, Yu Zhang, Wei-Nan Zhang, Ting Liu","Recent neural network models for Chinese zero pronoun resolution gain great performance by capturing semantic information for zero pronouns and candidate antecedents, but tend to be short-sighted, operating solely by making local decisions. They typically predict coreference links between the zero pronoun and one single candidate antecedent at a time while ignoring their influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is crucial for classifying later zero pronoun-candidate antecedent pairs, a need which leads traditional models of zero pronoun resolution to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to deal with the task. With the help of the reinforcement learning agent, our system learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 show that our approach substantially outperforms the state-of-the-art methods under three experimental settings.",,,,ACL
54,2018,Entity-Centric Joint Modeling of Japanese Coreference Resolution and Predicate Argument Structure Analysis,"Tomohide Shibata, Sadao Kurohashi","Predicate argument structure analysis is a task of identifying structured events. To improve this field, we need to identify a salient entity, which cannot be identified without performing coreference resolution and predicate argument structure analysis simultaneously. This paper presents an entity-centric joint model for Japanese coreference resolution and predicate argument structure analysis. Each entity is assigned an embedding, and when the result of both analyses refers to an entity, the entity embedding is updated. The analyses take the entity embedding into consideration to access the global information of entities. Our experimental results demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically, which is a notoriously difficult task in predicate argument structure analysis.",,,,ACL
55,2018,"Constraining MGbank: Agreement, L-Selection and Supertagging in Minimalist Grammars",John Torr,"This paper reports on two strategies that have been implemented for improving the efficiency and precision of wide-coverage Minimalist Grammar (MG) parsing. The first extends the formalism presented in Torr and Stabler (2016) with a mechanism for enforcing fine-grained selectional restrictions and agreements. The second is a method for factoring computationally costly null heads out from bottom-up MG parsing; this has the additional benefit of rendering the formalism fully compatible for the first time with highly efficient Markovian supertaggers. These techniques aided in the task of generating MGbank, the first wide-coverage corpus of Minimalist Grammar derivation trees.",,,,ACL
56,2018,Not that much power: Linguistic alignment is influenced more by low-level linguistic features rather than social power,"Yang Xu, Jeremy Cole, David Reitter","Linguistic alignment between dialogue partners has been claimed to be affected by their relative social power. A common finding has been that interlocutors of higher power tend to receive more alignment than those of lower power. However, these studies overlook some low-level linguistic features that can also affect alignment, which casts doubts on these findings. This work characterizes the effect of power on alignment with logistic regression models in two datasets, finding that the effect vanishes or is reversed after controlling for low-level features such as utterance length. Thus, linguistic alignment is explained better by low-level features than by social power. We argue that a wider range of factors, especially cognitive factors, need to be taken into account for future studies on observational data when social factors of language use are in question.",,,,ACL
57,2018,"TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation","Alexander Fabbri, Irene Li, Prawat Trairatvorakul, Yijiao He","The field of Natural Language Processing (NLP) is growing rapidly, with new research published daily along with an abundance of tutorials, codebases and other online resources. In order to learn this dynamic field or stay up-to-date on the latest research, students as well as educators and researchers must constantly sift through multiple sources to find valuable, relevant information. To address this situation, we introduce TutorialBank, a new, publicly available dataset which aims to facilitate NLP education and research. We have manually collected and categorized over 5,600 resources on NLP as well as the related fields of Artificial Intelligence (AI), Machine Learning (ML) and Information Retrieval (IR). Our dataset is notably the largest manually-picked corpus of resources intended for NLP education which does not include only academic papers. Additionally, we have created both a search engine and a command-line tool for the resources and have annotated the corpus to include lists of research topics, relevant resources for each topic, prerequisite relations among topics, relevant sub-parts of individual resources, among other annotations. We are releasing the dataset and present several avenues for further research.",,,,ACL
58,2018,Give Me More Feedback: Annotating Argument Persuasiveness and Related Attributes in Student Essays,"Winston Carlile, Nishant Gurrapadi, Zixuan Ke, Vincent Ng","While argument persuasiveness is one of the most important dimensions of argumentative essay quality, it is relatively little studied in automated essay scoring research. Progress on scoring argument persuasiveness is hindered in part by the scarcity of annotated corpora. We present the first corpus of essays that are simultaneously annotated with argument components, argument persuasiveness scores, and attributes of argument components that impact an argument’s persuasiveness. This corpus could trigger the development of novel computational models concerning argument persuasiveness that provide useful feedback to students on why their arguments are (un)persuasive in addition to how persuasive they are.",,,,ACL
59,2018,Inherent Biases in Reference-based Evaluation for Grammatical Error Correction,"Leshem Choshen, Omri Abend","The prevalent use of too few references for evaluating text-to-text generation is known to bias estimates of their quality (henceforth, low coverage bias or LCB). This paper shows that overcoming LCB in Grammatical Error Correction (GEC) evaluation cannot be attained by re-scaling or by increasing the number of references in any feasible range, contrary to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims.",,,,ACL
60,2018,The price of debiasing automatic metrics in natural language evalaution,"Arun Chaganty, Stephen Mussmann, Percy Liang","For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements. On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive. In this paper, we use control variates to combine automatic metrics with human evaluation to obtain an unbiased estimator with lower cost than human evaluation alone. In practice, however, we obtain only a 7-13% cost reduction on evaluating summarization and open-response question answering systems. We then prove that our estimator is optimal: there is no unbiased estimator with lower cost. Our theory further highlights the two fundamental bottlenecks—the automatic metric and the prompt shown to human evaluators—both of which need to be improved to obtain greater cost savings.",,,,ACL
61,2018,Neural Document Summarization by Jointly Learning to Score and Select Sentences,"Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang","Sentence scoring and sentence selection are two main steps in extractive document summarization systems. However, previous works treat them as two separated subtasks. In this paper, we present a novel end-to-end neural network framework for extractive document summarization by jointly learning to score and select sentences. It first reads the document sentences with a hierarchical encoder to obtain the representation of sentences. Then it builds the output summary by extracting sentences one by one. Different from previous methods, our approach integrates the selection strategy into the scoring model, which directly predicts the relative importance given previously selected sentences. Experiments on the CNN/Daily Mail dataset show that the proposed framework significantly outperforms the state-of-the-art extractive summarization models.",,,,ACL
62,2018,Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization,"Guokan Shang, Wensi Ding, Zekun Zhang, Antoine Tixier","We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our work combines the strengths of multiple recent approaches while addressing their weaknesses. Moreover, we leverage recent advances in word embeddings and graph degeneracy applied to NLP to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the AMI and ICSI corpus show that our system improves on the state-of-the-art. Code and data are publicly available, and our system can be interactively tested.",,,,ACL
63,2018,Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting,"Yen-Chun Chen, Mohit Bansal","Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (i.e., compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) on the CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores. Moreover, by first operating at the sentence-level and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC-2002 dataset, where we achieve higher scores than a state-of-the-art model.",,,,ACL
64,2018,Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation,"Han Guo, Ramakanth Pasunuru, Mohit Bansal","An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also propose novel multi-task architectures with high-level (semantic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution). Overall, we achieve statistically significant improvements over the state-of-the-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also present several quantitative and qualitative analysis studies of our model’s learned saliency and entailment skills.",,,,ACL
65,2018,Modeling and Prediction of Online Product Review Helpfulness: A Survey,"Gerardo Ocampo Diaz, Vincent Ng","As the amount of free-form user-generated reviews in e-commerce websites continues to increase, there is an increasing need for automatic mechanisms that sift through the vast amounts of user reviews and identify quality content. Review helpfulness modeling is a task which studies the mechanisms that affect review helpfulness and attempts to accurately predict it. This paper provides an overview of the most relevant work in helpfulness prediction and understanding in the past decade, discusses the insights gained from said work, and provides guidelines for future research.",,,,ACL
66,2018,Mining Cross-Cultural Differences and Similarities in Social Media,"Bill Yuchen Lin, Frank F. Xu, Kenny Zhu, Seung-won Hwang","Cross-cultural differences and similarities are common in cross-lingual natural language understanding, especially for research in social media. For instance, people of distinct cultures often hold different opinions on a single named entity. Also, understanding slang terms across languages requires knowledge of cross-cultural similarities. In this paper, we study the problem of computing such cross-cultural differences and similarities. We present a lightweight yet effective approach, and evaluate it on two novel tasks: 1) mining cross-cultural differences of named entities and 2) finding similar terms for slang across languages. Experimental results show that our framework substantially outperforms a number of baseline methods on both tasks. The framework could be useful for machine translation applications and research in computational social science.",,,,ACL
67,2018,Classification of Moral Foundations in Microblog Political Discourse,"Kristen Johnson, Dan Goldwasser","Previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. Additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. Based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on Twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. The contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.",,,,ACL
68,2018,Coarse-to-Fine Decoding for Neural Semantic Parsing,"Li Dong, Mirella Lapata","Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.",,,,ACL
69,2018,Confidence Modeling for Neural Semantic Parsing,"Li Dong, Chris Quirk, Mirella Lapata","In this work we focus on confidence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various metrics to quantify these factors. These metrics are then used to estimate confidence scores that indicate whether model predictions are likely to be correct. Beyond confidence estimation, we identify which parts of the input contribute to uncertain predictions allowing users to interpret their model, and verify or refine its input. Experimental results show that our confidence model significantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores.",,,,ACL
70,2018,StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing,"Pengcheng Yin, Chunting Zhou, Junxian He, Graham Neubig","Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures. Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. We introduce StructVAE, a variational auto-encoding model for semi-supervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances. StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models.",,,,ACL
71,2018,Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing,"Bo Chen, Le Sun, Xianpei Han","This paper proposes a neural semantic parsing approach – Sequence-to-Action, which models semantic parsing as an end-to-end semantic graph generation process. Our method simultaneously leverages the advantages from two recent promising directions of semantic parsing. Firstly, our model uses a semantic graph to represent the meaning of a sentence, which has a tight-coupling with knowledge bases. Secondly, by leveraging the powerful representation learning and prediction ability of neural network models, we propose a RNN model which can effectively map sentences to action sequences for semantic graph generation. Experiments show that our method achieves state-of-the-art performance on Overnight dataset and gets competitive performance on Geo and Atis datasets.",,,,ACL
72,2018,On the Limitations of Unsupervised Bilingual Dictionary Induction,"Anders Søgaard, Sebastian Ruder, Ivan Vulić","Unsupervised machine translation - i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora - seems impossible, but nevertheless, Lample et al. (2017) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised cross-lingual word embedding technique for bilingual dictionary induction (Conneau et al., 2017), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.",,,,ACL
73,2018,A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings,"Mikel Artetxe, Gorka Labaka, Eneko Agirre","Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap.",,,,ACL
74,2018,A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling,"Ying Lin, Shengqi Yang, Veselin Stoyanov, Heng Ji","We propose a multi-lingual multi-task architecture to develop supervised models with a minimal amount of labeled data for sequence labeling. In this new architecture, we combine various transfer models using two layers of parameter sharing. On the first layer, we construct the basis of the architecture to provide universal word representation and feature extraction capability for all models. On the second level, we adopt different parameter sharing strategies for different transfer schemes. This architecture proves to be particularly effective for low-resource settings, when there are less than 200 training sentences for the target task. Using Name Tagging as a target task, our approach achieved 4.3%-50.5% absolute F-score gains compared to the mono-lingual single-task baseline model.",,,,ACL
75,2018,Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable,"Viktor Hangya, Fabienne Braune, Alexander Fraser, Hinrich Schütze","Bilingual tasks, such as bilingual lexicon induction and cross-lingual classification, are crucial for overcoming data sparsity in the target language. Resources required for such tasks are often out-of-domain, thus domain adaptation is an important problem here. We make two contributions. First, we test a delightfully simple method for domain adaptation of bilingual word embeddings. We evaluate these embeddings on two bilingual tasks involving different domains: cross-lingual twitter sentiment classification and medical bilingual lexicon induction. Second, we tailor a broadly applicable semi-supervised classification method from computer vision to these tasks. We show that this method also helps in low-resource setups. Using both methods together we achieve large improvements over our baselines, by using only additional unlabeled data.",,,,ACL
76,2018,Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge,"Todor Mihaylov, Anette Frank","We introduce a neural reading comprehension model that integrates external commonsense knowledge, encoded as a key-value memory, in a cloze-style setting. Instead of relying only on document-to-question interaction or discrete features as in prior work, our model attends to relevant external knowledge and combines this knowledge with the context representation before inferring the answer. This allows the model to attract and imply knowledge from an external knowledge source that is not explicitly stated in the text, but that is relevant for inferring the answer. Our model improves results over a very strong baseline on a hard Common Nouns dataset, making it a strong competitor of much more complex models. By including knowledge explicitly, our model can also provide evidence about the background knowledge used in the RC process.",,,,ACL
77,2018,Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds,"Igor Labutov, Bishan Yang, Anusha Prakash, Amos Azaria","Question Answering (QA), as a research field, has primarily focused on either knowledge bases (KBs) or free text as a source of knowledge. These two sources have historically shaped the kinds of questions that are asked over these sources, and the methods developed to answer them. In this work, we look towards a practical use-case of QA over user-instructed knowledge that uniquely combines elements of both structured QA over knowledge bases, and unstructured QA over narrative, introducing the task of multi-relational QA over personal narrative. As a first step towards this goal, we make three key contributions: (i) we generate and release TextWorldsQA, a set of five diverse datasets, where each dataset contains dynamic narrative that describes entities and relations in a simulated world, paired with variably compositional questions over that knowledge, (ii) we perform a thorough evaluation and analysis of several state-of-the-art QA models and their variants at this task, and (iii) we release a lightweight Python-based framework we call TextWorlds for easily generating arbitrary additional worlds and narrative, with the goal of allowing the community to create and share a growing collection of diverse worlds as a test-bed for this task.",,,,ACL
78,2018,Simple and Effective Multi-Paragraph Reading Comprehension,"Christopher Clark, Matt Gardner","We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.",,,,ACL
79,2018,Semantically Equivalent Adversarial Rules for Debugging NLP models,"Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin","Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) – semantic-preserving perturbations that induce changes in the model’s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",,,,ACL
80,2018,Style Transfer Through Back-Translation,"Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, Alan W Black","Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new method for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations: sentiment, gender and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency.",,,,ACL
81,2018,Generating Fine-Grained Open Vocabulary Entity Type Descriptions,"Rajarshi Bhowmik, Gerard de Melo","While large-scale knowledge graphs provide vast amounts of structured facts about entities, a short textual description can often be useful to succinctly characterize an entity and its type. Unfortunately, many knowledge graphs entities lack such textual descriptions. In this paper, we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words. We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines.",,,,ACL
82,2018,Hierarchical Neural Story Generation,"Angela Fan, Mike Lewis, Yann Dauphin","We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",,,,ACL
83,2018,No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling,"Xin Wang, Wenhu Chen, Yuan-Fang Wang, William Yang Wang","Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.",,,,ACL
84,2018,Bridging Languages through Images with Deep Partial Canonical Correlation Analysis,"Guy Rotman, Ivan Vulić, Roi Reichart","We present a deep neural network that leverages images to improve bilingual text embeddings. Relying on bilingual image tags and descriptions, our approach conditions text embedding induction on the shared visual information for both languages, producing highly correlated bilingual embeddings. In particular, we propose a novel model based on Partial Canonical Correlation Analysis (PCCA). While the original PCCA finds linear projections of two views in order to maximize their canonical correlation conditioned on a shared third variable, we introduce a non-linear Deep PCCA (DPCCA) model, and develop a new stochastic iterative algorithm for its optimization. We evaluate PCCA and DPCCA on multilingual word similarity and cross-lingual image description retrieval. Our models outperform a large variety of previous methods, despite not having access to any visual signal during test time inference.",,,,ACL
85,2018,Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search,"Jamie Kiros, William Chan, Geoffrey Hinton","We introduce Picturebook, a large-scale lookup operation to ground language via ‘snapshots’ of our physical world accessed through image search. For each word in a vocabulary, we extract the top-k images from Google image search and feed the images through a convolutional network to extract a word embedding. We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations. We also introduce Inverse Picturebook, a mechanism to map a Picturebook embedding back into words. We experiment and report results across a wide range of tasks: word similarity, natural language inference, semantic relatedness, sentiment/topic classification, image-sentence ranking and machine translation. We also show that gate activations corresponding to Picturebook embeddings are highly correlated to human judgments of concreteness ratings.",,,,ACL
86,2018,What Action Causes This? Towards Naive Physical Action-Effect Prediction,"Qiaozi Gao, Shaohua Yang, Joyce Chai, Lucy Vanderwende","Despite recent advances in knowledge representation, automated reasoning, and machine learning, artificial agents still lack the ability to understand basic action-effect relations regarding the physical world, for example, the action of cutting a cucumber most likely leads to the state where the cucumber is broken apart into smaller pieces. If artificial agents (e.g., robots) ever become our partners in joint tasks, it is critical to empower them with such action-effect understanding so that they can reason about the state of the world and plan for actions. Towards this goal, this paper introduces a new task on naive physical action-effect prediction, which addresses the relations between concrete actions (expressed in the form of verb-noun pairs) and their effects on the state of the physical world as depicted by images. We collected a dataset for this task and developed an approach that harnesses web image data through distant supervision to facilitate learning for action-effect prediction. Our empirical results have shown that web data can be used to complement a small number of seed examples (e.g., three examples for each action) for model learning. This opens up possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans with a few examples.",,,,ACL
87,2018,Transformation Networks for Target-Oriented Sentiment Classification,"Xin Li, Lidong Bing, Wai Lam, Bei Shi","Target-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence. RNN with attention seems a good fit for the characteristics of this task, and indeed it achieves the state-of-the-art performance. After re-examining the drawbacks of attention mechanism and the obstacles that block CNN to perform well in this classification task, we propose a new model that achieves new state-of-the-art results on a few benchmarks. Instead of attention, our model employs a CNN layer to extract salient features from the transformed word representations originated from a bi-directional RNN layer. Between the two layers, we propose a component which first generates target-specific representations of words in the sentence, and then incorporates a mechanism for preserving the original contextual information from the RNN layer.",,,,ACL
88,2018,Target-Sensitive Memory Networks for Aspect Sentiment Classification,"Shuai Wang, Sahisnu Mazumder, Bing Liu, Mianwei Zhou","Aspect sentiment classification (ASC) is a fundamental task in sentiment analysis. Given an aspect/target and a sentence, the task classifies the sentiment polarity expressed on the target in the sentence. Memory networks (MNs) have been used for this task recently and have achieved state-of-the-art results. In MNs, attention mechanism plays a crucial role in detecting the sentiment context for the given target. However, we found an important problem with the current MNs in performing the ASC task. Simply improving the attention mechanism will not solve it. The problem is referred to as target-sensitive sentiment, which means that the sentiment polarity of the (detected) context is dependent on the given target and it cannot be inferred from the context alone. To tackle this problem, we propose the target-sensitive memory networks (TMNs). Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated.",,,,ACL
89,2018,Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification,"Raksha Sharma, Pushpak Bhattacharyya, Sandipan Dandapat, Himanshu Sharad Bhatt","Getting manually labeled data in each domain is always an expensive and a time consuming task. Cross-domain sentiment analysis has emerged as a demanding concept where a labeled source domain facilitates a sentiment classifier for an unlabeled target domain. However, polarity orientation (positive or negative) and the significance of a word to express an opinion often differ from one domain to another domain. Owing to these differences, cross-domain sentiment classification is still a challenging task. In this paper, we propose that words that do not change their polarity and significance represent the transferable (usable) information across domains for cross-domain sentiment classification. We present a novel approach based on χ2 test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains. Furthermore, we show that a weighted ensemble of the classifiers enhances the cross-domain classification performance.",,,,ACL
90,2018,Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach,"Jingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang","The goal of sentiment-to-sentiment “translation” is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of parallel data. To solve this problem, we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutralization module and an emotionalization module. We evaluate our approach on two review datasets, Yelp and Amazon. Experimental results show that our approach significantly outperforms the state-of-the-art systems. Especially, the proposed method substantially improves the content preservation performance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets, respectively.",,,,ACL
91,2018,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,"Boyuan Pan, Yazheng Yang, Zhou Zhao, Yueting Zhuang","Natural Language Inference (NLI), also known as Recognizing Textual Entailment (RTE), is one of the most important problems in natural language processing. It requires to infer the logical relationship between two given sentences. While current approaches mostly focus on the interaction architectures of the sentences, in this paper, we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model. We observe that people usually use some discourse markers such as “so” or “but” to represent the logical relationship between two sentences. These words potentially have deep connections with the meanings of the sentences, thus can be utilized to help improve the representations of them. Moreover, we use reinforcement learning to optimize a new objective function with a reward defined by the property of the NLI datasets to make full use of the labels information. Experiments show that our method achieves the state-of-the-art performance on several large-scale datasets.",,,,ACL
92,2018,Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module,"Juan Pavez, Héctor Allende, Héctor Allende-Cid","During the last years, there has been a lot of interest in achieving some kind of complex reasoning using deep neural networks. To do that, models like Memory Networks (MemNNs) have combined external memory storages and attention mechanisms. These architectures, however, lack of more complex reasoning mechanisms that could allow, for instance, relational reasoning. Relation Networks (RNs), on the other hand, have shown outstanding results in relational reasoning tasks. Unfortunately, their computational cost grows quadratically with the number of memories, something prohibitive for larger problems. To solve these issues, we introduce the Working Memory Network, a MemNN architecture with a novel working memory storage and reasoning module. Our model retains the relational reasoning abilities of the RN while reducing its computational complexity from quadratic to linear. We tested our model on the text QA dataset bAbI and the visual QA dataset NLVR. In the jointly trained bAbI-10k, we set a new state-of-the-art, achieving a mean error of less than 0.5%. Moreover, a simple ensemble of two of our models solves all 20 tasks in the joint version of the benchmark.",,,,ACL
93,2018,Reasoning with Sarcasm by Reading In-Between,"Yi Tay, Anh Tuan Luu, Siu Cheung Hui, Jian Su","Sarcasm is a sophisticated speech act which commonly manifests on social communities such as Twitter and Reddit. The prevalence of sarcasm on the social web is highly disruptive to opinion mining systems due to not only its tendency of polarity flipping but also usage of figurative language. Sarcasm commonly manifests with a contrastive theme either between positive-negative sentiments or between literal-figurative scenarios. In this paper, we revisit the notion of modeling contrast in order to reason with sarcasm. More specifically, we propose an attention-based neural model that looks in-between instead of across, enabling it to explicitly model contrast and incongruity. We conduct extensive experiments on six benchmark datasets from Twitter, Reddit and the Internet Argument Corpus. Our proposed model not only achieves state-of-the-art performance on all datasets but also enjoys improved interpretability.",,,,ACL
94,2018,Adversarial Contrastive Estimation,"Avishek Joey Bose, Huan Ling, Yanshuai Cao","Learning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sampler. The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data. We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics.",,,,ACL
95,2018,Adaptive Scaling for Sparse Detection in Information Extraction,"Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun","This paper focuses on detection tasks in information extraction, where positive instances are sparsely distributed and models are usually evaluated using F-measure on positive classes. These characteristics often result in deficient performance of neural network based detection models. In this paper, we propose adaptive scaling, an algorithm which can handle the positive sparsity problem and directly optimize over F-measure via dynamic cost-sensitive learning. To this end, we borrow the idea of marginal utility from economics and propose a theoretical framework for instance importance measuring without introducing any additional hyper-parameters. Experiments show that our algorithm leads to a more effective and stable training of neural network based detection models.",,,,ACL
96,2018,Strong Baselines for Neural Semi-Supervised Learning under Domain Shift,"Sebastian Ruder, Barbara Plank","Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks for part-of-speech tagging and sentiment analysis are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state-of-the-art for NLP. Hence classic approaches constitute an important and strong baseline.",,,,ACL
97,2018,Fluency Boost Learning and Inference for Neural Grammatical Error Correction,"Tao Ge, Furu Wei, Ming Zhou","Most of the neural sequence-to-sequence (seq2seq) models for grammatical error correction (GEC) have two limitations: (1) a seq2seq model may not be well generalized with only limited error-corrected data; (2) a seq2seq model may fail to completely correct a sentence with multiple errors through normal seq2seq inference. We attempt to address these limitations by proposing a fluency boost learning and inference mechanism. Fluency boosting learning generates fluency-boost sentence pairs during training, enabling the error correction model to learn how to improve a sentence’s fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally with multiple inference steps until the sentence’s fluency stops increasing. Experiments show our approaches improve the performance of seq2seq models for GEC, achieving state-of-the-art results on both CoNLL-2014 and JFLEG benchmark datasets.",,,,ACL
98,2018,A Neural Architecture for Automated ICD Coding,"Pengtao Xie, Eric Xing","The International Classification of Diseases (ICD) provides a hierarchy of diagnostic codes for classifying diseases. Medical coding – which assigns a subset of ICD codes to a patient visit – is a mandatory process that is crucial for patient care and billing. Manual coding is time-consuming, expensive, and error prone. In this paper, we build a neural architecture for automated coding. It takes the diagnosis descriptions (DDs) of a patient as inputs and selects the most relevant ICD codes. This architecture contains four major ingredients: (1) tree-of-sequences LSTM encoding of code descriptions (CDs), (2) adversarial learning for reconciling the different writing styles of DDs and CDs, (3) isotonic constraints for incorporating the importance order among the assigned codes, and (4) attentional matching for performing many-to-one and one-to-many mappings from DDs to CDs. We demonstrate the effectiveness of the proposed methods on a clinical datasets with 59K patient visits.",,,,ACL
99,2018,Domain Adaptation with Adversarial Training and Graph Embeddings,"Firoj Alam, Shafiq Joty, Muhammad Imran","The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.",,,,ACL
100,2018,TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring,"Cancan Jin, Ben He, Kai Hui, Le Sun","Existing automated essay scoring (AES) models rely on rated essays for the target prompt as training data. Despite their successes in prompt-dependent AES, how to effectively predict essay ratings under a prompt-independent setting remains a challenge, where the rated essays for the target prompt are not available. To close this gap, a two-stage deep neural network (TDNN) is proposed. In particular, in the first stage, using the rated essays for non-target prompts as the training data, a shallow model is learned to select essays with an extreme quality for the target prompt, serving as pseudo training data; in the second stage, an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the first step. Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task.",,,,ACL
101,2018,Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation,"Tiancheng Zhao, Kyusong Lee, Maxine Eskenazi","The encoder-decoder dialog model is one of the most prominent methods used to build dialog systems in complex domains. Yet it is limited because it cannot output interpretable actions as in traditional systems, which hinders humans from understanding its generation process. We present an unsupervised discrete sentence representation learning method that can integrate with any existing encoder-decoder dialog models for interpretable response generation. Building upon variational autoencoders (VAEs), we present two novel models, DI-VAE and DI-VST that improve VAEs and can discover interpretable semantics via either auto encoding or context predicting. Our methods have been validated on real-world dialog datasets to discover semantic representations and enhance encoder-decoder models with interpretable generation.",,,,ACL
102,2018,Learning to Control the Specificity in Neural Response Generation,"Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan","In conversation, a general response (e.g., “I don’t know”) could correspond to a large variety of input utterances. Previous generative conversational models usually employ a single model to learn the relationship between different utterance-response pairs, thus tend to favor general and trivial responses which appear frequently. To address this problem, we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity. Specifically, we introduce an explicit specificity control variable into a sequence-to-sequence model, which interacts with the usage representation of words through a Gaussian Kernel layer, to guide the model to generate responses at different specificity levels. We describe two ways to acquire distant labels for the specificity control variable in learning. Empirical studies show that our model can significantly outperform the state-of-the-art response generation models under both automatic and human evaluations.",,,,ACL
103,2018,Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network,"Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu","Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context. In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention. Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017) and we extend the attention mechanism in two ways. First, we construct representations of text segments at different granularities solely with stacked self-attention. Second, we try to extract the truly matched segment pairs with attention across the context and response. We jointly introduce those two kinds of attention in one uniform neural network. Experiments on two large-scale multi-turn response selection tasks show that our proposed model significantly outperforms the state-of-the-art models.",,,,ACL
104,2018,MojiTalk: Generating Emotional Responses at Scale,"Xianda Zhou, William Yang Wang","Generating emotional language is a key step towards building empathetic natural language processing agents. However, a major challenge for this line of research is the lack of large-scale labeled training data, and previous studies are limited to only small sets of human annotated sentiment labels. Additionally, explicitly controlling the emotion and sentiment of generated text is also difficult. In this paper, we take a more radical approach: we exploit the idea of leveraging Twitter data that are naturally labeled with emojis. We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence. We investigate several conditional variational autoencoders training on these conversations, which allow us to use emojis to control the emotion of the generated text. Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate high-quality abstractive conversation responses in accordance with designated emotions.",,,,ACL
105,2018,Taylor’s law for Human Linguistic Sequences,"Tatsuru Kobayashi, Kumiko Tanaka-Ishii","Taylor’s law describes the fluctuation characteristics underlying a system in which the variance of an event within a time span grows by a power law with respect to the mean. Although Taylor’s law has been applied in many natural and social systems, its application for language has been scarce. This article describes a new way to quantify Taylor’s law in natural language and conducts Taylor analysis of over 1100 texts across 14 languages. We found that the Taylor exponents of natural language written texts exhibit almost the same value. The exponent was also compared for other language-related data, such as the child-directed speech, music, and programming languages. The results show how the Taylor exponent serves to quantify the fundamental structural complexity underlying linguistic time series. The article also shows the applicability of these findings in evaluating language models.",,,,ACL
106,2018,A Framework for Representing Language Acquisition in a Population Setting,"Jordan Kodner, Christopher Cerezo Falco",Language variation and change are driven both by individuals’ internal cognitive processes and by the social structures through which language propagates. A wide range of computational frameworks have been proposed to connect these drivers. We compare the strengths and weaknesses of existing approaches and propose a new analytic framework which combines previous network models’ ability to capture realistic social structure with practically and more elegant computational properties. The framework privileges the process of language acquisition and embeds learners in a social network but is modular so that population structure can be combined with different acquisition models. We demonstrate two applications for the framework: a test of practical concerns that arise when modeling acquisition in a population setting and an application of the framework to recent work on phonological mergers in progress.,,,,ACL
107,2018,Prefix Lexicalization of Synchronous CFGs using Synchronous TAG,"Logan Born, Anoop Sarkar","We show that an epsilon-free, chain-free synchronous context-free grammar (SCFG) can be converted into a weakly equivalent synchronous tree-adjoining grammar (STAG) which is prefix lexicalized. This transformation at most doubles the grammar’s rank and cubes its size, but we show that in practice the size increase is only quadratic. Our results extend Greibach normal form from CFGs to SCFGs and prove new formal properties about SCFG, a formalism with many applications in natural language processing.",,,,ACL
108,2018,Straight to the Tree: Constituency Parsing with Neural Syntactic Distance,"Yikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessandro Sordoni","In this work, we propose a novel constituency parsing scheme. The model first predicts a real-valued scalar, named syntactic distance, for each split position in the sentence. The topology of grammar tree is then determined by the values of syntactic distances. Compared to traditional shift-reduce parsing schemes, our approach is free from the potentially disastrous compounding error. It is also easier to parallelize and much faster. Our model achieves the state-of-the-art single model F1 score of 92.1 on PTB and 86.4 on CTB dataset, which surpasses the previous single model results by a large margin.",,,,ACL
109,2018,Gaussian Mixture Latent Vector Grammars,"Yanpeng Zhao, Liwen Zhang, Kewei Tu","We introduce Latent Vector Grammars (LVeGs), a new framework that extends latent variable grammars such that each nonterminal symbol is associated with a continuous vector space representing the set of (infinitely many) subtypes of the nonterminal. We show that previous models such as latent variable grammars and compositional vector grammars can be interpreted as special cases of LVeGs. We then present Gaussian Mixture LVeGs (GM-LVeGs), a new special case of LVeGs that uses Gaussian mixtures to formulate the weights of production rules over subtypes of nonterminals. A major advantage of using Gaussian mixtures is that the partition function and the expectations of subtype rules can be computed using an extension of the inside-outside algorithm, which enables efficient inference and learning. We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies.",,,,ACL
110,2018,Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples,"Vidur Joshi, Matthew Peters, Mark Hopkins","We revisit domain adaptation for parsers in the neural era. First we show that recent advances in word representations greatly diminish the need for domain adaptation when the target domain is syntactically similar to the source domain. As evidence, we train a parser on the Wall Street Journal alone that achieves over 90% F1 on the Brown corpus. For more syntactically distant domains, we provide a simple way to adapt a parser using only dozens of partial annotations. For instance, we increase the percentage of error-free geometry-domain parses in a held-out set from 45% to 73% using approximately five dozen training examples. In the process, we demonstrate a new state-of-the-art single model result on the Wall Street Journal test set of 94.3%. This is an absolute increase of 1.7% over the previous state-of-the-art of 92.6%.",,,,ACL
111,2018,Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations,"Vered Shwartz, Ido Dagan","Revealing the implicit semantic relation between the constituents of a noun-compound is important for many NLP applications. It has been addressed in the literature either as a classification task to a set of pre-defined relations or by producing free text paraphrases explicating the relations. Most existing paraphrasing methods lack the ability to generalize, and have a hard time interpreting infrequent or new noun-compounds. We propose a neural model that generalizes better by representing paraphrases in a continuous space, generalizing for both unseen noun-compounds and rare paraphrases. Our model helps improving performance on both the noun-compound paraphrasing and classification tasks.",,,,ACL
112,2018,Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings,"Maksim Tkachenko, Chong Cher Chia, Hady Lauw","We explore the notion of subjectivity, and hypothesize that word embeddings learnt from input corpora of varying levels of subjectivity behave differently on natural language processing tasks such as classifying a sentence by sentiment, subjectivity, or topic. Through systematic comparative analyses, we establish this to be the case indeed. Moreover, based on the discovery of the outsized role that sentiment words play on subjectivity-sensitive tasks such as sentiment classification, we develop a novel word embedding SentiVec which is infused with sentiment information from a lexical resource, and is shown to outperform baselines on such tasks.",,,,ACL
113,2018,Word Embedding and WordNet Based Metaphor Identification and Interpretation,"Rui Mao, Chenghua Lin, Frank Guerin","Metaphoric expressions are widespread in natural language, posing a significant challenge for various natural language processing tasks such as Machine Translation. Current word embedding based metaphor identification models cannot identify the exact metaphorical words within a sentence. In this paper, we propose an unsupervised learning method that identifies and interprets metaphors at word-level without any preprocessing, outperforming strong baselines in the metaphor identification task. Our model extends to interpret the identified metaphors, paraphrasing them into their literal counterparts, so that they can be better translated by machines. We evaluated this with two popular translation systems for English to Chinese, showing that our model improved the systems significantly.",,,,ACL
114,2018,Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings,"Yang Xu, Jiawei Liu, Wei Yang, Liusheng Huang","Traditional word embedding approaches learn semantic information at word level while ignoring the meaningful internal structures of words like morphemes. Furthermore, existing morphology-based models directly incorporate morphemes to train word embeddings, but still neglect the latent meanings of morphemes. In this paper, we explore to employ the latent meanings of morphological compositions of words to train and enhance word embeddings. Based on this purpose, we propose three Latent Meaning Models (LMMs), named LMM-A, LMM-S and LMM-M respectively, which adopt different strategies to incorporate the latent meanings of morphemes during the training process. Experiments on word similarity, syntactic analogy and text classification are conducted to validate the feasibility of our models. The results demonstrate that our models outperform the baselines on five word similarity datasets. On Wordsim-353 and RG-65 datasets, our models nearly achieve 5% and 7% gains over the classic CBOW model, respectively. For the syntactic analogy and text classification tasks, our models also surpass all the baselines including a morphology-based model.",,,,ACL
115,2018,A Stochastic Decoder for Neural Machine Translation,"Philip Schulz, Wilker Aziz, Trevor Cohn","The process of translation is ambiguous, in that there are typically many valid translations for a given sentence. This gives rise to significant variation in parallel corpora, however, most current models of machine translation do not account for this variation, instead treating the problem as a deterministic process. To this end, we present a deep generative model of machine translation which incorporates a chain of latent variables, in order to account for local lexical and syntactic variation in parallel corpora. We provide an in-depth analysis of the pitfalls encountered in variational inference for training deep generative models. Experiments on several different language pairs demonstrate that the model consistently improves over strong baselines.",,,,ACL
116,2018,Forest-Based Neural Machine Translation,"Chunpeng Ma, Akihiro Tamura, Masao Utiyama, Tiejun Zhao","Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors. For statistical machine translation (SMT), forest-based methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-sequence NMT model). The BLEU score of the proposed method is higher than that of the sequence-to-sequence NMT, tree-based NMT, and forest-based SMT systems.",,,,ACL
117,2018,Context-Aware Neural Machine Translation Learns Anaphora Resolution,"Elena Voita, Pavel Serdyukov, Rico Sennrich, Ivan Titov","Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6).",,,,ACL
118,2018,Document Context Neural Machine Translation with Memory Networks,"Sameen Maruf, Gholamreza Haffari","We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model end-to-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",,,,ACL
119,2018,Which Melbourne? Augmenting Geocoding with Maps,"Milan Gritta, Mohammad Taher Pilehvar, Nigel Collier","The purpose of text geolocation is to associate geographic information contained in a document with a set (or sets) of coordinates, either implicitly by using linguistic features and/or explicitly by using geographic metadata combined with heuristics. We introduce a geocoder (location mention disambiguator) that achieves state-of-the-art (SOTA) results on three diverse datasets by exploiting the implicit lexical clues. Moreover, we propose a new method for systematic encoding of geographic metadata to generate two distinct views of the same text. To that end, we introduce the Map Vector (MapVec), a sparse representation obtained by plotting prior geographic probabilities, derived from population figures, on a World Map. We then integrate the implicit (language) and explicit (map) features to significantly improve a range of metrics. We also introduce an open-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing.",,,,ACL
120,2018,Learning Prototypical Goal Activities for Locations,"Tianyu Jiang, Ellen Riloff","People go to different places to engage in activities that reflect their goals. For example, people go to restaurants to eat, libraries to study, and churches to pray. We refer to an activity that represents a common reason why people typically go to a location as a prototypical goal activity (goal-act). Our research aims to learn goal-acts for specific locations using a text corpus and semi-supervised learning. First, we extract activities and locations that co-occur in goal-oriented syntactic patterns. Next, we create an activity profile matrix and apply a semi-supervised label propagation algorithm to iteratively revise the activity strengths for different locations using a small set of labeled data. We show that this approach outperforms several baseline methods when judged against goal-acts identified by human annotators.",,,,ACL
121,2018,Guess Me if You Can: Acronym Disambiguation for Enterprises,"Yang Li, Bo Zhao, Ariel Fuxman, Fangbo Tao","Acronyms are abbreviations formed from the initial components of words or phrases. In enterprises, people often use acronyms to make communications more efficient. However, acronyms could be difficult to understand for people who are not familiar with the subject matter (new employees, etc.), thereby affecting productivity. To alleviate such troubles, we study how to automatically resolve the true meanings of acronyms in a given context. Acronym disambiguation for enterprises is challenging for several reasons. First, acronyms may be highly ambiguous since an acronym used in the enterprise could have multiple internal and external meanings. Second, there are usually no comprehensive knowledge bases such as Wikipedia available in enterprises. Finally, the system should be generic to work for any enterprise. In this work we propose an end-to-end framework to tackle all these challenges. The framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output. Our disambiguation models are trained via distant supervised learning, without requiring any manually labeled training examples. Therefore, our proposed framework can be deployed to any enterprise to support high-quality acronym disambiguation. Experimental results on real world data justified the effectiveness of our system.",,,,ACL
122,2018,A Multi-Axis Annotation Scheme for Event Temporal Relations,"Qiang Ning, Hao Wu, Dan Roth","Existing temporal relation (TempRel) annotation schemes often have low inter-annotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition. This paper proposes a new multi-axis modeling to better capture the temporal structure of events. In addition, we identify that event end-points are a major source of confusion in annotation, so we also propose to annotate TempRels based on start-points only. A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60’s to 80’s (Cohen’s Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.",,,,ACL
123,2018,Exemplar Encoder-Decoder for Neural Conversation Generation,"Gaurav Pandey, Danish Contractor, Vineet Kumar, Sachindra Joshi","In this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses. Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model and the corresponding responses are used by our decoder to generate the ground truth response. The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context. As a result, our model learns to assign higher similarity scores to those retrieved contexts whose responses are crucial for generating the final response. We present detailed experiments on two large data sets and we find that our method out-performs state of the art sequence to sequence generative models on several recently proposed evaluation metrics.",,,,ACL
124,2018,DialSQL: Dialogue Based Structured Query Generation,"Izzeddin Gur, Semih Yavuz, Yu Su, Xifeng Yan","The recent advance in deep learning and semantic parsing has significantly improved the translation accuracy of natural language questions to structured queries. However, further improvement of the existing approaches turns out to be quite challenging. Rather than solely relying on algorithmic innovations, in this work, we introduce DialSQL, a dialogue-based structured query generation framework that leverages human intelligence to boost the performance of existing algorithms via user interaction. DialSQL is capable of identifying potential errors in a generated SQL query and asking users for validation via simple multi-choice questions. User feedback is then leveraged to revise the query. We design a generic simulator to bootstrap synthetic training dialogues and evaluate the performance of DialSQL on the WikiSQL dataset. Using SQLNet as a black box query generation tool, DialSQL improves its performance from 61.3% to 69.0% using only 2.4 validation questions per dialogue.",,,,ACL
125,2018,Conversations Gone Awry: Detecting Early Signs of Conversational Failure,"Justine Zhang, Jonathan Chang, Cristian Danescu-Niculescu-Mizil, Lucas Dixon","One of the main challenges online social systems face is the prevalence of antisocial behavior, such as harassment and personal attacks. In this work, we introduce the task of predicting from the very start of a conversation whether it will get out of hand. As opposed to detecting undesirable behavior after the fact, this task aims to enable early, actionable prediction at a time when the conversation might still be salvaged. To this end, we develop a framework for capturing pragmatic devices—such as politeness strategies and rhetorical prompts—used to start a conversation, and analyze their relation to its future trajectory. Applying this framework in a controlled setting, we demonstrate the feasibility of detecting early warning signs of antisocial behavior in online discussions.",,,,ACL
126,2018,Are BLEU and Meaning Representation in Opposition?,"Ondřej Cífka, Ondřej Bojar","One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.",,,,ACL
127,2018,Automatic Metric Validation for Grammatical Error Correction,"Leshem Choshen, Omri Abend","Metric validation in Grammatical Error Correction (GEC) is currently done by observing the correlation between human and metric-induced rankings. However, such correlation studies are costly, methodologically troublesome, and suffer from low inter-rater agreement. We propose MAEGE, an automatic methodology for GEC metric validation, that overcomes many of the difficulties in the existing methodology. Experiments with MAEGE shed a new light on metric quality, showing for example that the standard M2 metric fares poorly on corpus-level ranking. Moreover, we use MAEGE to perform a detailed analysis of metric behavior, showing that some types of valid edits are consistently penalized by existing metrics.",,,,ACL
128,2018,The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing,"Rotem Dror, Gili Baumer, Segev Shlomov, Roi Reichart","Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/ theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied. in NLP research in a statistically sound manner.",,,,ACL
129,2018,Distilling Knowledge for Search-based Structured Prediction,"Yijia Liu, Wanxiang Che, Huaipeng Zhao, Bing Qin","Many natural language processing tasks can be modeled into structured prediction and solved as a search problem. In this paper, we distill an ensemble of multiple models trained with different initialization into a single model. In addition to learning to match the ensemble’s probability output on the reference states, we also use the ensemble to explore the search space and learn from the encountered states in the exploration. Experimental results on two typical search-based structured prediction tasks – transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures.",,,,ACL
130,2018,Stack-Pointer Networks for Dependency Parsing,"Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng","We introduce a novel architecture for dependency parsing: stack-pointer networks (StackPtr). Combining pointer networks (Vinyals et al., 2015) with an internal stack, the proposed model first reads and encodes the whole sentence, then builds the dependency tree top-down (from root-to-leaf) in a depth-first fashion. The stack tracks the status of the depth-first search and the pointer networks select one child for the word at the top of the stack at each step. The StackPtr parser benefits from the information of whole sentence and all previously derived subtree structures, and removes the left-to-right restriction in classical transition-based parsers. Yet the number of steps for building any (non-projective) parse tree is linear in the length of the sentence just as other transition-based parsers, yielding an efficient decoding algorithm with O(n2) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-the-art performances on 21 of them",,,,ACL
131,2018,Twitter Universal Dependency Parsing for African-American and Mainstream American English,"Su Lin Blodgett, Johnny Wei, Brendan O’Connor","Due to the presence of both Twitter-specific conventions and non-standard and dialectal language, Twitter presents a significant parsing challenge to current dependency parsing tools. We broaden English dependency parsing to handle social media English, particularly social media African-American English (AAE), by developing and annotating a new dataset of 500 tweets, 250 of which are in AAE, within the Universal Dependencies 2.0 framework. We describe our standards for handling Twitter- and AAE-specific features and evaluate a variety of cross-domain strategies for improving parsing with no, or very little, in-domain labeled data, including a new data synthesis approach. We analyze these methods’ impact on performance disparities between AAE and Mainstream American English tweets, and assess parsing accuracy for specific AAE lexical and syntactic features. Our annotated data and a parsing model are available at: http://slanglab.cs.umass.edu/TwitterAAE/.",,,,ACL
132,2018,"LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better","Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yogatama","Language exhibits hierarchical structure, but recent work using a subject-verb agreement diagnostic argued that state-of-the-art language models, LSTMs, fail to learn long-range syntax sensitive dependencies. Using the same diagnostic, we show that, in fact, LSTMs do succeed in learning such dependencies—provided they have enough capacity. We then explore whether models that have access to explicit syntactic information learn agreement more effectively, and how the way in which this structural information is incorporated into the model impacts performance. We find that the mere presence of syntactic information does not improve accuracy, but when model architecture is determined by syntax, number agreement is improved. Further, we find that the choice of how syntactic structure is built affects how well number agreement is learned: top-down construction outperforms left-corner and bottom-up variants in capturing non-local structural dependencies.",,,,ACL
133,2018,Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures,"Wenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun Ren","Existing solutions to task-oriented dialogue systems follow pipeline designs which introduces architectural complexity and fragility. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude. It significantly outperforms state-of-the-art pipeline-based methods on large datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail.",,,,ACL
134,2018,An End-to-end Approach for Handling Unknown Slot Values in Dialogue State Tracking,"Puyang Xu, Qi Hu","We highlight a practical yet rarely discussed problem in dialogue state tracking (DST), namely handling unknown slot values. Previous approaches generally assume predefined candidate lists and thus are not designed to output unknown values, especially when the spoken language understanding (SLU) module is absent as in many end-to-end (E2E) systems. We describe in this paper an E2E architecture based on the pointer network (PtrNet) that can effectively extract unknown slot values while still obtains state-of-the-art accuracy on the standard DSTC2 benchmark. We also provide extensive empirical evidence to show that tracking unknown values can be challenging and our approach can bring significant improvement with the help of an effective feature dropout technique.",,,,ACL
135,2018,Global-Locally Self-Attentive Encoder for Dialogue State Tracking,"Victor Zhong, Caiming Xiong, Richard Socher","Dialogue state tracking, which estimates user goals and requests given the dialogue context, is an essential part of task-oriented dialogue systems. In this paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker (GLAD), which learns representations of the user utterance and previous system actions with global-local modules. Our model uses global modules to shares parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. We show that this significantly improves tracking of rare states. GLAD obtains 88.3% joint goal accuracy and 96.4% request accuracy on the WoZ state tracking task, outperforming prior work by 3.9% and 4.8%. On the DSTC2 task, our model obtains 74.7% joint goal accuracy and 97.3% request accuracy, outperforming prior work by 1.3% and 0.8%",,,,ACL
136,2018,Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems,"Andrea Madotto, Chien-Sheng Wu, Pascale Fung","End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.",,,,ACL
137,2018,Tailored Sequence to Sequence Models to Different Conversation Scenarios,"Hainan Zhang, Yanyan Lan, Jiafeng Guo, Jun Xu","Sequence to sequence (Seq2Seq) models have been widely used for response generation in the area of conversation. However, the requirements for different conversation scenarios are distinct. For example, customer service requires the generated responses to be specific and accurate, while chatbot prefers diverse responses so as to attract different users. The current Seq2Seq model fails to meet these diverse requirements, by using a general average likelihood as the optimization criteria. As a result, it usually generates safe and commonplace responses, such as ‘I don’t know’. In this paper, we propose two tailored optimization criteria for Seq2Seq to different conversation scenarios, i.e., the maximum generated likelihood for specific-requirement scenario, and the conditional value-at-risk for diverse-requirement scenario. Experimental results on the Ubuntu dialogue corpus (Ubuntu service scenario) and Chinese Weibo dataset (social chatbot scenario) show that our proposed models not only satisfies diverse requirements for different scenarios, but also yields better performances against traditional Seq2Seq models in terms of both metric-based and human evaluations.",,,,ACL
138,2018,Knowledge Diffusion for Neural Dialogue Generation,"Shuman Liu, Hongshen Chen, Zhaochun Ren, Yang Feng","End-to-end neural dialogue generation has shown promising results recently, but it does not employ knowledge to guide the generation and hence tends to generate short, general, and meaningless responses. In this paper, we propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities. With the help of facts matching and entity diffusion, the neural dialogue generation is augmented with the ability of convergent and divergent thinking over the knowledge base. Our empirical study on a real-world dataset prove that our model is capable of generating meaningful, diverse and natural responses for both factoid-questions and knowledge grounded chi-chats. The experiment results also show that our model outperforms competitive baseline models significantly.",,,,ACL
139,2018,Generating Informative Responses with Controlled Sentence Function,"Pei Ke, Jian Guan, Minlie Huang, Xiaoyan Zhu","Sentence function is a significant factor to achieve the purpose of the speaker, which, however, has not been touched in large-scale conversation generation so far. In this paper, we present a model to generate informative responses with controlled sentence function. Our model utilizes a continuous latent variable to capture various word patterns that realize the expected sentence function, and introduces a type controller to deal with the compatibility of controlling sentence function and generating informative content. Conditioned on the latent variable, the type controller determines the type (i.e., function-related, topic, and ordinary word) of a word to be generated at each decoding position. Experiments show that our model outperforms state-of-the-art baselines, and it has the ability to generate responses with both controlled sentence function and informative content.",,,,ACL
140,2018,Sentiment Adaptive End-to-End Dialog Systems,"Weiyan Shi, Zhou Yu","End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating. However, current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.",,,,ACL
141,2018,Embedding Learning Through Multilingual Concept Induction,"Philipp Dufter, Mengjie Zhao, Martin Schmitt, Alexander Fraser",We present a new method for estimating vector space representations of words: embedding learning by concept induction. We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space. An extensive experimental evaluation on crosslingual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches.,,,,ACL
142,2018,Isomorphic Transfer of Syntactic Structures in Cross-Lingual NLP,"Edoardo Maria Ponti, Roi Reichart, Anna Korhonen, Ivan Vulić","The transfer or share of knowledge between languages is a potential solution to resource scarcity in NLP. However, the effectiveness of cross-lingual transfer can be challenged by variation in syntactic structures. Frameworks such as Universal Dependencies (UD) are designed to be cross-lingually consistent, but even in carefully designed resources trees representing equivalent sentences may not always overlap. In this paper, we measure cross-lingual syntactic variation, or anisomorphism, in the UD treebank collection, considering both morphological and structural properties. We show that reducing the level of anisomorphism yields consistent gains in cross-lingual transfer tasks. We introduce a source language selection procedure that facilitates effective cross-lingual parser transfer, and propose a typologically driven method for syntactic tree processing which reduces anisomorphism. Our results show the effectiveness of this method for both machine translation and cross-lingual sentence similarity, demonstrating the importance of syntactic structure compatibility for boosting cross-lingual transfer in NLP.",,,,ACL
143,2018,Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data,"Adithya Pratapa, Gayatri Bhat, Monojit Choudhury, Sunayana Sitaram","Training language models for Code-mixed (CM) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language. We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory. We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can significantly reduce the perplexity of an RNN-based language model. We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs.",,,,ACL
144,2018,Chinese NER Using Lattice LSTM,"Yue Zhang, Jie Yang","We investigate a lattice-structured LSTM model for Chinese NER, which encodes a sequence of input characters as well as all potential words that match a lexicon. Compared with character-based methods, our model explicitly leverages word and word sequence information. Compared with word-based methods, lattice LSTM does not suffer from segmentation errors. Gated recurrent cells allow our model to choose the most relevant characters and words from a sentence for better NER results. Experiments on various datasets show that lattice LSTM outperforms both word-based and character-based LSTM baselines, achieving the best results.",,,,ACL
145,2018,Nugget Proposal Networks for Chinese Event Detection,"Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun","Neural network based models commonly regard event detection as a word-wise classification task, which suffer from the mismatch problem between words and event triggers, especially in languages without natural word delimiters such as Chinese. In this paper, we propose Nugget Proposal Networks (NPNs), which can solve the word-trigger mismatch problem by directly proposing entire trigger nuggets centered at each character regardless of word boundaries. Specifically, NPNs perform event detection in a character-wise paradigm, where a hybrid representation for each character is first learned to capture both structural and semantic information from both characters and words. Then based on learned representations, trigger nuggets are proposed and categorized by exploiting character compositional structures of Chinese event triggers. Experiments on both ACE2005 and TAC KBP 2017 datasets show that NPNs significantly outperform the state-of-the-art methods.",,,,ACL
146,2018,Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation,"Madhav Nimishakavi, Manish Gupta, Partha Talukdar","Relation Schema Induction (RSI) is the problem of identifying type signatures of arguments of relations from unlabeled text. Most of the previous work in this area have focused only on binary RSI, i.e., inducing only the subject and object type signatures per relation. However, in practice, many relations are high-order, i.e., they have more than two arguments and inducing type signatures of all arguments is necessary. For example, in the sports domain, inducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is more informative than inducing just win(WinningPlayer, OpponentPlayer). We refer to this problem as Higher-order Relation Schema Induction (HRSI). In this paper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a novel framework for the HRSI problem. To the best of our knowledge, this is the first attempt at inducing higher-order relation schemata from unlabeled text. Using the experimental analysis on three real world datasets we show how TFBA helps in dealing with sparsity and induce higher-order schemata.",,,,ACL
147,2018,Discovering Implicit Knowledge with Unary Relations,"Michael Glass, Alfio Gliozzo","State-of-the-art relation extraction approaches are only able to recognize relationships between mentions of entity arguments stated explicitly in the text and typically localized to the same sentence. However, the vast majority of relations are either implicit or not sententially localized. This is a major problem for Knowledge Base Population, severely limiting recall. In this paper we propose a new methodology to identify relations between two entities, consisting of detecting a very large number of unary relations, and using them to infer missing entities. We describe a deep learning architecture able to learn thousands of such relations very efficiently by using a common deep learning based representation. Our approach largely outperforms state of the art relation extraction technology on a newly introduced web scale knowledge base population benchmark, that we release to the research community.",,,,ACL
148,2018,Improving Entity Linking by Modeling Latent Relations between Mentions,"Phong Le, Ivan Titov","Entity linking involves aligning textual mentions of named entities to their corresponding entries in a knowledge base. Entity linking systems often exploit relations between textual mentions in a document (e.g., coreference) to decide if the linking decisions are compatible. Unlike previous approaches, which relied on supervised systems or heuristics to predict these relations, we treat relations as latent variables in our neural entity-linking model. We induce the relations without any supervision while optimizing the entity-linking system in an end-to-end fashion. Our multi-relational model achieves the best reported scores on the standard benchmark (AIDA-CoNLL) and substantially outperforms its relation-agnostic version. Its training also converges much faster, suggesting that the injected structural bias helps to explain regularities in the training data.",,,,ACL
149,2018,Dating Documents using Graph Convolution Networks,"Shikhar Vashishth, Shib Sankar Dasgupta, Swayambhu Nath Ray, Partha Talukdar","Document date is essential for many important tasks, such as document retrieval, summarization, event detection, etc. While existing approaches for these tasks assume accurate knowledge of the document date, this is not always available, especially for arbitrary documents from the Web. Document Dating is a challenging problem which requires inference over the temporal structure of the document. Prior document dating systems have largely relied on handcrafted features while ignoring such document-internal structures. In this paper, we propose NeuralDater, a Graph Convolutional Network (GCN) based document dating approach which jointly exploits syntactic and temporal graph structures of document in a principled way. To the best of our knowledge, this is the first application of deep learning for the problem of document dating. Through extensive experiments on real-world datasets, we find that NeuralDater significantly outperforms state-of-the-art baseline by 19% absolute (45% relative) accuracy points.",,,,ACL
150,2018,A Graph-to-Sequence Model for AMR-to-Text Generation,"Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea","The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus facing challenges with large-graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.",,,,ACL
151,2018,GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data,"Bayu Distiawan Trisedya, Jianzhong Qi, Rui Zhang, Wei Wang","A knowledge base is a large repository of facts that are mainly represented as RDF triples, each of which consists of a subject, a predicate (relationship), and an object. The RDF triple representation offers a simple interface for applications to access the facts. However, this representation is not in a natural language form, which is difficult for humans to understand. We address this problem by proposing a system to translate a set of RDF triples into natural sentences based on an encoder-decoder framework. To preserve as much information from RDF triples as possible, we propose a novel graph-based triple encoder. The proposed encoder encodes not only the elements of the triples but also the relationships both within a triple and between the triples. Experimental results show that the proposed encoder achieves a consistent improvement over the baseline models by up to 17.6%, 6.0%, and 16.4% in three common metrics BLEU, METEOR, and TER, respectively.",,,,ACL
152,2018,Learning to Write with Cooperative Discriminators,"Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut","Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators each specialize in a different principle of communication, such as Grice’s maxims, and are collectively combined with the base RNN generator through a composite decoding objective. Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.",,,,ACL
153,2018,A Neural Approach to Pun Generation,"Zhiwei Yu, Jiwei Tan, Xiaojun Wan","Automatic pun generation is an interesting and challenging text generation task. Previous efforts rely on templates or laboriously manually annotated pun datasets, which heavily constrains the quality and diversity of generated puns. Since sequence-to-sequence models provide an effective technique for text generation, it is promising to investigate these models on the pun generation task. In this paper, we propose neural network models for homographic pun generation, and they can generate puns without requiring any pun data for training. We first train a conditional neural language model from a general text corpus, and then generate puns from the language model with an elaborately designed decoding algorithm. Automatic and human evaluations show that our models are able to generate homographic puns of good readability and quality.",,,,ACL
154,2018,Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data,"Harsh Jhamtani, Varun Gangal, Eduard Hovy, Graham Neubig","This paper examines the problem of generating natural language descriptions of chess games. We introduce a new large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a chess game. The introduced dataset consists of more than 298K chess move-commentary pairs across 11K chess games. We highlight how this task poses unique research challenges in natural language generation: the data contain a large variety of styles of commentary and frequently depend on pragmatic context. We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of the game state that may be commented upon to describe a given chess move. Through a human study on predictions for a subset of the data which deals with direct move descriptions, we observe that outputs from our models are rated similar to ground truth commentary texts in terms of correctness and fluency.",,,,ACL
155,2018,From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction,"Zihang Dai, Qizhe Xie, Eduard Hovy","In this work, we study the credit assignment problem in reward augmented maximum likelihood (RAML) learning, and establish a theoretical equivalence between the token-level counterpart of RAML and the entropy regularized reinforcement learning. Inspired by the connection, we propose two sequence prediction algorithms, one extending RAML with fine-grained credit assignment and the other improving Actor-Critic with a systematic entropy regularization. On two benchmark datasets, we show the proposed algorithms outperform RAML and Actor-Critic respectively, providing new alternatives to sequence prediction.",,,,ACL
156,2018,DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension,"Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, Karthik Sankaranarayanan","We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding.",,,,ACL
157,2018,Stochastic Answer Networks for Machine Reading Comprehension,"Xiaodong Liu, Yelong Shen, Kevin Duh, Jianfeng Gao","We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. Compared to previous work such as ReasoNet which used reinforcement learning to determine the number of steps, the unique feature is the use of a kind of stochastic prediction dropout on the answer module (final layer) of the neural network during the training. We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD), the Adversarial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO).",,,,ACL
158,2018,Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering,"Wei Wang, Ming Yan, Chen Wu","This paper describes a novel hierarchical attention network for reading comprehension style question answering, which aims to answer questions for a given narrative paragraph. In the proposed method, attention and fusion are conducted horizontally and vertically across layers at different levels of granularity between question and paragraph. Specifically, it first encode the question and paragraph with fine-grained language embeddings, to better capture the respective representations at semantic level. Then it proposes a multi-granularity fusion approach to fully fuse information from both global and attended representations. Finally, it introduces a hierarchical attention network to focuses on the answer span progressively with multi-level soft-alignment. Extensive experiments on the large-scale SQuAD, TriviaQA dataset validate the effectiveness of the proposed method. At the time of writing the paper, our model achieves state-of-the-art on the both SQuAD and TriviaQA Wiki leaderboard as well as two adversarial SQuAD datasets.",,,,ACL
159,2018,Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension,"Zhen Wang, Jiachen Liu, Xinyan Xiao, Yajuan Lyu","While sophisticated neural-based techniques have been developed in reading comprehension, most approaches model the answer in an independent manner, ignoring its relations with other answer candidates. This problem can be even worse in open-domain scenarios, where candidates from multiple passages should be combined to answer a single question. In this paper, we formulate reading comprehension as an extract-then-select two-stage procedure. We first extract answer candidates from passages, then select the final answer by combining information from all the candidates. Furthermore, we regard candidate extraction as a latent variable and train the two-stage process jointly with reinforcement learning. As a result, our approach has improved the state-of-the-art performance significantly on two challenging open-domain reading comprehension datasets. Further analysis demonstrates the effectiveness of our model components, especially the information fusion of all the candidates and the joint training of the extract-then-select procedure.",,,,ACL
160,2018,Efficient and Robust Question Answering from Minimal Context over Documents,"Sewon Min, Victor Zhong, Richard Socher, Caiming Xiong","Neural models for question answering (QA) over documents have achieved significant performance improvements. Although effective, these models do not scale to large corpora due to their complex modeling of interactions between the document and the question. Moreover, recent work has shown that such models are sensitive to adversarial inputs. In this paper, we study the minimal context required to answer the question, and find that most questions in existing datasets can be answered with a small set of sentences. Inspired by this observation, we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model. Our overall system achieves significant reductions in training (up to 15 times) and inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs.",,,,ACL
161,2018,Denoising Distantly Supervised Open-Domain Question Answering,"Yankai Lin, Haozhe Ji, Zhiyuan Liu, Maosong Sun","Distantly supervised open-domain question answering (DS-QA) aims to find answers in collections of unlabeled text. Existing DS-QA models usually retrieve related paragraphs from a large-scale corpus and apply reading comprehension technique to extract answers from the most relevant paragraph. They ignore the rich information contained in other paragraphs. Moreover, distant supervision data inevitably accompanies with the wrong labeling problem, and these noisy data will substantially degrade the performance of DS-QA. To address these issues, we propose a novel DS-QA model which employs a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines.",,,,ACL
162,2018,Question Condensing Networks for Answer Selection in Community Question Answering,"Wei Wu, Xu Sun, Houfeng Wang","Answer selection is an important subtask of community question answering (CQA). In a real-world CQA forum, a question is often represented as two parts: a subject that summarizes the main points of the question, and a body that elaborates on the subject in detail. Previous researches on answer selection usually ignored the difference between these two parts and concatenated them as the question representation. In this paper, we propose the Question Condensing Networks (QCN) to make use of the subject-body relationship of community questions. In our model, the question subject is the primary part of the question representation, and the question body information is aggregated based on similarity and disparity with the question subject. Experimental results show that QCN outperforms all existing models on two CQA datasets.",,,,ACL
163,2018,Towards Robust Neural Machine Translation,"Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie Zhai","Small perturbations in the input can severely distort intermediate representations and thus impact translation quality of neural machine translation (NMT) models. In this paper, we propose to improve the robustness of NMT models with adversarial stability training. The basic idea is to make both the encoder and decoder in NMT models robust against input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. Experimental results on Chinese-English, English-German and English-French translation tasks show that our approaches can not only achieve significant improvements over strong NMT systems but also improve the robustness of NMT models.",,,,ACL
164,2018,Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings,"Shaohui Kuang, Junhui Li, António Branco, Weihua Luo","In neural machine translation, a source sequence of words is encoded into a vector from which a target sequence is generated in the decoding phase. Differently from statistical machine translation, the associations between source words and their possible target counterparts are not explicitly stored. Source and target words are at the two ends of a long information processing procedure, mediated by hidden states at both the source encoding and the target decoding phases. This makes it possible that a source word is incorrectly translated into a target word that is not any of its admissible equivalent counterparts in the target language. In this paper, we seek to somewhat shorten the distance between source and target words in that procedure, and thus strengthen their association, by means of a method we term bridging source and target word embeddings. We experiment with three strategies: (1) a source-side bridging model, where source word embeddings are moved one step closer to the output target sequence; (2) a target-side bridging model, which explores the more relevant source word embeddings for the prediction of the target sequence; and (3) a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation of ones by the others. Experiments and analysis presented in this paper demonstrate that the proposed bridging models are able to significantly improve quality of both sentence translation, in general, and alignment and translation of individual source words with target words, in particular.",,,,ACL
165,2018,Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning,"Julia Kreutzer, Joshua Uyheng, Stefan Riezler","We present a study on reinforcement learning (RL) from human bandit feedback for sequence-to-sequence learning, exemplified by the task of bandit neural machine translation (NMT). We investigate the reliability of human bandit feedback, and analyze the influence of reliability on the learnability of a reward estimator, and the effect of the quality of reward estimates on the overall RL task. Our analysis of cardinal (5-point ratings) and ordinal (pairwise preferences) feedback shows that their intra- and inter-annotator α-agreement is comparable. Best reliability is obtained for standardized cardinal feedback, and cardinal feedback is also easiest to learn and generalize from. Finally, improvements of over 1 BLEU can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into RL for NMT. This shows that RL is possible even from small amounts of fairly reliable human feedback, pointing to a great potential for applications at larger scale.",,,,ACL
166,2018,Accelerating Neural Transformer via an Average Attention Network,"Biao Zhang, Deyi Xiong, Jinsong Su","With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive architecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 different language pairs, we obtain robust and consistent speed-ups in decoding.",,,,ACL
167,2018,How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures,Tobias Domhan,"With recent advances in network architectures for Neural Machine Translation (NMT) recurrent models have effectively been replaced by either convolutional or self-attentional approaches, such as in the Transformer. While the main innovation of the Transformer architecture is its use of self-attentional layers, there are several other aspects, such as attention with multiple heads and the use of many attention layers, that distinguish the model from previous baselines. In this work we take a fine-grained look at the different architectures for NMT. We introduce an Architecture Definition Language (ADL) allowing for a flexible combination of common building blocks. Making use of this language we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention. Additionally, we find that self-attention is much more important on the encoder side than on the decoder side, where it can be replaced by a RNN or CNN without a loss in performance in most settings. Surprisingly, even a model without any target side self-attention performs well.",,,,ACL
168,2018,Weakly Supervised Semantic Parsing with Abstract Examples,"Omer Goldman, Veronica Latcinnik, Ehud Nave, Amir Globerson","Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways. First, a large search space of potential programs needs to be explored at training time to find a correct program. Second, spurious programs that accidentally lead to a correct denotation add noise to training. In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and program are lifted to an abstract form. We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training. To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spuriousness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation. Our method substantially improves performance, and reaches 82.5% accuracy, a 14.7% absolute accuracy improvement compared to the best reported accuracy so far.",,,,ACL
169,2018,Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback,"Carolin Lawrence, Stefan Riezler","Counterfactual learning from human bandit feedback describes a scenario where user feedback on the quality of outputs of a historic system is logged and used to improve a target system. We show how to apply this learning framework to neural semantic parsing. From a machine learning perspective, the key challenge lies in a proper reweighting of the estimator so as to avoid known degeneracies in counterfactual learning, while still being applicable to stochastic gradient optimization. To conduct experiments with human users, we devise an easy-to-use interface to collect human feedback on semantic parses. Our work is the first to show that semantic parsers can be improved significantly by counterfactual learning from logged human feedback data.",,,,ACL
170,2018,AMR dependency parsing with a typed semantic algebra,"Jonas Groschwitz, Matthias Lindemann, Meaghan Fowlie, Mark Johnson","We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph. This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system. We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and outperform strong baselines.",,,,ACL
171,2018,Sequence-to-sequence Models for Cache Transition Systems,"Xiaochang Peng, Linfeng Song, Daniel Gildea, Giorgio Satta","In this paper, we present a sequence-to-sequence based approach for mapping natural language sentences to AMR semantic graphs. We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system. To address the sparsity issue of neural AMR parsing, we feed feature embeddings from the transition state to provide relevant local information for each decoder state. We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.",,,,ACL
172,2018,Batch IS NOT Heavy: Learning Word Representations From All Samples,"Xin Xin, Fajie Yuan, Xiangnan He, Joemon M. Jose","Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations. However, it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution. Besides, SGD suffers from dramatic fluctuation due to the one-sample learning scheme. In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples. Remarkably, the time complexity of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples. We evaluate AllVec on several benchmark tasks. Experiments show that AllVec outperforms sampling-based SGD methods with comparable efficiency, especially for small training corpora.",,,,ACL
173,2018,Backpropagating through Structured Argmax using a SPIGOT,"Hao Peng, Sam Thomson, Noah A. Smith","We introduce structured projection of intermediate gradients (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers. SPIGOT requires no marginal inference, unlike structured attention networks and reinforcement learning-inspired solutions. Like so-called straight-through estimators, SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT’s proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed. We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.",,,,ACL
174,2018,Learning How to Actively Learn: A Deep Imitation Learning Approach,"Ming Liu, Wray Buntine, Gholamreza Haffari","Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary. We introduce a method that learns an AL “policy” using “imitation learning” (IL). Our IL-based approach makes use of an efficient and effective “algorithmic expert”, which provides the policy learner with good actions in the encountered AL situations. The AL strategy is then learned with a feedforward network, mapping situations to most informative query datapoints. We evaluate our method on two different tasks: text classification and named entity recognition. Experimental results show that our IL-based AL strategy is more effective than strong previous methods using heuristics and reinforcement learning.",,,,ACL
175,2018,Training Classifiers with Natural Language Explanations,"Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann","Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100 faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.",,,,ACL
176,2018,Did the Model Understand the Question?,"Pramod Kaushik Mudrakarta, Ankur Taly, Mukund Sundararajan, Kedar Dhamdhere","We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of “attribution” (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from 61.1% to 19%, and that of a tabular question answering model from 33.5% to 3.3%. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.",,,,ACL
177,2018,Harvesting Paragraph-level Question-Answer Pairs from Wikipedia,"Xinya Du, Claire Cardie","We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia.",,,,ACL
178,2018,Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification,"Yizhong Wang, Kai Liu, Jing Liu, Wei He","Machine reading comprehension (MRC) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine. Compared with MRC on a single passage, multi-passage MRC is more challenging, since we are likely to get multiple confusing answer candidates from different passages. To address this problem, we propose an end-to-end neural model that enables those answer candidates from different passages to verify each other based on their content representations. Specifically, we jointly train three modules that can predict the final answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification. The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset, both of which are designed for MRC in real-world settings.",,,,ACL
179,2018,Language Generation via DAG Transduction,"Yajie Ye, Weiwei Sun, Xiaojun Wan","A DAG automaton is a formal device for manipulating graphs. By augmenting a DAG automaton with transduction rules, a DAG transducer has potential applications in fundamental NLP tasks. In this paper, we propose a novel DAG transducer to perform graph-to-program transformation. The target structure of our transducer is a program licensed by a declarative programming language rather than linguistic structures. By executing such a program, we can easily get a surface string. Our transducer is designed especially for natural language generation (NLG) from type-logical semantic graphs. Taking Elementary Dependency Structures, a format of English Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design.",,,,ACL
180,2018,A Distributional and Orthographic Aggregation Model for English Derivational Morphology,"Daniel Deutsch, John Hewitt, Dan Roth","Modeling derivational morphology to generate words with particular semantics is useful in many text generation tasks, such as machine translation or abstractive question answering. In this work, we tackle the task of derived word generation. That is, we attempt to generate the word “runner” for “someone who runs.” We identify two key problems in generating derived words from root words and transformations. We contribute a novel aggregation model of derived word generation that learns derivational transformations both as orthographic functions using sequence-to-sequence models and as functions in distributional word embedding space. The model then learns to choose between the hypothesis of each system. We also present two ways of incorporating corpus information into derived word generation.",,,,ACL
181,2018,"Deep-speare: A joint neural model of poetic language, meter and rhyme","Jey Han Lau, Trevor Cohn, Timothy Baldwin, Julian Brooke","In this paper, we propose a joint architecture that captures language, rhyme and meter for sonnet modelling. We assess the quality of generated poems using crowd and expert judgements. The stress and rhyme models perform very well, as generated poems are largely indistinguishable from human-written poems. Expert evaluation, however, reveals that a vanilla language model captures meter implicitly, and that machine-generated poems still underperform in terms of readability and emotion. Our research shows the importance expert evaluation for poetry generation, and that future research should look beyond rhyme/meter and focus on poetic language.",,,,ACL
182,2018,NeuralREG: An end-to-end approach to referring expression generation,"Thiago Castro Ferreira, Diego Moussallem, Ákos Kádár, Sander Wubben","Traditionally, Referring Expression Generation (REG) models first decide on the form and then on the content of references to discourse entities in text, typically relying on features such as salience and grammatical function. In this paper, we present a new approach (NeuralREG), relying on deep neural networks, which makes decisions about form and content in one go without explicit feature extraction. Using a delexicalized version of the WebNLG corpus, we show that the neural model substantially improves over two strong baselines.",,,,ACL
183,2018,Stock Movement Prediction from Tweets and Historical Prices,"Yumo Xu, Shay B. Cohen","Stock movement prediction is a challenging problem: the market is highly stochastic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed model on a new stock movement prediction dataset which we collected.",,,,ACL
184,2018,Rumor Detection on Twitter with Tree-structured Recursive Neural Networks,"Jing Ma, Wei Gao, Kam-Fai Wong","Automatic rumor detection is technically very challenging. In this work, we try to learn discriminative features from tweets content by following their non-sequential propagation structure and generate more powerful representations for identifying different type of rumors. We propose two recursive neural models based on a bottom-up and a top-down tree-structured neural networks for rumor representation learning and classification, which naturally conform to the propagation layout of tweets. Results on two public Twitter datasets demonstrate that our recursive neural models 1) achieve much better performance than state-of-the-art approaches; 2) demonstrate superior capacity on detecting rumors at very early stage.",,,,ACL
185,2018,Visual Attention Model for Name Tagging in Multimodal Social Media,"Di Lu, Leonardo Neves, Vitor Carvalho, Ning Zhang","Everyday billions of multimodal posts containing both images and text are shared in social media sites such as Snapchat, Twitter or Instagram. This combination of image and text in a single message allows for more creative and expressive forms of communication, and has become increasingly common in such sites. This new paradigm brings new challenges for natural language understanding, as the textual component tends to be shorter, more informal, and often is only understood if combined with the visual context. In this paper, we explore the task of name tagging in multimodal social media posts. We start by creating two new multimodal datasets: the first based on Twitter posts and the second based on Snapchat captions (exclusively submitted to public and crowd-sourced stories). We then propose a novel model architecture based on Visual Attention that not only provides deeper visual understanding on the decisions of the model, but also significantly outperforms other state-of-the-art baseline methods for this task.",,,,ACL
186,2018,Multimodal Named Entity Disambiguation for Noisy Social Media Posts,"Seungwhan Moon, Leonardo Neves, Vitor Carvalho","We introduce the new Multimodal Named Entity Disambiguation (MNED) task for multimodal social media posts such as Snapchat or Instagram captions, which are composed of short captions with accompanying images. Social media posts bring significant challenges for disambiguation tasks because 1) ambiguity not only comes from polysemous entities, but also from inconsistent or incomplete notations, 2) very limited context is provided with surrounding words, and 3) there are many emerging entities often unseen during training. To this end, we build a new dataset called SnapCaptionsKB, a collection of Snapchat image captions submitted to public and crowd-sourced stories, with named entity mentions fully annotated and linked to entities in an external knowledge base. We then build a deep zeroshot multimodal network for MNED that 1) extracts contexts from both text and image, and 2) predicts correct entity in the knowledge graph embeddings space, allowing for zeroshot disambiguation of entities unseen in training set as well. The proposed model significantly outperforms the state-of-the-art text-only NED models, showing efficacy and potentials of the MNED task.",,,,ACL
187,2018,Semi-supervised User Geolocation via Graph Convolutional Networks,"Afshin Rahimi, Trevor Cohn, Timothy Baldwin","Social media user geolocation is vital to many applications such as event detection. In this paper, we propose GCN, a multiview geolocation model based on Graph Convolutional Networks, that uses both text and network context. We compare GCN to the state-of-the-art, and to two baselines we propose, and show that our model achieves or is competitive with the state-of-the-art over three benchmark geolocation datasets when sufficient supervision is available. We also evaluate GCN under a minimal supervision scenario, and show it outperforms baselines. We find that highway network gates are essential for controlling the amount of useful neighbourhood expansion in GCN.",,,,ACL
188,2018,Document Modeling with External Attention for Sentence Extraction,"Shashi Narayan, Ronald Cardenas, Nikos Papasarantopoulos, Shay B. Cohen","Document modeling is essential to a variety of natural language understanding tasks. We propose to use external information to improve document modeling for problems that can be framed as sentence extraction. We develop a framework composed of a hierarchical document encoder and an attention-based extractor with attention over external information. We evaluate our model on extractive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question). We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.",,,,ACL
189,2018,Neural Models for Documents with Metadata,"Dallas Card, Chenhao Tan, Noah A. Smith","Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.",,,,ACL
190,2018,NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing,"Dinghan Shen, Qinliang Su, Paidamoyo Chapfuwa, Wenlin Wang","Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled ad-hoc. In this paper, we present an end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary hashing codes are treated as Bernoulli latent variables. A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent variable to optimize the hash function. We also draw the connections between proposed method and rate-distortion theory, which provides a theoretical foundation for the effectiveness of our framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsupervised and supervised scenarios.",,,,ACL
191,2018,Large-Scale QA-SRL Parsing,"Nicholas FitzGerald, Julian Michael, Luheng He, Luke Zettlemoyer","We present a new large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations, and the first high-quality QA-SRL parser. Our corpus, QA-SRL Bank 2.0, consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with a new crowd-sourcing scheme that we show has high precision and good recall at modest cost. We also present neural models for two QA-SRL subtasks: detecting argument spans for a predicate and generating questions to label the semantic relationship. The best models achieve question accuracy of 82.6% and span-level accuracy of 77.6% (under human evaluation) on the full pipelined QA-SRL prediction task. They can also, as we show, be used to gather additional annotations at low cost.",,,,ACL
192,2018,"Syntax for Semantic Role Labeling, To Be, Or Not To Be","Shexia He, Zuchao Li, Hai Zhao, Hongxiao Bai","Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence. Previous studies have shown syntactic information has a remarkable contribution to SRL performance. However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone. This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework. We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information. Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both English and Chinese, showing the quantitative significance of syntax to neural SRL together with a thorough empirical survey over existing models.",,,,ACL
193,2018,Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation,"Alane Suhr, Yoav Artzi","We propose a learning approach for mapping context-dependent sequential instructions to actions. We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world. To train from start and goal states without access to demonstrations, we propose SESTRA, a learning algorithm that takes advantage of single-step reward observations and immediate expected reward maximization. We evaluate on the SCONE domains, and show absolute accuracy improvements of 9.8%-25.3% across the domains over approaches that use high-level logical representations.",,,,ACL
194,2018,Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding,"Bingfeng Luo, Yansong Feng, Zheng Wang, Songfang Huang","The success of many natural language processing (NLP) tasks is bound by the number and quality of annotated data, but there is often a shortage of such training data. In this paper, we ask the question: “Can we combine a neural network (NN) with regular expressions (RE) to improve supervised learning for NLP?”. In answer, we develop novel methods to exploit the rich expressiveness of REs at different levels within a NN, showing that the combination significantly enhances the learning effectiveness when a small number of training examples are available. We evaluate our approach by applying it to spoken language understanding for intent detection and slot filling. Experimental results show that our approach is highly effective in exploiting the available training data, giving a clear boost to the RE-unaware NN.",,,,ACL
195,2018,Token-level and sequence-level loss smoothing for RNN language models,"Maha Elbayad, Laurent Besacier, Jakob Verbeek","Despite the effectiveness of recurrent neural network language models, their maximum likelihood estimation suffers from two limitations. It treats all sentences that do not match the ground truth as equally poor, ignoring the structure of the output space. Second, it suffers from ’exposure bias’: during training tokens are predicted given ground-truth sequences, while at test time prediction is conditioned on generated output sequences. To overcome these limitations we build upon the recent reward augmented maximum likelihood approach that encourages the model to predict sentences that are close to the ground truth according to a given performance metric. We extend this approach to token-level loss smoothing, and propose improvements to the sequence-level smoothing approach. Our experiments on two different tasks, image captioning and machine translation, show that token-level and sequence-level loss smoothing are complementary, and significantly improve results.",,,,ACL
196,2018,Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers,"Georgios Spithourakis, Sebastian Riedel","Numeracy is the ability to understand and work with numbers. It is a necessary skill for composing and understanding documents in clinical, scientific, and other technical domains. In this paper, we explore different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary. Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude, respectively, over non-hierarchical models. A combination of strategies can further improve perplexity. Our continuous probability density function model reduces mean absolute percentage errors by 18% and 54% in comparison to the second best strategy for each dataset, respectively.",,,,ACL
197,2018,To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness,"Amulya Gupta, Zhu Zhang","With the recent success of Recurrent Neural Networks (RNNs) in Machine Translation (MT), attention mechanisms have become increasingly popular. The purpose of this paper is two-fold; firstly, we propose a novel attention model on Tree Long Short-Term Memory Networks (Tree-LSTMs), a tree-structured generalization of standard LSTM. Secondly, we study the interaction between attention and syntactic structures, by experimenting with three LSTM variants: bidirectional-LSTMs, Constituency Tree-LSTMs, and Dependency Tree-LSTMs. Our models are evaluated on two semantic relatedness tasks: semantic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and paraphrase detection for question pairs (Quora, 2017).",,,,ACL
198,2018,What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties,"Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault","Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. “Downstream” tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.",,,,ACL
199,2018,Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning,"Pengda Qin, Weiran Xu, William Yang Wang","Distant supervision has become the standard method for relation extraction. However, even though it is an efficient method, it does not come at no cost—The resulted distantly-supervised training samples are often very noisy. To combat the noise, most of the recent state-of-the-art approaches focus on selecting one-best sentence or calculating soft attention weights over the set of the sentences of one specific entity pair. However, these methods are suboptimal, and the false positive problem is still a key stumbling bottleneck for the performance. We argue that those incorrectly-labeled candidate sentences must be treated with a hard decision, rather than being dealt with soft attention weights. To do this, our paper describes a radical solution—We explore a deep reinforcement learning strategy to generate the false-positive indicator, where we automatically recognize false positives for each relation type without any supervised information. Unlike the removal operation in the previous studies, we redistribute them into the negative examples. The experimental results show that the proposed strategy significantly improves the performance of distant supervision comparing to state-of-the-art systems.",,,,ACL
200,2018,Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder,"Ryo Takahashi, Ran Tian, Kentaro Inui","Embedding models for entities and relations are extremely useful for recovering missing facts in a knowledge base. Intuitively, a relation can be modeled by a matrix mapping entity vectors. However, relations reside on low dimension sub-manifolds in the parameter space of arbitrary matrices – for one reason, composition of two relations M1, M2 may match a third M3 (e.g. composition of relations currency_of_country and country_of_film usually matches currency_of_film_budget), which imposes compositional constraints to be satisfied by the parameters (i.e. M1*M2=M3). In this paper we investigate a dimension reduction technique by training relations jointly with an autoencoder, which is expected to better capture compositional constraints. We achieve state-of-the-art on Knowledge Base Completion tasks with strongly improved Mean Rank, and show that joint training with an autoencoder leads to interpretable sparse codings of relations, helps discovering compositional constraints and benefits from compositional training. Our source code is released at github.com/tianran/glimvec.",,,,ACL
201,2018,Zero-Shot Transfer Learning for Event Extraction,"Lifu Huang, Heng Ji, Kyunghyun Cho, Ido Dagan","Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.",,,,ACL
202,2018,Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction,"Wenya Wang, Sinno Jialin Pan","Fine-grained opinion analysis aims to extract aspect and opinion terms from each sentence for opinion summarization. Supervised learning methods have proven to be effective for this task. However, in many domains, the lack of labeled data hinders the learning of a precise extraction model. In this case, unsupervised domain adaptation methods are desired to transfer knowledge from the source domain to any unlabeled target domain. In this paper, we develop a novel recursive neural network that could reduce domain shift effectively in word level through syntactic relations. We treat these relations as invariant “pivot information” across domains to build structural correspondences and generate an auxiliary task to predict the relation between any two adjacent words in the dependency tree. In the end, we demonstrate state-of-the-art results on three benchmark datasets.",,,,ACL
203,2018,Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning,"Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu","Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.",,,,ACL
204,2018,Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders,"Yansen Wang, Chenyi Liu, Minlie Huang, Liqiang Nie","Asking good questions in open-domain conversational systems is quite significant but rather untouched. This task, substantially different from traditional question generation, requires to question not only with various patterns but also on diverse and relevant topics. We observe that a good question is a natural composition of interrogatives, topic words, and ordinary words. Interrogatives lexicalize the pattern of questioning, topic words address the key information for topic transition in dialogue, and ordinary words play syntactical and grammatical roles in making a natural sentence. We devise two typed decoders (soft typed decoder and hard typed decoder) in which a type distribution over the three types is estimated and the type distribution is used to modulate the final generation distribution. Extensive experiments show that the typed decoders outperform state-of-the-art baselines and can generate more meaningful questions.",,,,ACL
205,2018,"Personalizing Dialogue Agents: I have a dog, do you have pets too?","Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam","Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",,,,ACL
206,2018,Efficient Large-Scale Neural Domain Classification with Personalized Attention,"Young-Bum Kim, Dongchan Kim, Anjishnu Kumar, Ruhi Sarikaya","In this paper, we explore the task of mapping spoken language utterances to one of thousands of natural language understanding domains in intelligent personal digital assistants (IPDAs). This scenario is observed in mainstream IPDAs in industry that allow third parties to develop thousands of new domains to augment built-in first party domains to rapidly increase domain coverage and overall IPDA capabilities. We propose a scalable neural model architecture with a shared encoder, a novel attention mechanism that incorporates personalization information and domain-specific classifiers that solves the problem efficiently. Our architecture is designed to efficiently accommodate incremental domain additions achieving two orders of magnitude speed up compared to full model retraining. We consider the practical constraints of real-time production systems, and design to minimize memory footprint and runtime latency. We demonstrate that incorporating personalization significantly improves domain classification accuracy in a setting with thousands of overlapping domains.",,,,ACL
207,2018,Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment,"Yue Gu, Kangning Yang, Shiyu Fu, Shuhong Chen","Multimodal affective computing, learning to recognize and interpret human affect and subjective information from multiple data sources, is still a challenge because: (i) it is hard to extract informative features to represent human affects from heterogeneous inputs; (ii) current fusion strategies only fuse different modalities at abstract levels, ignoring time-dependent interactions between modalities. Addressing such issues, we introduce a hierarchical multimodal architecture with attention and word-level fusion to classify utterance-level sentiment and emotion from text and audio data. Our introduced model outperforms state-of-the-art approaches on published datasets, and we demonstrate that our model is able to visualize and interpret synchronized attention over modalities.",,,,ACL
208,2018,Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph,"AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria","Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.",,,,ACL
209,2018,Efficient Low-rank Multimodal Fusion With Modality-Specific Factors,"Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang","Multimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is multimodal fusion. The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation. Previous research in this field has exploited the expressiveness of tensors for multimodal representation. However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor. In this paper, we propose the Low-rank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency. We evaluate our model on three different tasks: multimodal sentiment analysis, speaker trait analysis, and emotion recognition. Our model achieves competitive results on all these tasks while drastically reducing computational complexity. Additional experiments also show that our model can perform robustly for a wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations.",,,,ACL
210,2018,Discourse Coherence: Concurrent Explicit and Implicit Relations,"Hannah Rohde, Alexander Johnson, Nathan Schneider, Bonnie Webber","Theories of discourse coherence posit relations between discourse segments as a key feature of coherent text. Our prior work suggests that multiple discourse relations can be simultaneously operative between two segments for reasons not predicted by the literature. Here we test how this joint presence can lead participants to endorse seemingly divergent conjunctions (e.g., BUT and SO) to express the link they see between two segments. These apparent divergences are not symptomatic of participant naivety or bias, but arise reliably from the concurrent availability of multiple relations between segments – some available through explicit signals and some via inference. We believe that these new results can both inform future progress in theoretical work on discourse coherence and lead to higher levels of performance in discourse parsing.",,,,ACL
211,2018,A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text,"Shashank Srivastava, Nebojsa Jojic","We present a generative probabilistic model of documents as sequences of sentences, and show that inference in it can lead to extraction of long-range latent discourse structure from a collection of documents. The approach is based on embedding sequences of sentences from longer texts into a 2- or 3-D spatial grids, in which one or two coordinates model smooth topic transitions, while the third captures the sequential nature of the modeled text. A significant advantage of our approach is that the learned models are naturally visualizable and interpretable, as semantic similarity and sequential structure are modeled along orthogonal directions in the grid. We show that the method is effective in capturing discourse structures in narrative text across multiple genres, including biographies, stories, and newswire reports. In particular, our method outperforms or is competitive with state-of-the-art generative approaches on tasks such as predicting the outcome of a story, and sentence ordering.",,,,ACL
212,2018,Joint Reasoning for Temporal and Causal Relations,"Qiang Ning, Zhili Feng, Hao Wu, Dan Roth","Understanding temporal and causal relations between events is a fundamental natural language understanding task. Because a cause must occur earlier than its effect, temporal and causal relations are closely related and one relation often dictates the value of the other. However, limited attention has been paid to studying these two relations jointly. This paper presents a joint inference framework for them using constrained conditional models (CCMs). Specifically, we formulate the joint problem as an integer linear programming (ILP) problem, enforcing constraints that are inherent in the nature of time and causality. We show that the joint inference framework results in statistically significant improvement in the extraction of both temporal and causal relations from text.",,,,ACL
213,2018,Modeling Naive Psychology of Characters in Simple Commonsense Stories,"Hannah Rashkin, Antoine Bosselut, Maarten Sap, Kevin Knight","Understanding a narrative requires reading between the lines and reasoning about the unspoken but obvious implications about events and people’s mental states — a capability that is trivial for humans but remarkably hard for machines. To facilitate research addressing this challenge, we introduce a new annotation framework to explain naive psychology of story characters as fully-specified chains of mental states with respect to motivations and emotional reactions. Our work presents a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.",,,,ACL
214,2018,A Deep Relevance Model for Zero-Shot Document Filtering,"Chenliang Li, Wei Zhou, Feng Ji, Yu Duan","In the era of big data, focused analysis for diverse topics with a short response time becomes an urgent demand. As a fundamental task, information filtering therefore becomes a critical necessity. In this paper, we propose a novel deep relevance model for zero-shot document filtering, named DAZER. DAZER estimates the relevance between a document and a category by taking a small set of seed words relevant to the category. With pre-trained word embeddings from a large external corpus, DAZER is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process. The gate mechanism controls which convolution filters output the relevance signals in a category dependent manner. Experiments on two document collections of two different tasks (i.e., topic categorization and sentiment analysis) demonstrate that DAZER significantly outperforms the existing alternative solutions, including the state-of-the-art deep relevance ranking models.",,,,ACL
215,2018,Disconnected Recurrent Neural Networks for Text Categorization,Baoxin Wang,"Recurrent neural network (RNN) has achieved remarkable performance in text categorization. RNN can model the entire sequence and capture long-term dependencies, but it does not do well in extracting key patterns. In contrast, convolutional neural network (CNN) is good at extracting local and position-invariant features. In this paper, we present a novel model named disconnected recurrent neural network (DRNN), which incorporates position-invariance into RNN. By limiting the distance of information flow in RNN, the hidden state at each time step is restricted to represent words near the current position. The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text categorization.",,,,ACL
216,2018,Joint Embedding of Words and Labels for Text Classification,"Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe Zhang","Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels. The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both accuracy and speed.",,,,ACL
217,2018,Neural Sparse Topical Coding,"Min Peng, Qianqian Xie, Yanchun Zhang, Hua Wang","Topic models with sparsity enhancement have been proven to be effective at learning discriminative and coherent latent topics of short texts, which is critical to many scientific and engineering applications. However, the extensions of these models require carefully tailored graphical models and re-deduced inference algorithms, limiting their variations and applications. We propose a novel sparsity-enhanced topic model, Neural Sparse Topical Coding (NSTC) base on a sparsity-enhanced topic model called Sparse Topical Coding (STC). It focuses on replacing the complex inference process with the back propagation, which makes the model easy to explore extensions. Moreover, the external semantic information of words in word embeddings is incorporated to improve the representation of short texts. To illustrate the flexibility offered by the neural network based framework, we present three extensions base on NSTC without re-deduced inference algorithms. Experiments on Web Snippet and 20Newsgroups datasets demonstrate that our models outperform existing methods.",,,,ACL
218,2018,Document Similarity for Texts of Varying Lengths via Hidden Topics,"Hongyu Gong, Tarek Sakakini, Suma Bhat, JinJun Xiong","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",,,,ACL
219,2018,Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour,"Sandeep Mathias, Diptesh Kanojia, Kevin Patel, Samarth Agrawal","Predicting a reader’s rating of text quality is a challenging task that involves estimating different subjective aspects of the text, like structure, clarity, etc. Such subjective aspects are better handled using cognitive information. One such source of cognitive information is gaze behaviour. In this paper, we show that gaze behaviour does indeed help in effectively predicting the rating of text quality. To do this, we first we model text quality as a function of three properties - organization, coherence and cohesion. Then, we demonstrate how capturing gaze behaviour helps in predicting each of these properties, and hence the overall quality, by reporting improvements obtained by adding gaze features to traditional textual features for score prediction. We also hypothesize that if a reader has fully understood the text, the corresponding gaze behaviour would give a better indication of the assigned rating, as opposed to partial understanding. Our experiments validate this hypothesis by showing greater agreement between the given rating and the predicted rating when the reader has a full understanding of the text.",,,,ACL
220,2018,Multi-Input Attention for Unsupervised OCR Correction,"Rui Dong, David Smith","We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for unsupervised training and as a source of evidence when decoding. A sequence-to-sequence model with attention is applied for single-input correction, and a new decoder with multi-input attention averaging is developed to search for consensus among multiple sequences. We design two ways of training the correction model without human annotation, either training to match noisily observed textual variants or bootstrapping from a uniform error model. On two corpora of historical newspapers and books, we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and, with the addition of multi-input decoding, can rival supervised methods.",,,,ACL
221,2018,Building Language Models for Text with Named Entities,"Md Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang","Text in many domains involves a significant amount of named entities. Predicting the entity names is often challenging for a language model as they appear less frequent on the training corpus. In this paper, we propose a novel and effective approach to building a language model which can learn the entity names by leveraging their entity type information. We also introduce two benchmark datasets based on recipes and Java programming codes, on which we evaluate the proposed model. Experimental results show that our model achieves 52.2% better perplexity in recipe generation and 22.06% on code generation than state-of-the-art language models.",,,,ACL
222,2018,hyperdoc2vec: Distributed Representations of Hypertext Documents,"Jialong Han, Yan Song, Wayne Xin Zhao, Shuming Shi","Hypertext documents, such as web pages and academic papers, are of great importance in delivering information in our daily life. Although being effective on plain documents, conventional text embedding methods suffer from information loss if directly adapted to hyper-documents. In this paper, we propose a general embedding approach for hyper-documents, namely, hyperdoc2vec, along with four criteria characterizing necessary information that hyper-document embedding models should preserve. Systematic comparisons are conducted between hyperdoc2vec and several competitors on two tasks, i.e., paper classification and citation recommendation, in the academic paper domain. Analyses and experiments both validate the superiority of hyperdoc2vec to other models w.r.t. the four criteria.",,,,ACL
223,2018,Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval,"Zhenghao Liu, Chenyan Xiong, Maosong Sun, Zhiyuan Liu","This paper presents the Entity-Duet Neural Ranking Model (EDRM), which introduces knowledge graphs to neural search systems. EDRM represents queries and documents by their words and entity annotations. The semantics from knowledge graphs are integrated in the distributed representations of their entities, while the ranking is conducted by interaction-based neural ranking networks. The two components are learned end-to-end, making EDRM a natural combination of entity-oriented search and neural information retrieval. Our experiments on a commercial search log demonstrate the effectiveness of EDRM. Our analyses reveal that knowledge graph semantics significantly improve the generalization ability of neural ranking models.",,,,ACL
224,2018,Neural Natural Language Inference Models Enhanced with External Knowledge,"Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana Inkpen","Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these data? If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets.",,,,ACL
225,2018,AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples,"Dongyeop Kang, Tushar Khot, Ashish Sabharwal, Eduard Hovy","We consider the problem of learning textual entailment models with limited supervision (5K-10K training examples), and present two complementary approaches for it. First, we propose knowledge-guided adversarial example generators for incorporating large lexical resources in entailment models via only a handful of rule templates. Second, to make the entailment model—a discriminator—more robust, we propose the first GAN-style approach for training it using a natural language example generator that iteratively adjusts to the discriminator’s weaknesses. We demonstrate effectiveness using two entailment datasets, where the proposed methods increase accuracy by 4.7% on SciTail and by 2.8% on a 1% sub-sample of SNLI. Notably, even a single hand-written rule, negate, improves the accuracy of negation examples in SNLI by 6.1%.",,,,ACL
226,2018,Subword-level Word Vector Representations for Korean,"Sungjoon Park, Jeongmin Byun, Sion Baek, Yongseok Cho","Research on distributed word representations is focused on widely-used languages such as English. Although the same methods can be used for other languages, language-specific knowledge can enhance the accuracy and richness of word vector representations. In this paper, we look at improving distributed word representations for Korean using knowledge about the unique linguistic structure of Korean. Specifically, we decompose Korean words into the jamo-level, beyond the character-level, allowing a systematic use of subword information. To evaluate the vectors, we develop Korean test sets for word similarity and analogy and make them publicly available. The results show that our simple method outperforms word2vec and character-level Skip-Grams on semantic and syntactic similarity and analogy tasks and contributes positively toward downstream NLP tasks such as sentiment analysis.",,,,ACL
227,2018,Incorporating Chinese Characters of Words for Lexical Sememe Prediction,"Huiming Jin, Hao Zhu, Zhiyuan Liu, Ruobing Xie","Sememes are minimum semantic units of concepts in human languages, such that each word sense is composed of one or multiple sememes. Words are usually manually annotated with their sememes by linguists, and form linguistic common-sense knowledge bases widely used in various NLP tasks. Recently, the lexical sememe prediction task has been introduced. It consists of automatically recommending sememes for words, which is expected to improve annotation efficiency and consistency. However, existing methods of lexical sememe prediction typically rely on the external context of words to represent the meaning, which usually fails to deal with low-frequency and out-of-vocabulary words. To address this issue for Chinese, we propose a novel framework to take advantage of both internal character information and external context information of words. We experiment on HowNet, a Chinese sememe knowledge base, and demonstrate that our framework outperforms state-of-the-art baselines by a large margin, and maintains a robust performance even for low-frequency words.",,,,ACL
228,2018,SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment,"Jisun An, Haewoon Kwak, Yong-Yeol Ahn","Because word semantics can substantially change across communities and contexts, capturing domain-specific word semantics is an important challenge. Here, we propose SemAxis, a simple yet powerful framework to characterize word semantics using many semantic axes in word-vector spaces beyond sentiment. We demonstrate that SemAxis can capture nuanced semantic representations in multiple online communities. We also show that, when the sentiment axis is examined, SemAxis outperforms the state-of-the-art approaches in building domain-specific sentiment lexicons.",,,,ACL
229,2018,End-to-End Reinforcement Learning for Automatic Taxonomy Induction,"Yuning Mao, Xiang Ren, Jiaming Shen, Xiaotao Gu","We present a novel end-to-end reinforcement learning approach to automatic taxonomy induction from a set of terms. While prior methods treat the problem as a two-phase task (i.e.,, detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy), we argue that such two-phase methods may suffer from error propagation, and cannot effectively optimize metrics that capture the holistic structure of a taxonomy. In our approach, the representations of term pairs are learned using multiple sources of information and used to determine which term to select and where to place it on the taxonomy via a policy network. All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training taxonomies. Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6% on ancestor F1.",,,,ACL
230,2018,Incorporating Glosses into Neural Word Sense Disambiguation,"Fuli Luo, Tianyu Liu, Qiaolin Xia, Baobao Chang","Word Sense Disambiguation (WSD) aims to identify the correct meaning of polysemous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-the-art systems on several English all-words WSD datasets.",,,,ACL
231,2018,Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages,"Jeremy Barnes, Roman Klinger, Sabine Schulte im Walde","Sentiment analysis in low-resource languages suffers from a lack of annotated corpora to estimate high-performing models. Machine translation and bilingual word embeddings provide some relief through cross-lingual sentiment approaches. However, they either require large amounts of parallel data or do not sufficiently capture sentiment information. We introduce Bilingual Sentiment Embeddings (BLSE), which jointly represent sentiment information in a source and target language. This model only requires a small bilingual lexicon, a source-language corpus annotated for sentiment, and monolingual word embeddings for each language. We perform experiments on three language combinations (Spanish, Catalan, Basque) for sentence-level cross-lingual sentiment classification and find that our model significantly outperforms state-of-the-art methods on four out of six experimental setups, as well as capturing complementary information to machine translation. Our analysis of the resulting embedding space provides evidence that it represents sentiment information in the resource-poor target language without any annotated data in that language.",,,,ACL
232,2018,Learning Domain-Sensitive and Sentiment-Aware Word Embeddings,"Bei Shi, Zihao Fu, Lidong Bing, Wai Lam","Word embeddings have been widely used in sentiment classification because of their efficacy for semantic representations of words. Given reviews from different domains, some existing methods for word embeddings exploit sentiment information, but they cannot produce domain-sensitive embeddings. On the other hand, some other existing methods can generate domain-sensitive word embeddings, but they cannot distinguish words with similar contexts but opposite sentiment polarity. We propose a new method for learning domain-sensitive and sentiment-aware embeddings that simultaneously capture the information of sentiment semantics and domain sensitivity of individual words. Our method can automatically determine and produce domain-common embeddings and domain-specific embeddings. The differentiation of domain-common and domain-specific words enables the advantage of data augmentation of common semantics from multiple domains and capture the varied semantics of specific words from different domains at the same time. Experimental results show that our model provides an effective way to learn domain-sensitive and sentiment-aware word embeddings which benefit sentiment classification at both sentence level and lexicon term level.",,,,ACL
233,2018,Cross-Domain Sentiment Classification with Target Domain Specific Information,"Minlong Peng, Qi Zhang, Yu-gang Jiang, Xuanjing Huang","The task of adopting a model with good performance to a target domain that is different from the source domain used for training has received considerable attention in sentiment analysis. Most existing approaches mainly focus on learning representations that are domain-invariant in both the source and target domains. Few of them pay attention to domain-specific information, which should also be informative. In this work, we propose a method to simultaneously extract domain specific and invariant representations and train a classifier on each of the representation, respectively. And we introduce a few target domain labeled data for learning domain-specific information. To effectively utilize the target domain labeled data, we train the domain invariant representation based classifier with both the source and target domain labeled data and train the domain-specific representation based classifier with only the target domain labeled data. These two classifiers then boost each other in a co-training style. Extensive sentiment analysis experiments demonstrated that the proposed method could achieve better performance than state-of-the-art methods.",,,,ACL
234,2018,Aspect Based Sentiment Analysis with Gated Convolutional Networks,"Wei Xue, Tao Li","Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a model based on convolutional neural networks and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity. The architecture is much simpler than attention layer used in the existing models. Second, the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models.",,,,ACL
235,2018,A Helping Hand: Transfer Learning for Deep Sentiment Analysis,"Xin Dong, Gerard de Melo","Deep convolutional neural networks excel at sentiment polarity classification, but tend to require substantial amounts of training data, which moreover differs quite significantly between domains. In this work, we present an approach to feed generic cues into the training process of such networks, leading to better generalization abilities given limited training data. We propose to induce sentiment embeddings via supervision on extrinsic data, which are then fed into the model via a dedicated memory-based component. We observe significant gains in effectiveness on a range of different datasets in seven different languages.",,,,ACL
236,2018,Cold-Start Aware User and Product Attention for Sentiment Classification,"Reinald Kim Amplayo, Jihyeok Kim, Sua Sung, Seung-won Hwang","The use of user/product information in sentiment analysis is important, especially for cold-start users/products, whose number of reviews are very limited. However, current models do not deal with the cold-start problem which is typical in review websites. In this paper, we present Hybrid Contextualized Sentiment Classifier (HCSC), which contains two modules: (1) a fast word encoder that returns word vectors embedded with short and long range dependency features; and (2) Cold-Start Aware Attention (CSAA), an attention mechanism that considers the existence of cold-start problem when attentively pooling the encoded word vectors. HCSC introduces shared vectors that are constructed from similar users/products, and are used when the original distinct vectors do not have sufficient information (i.e. cold-start). This is decided by a frequency-guided selective gate vector. Our experiments show that in terms of RMSE, HCSC performs significantly better when compared with on famous datasets, despite having less complexity, and thus can be trained much faster. More importantly, our model performs significantly better than previous models when the training data is sparse and has cold-start problems.",,,,ACL
237,2018,Modeling Deliberative Argumentation Strategies on Wikipedia,"Khalid Al-Khatib, Henning Wachsmuth, Kevin Lang, Jakob Herpel","This paper studies how the argumentation strategies of participants in deliberative discussions can be supported computationally. Our ultimate goal is to predict the best next deliberative move of each participant. In this paper, we present a model for deliberative discussions and we illustrate its operationalization. Previous models have been built manually based on a small set of discussions, resulting in a level of abstraction that is not suitable for move recommendation. In contrast, we derive our model statistically from several types of metadata that can be used for move description. Applied to six million discussions from Wikipedia talk pages, our approach results in a model with 13 categories along three dimensions: discourse acts, argumentative relations, and frames. On this basis, we automatically generate a corpus with about 200,000 turns, labeled for the 13 categories. We then operationalize the model with three supervised classifiers and provide evidence that the proposed categories can be predicted.",,,,ACL
238,2018,"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning","Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut","We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",,,,ACL
239,2018,Learning Translations via Images with a Massively Multilingual Image Dataset,"John Hewitt, Daphne Ippolito, Brendan Callahan, Reno Kriz","We conduct the most comprehensive study to date into translating words via images. To facilitate research on the task, we introduce a large-scale multilingual corpus of images, each labeled with the word it represents. Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings. In contrast, we have collected by far the largest available dataset for this task, with images for approximately 10,000 words in each of 100 languages. We run experiments on a dozen high resource languages and 20 low resources languages, demonstrating the effect of word concreteness and part-of-speech on translation quality. %We find that while image features work best for concrete nouns, they are sometimes effective on other parts of speech. To improve image-based translation, we introduce a novel method of predicting word concreteness from images, which improves on a previous state-of-the-art unsupervised technique. This allows us to predict when image-based translation may be effective, enabling consistent improvements to a state-of-the-art text-based word translation system. Our code and the Massively Multilingual Image Dataset (MMID) are available at http://multilingual-images.org/.",,,,ACL
240,2018,On the Automatic Generation of Medical Imaging Reports,"Baoyu Jing, Pengtao Xie, Eric Xing","Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time-consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the reports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the prediction of tags and the generation of paragraphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available dataset.",,,,ACL
241,2018,Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning,"Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi","Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning. The proposed algorithm provides two evaluation approaches, which check if we can mislead neural image captioning systems to output some randomly chosen captions or keywords. Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords, and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.",,,,ACL
242,2018,Think Visually: Question Answering through Virtual Imagery,"Ankit Goyal, Jian Wang, Jia Deng","In this paper, we study the problem of geometric reasoning (a form of visual reasoning) in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture that specializes in answering questions that admit latent visual representations, and learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems. Experimental results validate the effectiveness of our proposed DSMN for visual thinking tasks.",,,,ACL
243,2018,Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game,"Haichao Zhang, Haonan Yu, Wei Xu","Building intelligent agents that can communicate with and learn from humans in natural language is of great value. Supervised language learning is limited by the ability of capturing mainly the statistics of training data, and is hardly adaptive to new scenarios or flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting. We highlight the perspective that conversational interaction serves as a natural interface both for language learning and for novel knowledge acquisition and propose a joint imitation and reinforcement approach for grounded language learning through an interactive conversational game. The agent trained with this approach is able to actively acquire information by asking questions about novel objects and use the just-learned knowledge in subsequent conversations in a one-shot fashion. Results compared with other methods verified the effectiveness of the proposed approach.",,,,ACL
244,2018,A Purely End-to-End System for Multi-speaker Speech Recognition,"Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Jonathan Le Roux","Recently, there has been growing interest in multi-speaker speech recognition, where the utterances of multiple speakers are recognized from their mixture. Promising techniques have been proposed for this task, but earlier works have required additional training data such as isolated source signals or senone alignments for effective learning. In this paper, we propose a new sequence-to-sequence framework to directly decode multiple label sequences from a single speech sequence by unifying source separation and speech recognition functions in an end-to-end manner. We further propose a new objective function to improve the contrast between the hidden vectors to avoid generating similar hypotheses. Experimental results show that the model is directly able to learn a mapping from a speech mixture to multiple label sequences, achieving 83.1% relative improvement compared to a model trained without the proposed objective. Interestingly, the results are comparable to those produced by previous end-to-end works featuring explicit separation and recognition modules.",,,,ACL
245,2018,A Structured Variational Autoencoder for Contextual Morphological Inflection,"Lawrence Wolf-Sonkin, Jason Naradowsky, Sebastian J. Mielke, Ryan Cotterell","Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases.",,,,ACL
246,2018,Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings,"Bernd Bohnet, Ryan McDonald, Gonçalo Simões, Daniel Andor","The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy. One characteristic common among these models is the presence of rich initial word encodings. These encodings typically are composed of a recurrent character-based representation with dynamically and pre-trained word embeddings. However, these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts. In this paper, we investigate models that use recurrent neural networks with sentence-level context for initial character and word-based representations. In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states.",,,,ACL
247,2018,Neural Factor Graph Models for Cross-lingual Morphological Tagging,"Chaitanya Malaviya, Matthew R. Gormley, Graham Neubig","Morphological analysis involves predicting the syntactic traits of a word (e.g. POS: Noun, Case: Acc, Gender: Fem). Previous work in morphological tagging improves performance for low-resource languages (LRLs) through cross-lingual training with a high-resource language (HRL) from the same family, but is limited by the strict, often false, assumption that tag sets exactly overlap between the HRL and LRL. In this paper we propose a method for cross-lingual morphological tagging that aims to improve information sharing between languages by relaxing this assumption. The proposed model uses factorial conditional random fields with neural network potentials, making it possible to (1) utilize the expressive power of neural network representations to smooth over superficial differences in the surface forms, (2) model pairwise and transitive relationships between tags, and (3) accurately generate tag sets that are unseen or rare in the training data. Experiments on four languages from the Universal Dependencies Treebank demonstrate superior tagging accuracies over existing cross-lingual approaches.",,,,ACL
248,2018,Global Transition-based Non-projective Dependency Parsing,"Carlos Gómez-Rodríguez, Tianze Shi, Lillian Lee","Shi, Huang, and Lee (2017a) obtained state-of-the-art results for English and Chinese dependency parsing by combining dynamic-programming implementations of transition-based dependency parsers with a minimal set of bidirectional LSTM features. However, their results were limited to projective parsing. In this paper, we extend their approach to support non-projectivity by providing the first practical implementation of the MH₄ algorithm, an O(n4) mildly nonprojective dynamic-programming parser with very high coverage on non-projective treebanks. To make MH₄ compatible with minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is effective than its projective counterpart in parsing a number of highly non-projective languages.",,,,ACL
249,2018,Constituency Parsing with a Self-Attentive Encoder,"Nikita Kitaev, Dan Klein","We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",,,,ACL
250,2018,Pre- and In-Parsing Models for Neural Empty Category Detection,"Yufei Chen, Yuanyuan Zhao, Weiwei Sun, Xiaojun Wan","Motivated by the positive impact of empty category on syntactic parsing, we study neural models for pre- and in-parsing detection of empty category, which has not previously been investigated. We find several non-obvious facts: (a) BiLSTM can capture non-local contextual information which is essential for detecting empty categories, (b) even with a BiLSTM, syntactic information is still able to enhance the detection, and (c) automatic detection of empty categories improves parsing quality for overt words. Our neural ECD models outperform the prior state-of-the-art by significant margins.",,,,ACL
251,2018,Composing Finite State Transducers on GPUs,"Arturo Argueta, David Chiang","Weighted finite state transducers (FSTs) are frequently used in language processing to handle tasks such as part-of-speech tagging and speech recognition. There has been previous work using multiple CPU cores to accelerate finite state algorithms, but limited attention has been given to parallel graphics processing unit (GPU) implementations. In this paper, we introduce the first (to our knowledge) GPU implementation of the FST composition operation, and we also discuss the optimizations used to achieve the best performance on this architecture. We show that our approach obtains speedups of up to 6 times over our serial implementation and 4.5 times over OpenFST.",,,,ACL
252,2018,Supervised Treebank Conversion: Data and Approaches,"Xinzhou Jiang, Zhenghua Li, Bo Zhang, Min Zhang","Treebank conversion is a straightforward and effective way to exploit various heterogeneous treebanks for boosting parsing performance. However, previous work mainly focuses on unsupervised treebank conversion and has made little progress due to the lack of manually labeled data where each sentence has two syntactic trees complying with two different guidelines at the same time, referred as bi-tree aligned data. In this work, we for the first time propose the task of supervised treebank conversion. First, we manually construct a bi-tree aligned dataset containing over ten thousand sentences. Then, we propose two simple yet effective conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two conversion approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multi-treebank exploitation and leads to significantly higher parsing accuracy.",,,,ACL
253,2018,Object-oriented Neural Programming (OONP) for Document Understanding,"Zhengdong Lu, Xianggen Liu, Haotian Cui, Yukun Yan","We propose Object-oriented Neural Programming (OONP), a framework for semantically parsing documents in specific domains. Basically, OONP reads a document and parses it into a predesigned object-oriented data structure that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the document, and builds and updates an intermediate ontology during the process to summarize its partial understanding of the text. OONP supports a big variety of forms (both symbolic and differentiable) for representing the state and the document, and a rich family of operations to compose the representation. An OONP parser can be trained with supervision of different forms and strength, including supervised learning (SL), reinforcement learning (RL) and hybrid of the two. Our experiments on both synthetic and real-world document parsing tasks have shown that OONP can learn to handle fairly complicated ontology with training data of modest sizes.",,,,ACL
254,2018,Finding syntax in human encephalography with beam search,"John Hale, Chris Dyer, Adhiguna Kuncoro, Jonathan Brennan","Recurrent neural network grammars (RNNGs) are generative models of (tree , string ) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.",,,,ACL
255,2018,Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information,"Sudha Rao, Hal Daumé III","Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. In this work, we build a neural network model for the task of ranking clarification questions. Our model is inspired by the idea of expected value of perfect information: a good question is one whose expected answer will be useful. We study this problem using data from StackExchange, a plentiful online resource in which people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster. We create a dataset of clarification questions consisting of 77K posts paired with a clarification question (and answer) from three domains of StackExchange: askubuntu, unix and superuser. We evaluate our model on 500 samples of this dataset against expert human judgments and demonstrate significant improvements over controlled baselines.",,,,ACL
256,2018,Let’s do it “again”: A First Computational Approach to Detecting Adverbial Presupposition Triggers,"Andre Cianflone, Yulan Feng, Jad Kabbara, Jackie Chi Kit Cheung","We introduce the novel task of predicting adverbial presupposition triggers, which is useful for natural language generation tasks such as summarization and dialogue systems. We introduce two new corpora, derived from the Penn Treebank and the Annotated English Gigaword dataset and investigate the use of a novel attention mechanism tailored to this task. Our attention mechanism augments a baseline recurrent neural network without the need for additional trainable parameters, minimizing the added computational cost of our mechanism. We demonstrate that this model statistically outperforms our baselines.",,,,ACL
1,2019,One Time of Interaction May Not Be Enough: Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues,"Chongyang Tao, Wei Wu, Can Xu, Wenpeng Hu, Dongyan Zhao","Currently, researchers have paid great attention to retrieval-based dialogues in open-domain. In particular, people study the problem by investigating context-response matching for multi-turn response selection based on publicly recognized benchmark data sets. State-of-the-art methods require a response to interact with each utterance in a context from the beginning, but the interaction is performed in a shallow way. In this work, we let utterance-response interaction go deep by proposing an interaction-over-interaction network (IoI). The model performs matching by stacking multiple interaction blocks in which residual information from one time of interaction initiates the interaction process again. Thus, matching information within an utterance-response pair is extracted from the interaction of the pair in an iterative fashion, and the information flows along the chain of the blocks via representations. Evaluation results on three benchmark data sets indicate that IoI can significantly outperform state-of-the-art methods in terms of various matching metrics. Through further analysis, we also unveil how the depth of interaction affects the performance of IoI.",,,,ACL
2,2019,Incremental Transformer with Deliberation Decoder for Document Grounded Conversations,"Zekang Li, Cheng Niu, Fandong Meng, Yang Feng, Qian Li","Document Grounded Conversations is a task to generate dialogue responses when chatting about the content of a given document. Obviously, document knowledge plays a critical role in Document Grounded Conversations, while existing dialogue models do not exploit this kind of knowledge effectively enough. In this paper, we propose a novel Transformer-based architecture for multi-turn document grounded conversations. In particular, we devise an Incremental Transformer to encode multi-turn utterances along with knowledge in related documents. Motivated by the human cognitive process, we design a two-pass decoder (Deliberation Decoder) to improve context coherence and knowledge correctness. Our empirical study on a real-world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance.",,,,ACL
3,2019,Improving Multi-turn Dialogue Modelling with Utterance ReWriter,"Hui Su, Xiaoyu Shen, Rongzhi Zhang, Fei Sun, Pengwei Hu","Recent research has achieved impressive results in single-turn dialogue modelling. In the multi-turn setting, however, current models are still far from satisfactory. One major challenge is the frequently occurred coreference and information omission in our daily conversation, making it hard for machines to understand the real intention. In this paper, we propose rewriting the human utterance as a pre-process to help multi-turn dialgoue modelling. Each utterance is first rewritten to recover all coreferred and omitted information. The next processing steps are then performed based on the rewritten utterance. To properly train the utterance rewriter, we collect a new dataset with human annotations and introduce a Transformer-based utterance rewriting architecture using the pointer network. We show the proposed architecture achieves remarkably good performance on the utterance rewriting task. The trained utterance rewriter can be easily integrated into online chatbots and brings general improvement over different domains.",,,,ACL
4,2019,Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study,"Chinnadhurai Sankar, Sandeep Subramanian, Chris Pal, Sarath Chandar, Yoshua Bengio","Neural generative models have been become increasingly popular when building conversational agents. They offer flexibility, can be easily adapted to new domains, and require minimal domain engineering. A common criticism of these systems is that they seldom understand or use the available dialog history effectively. In this paper, we take an empirical approach to understanding how these models use the available dialog history by studying the sensitivity of the models to artificially introduced unnatural changes or perturbations to their context at test time. We experiment with 10 different types of perturbations on 4 multi-turn dialog datasets and find that commonly used neural dialog architectures like recurrent and transformer-based seq2seq models are rarely sensitive to most perturbations such as missing or reordering utterances, shuffling words, etc. Also, by open-sourcing our code, we believe that it will serve as a useful diagnostic tool for evaluating dialog systems in the future.",,,,ACL
5,2019,Boosting Dialog Response Generation,"Wenchao Du, Alan W Black","Neural models have become one of the most important approaches to dialog response generation. However, they still tend to generate the most common and generic responses in the corpus all the time. To address this problem, we designed an iterative training process and ensemble method based on boosting. We combined our method with different training and decoding paradigms as the base model, including mutual-information-based decoding and reward-augmented maximum likelihood learning. Empirical results show that our approach can significantly improve the diversity and relevance of the responses generated by all base models, backed by objective measurements and human evaluation.",,,,ACL
6,2019,Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection,"Junyu Lu, Chenbin Zhang, Zeying Xie, Guang Ling, Tom Chao Zhou","Response selection plays an important role in fully automated dialogue systems. Given the dialogue context, the goal of response selection is to identify the best-matched next utterance (i.e., response) from multiple candidates. Despite the efforts of many previous useful models, this task remains challenging due to the huge semantic gap and also the large size of candidate set. To address these issues, we propose a Spatio-Temporal Matching network (STM) for response selection. In detail, soft alignment is first used to obtain the local relevance between the context and the response. And then, we construct spatio-temporal features by aggregating attention images in time dimension and make use of 3D convolution and pooling operations to extract matching information. Evaluation on two large-scale multi-turn response selection tasks has demonstrated that our proposed model significantly outperforms the state-of-the-art model. Particularly, visualization analysis shows that the spatio-temporal features enables matching information in segment pairs and time sequences, and have good interpretability for multi-turn text matching.",,,,ACL
7,2019,Semantic Parsing with Dual Learning,"Ruisheng Cao, Su Zhu, Chen Liu, Jieyu Li, Kai Yu","Semantic parsing converts natural language queries into structured logical forms. The lack of training data is still one of the most serious problems in this area. In this work, we develop a semantic parsing framework with the dual learning algorithm, which enables a semantic parser to make full use of data (labeled and even unlabeled) through a dual-learning game. This game between a primal model (semantic parsing) and a dual model (logical form to query) forces them to regularize each other, and can achieve feedback signals from some prior-knowledge. By utilizing the prior-knowledge of logical form structures, we propose a novel reward signal at the surface and semantic levels which tends to generate complete and reasonable logical forms. Experimental results show that our approach achieves new state-of-the-art performance on ATIS dataset and gets competitive performance on OVERNIGHT dataset.",,,,ACL
8,2019,Semantic Expressive Capacity with Bounded Memory,"Antoine Venant, Alexander Koller","We investigate the capacity of mechanisms for compositional semantic parsing to describe relations between sentences and semantic representations. We prove that in order to represent certain relations, mechanisms which are syntactically projective must be able to remember an unbounded number of locations in the semantic representations, where nonprojective mechanisms need not. This is the first result of this kind, and has consequences both for grammar-based and for neural systems.",,,,ACL
9,2019,AMR Parsing as Sequence-to-Graph Transduction,"Sheng Zhang, Xutai Ma, Kevin Duh, Benjamin Van Durme","We propose an attention-based model that treats AMR parsing as sequence-to-graph transduction. Unlike most AMR parsers that rely on pre-trained aligners, external semantic resources, or data augmentation, our proposed parser is aligner-free, and it can be effectively trained with limited amounts of labeled AMR data. Our experimental results outperform all previously reported SMATCH scores, on both AMR 2.0 (76.3% on LDC2017T10) and AMR 1.0 (70.2% on LDC2014T12).",,,,ACL
10,2019,Generating Logical Forms from Graph Representations of Text and Entities,"Peter Shaw, Philip Massey, Angelica Chen, Francesco Piccinno, Yasemin Altun","Structured information about entities is critical for many semantic parsing tasks. We present an approach that uses a Graph Neural Network (GNN) architecture to incorporate information about relevant entities and their relations during parsing. Combined with a decoder copy mechanism, this approach provides a conceptually simple mechanism to generate logical forms with entities. We demonstrate that this approach is competitive with the state-of-the-art across several tasks without pre-training, and outperforms existing approaches when combined with BERT pre-training.",,,,ACL
11,2019,Learning Compressed Sentence Representations for On-Device Text Processing,"Dinghan Shen, Pengyu Cheng, Dhanasekar Sundararaman, Xinyuan Zhang, Qian Yang","Vector representations of sentences, trained on massive text corpora, are widely used as generic sentence embeddings across a variety of NLP problems. The learned representations are generally assumed to be continuous and real-valued, giving rise to a large memory footprint and slow retrieval speed, which hinders their applicability to low-resource (memory and computation) platforms, such as mobile devices. In this paper, we propose four different strategies to transform continuous and generic sentence embeddings into a binarized form, while preserving their rich semantic information. The introduced methods are evaluated across a wide range of downstream tasks, where the binarized sentence embeddings are demonstrated to degrade performance by only about 2% relative to their continuous counterparts, while reducing the storage requirement by over 98%. Moreover, with the learned binary representations, the semantic relatedness of two sentences can be evaluated by simply calculating their Hamming distance, which is more computational efficient compared with the inner product operation between continuous embeddings. Detailed analysis and case study further validate the effectiveness of proposed methods.",,,,ACL
12,2019,The (Non-)Utility of Structural Features in BiLSTM-based Dependency Parsers,"Agnieszka Falenska, Jonas Kuhn","Classical non-neural dependency parsers put considerable effort on the design of feature functions. Especially, they benefit from information coming from structural features, such as features drawn from neighboring tokens in the dependency tree. In contrast, their BiLSTM-based successors achieve state-of-the-art performance without explicit information about the structural context. In this paper we aim to answer the question: How much structural context are the BiLSTM representations able to capture implicitly? We show that features drawn from partial subtrees become redundant when the BiLSTMs are used. We provide a deep insight into information flow in transition- and graph-based neural architectures to demonstrate where the implicit information comes from when the parsers make their decisions. Finally, with model ablations we demonstrate that the structural context is not only present in the models, but it significantly influences their performance.",,,,ACL
13,2019,Automatic Generation of High Quality CCGbanks for Parser Domain Adaptation,"Masashi Yoshikawa, Hiroshi Noji, Koji Mineshima, Daisuke Bekki","We propose a new domain adaptation method for Combinatory Categorial Grammar (CCG) parsing, based on the idea of automatic generation of CCG corpora exploiting cheaper resources of dependency trees. Our solution is conceptually simple, and not relying on a specific parser architecture, making it applicable to the current best-performing parsers. We conduct extensive parsing experiments with detailed discussion; on top of existing benchmark datasets on (1) biomedical texts and (2) question sentences, we create experimental datasets of (3) speech conversation and (4) math problems. When applied to the proposed method, an off-the-shelf CCG parser shows significant performance gains, improving from 90.7% to 96.6% on speech conversation, and from 88.5% to 96.8% on math problems.",,,,ACL
14,2019,A Joint Named-Entity Recognizer for Heterogeneous Tag-sets Using a Tag Hierarchy,"Genady Beryozkin, Yoel Drori, Oren Gilon, Tzvika Hartman, Idan Szpektor","We study a variant of domain adaptation for named-entity recognition where multiple, heterogeneously tagged training sets are available. Furthermore, the test tag-set is not identical to any individual training tag-set. Yet, the relations between all tags are provided in a tag hierarchy, covering the test tags as a combination of training tags. This setting occurs when various datasets are created using different annotation schemes. This is also the case of extending a tag-set with a new tag by annotating only the new tag in a new dataset. We propose to use the given tag hierarchy to jointly learn a neural network that shares its tagging layer among all tag-sets. We compare this model to combining independent models and to a model based on the multitasking approach. Our experiments show the benefit of the tag-hierarchy model, especially when facing non-trivial consolidation of tag-sets.",,,,ACL
15,2019,Massively Multilingual Transfer for NER,"Afshin Rahimi, Yuan Li, Trevor Cohn","In cross-lingual transfer, NLP models over one or more source languages are applied to a low-resource target language. While most prior work has used a single source model or a few carefully selected models, here we consider a “massive” setting with many such models. This setting raises the problem of poor transfer, particularly from distant languages. We propose two techniques for modulating the transfer, suitable for zero-shot or few-shot learning, respectively. Evaluating on named entity recognition, we show that our techniques are much more effective than strong baselines, including standard ensembling, and our unsupervised method rivals oracle selection of the single best individual model.",,,,ACL
16,2019,Reliability-aware Dynamic Feature Composition for Name Tagging,"Ying Lin, Liyuan Liu, Heng Ji, Dong Yu, Jiawei Han","Word embeddings are widely used on a variety of tasks and can substantially improve the performance. However, their quality is not consistent throughout the vocabulary due to the long-tail distribution of word frequency. Without sufficient contexts, rare word embeddings are usually less reliable than those of common words. However, current models typically trust all word embeddings equally regardless of their reliability and thus may introduce noise and hurt the performance. Since names often contain rare and uncommon words, this problem is particularly critical for name tagging. In this paper, we propose a novel reliability-aware name tagging model to tackle this issue. We design a set of word frequency-based reliability signals to indicate the quality of each word embedding. Guided by the reliability signals, the model is able to dynamically select and compose features such as word embedding and character-level representation using gating mechanisms. For example, if an input word is rare, the model relies less on its word embedding and assigns higher weights to its character and contextual features. Experiments on OntoNotes 5.0 show that our model outperforms the baseline model by 2.7% absolute gain in F-score. In cross-genre experiments on five genres in OntoNotes, our model improves the performance for most genre pairs and obtains up to 5% absolute F-score gain.",,,,ACL
17,2019,Unsupervised Pivot Translation for Distant Languages,"Yichong Leng, Xu Tan, Tao Qin, Xiang-Yang Li, Tie-Yan Liu","Unsupervised neural machine translation (NMT) has attracted a lot of attention recently. While state-of-the-art methods for unsupervised translation usually perform well between similar languages (e.g., English-German translation), they perform poorly between distant languages, because unsupervised alignment does not work well for distant languages. In this work, we introduce unsupervised pivot translation for distant languages, which translates a language to a distant language through multiple hops, and the unsupervised translation on each hop is relatively easier than the original direct translation. We propose a learning to route (LTR) method to choose the translation path between the source and target languages. LTR is trained on language pairs whose best translation path is available and is applied on the unseen language pairs for path selection. Experiments on 20 languages and 294 distant language pairs demonstrate the advantages of the unsupervised pivot translation for distant languages, as well as the effectiveness of the proposed LTR for path selection. Specifically, in the best case, LTR achieves an improvement of 5.58 BLEU points over the conventional direct unsupervised method.",,,,ACL
18,2019,Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces,"Barun Patra, Joel Ruben Antony Moniz, Sarthak Garg, Matthew R. Gormley, Graham Neubig","Recent work on bilingual lexicon induction (BLI) has frequently depended either on aligned bilingual lexicons or on distribution matching, often with an assumption about the isometry of the two spaces. We propose a technique to quantitatively estimate this assumption of the isometry between two embedding spaces and empirically show that this assumption weakens as the languages in question become increasingly etymologically distant. We then propose Bilingual Lexicon Induction with Semi-Supervision (BLISS) — a semi-supervised approach that relaxes the isometric assumption while leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don’t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.",,,,ACL
19,2019,An Effective Approach to Unsupervised Machine Translation,"Mikel Artetxe, Gorka Labaka, Eneko Agirre","While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014.",,,,ACL
20,2019,Effective Adversarial Regularization for Neural Machine Translation,"Motoki Sato, Jun Suzuki, Shun Kiyono","A regularization technique based on adversarial perturbation, which was initially developed in the field of image processing, has been successfully applied to text classification tasks and has yielded attractive improvements. We aim to further leverage this promising methodology into more sophisticated and critical neural models in the natural language processing field, i.e., neural machine translation (NMT) models. However, it is not trivial to apply this methodology to such models. Thus, this paper investigates the effectiveness of several possible configurations of applying the adversarial perturbation and reveals that the adversarial regularization technique can significantly and consistently improve the performance of widely used NMT models, such as LSTM-based and Transformer-based models.",,,,ACL
21,2019,Revisiting Low-Resource Neural Machine Translation: A Case Study,"Rico Sennrich, Biao Zhang","It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (PBSMT) and requiring large amounts of auxiliary data to achieve competitive results. In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. We discuss some pitfalls to be aware of when training low-resource NMT systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource NMT. In our experiments on German–English with different amounts of IWSLT14 training data, we show that, without the use of any auxiliary monolingual or multilingual data, an optimized NMT system can outperform PBSMT with far less data than previously claimed. We also apply these techniques to a low-resource Korean–English dataset, surpassing previously reported results by 4 BLEU.",,,,ACL
22,2019,Domain Adaptive Inference for Neural Machine Translation,"Danielle Saunders, Felix Stahlberg, Adrià de Gispert, Bill Byrne","We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and report strong improvements across test domains without access to the domain label.",,,,ACL
23,2019,Neural Relation Extraction for Knowledge Base Enrichment,"Bayu Distiawan Trisedya, Gerhard Weikum, Jianzhong Qi, Rui Zhang","We study relation extraction for knowledge base (KB) enrichment. Specifically, we aim to extract entities and their relationships from sentences in the form of triples and map the elements of the extracted triples to an existing KB in an end-to-end manner. Previous studies focus on the extraction itself and rely on Named Entity Disambiguation (NED) to map triples into the KB space. This way, NED errors may cause extraction errors that affect the overall precision and recall.To address this problem, we propose an end-to-end relation extraction model for KB enrichment based on a neural encoder-decoder model. We collect high-quality training data by distant supervision with co-reference resolution and paraphrase detection. We propose an n-gram based attention model that captures multi-word entity names in a sentence. Our model employs jointly learned word and entity embeddings to support named entity disambiguation. Finally, our model uses a modified beam search and a triple classifier to help generate high-quality triples. Our model outperforms state-of-the-art baselines by 15.51% and 8.38% in terms of F1 score on two real-world datasets.",,,,ACL
24,2019,Attention Guided Graph Convolutional Networks for Relation Extraction,"Zhijiang Guo, Yan Zhang, Wei Lu","Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches.",,,,ACL
25,2019,Spatial Aggregation Facilitates Discovery of Spatial Topics,"Aniruddha Maiti, Slobodan Vucetic","Spatial aggregation refers to merging of documents created at the same spatial location. We show that by spatial aggregation of a large collection of documents and applying a traditional topic discovery algorithm on the aggregated data we can efficiently discover spatially distinct topics. By looking at topic discovery through matrix factorization lenses we show that spatial aggregation allows low rank approximation of the original document-word matrix, in which spatially distinct topics are preserved and non-spatial topics are aggregated into a single topic. Our experiments on synthetic data confirm this observation. Our experiments on 4.7 million tweets collected during the Sandy Hurricane in 2012 show that spatial and temporal aggregation allows rapid discovery of relevant spatial and temporal topics during that period. Our work indicates that different forms of document aggregation might be effective in rapid discovery of various types of distinct topics from large collections of documents.",,,,ACL
26,2019,Relation Embedding with Dihedral Group in Knowledge Graph,"Canran Xu, Ruijiang Li","Link prediction is critical for the application of incomplete knowledge graph (KG) in the downstream tasks. As a family of effective approaches for link predictions, embedding methods try to learn low-rank representations for both entities and relations such that the bilinear form defined therein is a well-behaved scoring function. Despite of their successful performances, existing bilinear forms overlook the modeling of relation compositions, resulting in lacks of interpretability for reasoning on KG. To fulfill this gap, we propose a new model called DihEdral, named after dihedral symmetry group. This new model learns knowledge graph embeddings that can capture relation compositions by nature. Furthermore, our approach models the relation embeddings parametrized by discrete values, thereby decrease the solution space drastically. Our experiments show that DihEdral is able to capture all desired properties such as (skew-) symmetry, inversion and (non-) Abelian composition, and outperforms existing bilinear form based approach and is comparable to or better than deep learning models such as ConvE.",,,,ACL
27,2019,Sequence Tagging with Contextual and Non-Contextual Subword Representations: A Multilingual Evaluation,"Benjamin Heinzerling, Michael Strube","Pretrained contextual and non-contextual subword embeddings have become available in over 250 languages, allowing massively multilingual NLP. However, while there is no dearth of pretrained embeddings, the distinct lack of systematic evaluations makes it difficult for practitioners to choose between them. In this work, we conduct an extensive evaluation comparing non-contextual subword embeddings, namely FastText and BPEmb, and a contextual representation method, namely BERT, on multilingual named entity recognition and part-of-speech tagging. We find that overall, a combination of BERT, BPEmb, and character representations works best across languages and tasks. A more detailed analysis reveals different strengths and weaknesses: Multilingual BERT performs well in medium- to high-resource languages, but is outperformed by non-contextual subword embeddings in a low-resource setting.",,,,ACL
28,2019,Augmenting Neural Networks with First-order Logic,"Tao Li, Vivek Srikumar","Today, the dominant paradigm for training neural networks involves minimizing task loss on a large dataset. Using world knowledge to inform a model, and yet retain the ability to perform end-to-end training remains an open question. In this paper, we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction. Our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over baselines, especially in low-data regimes.",,,,ACL
29,2019,Self-Regulated Interactive Sequence-to-Sequence Learning,"Julia Kreutzer, Stefan Riezler","Not all types of supervision signals are created equal: Different types of feedback have different costs and effects on learning. We show how self-regulation strategies that decide when to ask for which kind of feedback from a teacher (or from oneself) can be cast as a learning-to-learn problem leading to improved cost-aware sequence-to-sequence learning. In experiments on interactive neural machine translation, we find that the self-regulator discovers an 𝜖-greedy strategy for the optimal cost-quality trade-off by mixing different feedback types including corrections, error markups, and self-supervision. Furthermore, we demonstrate its robustness under domain shift and identify it as a promising alternative to active learning.",,,,ACL
30,2019,You Only Need Attention to Traverse Trees,"Mahtab Ahmed, Muhammad Rifayat Samee, Robert E. Mercer","In recent NLP research, a topic of interest is universal sentence encoding, sentence representations that can be used in any supervised task. At the word sequence level, fully attention-based models suffer from two problems: a quadratic increase in memory consumption with respect to the sentence length and an inability to capture and use syntactic information. Recursive neural nets can extract very good syntactic information by traversing a tree structure. To this end, we propose Tree Transformer, a model that captures phrase level syntax for constituency trees as well as word-level dependencies for dependency trees by doing recursive traversal only with attention. Evaluation of this model on four tasks gets noteworthy results compared to the standard transformer and LSTM-based models as well as tree-structured LSTMs. Ablation studies to find whether positional information is inherently encoded in the trees and which type of attention is suitable for doing the recursive traversal are provided.",,,,ACL
31,2019,Cross-Domain Generalization of Neural Constituency Parsers,"Daniel Fried, Nikita Kitaev, Dan Klein","Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing—but to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.",,,,ACL
32,2019,Adaptive Attention Span in Transformers,"Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin","We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.",,,,ACL
33,2019,Neural News Recommendation with Long- and Short-term User Representations,"Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu","Personalized news recommendation is important to help users find their interested news and improve reading experience. A key problem in news recommendation is learning accurate user representations to capture their interests. Users usually have both long-term preferences and short-term interests. However, existing news recommendation methods usually learn single representations of users, which may be insufficient. In this paper, we propose a neural news recommendation approach which can learn both long- and short-term user representations. The core of our approach is a news encoder and a user encoder. In the news encoder, we learn representations of news from their titles and topic categories, and use attention network to select important words. In the user encoder, we propose to learn long-term user representations from the embeddings of their IDs.In addition, we propose to learn short-term user representations from their recently browsed news via GRU network. Besides, we propose two methods to combine long-term and short-term user representations. The first one is using the long-term user representation to initialize the hidden state of the GRU network in short-term user representation. The second one is concatenating both long- and short-term user representations as a unified user vector. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of neural news recommendation.",,,,ACL
34,2019,Automatic Domain Adaptation Outperforms Manual Domain Adaptation for Predicting Financial Outcomes,"Marina Sedinkina, Nikolas Breitkopf, Hinrich Schütze","In this paper, we automatically create sentiment dictionaries for predicting financial outcomes. We compare three approaches: (i) manual adaptation of the domain-general dictionary H4N, (ii) automatic adaptation of H4N and (iii) a combination consisting of first manual, then automatic adaptation. In our experiments, we demonstrate that the automatically adapted sentiment dictionary outperforms the previous state of the art in predicting the financial outcomes excess return and volatility. In particular, automatic adaptation performs better than manual adaptation. In our analysis, we find that annotation based on an expert’s a priori belief about a word’s meaning can be incorrect – annotation should be performed based on the word’s contexts in the target domain instead.",,,,ACL
35,2019,Manipulating the Difficulty of C-Tests,"Ji-Ung Lee, Erik Schwan, Christian M. Meyer","We propose two novel manipulation strategies for increasing and decreasing the difficulty of C-tests automatically. This is a crucial step towards generating learner-adaptive exercises for self-directed language learning and preparing language assessment tests. To reach the desired difficulty level, we manipulate the size and the distribution of gaps based on absolute and relative gap difficulty predictions. We evaluate our approach in corpus-based experiments and in a user study with 60 participants. We find that both strategies are able to generate C-tests with the desired difficulty level.",,,,ACL
36,2019,Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings,"Zied Haj-Yahia, Adrien Sieg, Léa A. Deleris","Text classification aims at mapping documents into a set of predefined categories. Supervised machine learning models have shown great success in this area but they require a large number of labeled documents to reach adequate accuracy. This is particularly true when the number of target categories is in the tens or the hundreds. In this work, we explore an unsupervised approach to classify documents into categories simply described by a label. The proposed method is inspired by the way a human proceeds in this situation: It draws on textual similarity between the most relevant words in each document and a dictionary of keywords for each category reflecting its semantics and lexical field. The novelty of our method hinges on the enrichment of the category labels through a combination of human expertise and language models, both generic and domain specific. Our experiments on 5 standard corpora show that the proposed method increases F1-score over relying solely on human expertise and can also be on par with simple supervised approaches. It thus provides a practical alternative to situations where low cost text categorization is needed, as we illustrate with our application to operational risk incidents classification.",,,,ACL
37,2019,Neural Text Simplification of Clinical Letters with a Domain Specific Phrase Table,"Matthew Shardlow, Raheel Nawaz","Clinical letters are infamously impenetrable for the lay patient. This work uses neural text simplification methods to automatically improve the understandability of clinical letters for patients. We take existing neural text simplification software and augment it with a new phrase table that links complex medical terminology to simpler vocabulary by mining SNOMED-CT. In an evaluation task using crowdsourcing, we show that the results of our new system are ranked easier to understand (average rank 1.93) than using the original system (2.34) without our phrase table. We also show improvement against baselines including the original text (2.79) and using the phrase table without the neural text simplification software (2.94). Our methods can easily be transferred outside of the clinical domain by using domain-appropriate resources to provide effective neural text simplification for any domain without the need for costly annotation.",,,,ACL
38,2019,What You Say and How You Say It Matters: Predicting Stock Volatility Using Verbal and Vocal Cues,"Yu Qin, Yi Yang","Predicting financial risk is an essential task in financial market. Prior research has shown that textual information in a firm’s financial statement can be used to predict its stock’s risk level. Nowadays, firm CEOs communicate information not only verbally through press releases and financial reports, but also nonverbally through investor meetings and earnings conference calls. There are anecdotal evidences that CEO’s vocal features, such as emotions and voice tones, can reveal the firm’s performance. However, how vocal features can be used to predict risk levels, and to what extent, is still unknown. To fill the gap, we obtain earnings call audio recordings and textual transcripts for S&P 500 companies in recent years. We propose a multimodal deep regression model (MDRM) that jointly model CEO’s verbal (from text) and vocal (from audio) information in a conference call. Empirical results show that our model that jointly considers verbal and vocal features achieves significant and substantial prediction error reduction. We also discuss several interesting findings and the implications to financial markets. The processed earnings conference calls data (text and audio) are released for readers who are interested in reproducing the results or designing trading strategy.",,,,ACL
39,2019,Detecting Concealed Information in Text and Speech,Shengli Hu,"Motivated by infamous cheating scandals in the media industry, the wine industry, and political campaigns, we address the problem of detecting concealed information in technical settings. In this work, we explore acoustic-prosodic and linguistic indicators of information concealment by collecting a unique corpus of professionals practicing for oral exams while concealing information. We reveal subtle signs of concealing information in speech and text, compare and contrast them with those in deception detection literature, uncovering the link between concealing information and deception. We then present a series of experiments that automatically detect concealed information from text and speech. We compare the use of acoustic-prosodic, linguistic, and individual feature sets, using different machine learning models. Finally, we present a multi-task learning framework with acoustic, linguistic, and individual features, that outperforms human performance by over 15%.",,,,ACL
40,2019,Evidence-based Trustworthiness,"Yi Zhang, Zachary Ives, Dan Roth","The information revolution brought with it information pollution. Information retrieval and extraction help us cope with abundant information from diverse sources. But some sources are of anonymous authorship, and some are of uncertain accuracy, so how can we determine what we should actually believe? Not all information sources are equally trustworthy, and simply accepting the majority view is often wrong. This paper develops a general framework for estimating the trustworthiness of information sources in an environment where multiple sources provide claims and supporting evidence, and each claim can potentially be produced by multiple sources. We consider two settings: one in which information sources directly assert claims, and a more realistic and challenging one, in which claims are inferred from evidence provided by sources, via (possibly noisy) NLP techniques. Our key contribution is to develop a family of probabilistic models that jointly estimate the trustworthiness of sources, and the credibility of claims they assert. This is done while accounting for the (possibly noisy) NLP needed to infer claims from evidence supplied by sources. We evaluate our framework on several datasets, showing strong results and significant improvement over baselines.",,,,ACL
41,2019,Disentangled Representation Learning for Non-Parallel Text Style Transfer,"Vineet John, Lili Mou, Hareesh Bahuleyan, Olga Vechtomova","This paper tackles the problem of disentangling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for style prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches.",,,,ACL
42,2019,Cross-Sentence Grammatical Error Correction,"Shamil Chollampatt, Weiqi Wang, Hwee Tou Ng","Automatic grammatical error correction (GEC) research has made remarkable progress in the past decade. However, all existing approaches to GEC correct errors by considering a single sentence alone and ignoring crucial cross-sentence context. Some errors can only be corrected reliably using cross-sentence context and models can also benefit from the additional contextual information in correcting other errors. In this paper, we address this serious limitation of existing approaches and improve strong neural encoder-decoder models by appropriately modeling wider contexts. We employ an auxiliary encoder that encodes previous sentences and incorporate the encoding in the decoder via attention and gating mechanisms. Our approach results in statistically significant improvements in overall GEC performance over strong baselines across multiple test sets. Analysis of our cross-sentence GEC model on a synthetic dataset shows high performance in verb tense corrections that require cross-sentence context.",,,,ACL
43,2019,This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation,"Rui Zhang, Joel Tetreault","Given the overwhelming number of emails, an effective subject line becomes essential to better inform the recipient of the email’s content. In this paper, we propose and study the task of email subject line generation: automatically generating an email subject line from the email body. We create the first dataset for this task and find that email subject line generation favor extremely abstractive summary which differentiates it from news headline generation or news single document summarization. We then develop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems. We also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric. Our system outperforms competitive baselines given both automatic and human evaluations. To our knowledge, this is the first work to tackle the problem of effective email subject line generation.",,,,ACL
44,2019,Time-Out: Temporal Referencing for Robust Modeling of Lexical Semantic Change,"Haim Dubossarsky, Simon Hengchen, Nina Tahmasebi, Dominik Schlechtweg","State-of-the-art models of lexical semantic change detection suffer from noise stemming from vector space alignment. We have empirically tested the Temporal Referencing method for lexical semantic change and show that, by avoiding alignment, it is less affected by this noise. We show that, trained on a diachronic corpus, the skip-gram with negative sampling architecture with temporal referencing outperforms alignment models on a synthetic task as well as a manual testset. We introduce a principled way to simulate lexical semantic change and systematically control for possible biases.",,,,ACL
45,2019,Adversarial Attention Modeling for Multi-dimensional Emotion Regression,"Suyang Zhu, Shoushan Li, Guodong Zhou","In this paper, we propose a neural network-based approach, namely Adversarial Attention Network, to the task of multi-dimensional emotion regression, which automatically rates multiple emotion dimension scores for an input text. Especially, to determine which words are valuable for a particular emotion dimension, an attention layer is trained to weight the words in an input sequence. Furthermore, adversarial training is employed between two attention layers to learn better word weights via a discriminator. In particular, a shared attention layer is incorporated to learn public word weights between two emotion dimensions. Empirical evaluation on the EMOBANK corpus shows that our approach achieves notable improvements in r-values on both EMOBANK Reader’s and Writer’s multi-dimensional emotion regression tasks in all domains over the state-of-the-art baselines.",,,,ACL
46,2019,"Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing","Sijie Mai, Haifeng Hu, Songlong Xing","We propose a general strategy named ‘divide, conquer and combine’ for multimodal fusion. Instead of directly fusing features at holistic level, we conduct fusion hierarchically so that both local and global interactions are considered for a comprehensive interpretation of multimodal embeddings. In the ‘divide’ and ‘conquer’ stages, we conduct local fusion by exploring the interaction of a portion of the aligned feature vectors across various modalities lying within a sliding window, which ensures that each part of multimodal embeddings are explored sufficiently. On its basis, global fusion is conducted in the ‘combine’ stage to explore the interconnection across local interactions, via an Attentive Bi-directional Skip-connected LSTM that directly connects distant local interactions and integrates two levels of attention mechanism. In this way, local interactions can exchange information sufficiently and thus obtain an overall view of multimodal information. Our method achieves state-of-the-art performance on multimodal affective computing with higher efficiency.",,,,ACL
47,2019,Modeling Financial Analysts’ Decision Making via the Pragmatics and Semantics of Earnings Calls,"Katherine Keith, Amanda Stent","Every fiscal quarter, companies hold earnings calls in which company executives respond to questions from analysts. After these calls, analysts often change their price target recommendations, which are used in equity re- search reports to help investors make deci- sions. In this paper, we examine analysts’ decision making behavior as it pertains to the language content of earnings calls. We identify a set of 20 pragmatic features of analysts’ questions which we correlate with analysts’ pre-call investor recommendations. We also analyze the degree to which semantic and pragmatic features from an earnings call complement market data in predicting analysts’ post-call changes in price targets. Our results show that earnings calls are moderately predictive of analysts’ decisions even though these decisions are influenced by a number of other factors including private communication with company executives and market conditions. A breakdown of model errors indicates disparate performance on calls from different market sectors.",,,,ACL
48,2019,An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis,"Ruidan He, Wee Sun Lee, Hwee Tou Ng, Daniel Dahlmeier","Aspect-based sentiment analysis produces a list of aspect terms and their corresponding sentiments for a natural language sentence. This task is usually done in a pipeline manner, with aspect term extraction performed first, followed by sentiment predictions toward the extracted aspect terms. While easier to develop, such an approach does not fully exploit joint information from the two subtasks and does not use all available sources of training information that might be helpful, such as document-level labeled sentiment corpus. In this paper, we propose an interactive multi-task learning network (IMN) which is able to jointly learn multiple related tasks simultaneously at both the token level as well as the document level. Unlike conventional multi-task learning methods that rely on learning common features for the different tasks, IMN introduces a message passing architecture where information is iteratively passed to different tasks through a shared set of latent variables. Experimental results demonstrate superior performance of the proposed method against multiple baselines on three benchmark datasets.",,,,ACL
49,2019,Decompositional Argument Mining: A General Purpose Approach for Argument Graph Construction,"Debela Gemechu, Chris Reed","This work presents an approach decomposing propositions into four functional components and identify the patterns linking those components to determine argument structure. The entities addressed by a proposition are target concepts and the features selected to make a point about the target concepts are aspects. A line of reasoning is followed by providing evidence for the points made about the target concepts via aspects. Opinions on target concepts and opinions on aspects are used to support or attack the ideas expressed by target concepts and aspects. The relations between aspects, target concepts, opinions on target concepts and aspects are used to infer the argument relations. Propositions are connected iteratively to form a graph structure. The approach is generic in that it is not tuned for a specific corpus and evaluated on three different corpora from the literature: AAEC, AMT, US2016G1tv and achieved an F score of 0.79, 0.77 and 0.64, respectively.",,,,ACL
50,2019,MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations,"Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria","Emotion recognition in conversations is a challenging task that has recently gained popularity due to its potential applications. Until now, however, a large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing. Thus, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends. Each utterance is annotated with emotion and sentiment labels, and encompasses audio, visual and textual modalities. We propose several strong multimodal baselines and show the importance of contextual and multimodal information for emotion recognition in conversations. The full dataset is available for use at http://affective-meld.github.io.",,,,ACL
51,2019,Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Classification,"Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, Yiwei Lv","Open-domain targeted sentiment analysis aims to detect opinion targets along with their sentiment polarities from a sentence. Prior work typically formulates this task as a sequence tagging problem. However, such formulation suffers from problems such as huge search space and sentiment inconsistency. To address these problems, we propose a span-based extract-then-classify framework, where multiple opinion targets are directly extracted from the sentence under the supervision of target span boundaries, and corresponding polarities are then classified using their span representations. We further investigate three approaches under this framework, namely the pipeline, joint, and collapsed models. Experiments on three benchmark datasets show that our approach consistently outperforms the sequence tagging baseline. Moreover, we find that the pipeline model achieves the best performance compared with the other two models.",,,,ACL
52,2019,Transfer Capsule Network for Aspect Level Sentiment Classification,"Zhuang Chen, Tieyun Qian","Aspect-level sentiment classification aims to determine the sentiment polarity of a sentence towards an aspect. Due to the high cost in annotation, the lack of aspect-level labeled data becomes a major obstacle in this area. On the other hand, document-level labeled data like reviews are easily accessible from online websites. These reviews encode sentiment knowledge in abundant contexts. In this paper, we propose a Transfer Capsule Network (TransCap) model for transferring document-level knowledge to aspect-level sentiment classification. To this end, we first develop an aspect routing approach to encapsulate the sentence-level semantic representations into semantic capsules from both the aspect-level and document-level data. We then extend the dynamic routing approach to adaptively couple the semantic capsules with the class capsules under the transfer learning framework. Experiments on SemEval datasets demonstrate the effectiveness of TransCap.",,,,ACL
53,2019,Progressive Self-Supervised Attention Learning for Aspect-Level Sentiment Analysis,"Jialong Tang, Ziyao Lu, Jinsong Su, Yubin Ge, Linfeng Song","In aspect-level sentiment classification (ASC), it is prevalent to equip dominant neural models with attention mechanisms, for the sake of acquiring the importance of each context word on the given aspect. However, such a mechanism tends to excessively focus on a few frequent words with sentiment polarities, while ignoring infrequent ones. In this paper, we propose a progressive self-supervised attention learning approach for neural ASC models, which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms. Specifically, we iteratively conduct sentiment predictions on all training instances. Particularly, at each iteration, the context word with the maximum attention weight is extracted as the one with active/misleading influence on the correct/incorrect prediction of every instance, and then the word itself is masked for subsequent iterations. Finally, we augment the conventional training objective with a regularization term, which enables ASC models to continue equally focusing on the extracted active context words while decreasing weights of those misleading ones. Experimental results on multiple datasets show that our proposed approach yields better attention mechanisms, leading to substantial improvements over the two state-of-the-art neural ASC models. Source code and trained models are available at https://github.com/DeepLearnXMU/PSSAttention.",,,,ACL
54,2019,Classification and Clustering of Arguments with Contextualized Word Embeddings,"Nils Reimers, Benjamin Schiller, Tilman Beck, Johannes Daxenberger, Christian Stab","We experiment with two recent contextualized word embedding methods (ELMo and BERT) in the context of open-domain argument search. For the first time, we show how to leverage the power of contextualized word embeddings to classify and cluster topic-dependent arguments, achieving impressive results on both tasks and across multiple datasets. For argument classification, we improve the state-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8 percentage points and for the IBM Debater - Evidence Sentences dataset by 7.4 percentage points. For the understudied task of argument clustering, we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the Argument Facet Similarity (AFS) Corpus.",,,,ACL
55,2019,Sentiment Tagging with Partial Labels using Modular Architectures,"Xiao Zhang, Dan Goldwasser","Many NLP learning tasks can be decomposed into several distinct sub-tasks, each associated with a partial label. In this paper we focus on a popular class of learning problems, sequence prediction applied to several sentiment analysis tasks, and suggest a modular learning approach in which different sub-tasks are learned using separate functional modules, combined to perform the final task while sharing information. Our experiments show this approach helps constrain the learning process and can alleviate some of the supervision efforts.",,,,ACL
56,2019,DOER: Dual Cross-Shared RNN for Aspect Term-Polarity Co-Extraction,"Huaishao Luo, Tianrui Li, Bing Liu, Junbo Zhang","This paper focuses on two related subtasks of aspect-based sentiment analysis, namely aspect term extraction and aspect sentiment classification, which we call aspect term-polarity co-extraction. The former task is to extract aspects of a product or service from an opinion document, and the latter is to identify the polarity expressed in the document about these extracted aspects. Most existing algorithms address them as two separate tasks and solve them one by one, or only perform one task, which can be complicated for real applications. In this paper, we treat these two tasks as two sequence labeling problems and propose a novel Dual crOss-sharEd RNN framework (DOER) to generate all aspect term-polarity pairs of the input sentence simultaneously. Specifically, DOER involves a dual recurrent neural network to extract the respective representation of each task, and a cross-shared unit to consider the relationship between them. Experimental results demonstrate that the proposed framework outperforms state-of-the-art baselines on three benchmark datasets.",,,,ACL
57,2019,A Corpus for Modeling User and Language Effects in Argumentation on Online Debating,"Esin Durmus, Claire Cardie","Existing argumentation datasets have succeeded in allowing researchers to develop computational methods for analyzing the content, structure and linguistic features of argumentative text. They have been much less successful in fostering studies of the effect of “user” traits — characteristics and beliefs of the participants — on the debate/argument outcome as this type of user information is generally not available. This paper presents a dataset of 78,376 debates generated over a 10-year period along with surprisingly comprehensive participant profiles. We also complete an example study using the dataset to analyze the effect of selected user traits on the debate outcome in comparison to the linguistic features typically employed in studies of this kind.",,,,ACL
58,2019,Topic Tensor Network for Implicit Discourse Relation Recognition in Chinese,"Sheng Xu, Peifeng Li, Fang Kong, Qiaoming Zhu, Guodong Zhou","In the literature, most of the previous studies on English implicit discourse relation recognition only use sentence-level representations, which cannot provide enough semantic information in Chinese due to its unique paratactic characteristics. In this paper, we propose a topic tensor network to recognize Chinese implicit discourse relations with both sentence-level and topic-level representations. In particular, besides encoding arguments (discourse units) using a gated convolutional network to obtain sentence-level representations, we train a simplified topic model to infer the latent topic-level representations. Moreover, we feed the two pairs of representations to two factored tensor networks, respectively, to capture both the sentence-level interactions and topic-level relevance using multi-slice tensors. Experimentation on CDTB, a Chinese discourse corpus, shows that our proposed model significantly outperforms several state-of-the-art baselines in both micro and macro F1-scores.",,,,ACL
59,2019,Learning from Omission,"Bill McDowell, Noah Goodman","Pragmatic reasoning allows humans to go beyond the literal meaning when interpret- ing language in context. Previous work has shown that such reasoning can improve the performance of already-trained language understanding systems. Here, we explore whether pragmatic reasoning during training can improve the quality of learned meanings. Our experiments on reference game data show that end-to-end pragmatic training produces more accurate utterance interpretation models, especially when data is sparse and language is complex.",,,,ACL
60,2019,Multi-Task Learning for Coherence Modeling,"Youmna Farag, Helen Yannakoudakis","We address the task of assessing discourse coherence, an aspect of text quality that is essential for many NLP tasks, such as summarization and language assessment. We propose a hierarchical neural network trained in a multi-task fashion that learns to predict a document-level coherence score (at the network’s top layers) along with word-level grammatical roles (at the bottom layers), taking advantage of inductive transfer between the two tasks. We assess the extent to which our framework generalizes to different domains and prediction tasks, and demonstrate its effectiveness not only on standard binary evaluation coherence tasks, but also on real-world tasks involving the prediction of varying degrees of coherence, achieving a new state of the art.",,,,ACL
61,2019,Data Programming for Learning Discourse Structure,"Sonia Badene, Kate Thompson, Jean-Pierre Lorré, Nicholas Asher","This paper investigates the advantages and limits of data programming for the task of learning discourse structure. The data programming paradigm implemented in the Snorkel framework allows a user to label training data using expert-composed heuristics, which are then transformed via the “generative step” into probability distributions of the class labels given the training candidates. These results are later generalized using a discriminative model. Snorkel’s attractive promise to create a large amount of annotated data from a smaller set of training data by unifying the output of a set of heuristics has yet to be used for computationally difficult tasks, such as that of discourse attachment, in which one must decide where a given discourse unit attaches to other units in a text in order to form a coherent discourse structure. Although approaching this problem using Snorkel requires significant modifications to the structure of the heuristics, we show that weak supervision methods can be more than competitive with classical supervised learning approaches to the attachment problem.",,,,ACL
62,2019,Evaluating Discourse in Structured Text Representations,"Elisa Ferracane, Greg Durrett, Junyi Jessy Li, Katrin Erk","Discourse structure is integral to understanding a text and is helpful in many NLP tasks. Learning latent representations of discourse is an attractive alternative to acquiring expensive labeled discourse data. Liu and Lapata (2018) propose a structured attention mechanism for text classification that derives a tree over a text, akin to an RST discourse tree. We examine this model in detail, and evaluate on additional discourse-relevant tasks and datasets, in order to assess whether the structured attention improves performance on the end task and whether it captures a text’s discourse structure. We find the learned latent trees have little to no structure and instead focus on lexical cues; even after obtaining more structured trees with proposed model modifications, the trees are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser. Finally, ablation studies show the structured attention provides little benefit, sometimes even hurting performance.",,,,ACL
63,2019,Know What You Don’t Know: Modeling a Pragmatic Speaker that Refers to Objects of Unknown Categories,"Sina Zarrieß, David Schlangen","Zero-shot learning in Language & Vision is the task of correctly labelling (or naming) objects of novel categories. Another strand of work in L&V aims at pragmatically informative rather than “correct” object descriptions, e.g. in reference games. We combine these lines of research and model zero-shot reference games, where a speaker needs to successfully refer to a novel object in an image. Inspired by models of “rational speech acts”, we extend a neural generator to become a pragmatic speaker reasoning about uncertain object categories. As a result of this reasoning, the generator produces fewer nouns and names of distractor categories as compared to a literal speaker. We show that this conversational strategy for dealing with novel objects often improves communicative success, in terms of resolution accuracy of an automatic listener.",,,,ACL
64,2019,End-to-end Deep Reinforcement Learning Based Coreference Resolution,"Hongliang Fei, Xu Li, Dingcheng Li, Ping Li","Recent neural network models have significantly advanced the task of coreference resolution. However, current neural coreference models are usually trained with heuristic loss functions that are computed over a sequence of local decisions. In this paper, we introduce an end-to-end reinforcement learning based coreference resolution model to directly optimize coreference evaluation metrics. Specifically, we modify the state-of-the-art higher-order mention ranking approach in Lee et al. (2018) to a reinforced policy gradient model by incorporating the reward associated with a sequence of coreference linking actions. Furthermore, we introduce maximum entropy regularization for adequate exploration to prevent the model from prematurely converging to a bad local optimum. Our proposed model achieves new state-of-the-art performance on the English OntoNotes v5.0 benchmark.",,,,ACL
65,2019,Implicit Discourse Relation Identification for Open-domain Dialogues,"Mingyu Derek Ma, Kevin Bowden, Jiaqi Wu, Wen Cui, Marilyn Walker","Discourse relation identification has been an active area of research for many years, and the challenge of identifying implicit relations remains largely an unsolved task, especially in the context of an open-domain dialogue system. Previous work primarily relies on a corpora of formal text which is inherently non-dialogic, i.e., news and journals. This data however is not suitable to handle the nuances of informal dialogue nor is it capable of navigating the plethora of valid topics present in open-domain dialogue. In this paper, we designed a novel discourse relation identification pipeline specifically tuned for open-domain dialogue systems. We firstly propose a method to automatically extract the implicit discourse relation argument pairs and labels from a dataset of dialogic turns, resulting in a novel corpus of discourse relation pairs; the first of its kind to attempt to identify the discourse relations connecting the dialogic turns in open-domain discourse. Moreover, we have taken the first steps to leverage the dialogue features unique to our task to further improve the identification of such relations by performing feature ablation and incorporating dialogue features to enhance the state-of-the-art model.",,,,ACL
66,2019,Coreference Resolution with Entity Equalization,"Ben Kantor, Amir Globerson","A key challenge in coreference resolution is to capture properties of entity clusters, and use those in the resolution process. Here we provide a simple and effective approach for achieving this, via an “Entity Equalization” mechanism. The Equalization approach represents each mention in a cluster via an approximation of the sum of all mentions in the cluster. We show how this can be done in a fully differentiable end-to-end manner, thus enabling high-order inferences in the resolution process. Our approach, which also employs BERT embeddings, results in new state-of-the-art results on the CoNLL-2012 coreference resolution task, improving average F1 by 3.6%.",,,,ACL
67,2019,A Cross-Domain Transferable Neural Coherence Model,"Peng Xu, Hamidreza Saghir, Jin Sung Kang, Teng Long, Avishek Joey Bose","Coherence is an important aspect of text quality and is crucial for ensuring its readability. One important limitation of existing coherence models is that training on one domain does not easily generalize to unseen categories of text. Previous work advocates for generative models for cross-domain generalization, because for discriminative models, the space of incoherent sentence orderings to discriminate against during training is prohibitively large. In this work, we propose a local discriminative neural model with a much smaller negative sampling space that can efficiently learn against incorrect orderings. The proposed coherence model is simple in structure, yet it significantly outperforms previous state-of-art methods on a standard benchmark dataset on the Wall Street Journal corpus, as well as in multiple new challenging settings of transfer to unseen categories of discourse on Wikipedia articles.",,,,ACL
68,2019,MOROCO: The Moldavian and Romanian Dialectal Corpus,"Andrei Butnaru, Radu Tudor Ionescu","In this work, we introduce the MOldavian and ROmanian Dialectal COrpus (MOROCO), which is freely available for download at https://github.com/butnaruandrei/MOROCO. The corpus contains 33564 samples of text (with over 10 million tokens) collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports and tech. The data set is divided into 21719 samples for training, 5921 samples for validation and another 5924 samples for testing. For each sample, we provide corresponding dialectal and category labels. This allows us to perform empirical studies on several classification tasks such as (i) binary discrimination of Moldavian versus Romanian text samples, (ii) intra-dialect multi-class categorization by topic and (iii) cross-dialect multi-class categorization by topic. We perform experiments using a shallow approach based on string kernels, as well as a novel deep approach based on character-level convolutional neural networks containing Squeeze-and-Excitation blocks. We also present and analyze the most discriminative features of our best performing model, before and after named entity removal.",,,,ACL
69,2019,Just “OneSeC” for Producing Multilingual Sense-Annotated Data,"Bianca Scarlini, Tommaso Pasini, Roberto Navigli","The well-known problem of knowledge acquisition is one of the biggest issues in Word Sense Disambiguation (WSD), where annotated data are still scarce in English and almost absent in other languages. In this paper we formulate the assumption of One Sense per Wikipedia Category and present OneSeC, a language-independent method for the automatic extraction of hundreds of thousands of sentences in which a target word is tagged with its meaning. Our automatically-generated data consistently lead a supervised WSD model to state-of-the-art performance when compared with other automatic and semi-automatic methods. Moreover, our approach outperforms its competitors on multilingual and domain-specific settings, where it beats the existing state of the art on all languages and most domains. All the training data are available for research purposes at http://trainomatic.org/onesec.",,,,ACL
70,2019,"How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions","Goran Glavaš, Robert Litschko, Sebastian Ruder, Ivan Vulić","Cross-lingual word embeddings (CLEs) facilitate cross-lingual transfer of NLP models. Despite their ubiquitous downstream usage, increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction (BLI). Even the BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we take the first step towards a comprehensive evaluation of CLE models: we thoroughly evaluate both supervised and unsupervised CLE models, for a large number of language pairs, on BLI and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI may hurt downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess simple baselines, which still display competitive performance across the board. We hope our work catalyzes further research on CLE evaluation and model analysis.",,,,ACL
71,2019,SP-10K: A Large-scale Evaluation Set for Selectional Preference Acquisition,"Hongming Zhang, Hantian Ding, Yangqiu Song","Selectional Preference (SP) is a commonly observed language phenomenon and proved to be useful in many natural language processing tasks. To provide a better evaluation method for SP models, we introduce SP-10K, a large-scale evaluation set that provides human ratings for the plausibility of 10,000 SP pairs over five SP relations, covering 2,500 most frequent verbs, nouns, and adjectives in American English. Three representative SP acquisition methods based on pseudo-disambiguation are evaluated with SP-10K. To demonstrate the importance of our dataset, we investigate the relationship between SP-10K and the commonsense knowledge in ConceptNet5 and show the potential of using SP to represent the commonsense knowledge. We also use the Winograd Schema Challenge to prove that the proposed new SP relations are essential for the hard pronoun coreference resolution problem.",,,,ACL
72,2019,A Wind of Change: Detecting and Evaluating Lexical Semantic Change across Times and Domains,"Dominik Schlechtweg, Anna Hätty, Marco Del Tredici, Sabine Schulte im Walde","We perform an interdisciplinary large-scale evaluation for detecting lexical semantic divergences in a diachronic and in a synchronic task: semantic sense changes across time, and semantic sense changes across domains. Our work addresses the superficialness and lack of comparison in assessing models of diachronic lexical change, by bringing together and extending benchmark models on a common state-of-the-art evaluation task. In addition, we demonstrate that the same evaluation task and modelling approaches can successfully be utilised for the synchronic detection of domain-specific sense divergences in the field of term extraction.",,,,ACL
73,2019,"Errudite: Scalable, Reproducible, and Testable Error Analysis","Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, Daniel Weld","Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.",,,,ACL
74,2019,DocRED: A Large-Scale Document-Level Relation Extraction Dataset,"Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin","Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED.",,,,ACL
75,2019,ChID: A Large-scale Chinese IDiom Dataset for Cloze Test,"Chujie Zheng, Minlie Huang, Aixin Sun","Cloze-style reading comprehension in Chinese is still limited due to the lack of various corpora. In this paper we propose a large-scale Chinese cloze test dataset ChID, which studies the comprehension of idiom, a unique language phenomenon in Chinese. In this corpus, the idioms in a passage are replaced by blank symbols and the correct answer needs to be chosen from well-designed candidate idioms. We carefully study how the design of candidate idioms and the representation of idioms affect the performance of state-of-the-art models. Results show that the machine accuracy is substantially worse than that of human, indicating a large space for further research.",,,,ACL
76,2019,Automatic Evaluation of Local Topic Quality,"Jeffrey Lund, Piper Armstrong, Wilson Fearn, Stephen Cowley, Courtni Byun","Topic models are typically evaluated with respect to the global topic distributions that they generate, using metrics such as coherence, but without regard to local (token-level) topic assignments. Token-level assignments are important for downstream tasks such as classification. Even recent models, which aim to improve the quality of these token-level topic assignments, have been evaluated only with respect to global metrics. We propose a task designed to elicit human judgments of token-level topic assignments. We use a variety of topic model types and parameters and discover that global metrics agree poorly with human assignments. Since human evaluation is expensive we propose a variety of automated metrics to evaluate topic models at a local level. Finally, we correlate our proposed metrics with human judgments from the task on several datasets. We show that an evaluation based on the percent of topic switches correlates most strongly with human judgment of local topic quality. We suggest that this new metric, which we call consistency, be adopted alongside global metrics such as topic coherence when evaluating new topic models.",,,,ACL
77,2019,Crowdsourcing and Aggregating Nested Markable Annotations,"Chris Madge, Juntao Yu, Jon Chamberlain, Udo Kruschwitz, Silviu Paun","One of the key steps in language resource creation is the identification of the text segments to be annotated, or markables, which depending on the task may vary from nominal chunks for named entity resolution to (potentially nested) noun phrases in coreference resolution (or mentions) to larger text segments in text segmentation. Markable identification is typically carried out semi-automatically, by running a markable identifier and correcting its output by hand–which is increasingly done via annotators recruited through crowdsourcing and aggregating their responses. In this paper, we present a method for identifying markables for coreference annotation that combines high-performance automatic markable detectors with checking with a Game-With-A-Purpose (GWAP) and aggregation using a Bayesian annotation model. The method was evaluated both on news data and data from a variety of other genres and results in an improvement on F1 of mention boundaries of over seven percentage points when compared with a state-of-the-art, domain-independent automatic mention detector, and almost three points over an in-domain mention detector. One of the key contributions of our proposal is its applicability to the case in which markables are nested, as is the case with coreference markables; but the GWAP and several of the proposed markable detectors are task and language-independent and are thus applicable to a variety of other annotation scenarios.",,,,ACL
78,2019,Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems,"Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher","Over-dependence on domain ontology and lack of sharing knowledge across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short when tracking unknown slot values during inference and often have difficulties in adapting to new domains. In this paper, we propose a Transferable Dialogue State Generator (TRADE) that generates dialogue states from utterances using copy mechanism, facilitating transfer when predicting (domain, slot, value) triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art 48.62% joint goal accuracy for the five domains of MultiWOZ, a human-human dialogue dataset. In addition, we show the transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves 60.58% joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains.",,,,ACL
79,2019,"Multi-Task Networks with Universe, Group, and Task Feature Learning","Shiva Pentyala, Mengwen Liu, Markus Dreyer","We present methods for multi-task learning that take advantage of natural groupings of related tasks. Task groups may be defined along known properties of the tasks, such as task domain or language. Such task groups represent supervised information at the inter-task level and can be encoded into the model. We investigate two variants of neural network architectures that accomplish this, learning different feature spaces at the levels of individual tasks, task groups, as well as the universe of all tasks: (1) parallel architectures encode each input simultaneously into feature spaces at different levels; (2) serial architectures encode each input successively into feature spaces at different levels in the task hierarchy. We demonstrate the methods on natural language understanding (NLU) tasks, where a grouping of tasks into different task domains leads to improved performance on ATIS, Snips, and a large in-house dataset.",,,,ACL
80,2019,Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue,"Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, Rajen Subba","Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Avenues like the E2E NLG Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem. The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset.",,,,ACL
81,2019,OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs,"Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba","We study a conversational reasoning model that strategically traverses through a large-scale common fact knowledge graph (KG) to introduce engaging and contextually diverse entities and attributes. For this study, we collect a new Open-ended Dialog <-> KG parallel corpus called OpenDialKG, where each utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. We then propose the DialKG Walker model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder. Automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-of-the-art baselines or rule-based models, in both in-domain and cross-domain tasks. The proposed model also generates a KG walk path for each entity retrieved, providing a natural way to explain conversational reasoning.",,,,ACL
82,2019,Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing,"Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, Jian Yin","In this paper, we present an approach to incorporate retrieved datapoints as supporting evidence for context-dependent semantic parsing, such as generating source code conditioned on the class environment. Our approach naturally combines a retrieval model and a meta-learner, where the former learns to find similar datapoints from the training data, and the latter considers retrieved datapoints as a pseudo task for fast adaptation. Specifically, our retriever is a context-aware encoder-decoder model with a latent variable which takes context environment into consideration, and our meta-learner learns to utilize retrieved datapoints in a model-agnostic meta-learning paradigm for fast adaptation. We conduct experiments on CONCODE and CSQA datasets, where the context refers to class environment in JAVA codes and conversational history, respectively. We use sequence-to-action model as the base semantic parser, which performs the state-of-the-art accuracy on both datasets. Results show that both the context-aware retriever and the meta-learning strategy improve accuracy, and our approach performs better than retrieve-and-edit baselines.",,,,ACL
83,2019,Knowledge-aware Pronoun Coreference Resolution,"Hongming Zhang, Yan Song, Yangqiu Song, Dong Yu","Resolving pronoun coreference requires knowledge support, especially for particular domains (e.g., medicine). In this paper, we explore how to leverage different types of knowledge to better resolve pronoun coreference with a neural model. To ensure the generalization ability of our model, we directly incorporate knowledge in the format of triplets, which is the most common format of modern knowledge graphs, instead of encoding it with features or rules as that in conventional approaches. Moreover, since not all knowledge is helpful in certain contexts, to selectively use them, we propose a knowledge attention module, which learns to select and use informative knowledge based on contexts, to enhance our model. Experimental results on two datasets from different domains prove the validity and effectiveness of our model, where it outperforms state-of-the-art baselines by a large margin. Moreover, since our model learns to use external knowledge rather than only fitting the training data, it also demonstrates superior performance to baselines in the cross-domain setting.",,,,ACL
84,2019,Don’t Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference,"Yonatan Belinkov, Adam Poliak, Stuart Shieber, Benjamin Van Durme, Alexander Rush","Natural Language Inference (NLI) datasets often contain hypothesis-only biases—artifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis. We propose two probabilistic methods to build models that are more robust to such biases and better transfer across datasets. In contrast to standard approaches to NLI, our methods predict the probability of a premise given a hypothesis and NLI label, discouraging models from ignoring the premise. We evaluate our methods on synthetic and existing NLI datasets by training on datasets containing biases and testing on datasets containing no (or different) hypothesis-only biases. Our results indicate that these methods can make NLI models more robust to dataset-specific artifacts, transferring better than a baseline architecture in 9 out of 12 NLI datasets. Additionally, we provide an extensive analysis of the interplay of our methods with known biases in NLI datasets, as well as the effects of encouraging models to ignore biases and fine-tuning on target datasets.",,,,ACL
85,2019,GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification,"Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang","Fact verification (FV) is a challenging task which requires to retrieve relevant evidence from plain text and use the evidence to verify given claims. Many claims require to simultaneously integrate and reason over several pieces of evidence for verification. However, previous work employs simple models to extract information from evidence without letting evidence communicate with each other, e.g., merely concatenate the evidence for processing. Therefore, these methods are unable to grasp sufficient relational and logical information among the evidence. To alleviate this issue, we propose a graph-based evidence aggregating and reasoning (GEAR) framework which enables information to transfer on a fully-connected evidence graph and then utilizes different aggregators to collect multi-evidence information. We further employ BERT, an effective pre-trained language representation model, to improve the performance. Experimental results on a large-scale benchmark dataset FEVER have demonstrated that GEAR could leverage multi-evidence information for FV and thus achieves the promising result with a test FEVER score of 67.10%. Our code is available at https://github.com/thunlp/GEAR.",,,,ACL
86,2019,SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference,"Martin Schmitt, Hinrich Schütze","We present SherLIiC, a testbed for lexical inference in context (LIiC), consisting of 3985 manually annotated inference rule candidates (InfCands), accompanied by (i) ~960k unlabeled InfCands, and (ii) ~190k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09. Each InfCand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more Freebase types. Due to our candidate selection process based on strong distributional evidence, SherLIiC is much harder than existing testbeds because distributional evidence is of little utility in the classification of InfCands. We also show that, due to its construction, many of SherLIiC’s correct InfCands are novel and missing from existing rule bases. We evaluate a large number of strong baselines on SherLIiC, ranging from semantic vector space models to state of the art neural models of natural language inference (NLI). We show that SherLIiC poses a tough challenge to existing NLI systems.",,,,ACL
87,2019,Extracting Symptoms and their Status from Clinical Conversations,"Nan Du, Kai Chen, Anjuli Kannan, Linh Tran, Yuhui Chen","This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models.",,,,ACL
88,2019,What Makes a Good Counselor? Learning to Distinguish between High-quality and Low-quality Counseling Conversations,"Verónica Pérez-Rosas, Xinyi Wu, Kenneth Resnicow, Rada Mihalcea","The quality of a counseling intervention relies highly on the active collaboration between clients and counselors. In this paper, we explore several linguistic aspects of the collaboration process occurring during counseling conversations. Specifically, we address the differences between high-quality and low-quality counseling. Our approach examines participants’ turn-by-turn interaction, their linguistic alignment, the sentiment expressed by speakers during the conversation, as well as the different topics being discussed. Our results suggest important language differences in low- and high-quality counseling, which we further use to derive linguistic features able to capture the differences between the two groups. These features are then used to build automatic classifiers that can predict counseling quality with accuracies of up to 88%.",,,,ACL
89,2019,Finding Your Voice: The Linguistic Development of Mental Health Counselors,"Justine Zhang, Robert Filbin, Christine Morrison, Jaclyn Weiser, Cristian Danescu-Niculescu-Mizil","Mental health counseling is an enterprise with profound societal importance where conversations play a primary role. In order to acquire the conversational skills needed to face a challenging range of situations, mental health counselors must rely on training and on continued experience with actual clients. However, in the absence of large scale longitudinal studies, the nature and significance of this developmental process remain unclear. For example, prior literature suggests that experience might not translate into consequential changes in counselor behavior. This has led some to even argue that counseling is a profession without expertise. In this work, we develop a computational framework to quantify the extent to which individuals change their linguistic behavior with experience and to study the nature of this evolution. We use our framework to conduct a large longitudinal study of mental health counseling conversations, tracking over 3,400 counselors across their tenure. We reveal that overall, counselors do indeed change their conversational behavior to become more diverse across interactions, developing an individual voice that distinguishes them from other counselors. Furthermore, a finer-grained investigation shows that the rate and nature of this diversification vary across functionally different conversational components.",,,,ACL
90,2019,Towards Automating Healthcare Question Answering in a Noisy Multilingual Low-Resource Setting,"Jeanne E. Daniel, Willie Brink, Ryan Eloff, Charles Copley","We discuss ongoing work into automating a multilingual digital helpdesk service available via text messaging to pregnant and breastfeeding mothers in South Africa. Our anonymized dataset consists of short informal questions, often in low-resource languages, with unreliable language labels, spelling errors and code-mixing, as well as template answers with some inconsistencies. We explore cross-lingual word embeddings, and train parametric and non-parametric models on 90K samples for answer selection from a set of 126 templates. Preliminary results indicate that LSTMs trained end-to-end perform best, with a test accuracy of 62.13% and a recall@5 of 89.56%, and demonstrate that we can accelerate response time by several orders of magnitude.",,,,ACL
91,2019,Joint Entity Extraction and Assertion Detection for Clinical Text,"Parminder Bhatia, Busra Celikkaya, Mohammed Khalilia","Negative medical findings are prevalent in clinical reports, yet discriminating them from positive findings remains a challenging task for in-formation extraction. Most of the existing systems treat this task as a pipeline of two separate tasks, i.e., named entity recognition (NER)and rule-based negation detection. We consider this as a multi-task problem and present a novel end-to-end neural model to jointly extract entities and negations. We extend a standard hierarchical encoder-decoder NER model and first adopt a shared encoder followed by separate decoders for the two tasks. This architecture performs considerably better than the previous rule-based and machine learning-based systems. To overcome the problem of increased parameter size especially for low-resource settings, we propose the Conditional Softmax Shared Decoder architecture which achieves state-of-art results for NER and negation detection on the 2010 i2b2/VA challenge dataset and a proprietary de-identified clinical dataset.",,,,ACL
92,2019,HEAD-QA: A Healthcare Dataset for Complex Reasoning,"David Vilares, Carlos Gómez-Rodríguez","We present HEAD-QA, a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. We then consider monolingual (Spanish) and cross-lingual (to English) experiments with information retrieval and neural techniques. We show that: (i) HEAD-QA challenges current methods, and (ii) the results lag well behind human performance, demonstrating its usefulness as a benchmark for future work.",,,,ACL
93,2019,Are You Convinced? Choosing the More Convincing Evidence with a Siamese Network,"Martin Gleize, Eyal Shnarch, Leshem Choshen, Lena Dankin, Guy Moshkowich","With the advancement in argument detection, we suggest to pay more attention to the challenging task of identifying the more convincing arguments. Machines capable of responding and interacting with humans in helpful ways have become ubiquitous. We now expect them to discuss with us the more delicate questions in our world, and they should do so armed with effective arguments. But what makes an argument more persuasive? What will convince you? In this paper, we present a new data set, IBM-EviConv, of pairs of evidence labeled for convincingness, designed to be more challenging than existing alternatives. We also propose a Siamese neural network architecture shown to outperform several baselines on both a prior convincingness data set and our own. Finally, we provide insights into our experimental results and the various kinds of argumentative value our method is capable of detecting.",,,,ACL
94,2019,From Surrogacy to Adoption; From Bitcoin to Cryptocurrency: Debate Topic Expansion,"Roy Bar-Haim, Dalia Krieger, Orith Toledo-Ronen, Lilach Edelstein, Yonatan Bilu","When debating a controversial topic, it is often desirable to expand the boundaries of discussion. For example, we may consider the pros and cons of possible alternatives to the debate topic, make generalizations, or give specific examples. We introduce the task of Debate Topic Expansion - finding such related topics for a given debate topic, along with a novel annotated dataset for the task. We focus on relations between Wikipedia concepts, and show that they differ from well-studied lexical-semantic relations such as hypernyms, hyponyms and antonyms. We present algorithms for finding both consistent and contrastive expansions and demonstrate their effectiveness empirically. We suggest that debate topic expansion may have various use cases in argumentation mining.",,,,ACL
95,2019,Multimodal and Multi-view Models for Emotion Recognition,"Gustavo Aguilar, Viktor Rozgic, Weiran Wang, Chao Wang","Studies on emotion recognition (ER) show that combining lexical and acoustic information results in more robust and accurate models. The majority of the studies focus on settings where both modalities are available in training and evaluation. However, in practice, this is not always the case; getting ASR output may represent a bottleneck in a deployment pipeline due to computational complexity or privacy-related constraints. To address this challenge, we study the problem of efficiently combining acoustic and lexical modalities during training while still providing a deployable acoustic model that does not require lexical inputs. We first experiment with multimodal models and two attention mechanisms to assess the extent of the benefits that lexical information can provide. Then, we frame the task as a multi-view learning problem to induce semantic information from a multimodal model into our acoustic-only network using a contrastive loss function. Our multimodal model outperforms the previous state of the art on the USC-IEMOCAP dataset reported on lexical and acoustic information. Additionally, our multi-view-trained acoustic network significantly surpasses models that have been exclusively trained with acoustic features.",,,,ACL
96,2019,Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts,"Rui Xia, Zixiang Ding","Emotion cause extraction (ECE), the task aimed at extracting the potential causes behind certain emotions in text, has gained much attention in recent years due to its wide applications. However, it suffers from two shortcomings: 1) the emotion must be annotated before cause extraction in ECE, which greatly limits its applications in real-world scenarios; 2) the way to first annotate emotion and then extract the cause ignores the fact that they are mutually indicative. In this work, we propose a new task: emotion-cause pair extraction (ECPE), which aims to extract the potential pairs of emotions and corresponding causes in a document. We propose a 2-step approach to address this new ECPE task, which first performs individual emotion extraction and cause extraction via multi-task learning, and then conduct emotion-cause pairing and filtering. The experimental results on a benchmark emotion cause corpus prove the feasibility of the ECPE task as well as the effectiveness of our approach.",,,,ACL
97,2019,Argument Invention from First Principles,"Yonatan Bilu, Ariel Gera, Daniel Hershcovich, Benjamin Sznajder, Dan Lahav","Competitive debaters often find themselves facing a challenging task – how to debate a topic they know very little about, with only minutes to prepare, and without access to books or the Internet? What they often do is rely on ”first principles”, commonplace arguments which are relevant to many topics, and which they have refined in past debates. In this work we aim to explicitly define a taxonomy of such principled recurring arguments, and, given a controversial topic, to automatically identify which of these arguments are relevant to the topic. As far as we know, this is the first time that this approach to argument invention is formalized and made explicit in the context of NLP. The main goal of this work is to show that it is possible to define such a taxonomy. While the taxonomy suggested here should be thought of as a ”first attempt” it is nonetheless coherent, covers well the relevant topics and coincides with what professional debaters actually argue in their speeches, and facilitates automatic argument invention for new topics.",,,,ACL
98,2019,Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization,"Sangwoo Cho, Logan Lebanoff, Hassan Foroosh, Fei Liu","The most important obstacles facing multi-document summarization include excessive redundancy in source descriptions and the looming shortage of training data. These obstacles prevent encoder-decoder models from being used directly, but optimization-based methods such as determinantal point processes (DPPs) are known to handle them well. In this paper we seek to strengthen a DPP-based method for extractive multi-document summarization by presenting a novel similarity measure inspired by capsule networks. The approach measures redundancy between a pair of sentences based on surface form and semantic information. We show that our DPP system with improved similarity measure performs competitively, outperforming strong summarization baselines on benchmark datasets. Our findings are particularly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions.",,,,ACL
99,2019,Global Optimization under Length Constraint for Neural Text Summarization,"Takuya Makino, Tomoya Iwakura, Hiroya Takamura, Manabu Okumura","We propose a global optimization method under length constraint (GOLC) for neural text summarization models. GOLC increases the probabilities of generating summaries that have high evaluation scores, ROUGE in this paper, within a desired length. We compared GOLC with two optimization methods, a maximum log-likelihood and a minimum risk training, on CNN/Daily Mail and a Japanese single document summarization data set of The Mainichi Shimbun Newspapers. The experimental results show that a state-of-the-art neural summarization model optimized with GOLC generates fewer overlength summaries while maintaining the fastest processing speed; only 6.70% overlength summaries on CNN/Daily and 7.8% on long summary of Mainichi, compared to the approximately 20% to 50% on CNN/Daily Mail and 10% to 30% on Mainichi with the other optimization methods. We also demonstrate the importance of the generation of in-length summaries for post-editing with the dataset Mainich that is created with strict length constraints. The ex- perimental results show approximately 30% to 40% improved post-editing time by use of in-length summaries.",,,,ACL
100,2019,Searching for Effective Neural Extractive Summarization: What Works and What’s Next,"Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu, Xuanjing Huang","The recent years have seen remarkable success in the use of deep neural networks on text summarization. However, there is no clear understanding of why they perform so well, or how they might be improved. In this paper, we seek to better understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Besides, we find an effective way to improve the current framework and achieve the state-of-the-art result on CNN/DailyMail by a large margin based on our observations and analysis. Hopefully, our work could provide more hints for future research on extractive summarization.",,,,ACL
101,2019,A Simple Theoretical Model of Importance for Summarization,Maxime Peyrard,"Research on summarization has mainly been driven by empirical approaches, crafting systems to perform well on standard datasets with the notion of information Importance remaining latent. We argue that establishing theoretical models of Importance will advance our understanding of the task and help to further improve summarization systems. To this end, we propose simple but rigorous definitions of several concepts that were previously used only intuitively in summarization: Redundancy, Relevance, and Informativeness. Importance arises as a single quantity naturally unifying these concepts. Additionally, we provide intuitions to interpret the proposed quantities and experiments to demonstrate the potential of the framework to inform and guide subsequent works.",,,,ACL
102,2019,Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model,"Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir Radev","Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",,,,ACL
103,2019,Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency,"Shuhuai Ren, Yihe Deng, Kun He, Wanxiang Che","We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples.",,,,ACL
104,2019,Heuristic Authorship Obfuscation,"Janek Bevendorff, Martin Potthast, Matthias Hagen, Benno Stein","Authorship verification is the task of determining whether two texts were written by the same author. We deal with the adversary task, called authorship obfuscation: preventing verification by altering a to-be-obfuscated text. Our new obfuscation approach (1) models writing style difference as the Jensen-Shannon distance between the character n-gram distributions of texts, and (2) manipulates an author’s subconsciously encoded writing style in a sophisticated manner using heuristic search. To obfuscate, we analyze the huge space of textual variants for a paraphrased version of the to-be-obfuscated text that has a sufficient Jensen-Shannon distance at minimal costs in terms of text quality. We analyze, quantify, and illustrate the rationale of this approach, define paraphrasing operators, derive obfuscation thresholds, and develop an effective obfuscation framework. Our authorship obfuscation approach defeats state-of-the-art verification approaches, including unmasking and compression models, while keeping text changes at a minimum.",,,,ACL
105,2019,Text Categorization by Learning Predominant Sense of Words as Auxiliary Task,"Kazuya Shimura, Jiyi Li, Fumiyo Fukumoto","Distributions of the senses of words are often highly skewed and give a strong influence of the domain of a document. This paper follows the assumption and presents a method for text categorization by leveraging the predominant sense of words depending on the domain, i.e., domain-specific senses. The key idea is that the features learned from predominant senses are possible to discriminate the domain of the document and thus improve the overall performance of text categorization. We propose multi-task learning framework based on the neural network model, transformer, which trains a model to simultaneously categorize documents and predicts a predominant sense for each word. The experimental results using four benchmark datasets show that our method is comparable to the state-of-the-art categorization approach, especially our model works well for categorization of multi-label documents.",,,,ACL
106,2019,DeepSentiPeer: Harnessing Sentiment in Review Texts to Recommend Peer Review Decisions,"Tirthankar Ghosal, Rajeev Verma, Asif Ekbal, Pushpak Bhattacharyya","Automatically validating a research artefact is one of the frontiers in Artificial Intelligence (AI) that directly brings it close to competing with human intellect and intuition. Although criticised sometimes, the existing peer review system still stands as the benchmark of research validation. The present-day peer review process is not straightforward and demands profound domain knowledge, expertise, and intelligence of human reviewer(s), which is somewhat elusive with the current state of AI. However, the peer review texts, which contains rich sentiment information of the reviewer, reflecting his/her overall attitude towards the research in the paper, could be a valuable entity to predict the acceptance or rejection of the manuscript under consideration. Here in this work, we investigate the role of reviewer sentiment embedded within peer review texts to predict the peer review outcome. Our proposed deep neural architecture takes into account three channels of information: the paper, the corresponding reviews, and review’s polarity to predict the overall recommendation score as well as the final decision. We achieve significant performance improvement over the baselines (∼ 29% error reduction) proposed in a recently released dataset of peer reviews. An AI of this kind could assist the editors/program chairs as an additional layer of confidence, especially when non-responding/missing reviewers are frequent in present day peer review.",,,,ACL
107,2019,Gated Embeddings in End-to-End Speech Recognition for Conversational-Context Fusion,"Suyoun Kim, Siddharth Dalmia, Florian Metze","We present a novel conversational-context aware end-to-end speech recognizer based on a gated neural network that incorporates conversational-context/word/speech embeddings. Unlike conventional speech recognition models, our model learns longer conversational-context information that spans across sentences and is consequently better at recognizing long conversations. Specifically, we propose to use text-based external word and/or sentence embeddings (i.e., fastText, BERT) within an end-to-end framework, yielding significant improvement in word error rate with better conversational-context representation. We evaluated the models on the Switchboard conversational speech corpus and show that our model outperforms standard end-to-end speech recognition models.",,,,ACL
108,2019,Figurative Usage Detection of Symptom Words to Improve Personal Health Mention Detection,"Adith Iyer, Aditya Joshi, Sarvnaz Karimi, Ross Sparks, Cecile Paris","Personal health mention detection deals with predicting whether or not a given sentence is a report of a health condition. Past work mentions errors in this prediction when symptom words, i.e., names of symptoms of interest, are used in a figurative sense. Therefore, we combine a state-of-the-art figurative usage detection with CNN-based personal health mention detection. To do so, we present two methods: a pipeline-based approach and a feature augmentation-based approach. The introduction of figurative usage detection results in an average improvement of 2.21% F-score of personal health mention detection, in the case of the feature augmentation-based approach. This paper demonstrates the promise of using figurative usage detection to improve personal health mention detection.",,,,ACL
109,2019,Complex Word Identification as a Sequence Labelling Task,"Sian Gooding, Ekaterina Kochmar","Complex Word Identification (CWI) is concerned with detection of words in need of simplification and is a crucial first step in a simplification pipeline. It has been shown that reliable CWI systems considerably improve text simplification. However, most CWI systems to date address the task on a word-by-word basis, not taking the context into account. In this paper, we present a novel approach to CWI based on sequence modelling. Our system is capable of performing CWI in context, does not require extensive feature engineering and outperforms state-of-the-art systems on this task.",,,,ACL
110,2019,Neural News Recommendation with Topic-Aware News Representation,"Chuhan Wu, Fangzhao Wu, Mingxiao An, Yongfeng Huang, Xing Xie","News recommendation can help users find interested news and alleviate information overload. The topic information of news is critical for learning accurate news and user representations for news recommendation. However, it is not considered in many existing news recommendation methods. In this paper, we propose a neural news recommendation approach with topic-aware news representations. The core of our approach is a topic-aware news encoder and a user encoder. In the news encoder we learn representations of news from their titles via CNN networks and apply attention networks to select important words. In addition, we propose to learn topic-aware news representations by jointly training the news encoder with an auxiliary topic classification task. In the user encoder we learn the representations of users from their browsed news and use attention networks to select informative news for user representation learning. Extensive experiments on a real-world dataset validate the effectiveness of our approach.",,,,ACL
111,2019,Poetry to Prose Conversion in Sanskrit as a Linearisation Task: A Case for Low-Resource Languages,"Amrith Krishna, Vishnu Sharma, Bishal Santra, Aishik Chakraborty, Pavankumar Satuluri","The word ordering in a Sanskrit verse is often not aligned with its corresponding prose order. Conversion of the verse to its corresponding prose helps in better comprehension of the construction. Owing to the resource constraints, we formulate this task as a word ordering (linearisation) task. In doing so, we completely ignore the word arrangement at the verse side. kāvya guru, the approach we propose, essentially consists of a pipeline of two pretraining steps followed by a seq2seq model. The first pretraining step learns task-specific token embeddings from pretrained embeddings. In the next step, we generate multiple possible hypotheses for possible word arrangements of the input %using another pretraining step. We then use them as inputs to a neural seq2seq model for the final prediction. We empirically show that the hypotheses generated by our pretraining step result in predictions that consistently outperform predictions based on the original order in the verse. Overall, kāvya guru outperforms current state of the art models in linearisation for the poetry to prose conversion task in Sanskrit.",,,,ACL
112,2019,Learning Emphasis Selection for Written Text in Visual Media from Crowd-Sourced Label Distributions,"Amirreza Shirani, Franck Dernoncourt, Paul Asente, Nedim Lipka, Seokhwan Kim","In visual communication, text emphasis is used to increase the comprehension of written text to convey the author’s intent. We study the problem of emphasis selection, i.e. choosing candidates for emphasis in short written text, to enable automated design assistance in authoring. Without knowing the author’s intent and only considering the input text, multiple emphasis selections are valid. We propose a model that employs end-to-end label distribution learning (LDL) on crowd-sourced data and predicts a selection distribution, capturing the inter-subjectivity (common-sense) in the audience as well as the ambiguity of the input. We compare the model with several baselines in which the problem is transformed to single-label learning by mapping label distributions to absolute labels via majority voting.",,,,ACL
113,2019,"Rumor Detection by Exploiting User Credibility Information, Attention and Multi-task Learning","Quanzhi Li, Qiong Zhang, Luo Si","In this study, we propose a new multi-task learning approach for rumor detection and stance classification tasks. This neural network model has a shared layer and two task specific layers. We incorporate the user credibility information into the rumor detection layer, and we also apply attention mechanism in the rumor detection process. The attended information include not only the hidden states in the rumor detection layer, but also the hidden states from the stance detection layer. The experiments on two datasets show that our proposed model outperforms the state-of-the-art rumor detection approaches.",,,,ACL
114,2019,Context-specific Language Modeling for Human Trafficking Detection from Online Advertisements,"Saeideh Shahrokh Esfahani, Michael J. Cafarella, Maziyar Baran Pouyan, Gregory DeAngelo, Elena Eneva","Human trafficking is a worldwide crisis. Traffickers exploit their victims by anonymously offering sexual services through online advertisements. These ads often contain clues that law enforcement can use to separate out potential trafficking cases from volunteer sex advertisements. The problem is that the sheer volume of ads is too overwhelming for manual processing. Ideally, a centralized semi-automated tool can be used to assist law enforcement agencies with this task. Here, we present an approach using natural language processing to identify trafficking ads on these websites. We propose a classifier by integrating multiple text feature sets, including the publicly available pre-trained textual language model Bi-directional Encoder Representation from transformers (BERT). In this paper, we demonstrate that a classifier using this composite feature set has significantly better performance compared to any single feature set alone.",,,,ACL
115,2019,Self-Attentional Models for Lattice Inputs,"Matthias Sperber, Graham Neubig, Ngoc-Quan Pham, Alex Waibel","Lattices are an efficient and effective method to encode ambiguity of upstream systems in natural language processing tasks, for example to compactly capture multiple speech recognition hypotheses, or to represent multiple linguistic analyses. Previous work has extended recurrent neural networks to model lattice inputs and achieved improvements in various tasks, but these models suffer from very slow computation speeds. This paper extends the recently proposed paradigm of self-attention to handle lattice inputs. Self-attention is a sequence modeling technique that relates inputs to one another by computing pairwise similarities and has gained popularity for both its strong results and its computational efficiency. To extend such models to handle lattices, we introduce probabilistic reachability masks that incorporate lattice structure into the model and support lattice scores if available. We also propose a method for adapting positional embeddings to lattice structures. We apply the proposed model to a speech translation task and find that it outperforms all examined baselines while being much faster to compute than previous neural lattice models during both training and inference.",,,,ACL
116,2019,"When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion","Elena Voita, Rico Sennrich, Ivan Titov","Though machine translation errors caused by the lack of context beyond one sentence have long been acknowledged, the development of context-aware NMT systems is hampered by several problems. Firstly, standard metrics are not sensitive to improvements in consistency in document-level translations. Secondly, previous work on context-aware NMT assumed that the sentence-aligned parallel data consisted of complete documents while in most practical scenarios such document-level data constitutes only a fraction of the available parallel data. To address the first issue, we perform a human study on an English-Russian subtitles dataset and identify deixis, ellipsis and lexical cohesion as three main sources of inconsistency. We then create test sets targeting these phenomena. To address the second shortcoming, we consider a set-up in which a much larger amount of sentence-level data is available compared to that aligned at the document level. We introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU.",,,,ACL
117,2019,A Compact and Language-Sensitive Multilingual Translation Method,"Yining Wang, Long Zhou, Jiajun Zhang, Feifei Zhai, Jingfang Xu","Multilingual neural machine translation (Multi-NMT) with one encoder-decoder model has made remarkable progress due to its simple deployment. However, this multilingual translation paradigm does not make full use of language commonality and parameter sharing between encoder and decoder. Furthermore, this kind of paradigm cannot outperform the individual models trained on bilingual corpus in most cases. In this paper, we propose a compact and language-sensitive method for multilingual translation. To maximize parameter sharing, we first present a universal representor to replace both encoder and decoder models. To make the representor sensitive for specific languages, we further introduce language-sensitive embedding, attention, and discriminator with the ability to enhance model performance. We verify our methods on various translation scenarios, including one-to-many, many-to-many and zero-shot. Extensive experiments demonstrate that our proposed methods remarkably outperform strong standard multilingual translation systems on WMT and IWSLT datasets. Moreover, we find that our model is especially helpful in low-resource and zero-shot translation scenarios.",,,,ACL
118,2019,Unsupervised Parallel Sentence Extraction with Parallel Segment Detection Helps Machine Translation,"Viktor Hangya, Alexander Fraser","Mining parallel sentences from comparable corpora is important. Most previous work relies on supervised systems, which are trained on parallel data, thus their applicability is problematic in low-resource scenarios. Recent developments in building unsupervised bilingual word embeddings made it possible to mine parallel sentences based on cosine similarities of source and target language words. We show that relying only on this information is not enough, since sentences often have similar words but different meanings. We detect continuous parallel segments in sentence pair candidates and rely on them when mining parallel sentences. We show better mining accuracy on three language pairs in a standard shared task on artificial data. We also provide the first experiments showing that parallel sentences mined from real life sources improve unsupervised MT. Our code is available, we hope it will be used to support low-resource MT research.",,,,ACL
119,2019,Unsupervised Bilingual Word Embedding Agreement for Unsupervised Neural Machine Translation,"Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita","Unsupervised bilingual word embedding (UBWE), together with other technologies such as back-translation and denoising, has helped unsupervised neural machine translation (UNMT) achieve remarkable results in several language pairs. In previous methods, UBWE is first trained using non-parallel monolingual corpora and then this pre-trained UBWE is used to initialize the word embedding in the encoder and decoder of UNMT. That is, the training of UBWE and UNMT are separate. In this paper, we first empirically investigate the relationship between UBWE and UNMT. The empirical findings show that the performance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT.",,,,ACL
120,2019,Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies,"Yunsu Kim, Yingbo Gao, Hermann Ney","Transfer learning or multilingual model is essential for low-resource neural machine translation (NMT), but the applicability is limited to cognate languages by sharing their vocabularies. This paper shows effective techniques to transfer a pretrained NMT model to a new, unrelated language without shared vocabularies. We relieve the vocabulary mismatch by using cross-lingual word embedding, train a more language-agnostic encoder by injecting artificial noises, and generate synthetic data easily from the pretraining data without back-translation. Our methods do not require restructuring the vocabulary or retraining the model. We improve plain NMT transfer by up to +5.1% BLEU in five low-resource translation tasks, outperforming multilingual joint training by a large margin. We also provide extensive ablation studies on pretrained embedding, synthetic data, vocabulary size, and parameter freezing for a better understanding of NMT transfer.",,,,ACL
121,2019,Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations,"Jiatao Gu, Yong Wang, Kyunghyun Cho, Victor O.K. Li","Zero-shot translation, translating between language pairs on which a Neural Machine Translation (NMT) system has never been trained, is an emergent property when training the system in multilingual settings. However, naive training for zero-shot NMT easily fails, and is sensitive to hyper-parameter setting. The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot. In this work, we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the mutual information between language IDs of the source and decoded sentences. Inspired by this analysis, we propose to use two simple but effective approaches: (1) decoder pre-training; (2) back-translation. These methods show significant improvement (4 22 BLEU points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach.",,,,ACL
122,2019,Syntactically Supervised Transformers for Faster Neural Machine Translation,"Nader Akoury, Kalpesh Krishna, Mohit Iyyer","Standard decoders for neural machine translation autoregressively generate a single target token per timestep, which slows inference especially for long outputs. While architectural advances such as the Transformer fully parallelize the decoder computations at training time, inference still proceeds sequentially. Recent developments in non- and semi-autoregressive decoding produce multiple tokens per timestep independently of the others, which improves inference speed but deteriorates translation quality. In this work, we propose the syntactically supervised Transformer (SynST), which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse. A series of controlled experiments demonstrates that SynST decodes sentences ~5x faster than the baseline autoregressive Transformer while achieving higher BLEU scores than most competing methods on En-De and En-Fr datasets.",,,,ACL
123,2019,Dynamically Composing Domain-Data Selection with Clean-Data Selection by “Co-Curricular Learning” for Neural Machine Translation,"Wei Wang, Isaac Caswell, Ciprian Chelba","Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a “co-curricular learning” method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the “co-curriculum”. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.",,,,ACL
124,2019,On the Word Alignment from Neural Machine Translation,"Xintong Li, Guanlin Li, Lemao Liu, Max Meng, Shuming Shi","Prior researches suggest that neural machine translation (NMT) captures word alignment through its attention mechanism, however, this paper finds attention may almost fail to capture word alignment for some NMT models. This paper thereby proposes two methods to induce word alignment which are general and agnostic to specific NMT models. Experiments show that both methods induce much better word alignment than attention. This paper further visualizes the translation through the word alignment induced by NMT. In particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics.",,,,ACL
125,2019,Imitation Learning for Non-Autoregressive Neural Machine Translation,"Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, Xu Sun","Non-autoregressive translation models (NAT) have achieved impressive inference speedup. A potential issue of the existing NAT algorithms, however, is that the decoding is conducted in parallel, without directly considering previous context. In this paper, we propose an imitation learning framework for non-autoregressive machine translation, which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart. We conduct experiments on the IWSLT16, WMT14 and WMT16 datasets. Our proposed model achieves a significant speedup over the autoregressive models, while keeping the translation quality comparable to the autoregressive models. By sampling sentence length in parallel at inference time, we achieve the performance of 31.85 BLEU on WMT16 Ro→En and 30.68 BLEU on IWSLT16 En→De.",,,,ACL
126,2019,Monotonic Infinite Lookback Attention for Simultaneous Machine Translation,"Naveen Arivazhagan, Colin Cherry, Wolfgang Macherey, Chung-Cheng Chiu, Semih Yavuz","Simultaneous machine translation begins to translate each source sentence before the source speaker is finished speaking, with applications to live and streaming scenarios. Simultaneous systems must carefully schedule their reading of the source sentence to balance quality against latency. We present the first simultaneous translation system to learn an adaptive schedule jointly with a neural machine translation (NMT) model that attends over all source tokens read thus far. We do so by introducing Monotonic Infinite Lookback (MILk) attention, which maintains both a hard, monotonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source. We show that MILk’s adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values.",,,,ACL
127,2019,Global Textual Relation Embedding for Relational Understanding,"Zhiyu Chen, Hanwen Zha, Honglei Liu, Wenhu Chen, Xifeng Yan","Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding. Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding. The data and code can be found at https://github.com/czyssrs/GloREPlus",,,,ACL
128,2019,Graph Neural Networks with Generated Parameters for Relation Extraction,"Hao Zhu, Yankai Lin, Zhiyuan Liu, Jie Fu, Tat-Seng Chua","In this paper, we propose a novel graph neural network with generated parameters (GP-GNNs). The parameters in the propagation module, i.e. the transition matrices used in message passing procedure, are produced by a generator taking natural language sentences as inputs. We verify GP-GNNs in relation extraction from text, both on bag- and instance-settings. Experimental results on a human-annotated dataset and two distantly supervised datasets show that multi-hop reasoning mechanism yields significant improvements. We also perform a qualitative analysis to demonstrate that our model could discover more accurate relations by multi-hop relational reasoning.",,,,ACL
129,2019,Entity-Relation Extraction as Multi-Turn Question Answering,"Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan","In this paper, we propose a new paradigm for the task of entity-relation extraction. We cast the task as a multi-turn question answering problem, i.e., the extraction of entities and elations is transformed to the task of identifying answer spans from the context. This multi-turn QA formalization comes with several key advantages: firstly, the question query encodes important information for the entity/relation class we want to identify; secondly, QA provides a natural way of jointly modeling entity and relation; and thirdly, it allows us to exploit the well developed machine reading comprehension (MRC) models. Experiments on the ACE and the CoNLL04 corpora demonstrate that the proposed paradigm significantly outperforms previous best models. We are able to obtain the state-of-the-art results on all of the ACE04, ACE05 and CoNLL04 datasets, increasing the SOTA results on the three datasets to 49.6 (+1.2), 60.3 (+0.7) and 69.2 (+1.4), respectively. Additionally, we construct and will release a newly developed dataset RESUME, which requires multi-step reasoning to construct entity dependencies, as opposed to the single-step dependency extraction in the triplet exaction in previous datasets. The proposed multi-turn QA model also achieves the best performance on the RESUME dataset.",,,,ACL
130,2019,Exploiting Entity BIO Tag Embeddings and Multi-task Learning for Relation Extraction with Imbalanced Data,"Wei Ye, Bo Li, Rui Xie, Zhonghao Sheng, Long Chen","In practical scenario, relation extraction needs to first identify entity pairs that have relation and then assign a correct relation class. However, the number of non-relation entity pairs in context (negative instances) usually far exceeds the others (positive instances), which negatively affects a model’s performance. To mitigate this problem, we propose a multi-task architecture which jointly trains a model to perform relation identification with cross-entropy loss and relation classification with ranking loss. Meanwhile, we observe that a sentence may have multiple entities and relation mentions, and the patterns in which the entities appear in a sentence may contain useful semantic information that can be utilized to distinguish between positive and negative instances. Thus we further incorporate the embeddings of character-wise/word-wise BIO tag from the named entity recognition task into character/word embeddings to enrich the input representation. Experiment results show that our proposed approach can significantly improve the performance of a baseline model with more than 10% absolute increase in F1-score, and outperform the state-of-the-art models on ACE 2005 Chinese and English corpus. Moreover, BIO tag embeddings are particularly effective and can be used to improve other models as well.",,,,ACL
131,2019,Joint Type Inference on Entities and Relations via Graph Convolutional Networks,"Changzhi Sun, Yeyun Gong, Yuanbin Wu, Ming Gong, Daxin Jiang","We develop a new paradigm for the task of joint entity relation extraction. It first identifies entity spans, then performs a joint inference on entity types and relation types. To tackle the joint type inference task, we propose a novel graph convolutional network (GCN) running on an entity-relation bipartite graph. By introducing a binary relation classification task, we are able to utilize the structure of entity-relation bipartite graph in a more efficient and interpretable way. Experiments on ACE05 show that our model outperforms existing joint models in entity performance and is competitive with the state-of-the-art in relation performance.",,,,ACL
132,2019,Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers,"Haoyu Wang, Ming Tan, Mo Yu, Shiyu Chang, Dakuo Wang","Many approaches to extract multiple relations from a paragraph require multiple passes over the paragraph. In practice, multiple passes are computationally expensive and this makes difficult to scale to longer paragraphs and larger text corpora. In this work, we focus on the task of multiple relation extractions by encoding the paragraph only once. We build our solution upon the pre-trained self-attentive models (Transformer), where we first add a structured prediction layer to handle extraction between multiple entity pairs, then enhance the paragraph embedding to capture multiple relational information associated with each entity with entity-aware attention. We show that our approach is not only scalable but can also perform state-of-the-art on the standard benchmark ACE 2005.",,,,ACL
133,2019,Unsupervised Information Extraction: Regularizing Discriminative Approaches with Relation Distribution Losses,"Étienne Simon, Vincent Guigue, Benjamin Piwowarski","Unsupervised relation extraction aims at extracting relations between entities in text. Previous unsupervised approaches are either generative or discriminative. In a supervised setting, discriminative approaches, such as deep neural network classifiers, have demonstrated substantial improvement. However, these models are hard to train without supervision, and the currently proposed solutions are unstable. To overcome this limitation, we introduce a skewness loss which encourages the classifier to predict a relation with confidence given a sentence, and a distribution distance loss enforcing that all relations are predicted in average. These losses improve the performance of discriminative based models, and enable us to train deep neural networks satisfactorily, surpassing current state of the art on three different datasets.",,,,ACL
134,2019,Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction,"Christoph Alt, Marc Hübner, Leonhard Hennig","Distantly supervised relation extraction is widely used to extract relational facts from text, but suffers from noisy labels. Current relation extraction methods try to alleviate the noise by multi-instance learning and by providing supporting linguistic and contextual information to more efficiently guide the relation classification. While achieving state-of-the-art results, we observed these models to be biased towards recognizing a limited set of relations with high precision, while ignoring those in the long tail. To address this gap, we utilize a pre-trained language model, the OpenAI Generative Pre-trained Transformer (GPT) (Radford et al., 2018). The GPT and similar models have been shown to capture semantic and syntactic features, and also a notable amount of “common-sense” knowledge, which we hypothesize are important features for recognizing a more diverse set of relations. By extending the GPT to the distantly supervised setting, and fine-tuning it on the NYT10 dataset, we show that it predicts a larger set of distinct relation types with high confidence. Manual and automated evaluation of our model shows that it achieves a state-of-the-art AUC score of 0.422 on the NYT10 dataset, and performs especially well at higher recall levels.",,,,ACL
135,2019,ARNOR: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification,"Wei Jia, Dai Dai, Xinyan Xiao, Hua Wu","Distant supervision is widely used in relation classification in order to create large-scale training data by aligning a knowledge base with an unlabeled corpus. However, it also introduces amounts of noisy labels where a contextual sentence actually does not express the labeled relation. In this paper, we propose ARNOR, a novel Attention Regularization based NOise Reduction framework for distant supervision relation classification. ARNOR assumes that a trustable relation label should be explained by the neural attention model. Specifically, our ARNOR framework iteratively learns an interpretable model and utilizes it to select trustable instances. We first introduce attention regularization to force the model to pay attention to the patterns which explain the relation labels, so as to make the model more interpretable. Then, if the learned model can clearly locate the relation patterns of a candidate instance in the training set, we will select it as a trustable instance for further training step. According to the experiments on NYT data, our ARNOR framework achieves significant improvements over state-of-the-art methods in both relation classification performance and noise reduction effect.",,,,ACL
136,2019,GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction,"Tsu-Jui Fu, Peng-Hsuan Li, Wei-Yun Ma","In this paper, we present GraphRel, an end-to-end relation extraction model which uses graph convolutional networks (GCNs) to jointly learn named entities and relations. In contrast to previous baselines, we consider the interaction between named entities and relations via a 2nd-phase relation-weighted GCN to better extract relations. Linear and dependency structures are both used to extract both sequential and regional features of the text, and a complete word graph is further utilized to extract implicit features among all word pairs of the text. With the graph-based approach, the prediction for overlapping relations is substantially improved over previous sequential approaches. We evaluate GraphRel on two public datasets: NYT and WebNLG. Results show that GraphRel maintains high precision while increasing recall substantially. Also, GraphRel outperforms previous work by 3.2% and 5.8% (F1 score), achieving a new state-of-the-art for relation extraction.",,,,ACL
137,2019,DIAG-NRE: A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction,"Shun Zheng, Xu Han, Yankai Lin, Peilin Yu, Lu Chen","Pattern-based labeling methods have achieved promising results in alleviating the inevitable labeling noises of distantly supervised neural relation extraction. However, these methods require significant expert labor to write relation-specific patterns, which makes them too sophisticated to generalize quickly. To ease the labor-intensive workload of pattern writing and enable the quick generalization to new relation types, we propose a neural pattern diagnosis framework, DIAG-NRE, that can automatically summarize and refine high-quality relational patterns from noise data with human experts in the loop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two real-world datasets and present both significant and interpretable improvements over state-of-the-art methods.",,,,ACL
138,2019,Multi-grained Named Entity Recognition,"Congying Xia, Chenwei Zhang, Tao Yang, Yaliang Li, Nan Du","This paper presents a novel framework, MGNER, for Multi-Grained Named Entity Recognition where multiple entities or entity mentions in a sentence could be non-overlapping or totally nested. Different from traditional approaches regarding NER as a sequential labeling task and annotate entities consecutively, MGNER detects and recognizes entities on multiple granularities: it is able to recognize named entities without explicitly assuming non-overlapping or totally nested structures. MGNER consists of a Detector that examines all possible word segments and a Classifier that categorizes entities. In addition, contextual information and a self-attention mechanism are utilized throughout the framework to improve the NER performance. Experimental results show that MGNER outperforms current state-of-the-art baselines up to 4.4% in terms of the F1 score among nested/non-overlapping NER tasks.",,,,ACL
139,2019,ERNIE: Enhanced Language Representation with Informative Entities,"Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun","Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.",,,,ACL
140,2019,Multi-Channel Graph Neural Network for Entity Alignment,"Yixin Cao, Zhiyuan Liu, Chengjiang Li, Zhiyuan Liu, Juanzi Li","Entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments. In this paper, we propose a novel Multi-channel Graph Neural Network model (MuGNN) to learn alignment-oriented knowledge graph (KG) embeddings by robustly encoding two KGs via multiple channels. Each channel encodes KGs via different relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive entities respectively, which are further combined via pooling techniques. Moreover, we also infer and transfer rule knowledge for completing two KGs consistently. MuGNN is expected to reconcile the structural differences of two KGs, and thus make better use of seed alignments. Extensive experiments on five publicly available datasets demonstrate our superior performance (5% Hits@1 up on average). Source code and data used in the experiments can be accessed at https://github.com/thunlp/MuGNN .",,,,ACL
141,2019,A Neural Multi-digraph Model for Chinese NER with Gazetteers,"Ruixue Ding, Pengjun Xie, Xiaoyan Zhang, Wei Lu, Linlin Li","Gazetteers were shown to be useful resources for named entity recognition (NER). Many existing approaches to incorporating gazetteers into machine learning based NER systems rely on manually defined selection strategies or handcrafted templates, which may not always lead to optimal effectiveness, especially when multiple gazetteers are involved. This is especially the case for the task of Chinese NER, where the words are not naturally tokenized, leading to additional ambiguities. To automatically learn how to incorporate multiple gazetteers into an NER system, we propose a novel approach based on graph neural networks with a multi-digraph structure that captures the information that the gazetteers offer. Experiments on various datasets show that our model is effective in incorporating rich gazetteer information while resolving ambiguities, outperforming previous approaches.",,,,ACL
142,2019,Improved Language Modeling by Decoding the Past,Siddhartha Brahma,"Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method improves perplexity on the Penn Treebank dataset by up to 1.8 points and by up to 2.3 points on the WikiText-2 dataset, over strong regularized baselines using a single softmax. With a mixture-of-softmax model, we show gains of up to 1.0 perplexity points on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.",,,,ACL
143,2019,Training Hybrid Language Models by Marginalizing over Segmentations,"Edouard Grave, Sainbayar Sukhbaatar, Piotr Bojanowski, Armand Joulin","In this paper, we study the problem of hybrid language modeling, that is using models which can predict both characters and larger units such as character ngrams or words. Using such models, multiple potential segmentations usually exist for a given string, for example one using words and one using characters only. Thus, the probability of a string is the sum of the probabilities of all the possible segmentations. Here, we show how it is possible to marginalize over the segmentations efficiently, in order to compute the true probability of a sequence. We apply our technique on three datasets, comprising seven languages, showing improvements over a strong character level language model.",,,,ACL
144,2019,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future","Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass","Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.",,,,ACL
145,2019,Lightweight and Efficient Neural Natural Language Processing with Quaternion Networks,"Yi Tay, Aston Zhang, Anh Tuan Luu, Jinfeng Rao, Shuai Zhang","Many state-of-the-art neural models for NLP are heavily parameterized and thus memory inefficient. This paper proposes a series of lightweight and memory efficient neural architectures for a potpourri of natural language processing (NLP) tasks. To this end, our models exploit computation using Quaternion algebra and hypercomplex spaces, enabling not only expressive inter-component interactions but also significantly (75%) reduced parameter size due to lesser degrees of freedom in the Hamilton product. We propose Quaternion variants of models, giving rise to new architectures such as the Quaternion attention Model and Quaternion Transformer. Extensive experiments on a battery of NLP tasks demonstrates the utility of proposed Quaternion-inspired models, enabling up to 75% reduction in parameter size without significant loss in performance.",,,,ACL
146,2019,Sparse Sequence-to-Sequence Models,"Ben Peters, Vlad Niculae, André F. T. Martins","Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of 𝛼-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any 𝛼 > 1. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models.",,,,ACL
147,2019,On the Robustness of Self-Attentive Models,"Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei Wei, Wen-Lian Hsu","This work examines the robustness of self-attentive neural networks against adversarial input perturbations. Specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks. We also propose a novel attack algorithm for generating more natural adversarial examples that could mislead neural models but not humans. Experimental results show that, compared to recurrent neural models, self-attentive models are more robust against adversarial perturbation. In addition, we provide theoretical explanations for their superior robustness to support our claims.",,,,ACL
148,2019,Exact Hard Monotonic Attention for Character-Level Transduction,"Shijie Wu, Ryan Cotterell","Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.",,,,ACL
149,2019,A Lightweight Recurrent Network for Sequence Modeling,"Biao Zhang, Rico Sennrich","Recurrent networks have achieved great success on various sequential tasks with the assistance of complex recurrent units, but suffer from severe computational inefficiency due to weak parallelization. One direction to alleviate this issue is to shift heavy computations outside the recurrence. In this paper, we propose a lightweight recurrent network, or LRN. LRN uses input and forget gates to handle long-range dependencies as well as gradient vanishing and explosion, with all parameter related calculations factored outside the recurrence. The recurrence in LRN only manipulates the weight assigned to each token, tightly connecting LRN with self-attention networks. We apply LRN as a drop-in replacement of existing recurrent units in several neural sequential models. Extensive experiments on six NLP tasks show that LRN yields the best running efficiency with little or no loss in model performance.",,,,ACL
150,2019,Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications,"Wei Zhao, Haiyun Peng, Steffen Eger, Erik Cambria, Min Yang","Obstacles hindering the development of capsule networks for challenging NLP applications include poor scalability to large output spaces and less reliable routing processes. In this paper, we introduce: (i) an agreement score to evaluate the performance of routing processes at instance-level; (ii) an adaptive optimizer to enhance the reliability of routing; (iii) capsule compression and partial routing to improve the scalability of capsule networks. We validate our approach on two NLP tasks, namely: multi-label text classification and question answering. Experimental results show that our approach considerably improves over strong competitors on both tasks. In addition, we gain the best results in low-resource settings with few training instances.",,,,ACL
151,2019,Soft Representation Learning for Sparse Transfer,"Haeju Park, Jinyoung Yeo, Gengyu Wang, Seung-won Hwang","Transfer learning is effective for improving the performance of tasks that are related, and Multi-task learning (MTL) and Cross-lingual learning (CLL) are important instances. This paper argues that hard-parameter sharing, of hard-coding layers shared across different tasks or languages, cannot generalize well, when sharing with a loosely related task. Such case, which we call sparse transfer, might actually hurt performance, a phenomenon known as negative transfer. Our contribution is using adversarial training across tasks, to “soft-code” shared and private spaces, to avoid the shared space gets too sparse. In CLL, our proposed architecture considers another challenge of dealing with low-quality input.",,,,ACL
152,2019,Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization,"Paul Pu Liang, Zhun Liu, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan Salakhutdinov","There has been an increased interest in multimodal language processing including multimodal dialog, question answering, sentiment analysis, and speech recognition. However, naturally occurring multimodal data is often imperfect as a result of imperfect modalities, missing entries or noise corruption. To address these concerns, we present a regularization method based on tensor rank minimization. Our method is based on the observation that high-dimensional multimodal time series data often exhibit correlations across time and modalities which leads to low-rank tensor representations. However, the presence of noise or incomplete values breaks these correlations and results in tensor representations of higher rank. We design a model to learn such tensor representations and effectively regularize their rank. Experiments on multimodal language data show that our model achieves good results across various levels of imperfection.",,,,ACL
153,2019,Towards Lossless Encoding of Sentences,"Gabriele Prato, Mathieu Duchesneau, Sarath Chandar, Alain Tapp","A lot of work has been done in the field of image compression via machine learning, but not much attention has been given to the compression of natural language. Compressing text into lossless representations while making features easily retrievable is not a trivial task, yet has huge benefits. Most methods designed to produce feature rich sentence embeddings focus solely on performing well on downstream tasks and are unable to properly reconstruct the original sequence from the learned embedding. In this work, we propose a near lossless method for encoding long sequences of texts as well as all of their sub-sequences into feature rich representations. We test our method on sentiment analysis and show good performance across all sub-sentence and sentence embeddings.",,,,ACL
154,2019,Open Vocabulary Learning for Neural Chinese Pinyin IME,"Zhuosheng Zhang, Yafang Huang, Hai Zhao","Pinyin-to-character (P2C) conversion is the core component of pinyin-based Chinese input method engine (IME). However, the conversion is seriously compromised by the ambiguities of Chinese characters corresponding to pinyin as well as the predefined fixed vocabularies. To alleviate such inconveniences, we propose a neural P2C conversion model augmented by an online updated vocabulary with a sampling mechanism to support open vocabulary learning during IME working. Our experiments show that the proposed method outperforms commercial IMEs and state-of-the-art traditional models on standard corpus and true inputting history dataset in terms of multiple metrics and thus the online updated vocabulary indeed helps our IME effectively follows user inputting behavior.",,,,ACL
155,2019,Using LSTMs to Assess the Obligatoriness of Phonological Distinctive Features for Phonotactic Learning,"Nicole Mirea, Klinton Bicknell","To ascertain the importance of phonetic information in the form of phonological distinctive features for the purpose of segment-level phonotactic acquisition, we compare the performance of two recurrent neural network models of phonotactic learning: one that has access to distinctive features at the start of the learning process, and one that does not. Though the predictions of both models are significantly correlated with human judgments of non-words, the feature-naive model significantly outperforms the feature-aware one in terms of probability assigned to a held-out test set of English words, suggesting that distinctive features are not obligatory for learning phonotactic patterns at the segment level.",,,,ACL
156,2019,Better Character Language Modeling through Morphology,"Terra Blevins, Luke Zettlemoyer","We incorporate morphological supervision into character language models (CLMs) via multitasking and show that this addition improves bits-per-character (BPC) performance across 24 languages, even when the morphology data and language modeling data are disjoint. Analyzing the CLMs shows that inflected words benefit more from explicitly modeling morphology than uninflected words, and that morphological supervision improves performance even as the amount of language modeling data grows. We then transfer morphological supervision across languages to improve performance in the low-resource setting.",,,,ACL
157,2019,Historical Text Normalization with Delayed Rewards,"Simon Flachs, Marcel Bollmann, Anders Søgaard","Training neural sequence-to-sequence models with simple token-level log-likelihood is now a standard approach to historical text normalization, albeit often outperformed by phrase-based models. Policy gradient training enables direct optimization for exact matches, and while the small datasets in historical text normalization are prohibitive of from-scratch reinforcement learning, we show that policy gradient fine-tuning leads to significant improvements across the board. Policy gradient training, in particular, leads to more accurate normalizations for long or unseen words.",,,,ACL
158,2019,Stochastic Tokenization with a Language Model for Neural Text Classification,"Tatsuya Hiraoka, Hiroyuki Shindo, Yuji Matsumoto","For unsegmented languages such as Japanese and Chinese, tokenization of a sentence has a significant impact on the performance of text classification. Sentences are usually segmented with words or subwords by a morphological analyzer or byte pair encoding and then encoded with word (or subword) representations for neural networks. However, segmentation is potentially ambiguous, and it is unclear whether the segmented tokens achieve the best performance for the target task. In this paper, we propose a method to simultaneously learn tokenization and text classification to address these problems. Our model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods.",,,,ACL
159,2019,Mitigating Gender Bias in Natural Language Processing: Literature Review,"Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief","As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.",,,,ACL
160,2019,Gender-preserving Debiasing for Pre-trained Word Embeddings,"Masahiro Kaneko, Danushka Bollegala","Word embeddings learnt from massive text collections have demonstrated significant levels of discriminative biases such as gender, racial or ethnic biases, which in turn bias the down-stream NLP applications that use those word embeddings. Taking gender-bias as a working example, we propose a debiasing method that preserves non-discriminative gender-related information, while removing stereotypical discriminative gender biases from pre-trained word embeddings. Specifically, we consider four types of information: feminine, masculine, gender-neutral and stereotypical, which represent the relationship between gender vs. bias, and propose a debiasing method that (a) preserves the gender-related information in feminine and masculine words, (b) preserves the neutrality in gender-neutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information.",,,,ACL
161,2019,Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology,"Ran Zmigrod, Sabrina J. Mielke, Hanna Wallach, Ryan Cotterell","Gender stereotypes are manifest in most of the world’s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.",,,,ACL
162,2019,A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings,"Chris Sweeney, Maryam Najafian","Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.",,,,ACL
163,2019,The Risk of Racial Bias in Hate Speech Detection,"Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, Noah A. Smith","We investigate how annotators’ insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet’s dialect they are significantly less likely to label the tweet as offensive.",,,,ACL
164,2019,Evaluating Gender Bias in Machine Translation,"Gabriel Stanovsky, Noah A. Smith, Luke Zettlemoyer","We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., “The doctor asked the nurse to help her in the operation”). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word “doctor”). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are publicly available at https://github.com/gabrielStanovsky/mt_gender.",,,,ACL
165,2019,LSTMEmbed: Learning Word and Sense Representations from a Large Semantically Annotated Corpus with Long Short-Term Memories,"Ignacio Iacobacci, Roberto Navigli","While word embeddings are now a de facto standard representation of words in most NLP tasks, recently the attention has been shifting towards vector representations which capture the different meanings, i.e., senses, of words. In this paper we explore the capabilities of a bidirectional LSTM model to learn representations of word senses from semantically annotated corpora. We show that the utilization of an architecture that is aware of word order, like an LSTM, enables us to create better representations. We assess our proposed model on various standard benchmarks for evaluating semantic representations, reaching state-of-the-art performance on the SemEval-2014 word-to-sense similarity task. We release the code and the resulting word and sense embeddings at http://lcl.uniroma1.it/LSTMEmbed.",,,,ACL
166,2019,Understanding Undesirable Word Embedding Associations,"Kawin Ethayarajh, David Duvenaud, Graeme Hirst","Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.",,,,ACL
167,2019,Unsupervised Discovery of Gendered Language through Latent-Variable Modeling,"Alexander Miserlis Hoyle, Lawrence Wolf-Sonkin, Hanna Wallach, Isabelle Augenstein, Ryan Cotterell","Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.",,,,ACL
168,2019,Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings,"Vihari Piratla, Sunita Sarawagi, Soumen Chakrabarti","Given a small corpus D_T pertaining to a limited set of focused topics, our goal is to train embeddings that accurately capture the sense of words in the topic in spite of the limited size of D_T. These embeddings may be used in various tasks involving D_T. A popular strategy in limited data settings is to adapt pretrained embeddings E trained on a large corpus. To correct for sense drift, fine-tuning, regularization, projection, and pivoting have been proposed recently. Among these, regularization informed by a word’s corpus frequency performed well, but we improve upon it using a new regularizer based on the stability of its cooccurrence with other words. However, a thorough comparison across ten topics, spanning three tasks, with standardized settings of hyper-parameters, reveals that even the best embedding adaptation strategies provide small gains beyond well-tuned baselines, which many earlier comparisons ignored. In a bold departure from adapting pretrained embeddings, we propose using D_T to probe, attend to, and borrow fragments from any large, topic-rich source corpus (such as Wikipedia), which need not be the corpus used to pretrain embeddings. This step is made scalable and practical by suitable indexing. We reach the surprising conclusion that even limited corpus augmentation is more useful than adapting embeddings, which suggests that non-dominant sense information may be irrevocably obliterated from pretrained embeddings and cannot be salvaged by adaptation.",,,,ACL
169,2019,SphereRE: Distinguishing Lexical Relations with Hyperspherical Relation Embeddings,"Chengyu Wang, Xiaofeng He, Aoying Zhou","Lexical relations describe how meanings of terms relate to each other. Typical examples include hypernymy, synonymy, meronymy, etc. Automatic distinction of lexical relations is vital for NLP applications, and also challenging due to the lack of contextual signals to discriminate between such relations. In this work, we present a neural representation learning model to distinguish lexical relations among term pairs based on Hyperspherical Relation Embeddings (SphereRE). Rather than learning embeddings for individual terms, the model learns representations of relation triples by mapping them to the hyperspherical embedding space, where relation triples of different lexical relations are well separated. Experiments over several benchmarks confirm SphereRE outperforms state-of-the-arts.",,,,ACL
170,2019,Multilingual Factor Analysis,"Francisco Vargas, Kamen Brestnichki, Alex Papadopoulos Korfiatis, Nils Hammerla",In this work we approach the task of learning multilingual word representations in an offline manner by fitting a generative latent variable model to a multilingual dictionary. We model equivalent words in different languages as different views of the same word generated by a common latent variable representing their latent lexical meaning. We explore the task of alignment by querying the fitted model for multilingual embeddings achieving competitive results across a variety of tasks. The proposed model is robust to noise in the embedding space making it a suitable method for distributed representations learned from noisy corpora.,,,,ACL
171,2019,Meaning to Form: Measuring Systematicity as Information,"Tiago Pimentel, Arya D. McCarthy, Damian Blasi, Brian Roark, Ryan Cotterell","A longstanding debate in semiotics centers on the relationship between linguistic signs and their corresponding semantics: is there an arbitrary relationship between a word form and its meaning, or does some systematic phenomenon pervade? For instance, does the character bigram ‘gl’ have any systematic relationship to the meaning of words like ‘glisten’, ‘gleam’ and ‘glow’? In this work, we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks. We employ these in a data-driven and massively multilingual approach to the question, examining 106 languages. We find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation. Encouragingly, we also recover well-attested English examples of systematic affixes. We conclude with the meta-point: Our approximate effect size (measured in bits) is quite small—despite some amount of systematicity between form and meaning, an arbitrary relationship and its resulting benefits dominate human language.",,,,ACL
172,2019,Learning Morphosyntactic Analyzers from the Bible via Iterative Annotation Projection across 26 Languages,"Garrett Nicolai, David Yarowsky","A large percentage of computational tools are concentrated in a very small subset of the planet’s languages. Compounding the issue, many languages lack the high-quality linguistic annotation necessary for the construction of such tools with current machine learning methods. In this paper, we address both issues simultaneously: leveraging the high accuracy of English taggers and parsers, we project morphological information onto translations of the Bible in 26 varied test languages. Using an iterative discovery, constraint, and training process, we build inflectional lexica in the target languages. Through a combination of iteration, ensembling, and reranking, we see double-digit relative error reductions in lemmatization and morphological analysis over a strong initial system.",,,,ACL
173,2019,Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling,"Nasser Zalmout, Nizar Habash","Morphological tagging is challenging for morphologically rich languages due to the large target space and the need for more training data to minimize model sparsity. Dialectal variants of morphologically rich languages suffer more as they tend to be more noisy and have less resources. In this paper we explore the use of multitask learning and adversarial training to address morphological richness and dialectal variations in the context of full morphological tagging. We use multitask learning for joint morphological modeling for the features within two dialects, and as a knowledge-transfer scheme for cross-dialectal modeling. We use adversarial training to learn dialect invariant features that can help the knowledge-transfer scheme from the high to low-resource variants. We work with two dialectal variants: Modern Standard Arabic (high-resource “dialect’”) and Egyptian Arabic (low-resource dialect) as a case study. Our models achieve state-of-the-art results for both. Furthermore, adversarial training provides more significant improvement when using smaller training datasets in particular.",,,,ACL
174,2019,Neural Machine Translation with Reordering Embeddings,"Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita","The reordering model plays an important role in phrase-based statistical machine translation. However, there are few works that exploit the reordering information in neural machine translation. In this paper, we propose a reordering mechanism to learn the reordering embedding of a word based on its contextual information. These learned reordering embeddings are stacked together with self-attention networks to learn sentence representation for machine translation. The reordering mechanism can be easily integrated into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT’14 English-to-German, NIST Chinese-to-English, and WAT Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer.",,,,ACL
175,2019,Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation,"Bram Bulte, Arda Tezcan","We present a simple yet powerful data augmentation method for boosting Neural Machine Translation (NMT) performance by leveraging information retrieved from a Translation Memory (TM). We propose and test two methods for augmenting NMT training data with fuzzy TM matches. Tests on the DGT-TM data set for two language pairs show consistent and substantial improvements over a range of baseline systems. The results suggest that this method is promising for any translation environment in which a sizeable TM is available and a certain amount of repetition across translations is to be expected, especially considering its ease of implementation.",,,,ACL
176,2019,Learning Deep Transformer Models for Machine Translation,"Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li","Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT’16 English-German and NIST OpenMT’12 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big.",,,,ACL
177,2019,Generating Diverse Translations with Sentence Codes,"Raphael Shu, Hideki Nakayama, Kyunghyun Cho","Users of machine translation systems may desire to obtain multiple candidates translated in different ways. In this work, we attempt to obtain diverse translations by using sentence codes to condition the sentence generation. We describe two methods to extract the codes, either with or without the help of syntax information. For diverse generation, we sample multiple candidates, each of which conditioned on a unique code. Experiments show that the sampled translations have much higher diversity scores when using reasonable sentence codes, where the translation quality is still on par with the baselines even under strong constraint imposed by the codes. In qualitative analysis, we show that our method is able to generate paraphrase translations with drastically different structures. The proposed approach can be easily adopted to existing translation systems as no modification to the model is required.",,,,ACL
178,2019,Self-Supervised Neural Machine Translation,"Dana Ruiter, Cristina España-Bonet, Josef van Genabith","We present a simple new method where an emergent NMT system is used for simultaneously selecting training data and learning internal NMT representations. This is done in a self-supervised way without parallel data, in such a way that both tasks enhance each other during training. The method is language independent, introduces no additional hyper-parameters, and achieves BLEU scores of 29.21 (en2fr) and 27.36 (fr2en) on newstest2014 using English and French Wikipedia data for training.",,,,ACL
179,2019,Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation,"Elizabeth Salesky, Matthias Sperber, Alan W Black","Previous work on end-to-end translation from speech has primarily used frame-level features as speech representations, which creates longer, sparser sequences than text. We show that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features. Specifically, we generate phoneme labels for speech frames and average consecutive frames with the same label to create shorter, higher-level source sequences for translation. We see improvements of up to 5 BLEU on both our high and low resource language pairs, with a reduction in training time of 60%. Our improvements hold across multiple data sizes and two language pairs.",,,,ACL
180,2019,Visually Grounded Neural Syntax Acquisition,"Haoyue Shi, Jiayuan Mao, Kevin Gimpel, Karen Livescu","We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without any explicit supervision. The model learns by looking at natural images and reading paired captions. VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. We define concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F1 scores against gold parse trees. We find that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data. We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists. Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches.",,,,ACL
181,2019,Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation,"Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie","Advances in learning and representations have reinvigorated work that connects language to other modalities. A particularly exciting direction is Vision-and-Language Navigation(VLN), in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. Despite recent progress, current research leaves unclear how much of a role language under-standing plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. Here, we highlight shortcomings of current metrics for the Room-to-Room dataset (Anderson et al.,2018b) and propose a new metric, Coverage weighted by Length Score (CLS). We also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. We join existing short paths to form more challenging extended paths to create a new data set, Room-for-Room (R4R). Using R4R and CLS, we show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion.",,,,ACL
182,2019,Expressing Visual Relationships via Language,"Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, Mohit Bansal","Describing images with text is a fundamental problem in vision-language research. Current studies in this domain mostly focus on single image captioning. However, in various real applications (e.g., image editing, difference interpretation, and retrieval), generating relational captions for two images, can also be very useful. This important problem has not been explored mostly due to lack of datasets and effective models. To push forward the research in this direction, we first introduce a new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. We then propose a new relational speaker model based on an encoder-decoder architecture with static relational attention and sequential multi-head attention. We also extend the model with dynamic relational attention, which calculates visual alignment while decoding. Our models are evaluated on our newly collected and two public datasets consisting of image pairs annotated with relationship sentences. Experimental results, based on both automatic and human evaluation, demonstrate that our model outperforms all baselines and existing methods on all the datasets.",,,,ACL
183,2019,Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video,"Zhenfang Chen, Lin Ma, Wenhan Luo, Kwan-Yee Kenneth Wong","In this paper, we address a novel task, namely weakly-supervised spatio-temporally grounding natural sentence in video. Specifically, given a natural sentence and a video, we localize a spatio-temporal tube in the video that semantically corresponds to the given sentence, with no reliance on any spatio-temporal annotations during training. First, a set of spatio-temporal tubes, referred to as instances, are extracted from the video. We then encode these instances and the sentence using our newly proposed attentive interactor which can exploit their fine-grained relationships to characterize their matching behaviors. Besides a ranking loss, a novel diversity loss is introduced to train our attentive interactor to strengthen the matching behaviors of reliable instance-sentence pairs and penalize the unreliable ones. We also contribute a dataset, called VID-sentence, based on the ImageNet video object detection dataset, to serve as a benchmark for our task. Results from extensive experiments demonstrate the superiority of our model over the baseline approaches.",,,,ACL
184,2019,The PhotoBook Dataset: Building Common Ground through Visually-Grounded Dialogue,"Janosch Haber, Tim Baumgärtner, Ece Takmaz, Lieke Gelderloos, Elia Bruni","This paper introduces the PhotoBook dataset, a large-scale collection of visually-grounded, task-oriented dialogues in English designed to investigate shared dialogue history accumulating during conversation. Taking inspiration from seminal work on dialogue analysis, we propose a data-collection task formulated as a collaborative game prompting two online participants to refer to images utilising both their visual context as well as previously established referring expressions. We provide a detailed description of the task setup and a thorough analysis of the 2,500 dialogues collected. To further illustrate the novel features of the dataset, we propose a baseline model for reference resolution which uses a simple method to take into account shared information accumulated in a reference chain. Our results show that this information is particularly important to resolve later descriptions and underline the need to develop more sophisticated models of common ground in dialogue interaction.",,,,ACL
185,2019,Continual and Multi-Task Architecture Search,"Ramakanth Pasunuru, Mohit Bansal","Architecture search is the process of automatically learning the neural model or cell structure that best suits the given task. Recently, this approach has shown promising performance improvements (on language modeling and image classification) with reasonable training speed, using a weight sharing strategy called Efficient Neural Architecture Search (ENAS). In our work, we first introduce a novel continual architecture search (CAS) approach, so as to continually evolve the model parameters during the sequential training of several tasks, without losing performance on previously learned tasks (via block-sparsity and orthogonality constraints), thus enabling life-long learning. Next, we explore a multi-task architecture search (MAS) approach over ENAS for finding a unified, single cell structure that performs well across multiple tasks (via joint controller rewards), and hence allows more generalizable transfer of the cell structure knowledge to an unseen new task. We empirically show the effectiveness of our sequential continual learning and parallel multi-task learning based architecture search approaches on diverse sentence-pair classification tasks (GLUE) and multimodal-generation based video captioning tasks. Further, we present several ablations and analyses on the learned cell structures.",,,,ACL
186,2019,Semi-supervised Stochastic Multi-Domain Learning using Variational Inference,"Yitong Li, Timothy Baldwin, Trevor Cohn","Supervised models of NLP rely on large collections of text which closely resemble the intended testing setting. Unfortunately matching text is often not available in sufficient quantity, and moreover, within any domain of text, data is often highly heterogenous. In this paper we propose a method to distill the important domain signal as part of a multi-domain learning system, using a latent variable model in which parts of a neural model are stochastically gated based on the inferred domain. We compare the use of discrete versus continuous latent variables, operating in a domain-supervised or a domain semi-supervised setting, where the domain is known only for a subset of training inputs. We show that our model leads to substantial performance improvements over competitive benchmark domain adaptation methods, including methods using adversarial learning.",,,,ACL
187,2019,Boosting Entity Linking Performance by Leveraging Unlabeled Documents,"Phong Le, Ivan Titov","Modern entity linking systems rely on large collections of documents specifically annotated for the task (e.g., AIDA CoNLL). In contrast, we propose an approach which exploits only naturally occurring information: unlabeled documents and Wikipedia. Our approach consists of two stages. First, we construct a high recall list of candidate entities for each mention in an unlabeled document. Second, we use the candidate lists as weak supervision to constrain our document-level entity linking model. The model treats entities as latent variables and, when estimated on a collection of unlabelled texts, learns to choose entities relying both on local context of each mention and on coherence with other entities in the document. The resulting approach rivals fully-supervised state-of-the-art systems on standard test sets. It also approaches their performance in the very challenging setting: when tested on a test set sampled from the data used to estimate the supervised systems. By comparing to Wikipedia-only training of our model, we demonstrate that modeling unlabeled documents is beneficial.",,,,ACL
188,2019,Pre-Learning Environment Representations for Data-Efficient Neural Instruction Following,"David Gaddy, Dan Klein",We consider the problem of learning to map from natural language instructions to state transitions (actions) in a data-efficient manner. Our method takes inspiration from the idea that it should be easier to ground language to concepts that have already been formed through pre-linguistic observation. We augment a baseline instruction-following learner with an initial environment-learning phase that uses observations of language-free state transitions to induce a suitable latent representation of actions before processing the instruction-following training data. We show that mapping to pre-learned representations substantially improves performance over systems whose representations are learned from limited instructional data alone.,,,,ACL
189,2019,Reinforced Training Data Selection for Domain Adaptation,"Miaofeng Liu, Yan Song, Hongbin Zou, Tong Zhang","Supervised models suffer from the problem of domain shifting where distribution mismatch in the data across domains greatly affect model performance. To solve the problem, training data selection (TDS) has been proven to be a prospective solution for domain adaptation in leveraging appropriate data. However, conventional TDS methods normally requires a predefined threshold which is neither easy to set nor can be applied across tasks, and models are trained separately with the TDS process. To make TDS self-adapted to data and task, and to combine it with model training, in this paper, we propose a reinforcement learning (RL) framework that synchronously searches for training instances relevant to the target domain and learns better representations for them. A selection distribution generator (SDG) is designed to perform the selection and is updated according to the rewards computed from the selected data, where a predictor is included in the framework to ensure a task-specific model can be trained on the selected data and provides feedback to rewards. Experimental results from part-of-speech tagging, dependency parsing, and sentiment analysis, as well as ablation studies, illustrate that the proposed framework is not only effective in data selection and representation, but also generalized to accommodate different NLP tasks.",,,,ACL
190,2019,Generating Long and Informative Reviews with Aspect-Aware Coarse-to-Fine Decoding,"Junyi Li, Wayne Xin Zhao, Ji-Rong Wen, Yang Song","Generating long and informative review text is a challenging natural language generation task. Previous work focuses on word-level generation, neglecting the importance of topical and syntactic characteristics from natural languages. In this paper, we propose a novel review generation model by characterizing an elaborately designed aspect-aware coarse-to-fine generation process. First, we model the aspect transitions to capture the overall content flow. Then, to generate a sentence, an aspect-aware sketch will be predicted using an aspect-aware decoder. Finally, another decoder fills in the semantic slots by generating corresponding words. Our approach is able to jointly utilize aspect semantics, syntactic sketch, and context information. Extensive experiments results have demonstrated the effectiveness of the proposed model.",,,,ACL
191,2019,PaperRobot: Incremental Draft Generation of Scientific Ideas,"Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji","We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30%, 24% and 12% of the time, respectively.",,,,ACL
192,2019,Rhetorically Controlled Encoder-Decoder for Modern Chinese Poetry Generation,"Zhiqiang Liu, Zuohui Fu, Jie Cao, Gerard de Melo, Yik-Cheung Tam","Rhetoric is a vital element in modern poetry, and plays an essential role in improving its aesthetics. However, to date, it has not been considered in research on automatic poetry generation. In this paper, we propose a rhetorically controlled encoder-decoder for modern Chinese poetry generation. Our model relies on a continuous latent variable as a rhetoric controller to capture various rhetorical patterns in an encoder, and then incorporates rhetoric-based mixtures while generating modern Chinese poetry. For metaphor and personification, an automated evaluation shows that our model outperforms state-of-the-art baselines by a substantial margin, while human evaluation shows that our model generates better poems than baseline methods in terms of fluency, coherence, meaningfulness, and rhetorical aesthetics.",,,,ACL
193,2019,Enhancing Topic-to-Essay Generation with External Commonsense Knowledge,"Pengcheng Yang, Lei Li, Fuli Luo, Tianyu Liu, Xu Sun","Automatic topic-to-essay generation is a challenging task since it requires generating novel, diverse, and topic-consistent paragraph-level text with a set of topics as input. Previous work tends to perform essay generation based solely on the given topics while ignoring massive commonsense knowledge. However, this commonsense knowledge provides additional background information, which can help to generate essays that are more novel and diverse. Towards filling this gap, we propose to integrate commonsense from the external knowledge base into the generator through dynamic memory mechanism. Besides, the adversarial training based on a multi-label discriminator is employed to further improve topic-consistency. We also develop a series of automatic evaluation metrics to comprehensively assess the quality of the generated essay. Experiments show that with external commonsense knowledge and adversarial training, the generated essays are more novel, diverse, and topic-consistent than existing methods in terms of both automatic and human evaluation.",,,,ACL
194,2019,Towards Fine-grained Text Sentiment Transfer,"Fuli Luo, Peng Li, Pengcheng Yang, Jie Zhou, Yutong Tan","In this paper, we focus on the task of fine-grained text sentiment transfer (FGST). This task aims to revise an input sequence to satisfy a given sentiment intensity, while preserving the original semantic content. Different from the conventional sentiment transfer task that only reverses the sentiment polarity (positive/negative) of text, the FTST task requires more nuanced and fine-grained control of sentiment. To remedy this, we propose a novel Seq2SentiSeq model. Specifically, the numeric sentiment intensity value is incorporated into the decoder via a Gaussian kernel layer to finely control the sentiment intensity of the output. Moreover, to tackle the problem of lacking parallel data, we propose a cycle reinforcement learning algorithm to guide the model training. In this framework, the elaborately designed rewards can balance both sentiment transformation and content preservation, while not requiring any ground truth output. Experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation.",,,,ACL
195,2019,Data-to-text Generation with Entity Modeling,"Ratish Puduppully, Li Dong, Mirella Lapata","Recent approaches to data-to-text generation have shown great promise thanks to the use of large-scale datasets and the application of neural network architectures which are trained end-to-end. These models rely on representation learning to select content appropriately, structure it coherently, and verbalize it grammatically, treating entities as nothing more than vocabulary tokens. In this work we propose an entity-centric neural architecture for data-to-text generation. Our model creates entity-specific representations which are dynamically updated. Text is generated conditioned on the data input and entity memory representations using hierarchical attention at each time step. We present experiments on the RotoWire benchmark and a (five times larger) new dataset on the baseball domain which we create. Our results show that the proposed model outperforms competitive baselines in automatic and human evaluation.",,,,ACL
196,2019,Ensuring Readability and Data-fidelity using Head-modifier Templates in Deep Type Description Generation,"Jiangjie Chen, Ao Wang, Haiyun Jiang, Suo Feng, Chenguang Li","A type description is a succinct noun compound which helps human and machines to quickly grasp the informative and distinctive information of an entity. Entities in most knowledge graphs (KGs) still lack such descriptions, thus calling for automatic methods to supplement such information. However, existing generative methods either overlook the grammatical structure or make factual mistakes in generated texts. To solve these problems, we propose a head-modifier template based method to ensure the readability and data fidelity of generated type descriptions. We also propose a new dataset and two metrics for this task. Experiments show that our method improves substantially compared with baselines and achieves state-of-the-art performance on both datasets.",,,,ACL
197,2019,Key Fact as Pivot: A Two-Stage Model for Low Resource Table-to-Text Generation,"Shuming Ma, Pengcheng Yang, Tianyu Liu, Peng Li, Jie Zhou","Table-to-text generation aims to translate the structured data into the unstructured text. Most existing methods adopt the encoder-decoder framework to learn the transformation, which requires large-scale training samples. However, the lack of large parallel data is a major practical problem for many domains. In this work, we consider the scenario of low resource table-to-text generation, where only limited parallel data is available. We propose a novel model to separate the generation into two stages: key fact prediction and surface realization. It first predicts the key facts from the tables, and then generates the text with the key facts. The training of key fact prediction needs much fewer annotated data, while surface realization can be trained with pseudo parallel corpus. We evaluate our model on a biography generation dataset. Our model can achieve 27.34 BLEU score with only 1,000 parallel data, while the baseline model only obtain the performance of 9.71 BLEU score.",,,,ACL
198,2019,Unsupervised Neural Text Simplification,"Sai Surya, Abhijit Mishra, Anirban Laha, Parag Jain, Karthik Sankaranarayanan","The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabeled text corpora. The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based losses and denoising. The framework is trained using unlabeled text collected from en-Wikipedia dump. Our analysis (both quantitative and qualitative involving human evaluators) on public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels, competitive to existing supervised methods. It also outperforms viable unsupervised baselines. Adding a few labeled pairs helps improve the performance further.",,,,ACL
199,2019,Syntax-Infused Variational Autoencoder for Text Generation,"Xinyuan Zhang, Yi Yang, Siyang Yuan, Dinghan Shen, Lawrence Carin","We present a syntax-infused variational autoencoder (SIVAE), that integrates sentences with their syntactic trees to improve the grammar of generated sentences. Distinct from existing VAE-based text generative models, SIVAE contains two separate latent spaces, for sentences and syntactic trees. The evidence lower bound objective is redesigned correspondingly, by optimizing a joint distribution that accommodates two encoders and two decoders. SIVAE works with long short-term memory architectures to simultaneously generate sentences and syntactic trees. Two versions of SIVAE are proposed: one captures the dependencies between the latent variables through a conditional prior network, and the other treats the latent variables independently such that syntactically-controlled sentence generation can be performed. Experimental results demonstrate the generative superiority of SIVAE on both reconstruction and targeted syntactic evaluations. Finally, we show that the proposed models can be used for unsupervised paraphrasing given different syntactic tree templates.",,,,ACL
200,2019,Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models,"Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun Chen, Xin Wang","Variational autoencoders (VAEs) have received much attention recently as an end-to-end architecture for text generation with latent variables. However, previous works typically focus on synthesizing relatively short sentences (up to 20 words), and the posterior collapse issue has been widely identified in text-VAEs. In this paper, we propose to leverage several multi-level structures to learn a VAE model for generating long, and coherent text. In particular, a hierarchy of stochastic layers between the encoder and decoder networks is employed to abstract more informative and semantic-rich latent codes. Besides, we utilize a multi-level decoder structure to capture the coherent long-term structure inherent in long-form texts, by generating intermediate sentence representations as high-level plan vectors. Extensive experimental results demonstrate that the proposed multi-level VAE model produces more coherent and less repetitive long text compared to baselines as well as can mitigate the posterior-collapse issue.",,,,ACL
201,2019,Jointly Learning Semantic Parser and Natural Language Generator via Dual Information Maximization,"Hai Ye, Wenjie Li, Lu Wang","Semantic parsing aims to transform natural language (NL) utterances into formal meaning representations (MRs), whereas an NL generator achieves the reverse: producing an NL description for some given MRs. Despite this intrinsic connection, the two tasks are often studied separately in prior work. In this paper, we model the duality of these two tasks via a joint learning framework, and demonstrate its effectiveness of boosting the performance on both tasks. Concretely, we propose a novel method of dual information maximization (DIM) to regularize the learning process, where DIM empirically maximizes the variational lower bounds of expected joint distributions of NL and MRs. We further extend DIM to a semi-supervision setup (SemiDIM), which leverages unlabeled data of both tasks. Experiments on three datasets of dialogue management and code generation (and summarization) show that performance on both semantic parsing and NL generation can be consistently improved by DIM, in both supervised and semi-supervised setups.",,,,ACL
202,2019,"Learning to Select, Track, and Generate for Data-to-Text","Hayate Iso, Yui Uehara, Tatsuya Ishigaki, Hiroshi Noji, Eiji Aramaki","We propose a data-to-text generation model with two modules, one for tracking and the other for text generation. Our tracking module selects and keeps track of salient information and memorizes which record has been mentioned. Our generation module generates a summary conditioned on the state of tracking module. Our proposed model is considered to simulate the human-like writing process that gradually selects the information by determining the intermediate variables while writing the summary. In addition, we also explore the effectiveness of the writer information for generations. Experimental results show that our proposed model outperforms existing models in all evaluation metrics even without writer information. Incorporating writer information further improves the performance, contributing to content planning and surface realization.",,,,ACL
203,2019,Reinforced Dynamic Reasoning for Conversational Question Generation,"Boyuan Pan, Hao Li, Ziyu Yao, Deng Cai, Huan Sun","This paper investigates a new task named Conversational Question Generation (CQG) which is to generate a question based on a passage and a conversation history (i.e., previous turns of question-answer pairs). CQG is a crucial task for developing intelligent agents that can drive question-answering style conversations or test user understanding of a given passage. Towards that end, we propose a new approach named Reinforced Dynamic Reasoning network, which is based on the general encoder-decoder framework but incorporates a reasoning procedure in a dynamic manner to better understand what has been asked and what to ask next about the passage into the general encoder-decoder framework. To encourage producing meaningful questions, we leverage a popular question answering (QA) model to provide feedback and fine-tune the question generator using a reinforcement learning mechanism. Empirical results on the recently released CoQA dataset demonstrate the effectiveness of our method in comparison with various baselines and model variants. Moreover, to show the applicability of our method, we also apply it to create multi-turn question-answering conversations for passages in SQuAD.",,,,ACL
204,2019,TalkSumm: A Dataset and Scalable Annotation Method for Scientific Paper Summarization Based on Conference Talks,"Guy Lev, Michal Shmueli-Scheuer, Jonathan Herzig, Achiya Jerbi, David Konopnicki","Currently, no large-scale training data is available for the task of scientific paper summarization. In this paper, we propose a novel method that automatically generates summaries for scientific papers, by utilizing videos of talks at scientific conferences. We hypothesize that such talks constitute a coherent and concise description of the papers’ content, and can form the basis for good summaries. We collected 1716 papers and their corresponding videos, and created a dataset of paper summaries. A model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually. In addition, we validated the quality of our summaries by human experts.",,,,ACL
205,2019,Improving Abstractive Document Summarization with Salient Information Modeling,"Yongjian You, Weijia Jia, Tianyi Liu, Wenmian Yang","Comprehensive document encoding and salient information selection are two major difficulties for generating summaries with adequate salient information. To tackle the above difficulties, we propose a Transformer-based encoder-decoder framework with two novel extensions for abstractive document summarization. Specifically, (1) to encode the documents comprehensively, we design a focus-attention mechanism and incorporate it into the encoder. This mechanism models a Gaussian focal bias on attention scores to enhance the perception of local context, which contributes to producing salient and informative summaries. (2) To distinguish salient information precisely, we design an independent saliency-selection network which manages the information flow from encoder to decoder. This network effectively reduces the influences of secondary information on the generated summaries. Experimental results on the popular CNN/Daily Mail benchmark demonstrate that our model outperforms other state-of-the-art baselines on the ROUGE metrics.",,,,ACL
206,2019,Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking,"Masaru Isonuma, Junichiro Mori, Ichiro Sakata","This paper focuses on the end-to-end abstractive summarization of a single product review without supervision. We assume that a review can be described as a discourse tree, in which the summary is the root, and the child sentences explain their parent in detail. By recursively estimating a parent from its children, our model learns the latent discourse tree without an external parser and generates a concise summary. We also introduce an architecture that ranks the importance of each sentence on the tree to support summary generation focusing on the main review point. The experimental results demonstrate that our model is competitive with or outperforms other unsupervised approaches. In particular, for relatively long reviews, it achieves a competitive or better performance than supervised models. The induced tree shows that the child sentences provide additional information about their parent, and the generated summary abstracts the entire review.",,,,ACL
207,2019,BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization,"Kai Wang, Xiaojun Quan, Rui Wang","The success of neural summarization models stems from the meticulous encodings of source articles. To overcome the impediments of limited and sometimes noisy training data, one promising direction is to make better use of the available training data by applying filters during summarization. In this paper, we propose a novel Bi-directional Selective Encoding with Template (BiSET) model, which leverages template discovered from training data to softly select key information from each source article to guide its summarization process. Extensive experiments on a standard summarization dataset are conducted and the results show that the template-equipped BiSET model manages to improve the summarization performance significantly with a new state of the art.",,,,ACL
208,2019,Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards,"Hou Pong Chan, Wang Chen, Lu Wang, Irwin King","Generating keyphrases that summarize the main points of a document is a fundamental task in natural language processing. Although existing generative models are capable of predicting multiple keyphrases for an input document as well as determining the number of keyphrases to generate, they still suffer from the problem of generating too few keyphrases. To address this problem, we propose a reinforcement learning (RL) approach for keyphrase generation, with an adaptive reward function that encourages a model to generate both sufficient and accurate keyphrases. Furthermore, we introduce a new evaluation method that incorporates name variations of the ground-truth keyphrases using the Wikipedia knowledge base. Thus, our evaluation method can more robustly evaluate the quality of predicted keyphrases. Extensive experiments on five real-world datasets of different scales demonstrate that our RL approach consistently and significantly improves the performance of the state-of-the-art generative models with both conventional and new evaluation methods.",,,,ACL
209,2019,Scoring Sentence Singletons and Pairs for Abstractive Summarization,"Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon Kim, Seokhwan Kim","When writing a summary, humans tend to choose content from one or two sentences and merge them into a single summary sentence. However, the mechanisms behind the selection of one or multiple source sentences remain poorly understood. Sentence fusion assumes multi-sentence input; yet sentence selection methods only work with single sentences and not combinations of them. There is thus a crucial gap between sentence selection and fusion to support summarizing by both compressing single sentences and fusing pairs. This paper attempts to bridge the gap by ranking sentence singletons and pairs together in a unified space. Our proposed framework attempts to model human methodology by selecting either a single sentence or a pair of sentences, then compressing or fusing the sentence(s) to produce a summary sentence. We conduct extensive experiments on both single- and multi-document summarization datasets and report findings on sentence selection and abstraction.",,,,ACL
210,2019,Keep Meeting Summaries on Topic: Abstractive Multi-Modal Meeting Summarization,"Manling Li, Lingyu Zhang, Heng Ji, Richard J. Radke","Transcripts of natural, multi-person meetings differ significantly from documents like news articles, which can make Natural Language Generation models for generating summaries unfocused. We develop an abstractive meeting summarizer from both videos and audios of meeting recordings. Specifically, we propose a multi-modal hierarchical attention across three levels: segment, utterance and word. To narrow down the focus into topically-relevant segments, we jointly model topic segmentation and summarization. In addition to traditional text features, we introduce new multi-modal features derived from visual focus of attention, based on the assumption that the utterance is more important if the speaker receives more attention. Experiments show that our model significantly outperforms the state-of-the-art with both BLEU and ROUGE measures.",,,,ACL
211,2019,Adversarial Domain Adaptation Using Artificial Titles for Abstractive Title Generation,"Francine Chen, Yan-Ying Chen","A common issue in training a deep learning, abstractive summarization model is lack of a large set of training summaries. This paper examines techniques for adapting from a labeled source domain to an unlabeled target domain in the context of an encoder-decoder model for text generation. In addition to adversarial domain adaptation (ADA), we introduce the use of artificial titles and sequential training to capture the grammatical style of the unlabeled target domain. Evaluation on adapting to/from news articles and Stack Exchange posts indicates that the use of these techniques can boost performance for both unsupervised adaptation as well as fine-tuning with limited target data.",,,,ACL
212,2019,BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization,"Eva Sharma, Chen Li, Lu Wang","Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article’s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.",,,,ACL
213,2019,Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference,"Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, Iryna Gurevych","While recent progress on abstractive summarization has led to remarkably fluent summaries, factual errors in generated summaries still severely limit their use in practice. In this paper, we evaluate summaries produced by state-of-the-art models via crowdsourcing and show that such errors occur frequently, in particular with more abstractive models. We study whether textual entailment predictions can be used to detect such errors and if they can be reduced by reranking alternative predicted summaries. That leads to an interesting downstream application for entailment models. In our experiments, we find that out-of-the-box entailment models trained on NLI datasets do not yet offer the desired performance for the downstream task and we therefore release our annotations as additional test data for future extrinsic evaluations of NLI.",,,,ACL
214,2019,Self-Supervised Learning for Contextualized Extractive Summarization,"Hong Wang, Xin Wang, Wenhan Xiong, Mo Yu, Xiaoxiao Guo","Existing models for extractive summarization are usually trained from scratch with a cross-entropy loss, which does not explicitly capture the global context at the document level. In this paper, we aim to improve this task by introducing three auxiliary pre-training tasks that learn to capture the document-level context in a self-supervised fashion. Experiments on the widely-used CNN/DM dataset validate the effectiveness of the proposed auxiliary tasks. Furthermore, we show that after pre-training, a clean model with simple building blocks is able to outperform previous state-of-the-art that are carefully designed.",,,,ACL
215,2019,On the Summarization of Consumer Health Questions,"Asma Ben Abacha, Dina Demner-Fushman","Question understanding is one of the main challenges in question answering. In real world applications, users often submit natural language questions that are longer than needed and include peripheral information that increases the complexity of the question, leading to substantially more false positives in answer retrieval. In this paper, we study neural abstractive models for medical question summarization. We introduce the MeQSum corpus of 1,000 summarized consumer health questions. We explore data augmentation methods and evaluate state-of-the-art neural abstractive models on this new task. In particular, we show that semantic augmentation from question datasets improves the overall performance, and that pointer-generator networks outperform sequence-to-sequence attentional models on this task, with a ROUGE-1 score of 44.16%. We also present a detailed error analysis and discuss directions for improvement that are specific to question summarization.",,,,ACL
216,2019,Unsupervised Rewriter for Multi-Sentence Compression,"Yang Zhao, Xiaoyu Shen, Wei Bi, Akiko Aizawa","Multi-sentence compression (MSC) aims to generate a grammatical but reduced compression from multiple input sentences while retaining their key information. Previous dominating approach for MSC is the extraction-based word graph approach. A few variants further leveraged lexical substitution to yield more abstractive compression. However, two limitations exist. First, the word graph approach that simply concatenates fragments from multiple sentences may yield non-fluent or ungrammatical compression. Second, lexical substitution is often inappropriate without the consideration of context information. To tackle the above-mentioned issues, we present a neural rewriter for multi-sentence compression that does not need any parallel corpus. Empirical studies have shown that our approach achieves comparable results upon automatic evaluation and improves the grammaticality of compression based on human evaluation. A parallel corpus with more than 140,000 (sentence group, compression) pairs is also constructed as a by-product for future research.",,,,ACL
217,2019,Inferential Machine Comprehension: Answering Questions by Recursively Deducing the Evidence Chain from Text,"Jianxing Yu, Zhengjun Zha, Jian Yin","This paper focuses on the topic of inferential machine comprehension, which aims to fully understand the meanings of given text to answer generic questions, especially the ones needed reasoning skills. In particular, we first encode the given document, question and options in a context aware way. We then propose a new network to solve the inference problem by decomposing it into a series of attention-based reasoning steps. The result of the previous step acts as the context of next step. To make each step can be directly inferred from the text, we design an operational cell with prior structure. By recursively linking the cells, the inferred results are synthesized together to form the evidence chain for reasoning, where the reasoning direction can be guided by imposing structural constraints to regulate interactions on the cells. Moreover, a termination mechanism is introduced to dynamically determine the uncertain reasoning depth, and the network is trained by reinforcement learning. Experimental results on 3 popular data sets, including MCTest, RACE and MultiRC, demonstrate the effectiveness of our approach.",,,,ACL
218,2019,Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension,"Yimeng Zhuang, Huadong Wang","Multi-passage reading comprehension requires the ability to combine cross-passage information and reason over multiple passages to infer the answer. In this paper, we introduce the Dynamic Self-attention Network (DynSAN) for multi-passage reading comprehension task, which processes cross-passage information at token-level and meanwhile avoids substantial computational costs. The core module of the dynamic self-attention is a proposed gated token selection mechanism, which dynamically selects important tokens from a sequence. These chosen tokens will attend to each other via a self-attention mechanism to model long-range dependencies. Besides, convolutional layers are combined with the dynamic self-attention to enhance the model’s capacity of extracting local semantic. The experimental results show that the proposed DynSAN achieves new state-of-the-art performance on the SearchQA, Quasar-T and WikiHop datasets. Further ablation study also validates the effectiveness of our model components.",,,,ACL
219,2019,Explicit Utilization of General Knowledge in Machine Reading Comprehension,"Chao Wang, Hui Jiang","To bridge the gap between Machine Reading Comprehension (MRC) models and human beings, which is mainly reflected in the hunger for data and the robustness to noise, in this paper, we explore how to integrate the neural networks of MRC models with the general knowledge of human beings. On the one hand, we propose a data enrichment method, which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage-question pair. On the other hand, we propose an end-to-end MRC model named as Knowledge Aided Reader (KAR), which explicitly uses the above extracted general knowledge to assist its attention mechanisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them. When only a subset (20%-80%) of the training examples are available, KAR outperforms the state-of-the-art MRC models by a large margin, and is still reasonably robust to noise.",,,,ACL
220,2019,Multi-style Generative Reading Comprehension,"Kyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda, Atsushi Otsuka","This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We propose a multi-style abstractive summarization model for question answering, called Masque. The proposed model has two key characteristics. First, unlike most studies on RC that have focused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications. Second, whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model, our approach learns multi-style answers within a model to improve the NLG capability for all styles involved. This also enables our model to give an answer in the target style. Experiments show that our model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the transfer of the style-independent NLG capability to the target style is the key to its success.",,,,ACL
221,2019,"Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension","Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li","This paper considers the reading comprehension task in which multiple documents are given as input. Prior work has shown that a pipeline of retriever, reader, and reranker can improve the overall performance. However, the pipeline system is inefficient since the input is re-encoded within each module, and is unable to leverage upstream components to help downstream training. In this work, we present RE3QA, a unified question answering model that combines context retrieving, reading comprehension, and answer reranking to predict the final answer. Unlike previous pipelined approaches, RE3QA shares contextualized text representation across different components, and is carefully designed to use high-quality upstream outputs (e.g., retrieved context or candidate answers) for directly supervising downstream modules (e.g., the reader or the reranker). As a result, the whole network can be trained end-to-end to avoid the context inconsistency problem. Experiments show that our model outperforms the pipelined baseline and achieves state-of-the-art results on two versions of TriviaQA and two variants of SQuAD.",,,,ACL
222,2019,Multi-Hop Paragraph Retrieval for Open-Domain Question Answering,"Yair Feldman, Ran El-Yaniv","This paper is concerned with the task of multi-hop open-domain Question Answering (QA). This task is particularly challenging since it requires the simultaneous performance of textual reasoning and efficient searching. We present a method for retrieving multiple supporting paragraphs, nested amidst a large knowledge base, which contain the necessary evidence to answer a given question. Our method iteratively retrieves supporting paragraphs by forming a joint vector representation of both a question and a paragraph. The retrieval is performed by considering contextualized sentence-level representations of the paragraphs in the knowledge source. Our method achieves state-of-the-art performance over two well-known datasets, SQuAD-Open and HotpotQA, which serve as our single- and multi-hop open-domain QA benchmarks, respectively.",,,,ACL
223,2019,E3: Entailment-driven Extracting and Editing for Conversational Machine Reading,"Victor Zhong, Luke Zettlemoyer","Conversational machine reading systems help users answer high-level questions (e.g. determine if they qualify for particular government benefits) when they do not know the exact rules by which the determination is made (e.g. whether they need certain income levels or veteran status). The key challenge is that these rules are only provided in the form of a procedural text (e.g. guidelines from government website) which the system must read to figure out what to ask the user. We present a new conversational machine reading model that jointly extracts a set of decision rules from the procedural text while reasoning about which are entailed by the conversational history and which still need to be edited to create questions for the user. On the recently introduced ShARC conversational machine reading dataset, our Entailment-driven Extract and Edit network (E3) achieves a new state-of-the-art, outperforming existing systems as well as a new BERT-based baseline. In addition, by explicitly highlighting which information still needs to be gathered, E3 provides a more explainable alternative to prior work. We release source code for our models and experiments at https://github.com/vzhong/e3.",,,,ACL
224,2019,Generating Question-Answer Hierarchies,"Kalpesh Krishna, Mohit Iyyer","The process of knowledge acquisition can be viewed as a question-answer game between a student and a teacher in which the student typically starts by asking broad, open-ended questions before drilling down into specifics (Hintikka, 1981; Hakkarainen and Sintonen, 2002). This pedagogical perspective motivates a new way of representing documents. In this paper, we present SQUASH (Specificity-controlled Question-Answer Hierarchies), a novel and challenging text generation task that converts an input document into a hierarchy of question-answer pairs. Users can click on high-level questions (e.g., “Why did Frodo leave the Fellowship?”) to reveal related but more specific questions (e.g., “Who did Frodo leave with?”). Using a question taxonomy loosely based on Lehnert (1978), we classify questions in existing reading comprehension datasets as either GENERAL or SPECIFIC . We then use these labels as input to a pipelined system centered around a conditional neural language model. We extensively evaluate the quality of the generated QA hierarchies through crowdsourced experiments and report strong empirical results.",,,,ACL
225,2019,Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction,"Kosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito","Question answering (QA) using textual sources for purposes such as reading comprehension (RC) has attracted much attention. This study focuses on the task of explainable multi-hop QA, which requires the system to return the answer with evidence sentences by reasoning and gathering disjoint pieces of the reference texts. It proposes the Query Focused Extractor (QFE) model for evidence extraction and uses multi-task learning with the QA model. QFE is inspired by extractive summarization models; compared with the existing method, which extracts each evidence sentence independently, it sequentially extracts evidence sentences by using an RNN with an attention mechanism on the question sentence. It enables QFE to consider the dependency among the evidence sentences and cover important information in the question sentence. Experimental results show that QFE with a simple RC baseline model achieves a state-of-the-art evidence extraction score on HotpotQA. Although designed for RC, it also achieves a state-of-the-art evidence extraction score on FEVER, which is a recognizing textual entailment task on a large textual database.",,,,ACL
226,2019,Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension,"An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu","Machine reading comprehension (MRC) is a crucial and challenging task in NLP. Recently, pre-trained language models (LMs), especially BERT, have achieved remarkable success, presenting new state-of-the-art results in MRC. In this work, we investigate the potential of leveraging external knowledge bases (KBs) to further improve BERT for MRC. We introduce KT-NET, which employs an attention mechanism to adaptively select desired knowledge from KBs, and then fuses selected knowledge with BERT to enable context- and knowledge-aware predictions. We believe this would combine the merits of both deep LMs and curated KBs towards better MRC. Experimental results indicate that KT-NET offers significant and consistent improvements over BERT, outperforming competitive baselines on ReCoRD and SQuAD1.1 benchmarks. Notably, it ranks the 1st place on the ReCoRD leaderboard, and is also the best single model on the SQuAD1.1 leaderboard at the time of submission (March 4th, 2019).",,,,ACL
227,2019,XQA: A Cross-lingual Open-domain Question Answering Dataset,"Jiahua Liu, Yankai Lin, Zhiyuan Liu, Maosong Sun","Open-domain question answering (OpenQA) aims to answer questions through text retrieval and reading comprehension. Recently, lots of neural network-based models have been proposed and achieved promising results in OpenQA. However, the success of these models relies on a massive volume of training data (usually in English), which is not available in many other languages, especially for those low-resource languages. Therefore, it is essential to investigate cross-lingual OpenQA. In this paper, we construct a novel dataset XQA for cross-lingual OpenQA research. It consists of a training set in English as well as development and test sets in eight other languages. Besides, we provide several baseline systems for cross-lingual OpenQA, including two machine translation-based methods and one zero-shot cross-lingual method (multilingual BERT). Experimental results show that the multilingual BERT model achieves the best results in almost all target languages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA.",,,,ACL
228,2019,Compound Probabilistic Context-Free Grammars for Grammar Induction,"Yoon Kim, Chris Dyer, Alexander Rush","We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models.",,,,ACL
229,2019,Semi-supervised Domain Adaptation for Dependency Parsing,"Zhenghua Li, Xue Peng, Min Zhang, Rui Wang, Luo Si","During the past decades, due to the lack of sufficient labeled data, most studies on cross-domain parsing focus on unsupervised domain adaptation, assuming there is no target-domain training data. However, unsupervised approaches make limited progress so far due to the intrinsic difficulty of both domain adaptation and parsing. This paper tackles the semi-supervised domain adaptation problem for Chinese dependency parsing, based on two newly-annotated large-scale domain-aware datasets. We propose a simple domain embedding approach to merge the source- and target-domain training data, which is shown to be more effective than both direct corpus concatenation and multi-task learning. In order to utilize unlabeled target-domain data, we employ the recent contextualized word representations and show that a simple fine-tuning procedure can further boost cross-domain parsing accuracy by large margin.",,,,ACL
230,2019,Head-Driven Phrase Structure Grammar Parsing on Penn Treebank,"Junru Zhou, Hai Zhao","Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings. This paper makes the first attempt to formulate a simplified HPSG by integrating constituent and dependency formal representations into head-driven phrase structure. Then two parsing algorithms are respectively proposed for two converted tree representations, division span and joint span. As HPSG encodes both constituent and dependency structure information, the proposed HPSG parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees. Our parser achieves new state-of-the-art performance for both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank, verifying the effectiveness of joint learning constituent and dependency structures. In details, we report 95.84 F1 of constituent parsing and 97.00% UAS of dependency parsing on PTB.",,,,ACL
231,2019,Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning,"Minlong Peng, Xiaoyu Xing, Qi Zhang, Jinlan Fu, Xuanjing Huang","In this work, we explore the way to perform named entity recognition (NER) using only unlabeled data and named entity dictionaries. To this end, we formulate the task as a positive-unlabeled (PU) learning problem and accordingly propose a novel PU learning algorithm to perform the task. We prove that the proposed algorithm can unbiasedly and consistently estimate the task loss as if there is fully labeled data. A key feature of the proposed method is that it does not require the dictionaries to label every entity within a sentence, and it even does not require the dictionaries to label all of the words constituting an entity. This greatly reduces the requirement on the quality of the dictionaries and makes our method generalize well with quite simple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at https://github.com/v-mipeng/LexiconNER.",,,,ACL
232,2019,Multi-Task Semantic Dependency Parsing with Policy Gradient for Learning Easy-First Strategies,"Shuhei Kurita, Anders Søgaard","In Semantic Dependency Parsing (SDP), semantic relations form directed acyclic graphs, rather than trees. We propose a new iterative predicate selection (IPS) algorithm for SDP. Our IPS algorithm combines the graph-based and transition-based parsing approaches in order to handle multiple semantic head words. We train the IPS model using a combination of multi-task learning and task-specific policy gradient training. Trained this way, IPS achieves a new state of the art on the SemEval 2015 Task 18 datasets. Furthermore, we observe that policy gradient training learns an easy-first strategy.",,,,ACL
233,2019,GCDT: A Global Context Enhanced Deep Transition Architecture for Sequence Labeling,"Yijin Liu, Fandong Meng, Jinchao Zhang, Jinan Xu, Yufeng Chen","Current state-of-the-art systems for sequence labeling are typically based on the family of Recurrent Neural Networks (RNNs). However, the shallow connections between consecutive hidden states of RNNs and insufficient modeling of global information restrict the potential performance of those models. In this paper, we try to address these issues, and thus propose a Global Context enhanced Deep Transition architecture for sequence labeling named GCDT. We deepen the state transition path at each position in a sentence, and further assign every token with a global representation learned from the entire sentence. Experiments on two standard sequence labeling tasks show that, given only training data and the ubiquitous word embeddings (Glove), our GCDT achieves 91.96 F1 on the CoNLL03 NER task and 95.43 F1 on the CoNLL2000 Chunking task, which outperforms the best reported results under the same settings. Furthermore, by leveraging BERT as an additional resource, we establish new state-of-the-art results with 93.47 F1 on NER and 97.30 F1 on Chunking.",,,,ACL
234,2019,Unsupervised Learning of PCFGs with Normalizing Flow,"Lifeng Jin, Finale Doshi-Velez, Timothy Miller, Lane Schwartz, William Schuler","Unsupervised PCFG inducers hypothesize sets of compact context-free rules as explanations for sentences. PCFG induction not only provides tools for low-resource languages, but also plays an important role in modeling language acquisition (Bannard et al., 2009; Abend et al. 2017). However, current PCFG induction models, using word tokens as input, are unable to incorporate semantics and morphology into induction, and may encounter issues of sparse vocabulary when facing morphologically rich languages. This paper describes a neural PCFG inducer which employs context embeddings (Peters et al., 2018) in a normalizing flow model (Dinh et al., 2015) to extend PCFG induction to use semantic and morphological information. Linguistically motivated sparsity and categorical distance constraints are imposed on the inducer as regularization. Experiments show that the PCFG induction model with normalizing flow produces grammars with state-of-the-art accuracy on a variety of different languages. Ablation further shows a positive effect of normalizing flow, context embeddings and proposed regularizers.",,,,ACL
235,2019,Variance of Average Surprisal: A Better Predictor for Quality of Grammar from Unsupervised PCFG Induction,"Lifeng Jin, William Schuler","In unsupervised grammar induction, data likelihood is known to be only weakly correlated with parsing accuracy, especially at convergence after multiple runs. In order to find a better indicator for quality of induced grammars, this paper correlates several linguistically- and psycholinguistically-motivated predictors to parsing accuracy on a large multilingual grammar induction evaluation data set. Results show that variance of average surprisal (VAS) better correlates with parsing accuracy than data likelihood and that using VAS instead of data likelihood for model selection provides a significant accuracy boost. Further evidence shows VAS to be a better candidate than data likelihood for predicting word order typology classification. Analyses show that VAS seems to separate content words from function words in natural language grammars, and to better arrange words with different frequencies into separate classes that are more consistent with linguistic theory.",,,,ACL
236,2019,Cross-Domain NER using Cross-Domain Language Modeling,"Chen Jia, Xiaobo Liang, Yue Zhang","Due to limitation of labeled resources, cross-domain named entity recognition (NER) has been a challenging task. Most existing work considers a supervised setting, making use of labeled data for both the source and target domains. A disadvantage of such methods is that they cannot train for domains without NER data. To address this issue, we consider using cross-domain LM as a bridge cross-domains for NER domain adaptation, performing cross-domain and cross-task knowledge transfer by designing a novel parameter generation network. Results show that our method can effectively extract domain differences from cross-domain LM contrast, allowing unsupervised domain adaptation while also giving state-of-the-art results among supervised domain adaptation methods.",,,,ACL
237,2019,Graph-based Dependency Parsing with Graph Neural Networks,"Tao Ji, Yuanbin Wu, Man Lan","We investigate the problem of efficiently incorporating high-order features into neural graph-based dependency parsing. Instead of explicitly extracting high-order features from intermediate parse trees, we develop a more powerful dependency tree node representation which captures high-order information concisely and efficiently. We use graph neural networks (GNNs) to learn the representations and discuss several new configurations of GNN’s updating and aggregation functions. Experiments on PTB show that our parser achieves the best UAS and LAS on PTB (96.0%, 94.3%) among systems without using any external resources.",,,,ACL
238,2019,Wide-Coverage Neural A* Parsing for Minimalist Grammars,"John Torr, Milos Stanojevic, Mark Steedman, Shay B. Cohen","Minimalist Grammars (Stabler, 1997) are a computationally oriented, and rigorous formalisation of many aspects of Chomsky’s (1995) Minimalist Program. This paper presents the first ever application of this formalism to the task of realistic wide-coverage parsing. The parser uses a linguistically expressive yet highly constrained grammar, together with an adaptation of the A* search algorithm currently used in CCG parsing (Lewis and Steedman, 2014; Lewis et al., 2016), with supertag probabilities provided by a bi-LSTM neural network supertagger trained on MGbank, a corpus of MG derivation trees. We report on some promising initial experimental results for overall dependency recovery as well as on the recovery of certain unbounded long distance dependencies. Finally, although like other MG parsers, ours has a high order polynomial worst case time complexity, we show that in practice its expected time complexity is cubic in the length of the sentence. The parser is publicly available.",,,,ACL
239,2019,Multi-Modal Sarcasm Detection in Twitter with Hierarchical Fusion Model,"Yitao Cai, Huiyu Cai, Xiaojun Wan","Sarcasm is a subtle form of language in which people express the opposite of what is implied. Previous works of sarcasm detection focused on texts. However, more and more social media platforms like Twitter allow users to create multi-modal messages, including texts, images, and videos. It is insufficient to detect sarcasm from multi-model messages based only on texts. In this paper, we focus on multi-modal sarcasm detection for tweets consisting of texts and images in Twitter. We treat text features, image features and image attributes as three modalities and propose a multi-modal hierarchical fusion model to address this task. Our model first extracts image features and attribute features, and then leverages attribute features and bidirectional LSTM network to extract text features. Features of three modalities are then reconstructed and fused into one feature vector for prediction. We create a multi-modal sarcasm detection dataset based on Twitter. Evaluation results on the dataset demonstrate the efficacy of our proposed model and the usefulness of the three modalities.",,,,ACL
240,2019,Topic-Aware Neural Keyphrase Generation for Social Media Language,"Yue Wang, Jing Li, Hou Pong Chan, Irwin King, Michael R. Lyu","A huge volume of user-generated content is daily produced on social media. To facilitate automatic language understanding, we study keyphrase prediction, distilling salient information from massive posts. While most existing methods extract words from source posts to form keyphrases, we propose a sequence-to-sequence (seq2seq) based neural keyphrase generation framework, enabling absent keyphrases to be created. Moreover, our model, being topic-aware, allows joint modeling of corpus-level latent topic representations, which helps alleviate data sparsity widely exhibited in social media language. Experiments on three datasets collected from English and Chinese social media platforms show that our model significantly outperforms both extraction and generation models without exploiting latent topics. Further discussions show that our model learns meaningful topics, which interprets its superiority in social media keyphrase generation.",,,,ACL
241,2019,#YouToo? Detection of Personal Recollections of Sexual Harassment on Social Media,"Arijit Ghosh Chowdhury, Ramit Sawhney, Rajiv Ratn Shah, Debanjan Mahata","The availability of large-scale online social data, coupled with computational methods can help us answer fundamental questions relat- ing to our social lives, particularly our health and well-being. The #MeToo trend has led to people talking about personal experiences of harassment more openly. This work at- tempts to aggregate such experiences of sex- ual abuse to facilitate a better understanding of social media constructs and to bring about social change. It has been found that disclo- sure of abuse has positive psychological im- pacts. Hence, we contend that such informa- tion can leveraged to create better campaigns for social change by analyzing how users react to these stories and to obtain a better insight into the consequences of sexual abuse. We use a three part Twitter-Specific Social Media Lan- guage Model to segregate personal recollec- tions of sexual harassment from Twitter posts. An extensive comparison with state-of-the-art generic and specific models along with a de- tailed error analysis explores the merit of our proposed model.",,,,ACL
242,2019,Multi-task Pairwise Neural Ranking for Hashtag Segmentation,"Mounica Maddela, Wei Xu, Daniel Preoţiuc-Pietro","Hashtags are often employed on social media and beyond to add metadata to a textual utterance with the goal of increasing discoverability, aiding search, or providing additional semantics. However, the semantic content of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation by framing it as a pairwise ranking problem between candidate segmentations. Our novel neural approaches demonstrate 24.6% error reduction in hashtag segmentation accuracy compared to the current state-of-the-art method. Finally, we demonstrate that a deeper understanding of hashtag semantics obtained through segmentation is useful for downstream applications such as sentiment analysis, for which we achieved a 2.6% increase in average recall on the SemEval 2017 sentiment analysis dataset.",,,,ACL
243,2019,Entity-Centric Contextual Affective Analysis,"Anjalie Field, Yulia Tsvetkov","While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks, their potential usefulness for social-oriented tasks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women.",,,,ACL
244,2019,Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks,"Jing Ma, Wei Gao, Shafiq Joty, Kam-Fai Wong","Claim verification is generally a task of verifying the veracity of a given claim, which is critical to many downstream applications. It is cumbersome and inefficient for human fact-checkers to find consistent pieces of evidence, from which solid verdict could be inferred against the claim. In this paper, we propose a novel end-to-end hierarchical attention network focusing on learning to represent coherent evidence as well as their semantic relatedness with the claim. Our model consists of three main components: 1) A coherence-based attention layer embeds coherent evidence considering the claim and sentences from relevant articles; 2) An entailment-based attention layer attends on sentences that can semantically infer the claim on top of the first attention; and 3) An output layer predicts the verdict based on the embedded evidence. Experimental results on three public benchmark datasets show that our proposed model outperforms a set of state-of-the-art baselines.",,,,ACL
245,2019,Predicting Human Activities from User-Generated Content,"Steven Wilson, Rada Mihalcea","The activities we do are linked to our interests, personality, political preferences, and decisions we make about the future. In this paper, we explore the task of predicting human activities from user-generated content. We collect a dataset containing instances of social media users writing about a range of everyday activities. We then use a state-of-the-art sentence embedding framework tailored to recognize the semantics of human activities and perform an automatic clustering of these activities. We train a neural network model to make predictions about which clusters contain activities that were performed by a given user based on the text of their previous posts and self-description. Additionally, we explore the degree to which incorporating inferred user traits into our model helps with this prediction task.",,,,ACL
246,2019,You Write like You Eat: Stylistic Variation as a Predictor of Social Stratification,"Angelo Basile, Albert Gatt, Malvina Nissim","Inspired by Labov’s seminal work on stylisticvariation as a function of social stratification,we develop and compare neural models thatpredict a person’s presumed socio-economicstatus, obtained through distant supervision,from their writing style on social media. Thefocus of our work is on identifying the mostimportant stylistic parameters to predict socio-economic group. In particular, we show theeffectiveness of morpho-syntactic features aspredictors of style, in contrast to lexical fea-tures, which are good predictors of topic",,,,ACL
247,2019,Encoding Social Information with Graph Convolutional Networks forPolitical Perspective Detection in News Media,"Chang Li, Dan Goldwasser","Identifying the political perspective shaping the way news events are discussed in the media is an important and challenging task. In this paper, we highlight the importance of contextualizing social information, capturing how this information is disseminated in social networks. We use Graph Convolutional Networks, a recently proposed neural architecture for representing relational information, to capture the documents’ social context. We show that social information can be used effectively as a source of distant supervision, and when direct supervision is available, even little social information can significantly improve performance.",,,,ACL
248,2019,Fine-Grained Spoiler Detection from Large-Scale Review Corpora,"Mengting Wan, Rishabh Misra, Ndapa Nakashole, Julian McAuley","This paper presents computational approaches for automatically detecting critical plot twists in reviews of media products. First, we created a large-scale book review dataset that includes fine-grained spoiler annotations at the sentence-level, as well as book and (anonymized) user information. Second, we carefully analyzed this dataset, and found that: spoiler language tends to be book-specific; spoiler distributions vary greatly across books and review authors; and spoiler sentences tend to jointly appear in the latter part of reviews. Third, inspired by these findings, we developed an end-to-end neural network architecture to detect spoiler sentences in review corpora. Quantitative and qualitative results demonstrate that the proposed method substantially outperforms existing baselines.",,,,ACL
249,2019,Celebrity Profiling,"Matti Wiegmann, Benno Stein, Martin Potthast","Celebrities are among the most prolific users of social media, promoting their personas and rallying followers. This activity is closely tied to genuine writing samples, which makes them worthy research subjects in many respects, not least profiling. With this paper we introduce the Webis Celebrity Corpus 2019. For its construction the Twitter feeds of 71,706 verified accounts have been carefully linked with their respective Wikidata items, crawling both. After cleansing, the resulting profiles contain an average of 29,968 words per profile and up to 239 pieces of personal information. A cross-evaluation that checked the correct association of Twitter account and Wikidata item revealed an error rate of only 0.6%, rendering the profiles highly reliable. Our corpus comprises a wide cross-section of local and global celebrities, forming a unique combination of scale, profile comprehensiveness, and label reliability. We further establish the state of the art’s profiling performance by evaluating the winning approaches submitted to the PAN gender prediction tasks in a transfer learning experiment. They are only outperformed by our own deep learning approach, which we also use to exemplify celebrity occupation prediction for the first time.",,,,ACL
250,2019,Dataset Creation for Ranking Constructive News Comments,"Soichiro Fujita, Hayato Kobayashi, Manabu Okumura","Ranking comments on an online news service is a practically important task for the service provider, and thus there have been many studies on this task. However, most of them considered users’ positive feedback, such as “Like”-button clicks, as a quality measure. In this paper, we address directly evaluating the quality of comments on the basis of “constructiveness,” separately from user feedback. To this end, we create a new dataset including 100K+ Japanese comments with constructiveness scores (C-scores). Our experiments clarify that C-scores are not always related to users’ positive feedback, and the performance of pairwise ranking models tends to be enhanced by the variation of comments rather than articles.",,,,ACL
251,2019,Enhancing Air Quality Prediction with Social Media and Natural Language Processing,"Jyun-Yu Jiang, Xue Sun, Wei Wang, Sean Young","Accompanied by modern industrial developments, air pollution has already become a major concern for human health. Hence, air quality measures, such as the concentration of PM2.5, have attracted increasing attention. Even some studies apply historical measurements into air quality forecast, the changes of air quality conditions are still hard to monitor. In this paper, we propose to exploit social media and natural language processing techniques to enhance air quality prediction. Social media users are treated as social sensors with their findings and locations. After filtering noisy tweets using word selection and topic modeling, a deep learning model based on convolutional neural networks and over-tweet-pooling is proposed to enhance air quality prediction. We conduct experiments on 7-month real-world Twitter datasets in the five most heavily polluted states in the USA. The results show that our approach significantly improves air quality prediction over the baseline that does not use social media by 6.9% to 17.7% in macro-F1 scores.",,,,ACL
252,2019,Twitter Homophily: Network Based Prediction of User’s Occupation,"Jiaqi Pan, Rishabh Bhardwaj, Wei Lu, Hai Leong Chieu, Xinghao Pan","In this paper, we investigate the importance of social network information compared to content information in the prediction of a Twitter user’s occupational class. We show that the content information of a user’s tweets, the profile descriptions of a user’s follower/following community, and the user’s social network provide useful information for classifying a user’s occupational group. In our study, we extend an existing data set for this problem, and we achieve significantly better performance by using social network homophily that has not been fully exploited in previous work. In our analysis, we found that by using the graph convolutional network to exploit social homophily, we can achieve competitive performance on this data set with just a small fraction of the training data.",,,,ACL
253,2019,Domain Adaptive Dialog Generation via Meta Learning,"Kun Qian, Zhou Yu","Domain adaptation is an essential task in dialog system building because there are so many new dialog tasks created for different needs every day. Collecting and annotating training data for these new tasks is costly since it involves real user interactions. We propose a domain adaptive dialog generation method based on meta-learning (DAML). DAML is an end-to-end trainable dialog system model that learns from multiple rich-resource tasks and then adapts to new domains with minimal training samples. We train a dialog system model using multiple rich-resource single-domain dialog data by applying the model-agnostic meta-learning algorithm to dialog domain. The model is capable of learning a competitive dialog system on a new domain with only a few training examples in an efficient manner. The two-step gradient updates in DAML enable the model to learn general features across multiple tasks. We evaluate our method on a simulated dialog dataset and achieve state-of-the-art performance, which is generalizable to new tasks.",,,,ACL
254,2019,Strategies for Structuring Story Generation,"Angela Fan, Mike Lewis, Yann Dauphin","Writers often rely on plans or sketches to write long stories, but most current language models generate word by word from left to right. We explore coarse-to-fine models for creating narrative texts of several hundred words, and introduce new models which decompose stories by abstracting over actions and entities. The model first generates the predicate-argument structure of the text, where different mentions of the same entity are marked with placeholder tokens. It then generates a surface realization of the predicate-argument structure, and finally replaces the entity placeholders with context-sensitive names and references. Human judges prefer the stories from our models to a wide range of previous approaches to hierarchical text generation. Extensive analysis shows that our methods can help improve the diversity and coherence of events and entities in generated stories.",,,,ACL
255,2019,"Argument Generation with Retrieval, Planning, and Realization","Xinyu Hua, Zhe Hu, Lu Wang","Automatic argument generation is an appealing but challenging task. In this paper, we study the specific problem of counter-argument generation, and present a novel framework, CANDELA. It consists of a powerful retrieval system and a novel two-step generation model, where a text planning decoder first decides on the main talking points and a proper language style for each sentence, then a content realization decoder reflects the decisions and constructs an informative paragraph-level argument. Furthermore, our generation model is empowered by a retrieval system indexed with 12 million articles collected from Wikipedia and popular English news media, which provides access to high-quality content with diversity. Automatic evaluation on a large-scale dataset collected from Reddit shows that our model yields significantly higher BLEU, ROUGE, and METEOR scores than the state-of-the-art and non-trivial comparisons. Human evaluation further indicates that our system arguments are more appropriate for refutation and richer in content.",,,,ACL
256,2019,A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation,"Feng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, Chin-Yew Lin","Recent neural language generation systems often hallucinate contents (i.e., producing irrelevant or contradicted facts), especially when trained on loosely corresponding pairs of the input structure and text. To mitigate this issue, we propose to integrate a language understanding module for data refinement with self-training iterations to effectively induce strong equivalence between the input data and the paired text. Experiments on the E2E challenge dataset show that our proposed framework can reduce more than 50% relative unaligned noise from the original data-text pairs. A vanilla sequence-to-sequence neural NLG model trained on the refined data has improved on content correctness compared with the current state-of-the-art ensemble generator.",,,,ACL
257,2019,Cross-Modal Commentator: Automatic Machine Commenting Based on Cross-Modal Information,"Pengcheng Yang, Zhihan Zhang, Fuli Luo, Lei Li, Chengyang Huang","Automatic commenting of online articles can provide additional opinions and facts to the reader, which improves user experience and engagement on social media platforms. Previous work focuses on automatic commenting based solely on textual content. However, in real-scenarios, online articles usually contain multiple modal contents. For instance, graphic news contains plenty of images in addition to text. Contents other than text are also vital because they are not only more attractive to the reader but also may provide critical information. To remedy this, we propose a new task: cross-model automatic commenting (CMAC), which aims to make comments by integrating multiple modal contents. We construct a large-scale dataset for this task and explore several representative methods. Going a step further, an effective co-attention model is presented to capture the dependency between textual and visual information. Evaluation results show that our proposed model can achieve better performance than competitive baselines.",,,,ACL
258,2019,A Working Memory Model for Task-oriented Dialog Response Generation,"Xiuyi Chen, Jiaming Xu, Bo Xu","Recently, to incorporate external Knowledge Base (KB) information, one form of world knowledge, several end-to-end task-oriented dialog systems have been proposed. These models, however, tend to confound the dialog history with KB tuples and simply store them into one memory. Inspired by the psychological studies on working memory, we propose a working memory model (WMM2Seq) for dialog response generation. Our WMM2Seq adopts a working memory to interact with two separated long-term memories, which are the episodic memory for memorizing dialog history and the semantic memory for storing KB tuples. The working memory consists of a central executive to attend to the aforementioned memories, and a short-term storage system to store the “activated” contents from the long-term memories. Furthermore, we introduce a context-sensitive perceptual process for the token representations of dialog history, and then feed them into the episodic memory. Extensive experiments on two task-oriented dialog datasets demonstrate that our WMM2Seq significantly outperforms the state-of-the-art results in several evaluation metrics.",,,,ACL
259,2019,Cognitive Graph for Multi-Hop Reading Comprehension at Scale,"Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, Jie Tang","We propose a new CogQA framework for multi-hop reading comprehension question answering in web-scale documents. Founded on the dual process theory in cognitive science, the framework gradually builds a cognitive graph in an iterative process by coordinating an implicit extraction module (System 1) and an explicit reasoning module (System 2). While giving accurate answers, our framework further provides explainable reasoning paths. Specifically, our implementation based on BERT and graph neural network efficiently handles millions of documents for multi-hop reasoning questions in the HotpotQA fullwiki dataset, achieving a winning joint F1 score of 34.9 on the leaderboard, compared to 23.1 of the best competitor.",,,,ACL
260,2019,Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs,"Ming Tu, Guangtao Wang, Jing Huang, Yun Tang, Xiaodong He","Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.",,,,ACL
261,2019,"Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension","Yichen Jiang, Nitish Joshi, Yen-Chun Chen, Mohit Bansal","Multi-hop reading comprehension requires the model to explore and connect relevant information from multiple sentences/documents in order to answer the question about the context. To achieve this, we propose an interpretable 3-module system called Explore-Propose-Assemble reader (EPAr). First, the Document Explorer iteratively selects relevant documents and represents divergent reasoning chains in a tree structure so as to allow assimilating information from all chains. The Answer Proposer then proposes an answer from every root-to-leaf path in the reasoning tree. Finally, the Evidence Assembler extracts a key sentence containing the proposed answer from every path and combines them to predict the final answer. Intuitively, EPAr approximates the coarse-to-fine-grained comprehension behavior of human readers when facing multiple long documents. We jointly optimize our 3 modules by minimizing the sum of losses from each stage conditioned on the previous stage’s output. On two multi-hop reading comprehension datasets WikiHop and MedHop, our EPAr model achieves significant improvements over the baseline and competitive results compared to the state-of-the-art model. We also present multiple reasoning-chain-recovery tests and ablation studies to demonstrate our system’s ability to perform interpretable and accurate reasoning.",,,,ACL
262,2019,"Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA","Yichen Jiang, Mohit Bansal","Multi-hop question answering requires a model to connect multiple pieces of evidence scattered in a long context to answer the question. In this paper, we show that in the multi-hop HotpotQA (Yang et al., 2018) dataset, the examples often contain reasoning shortcuts through which models can directly locate the answer by word-matching the question with a sentence in the context. We demonstrate this issue by constructing adversarial documents that create contradicting answers to the shortcut but do not affect the validity of the original answer. The performance of strong baseline models drops significantly on our adversarial test, indicating that they are indeed exploiting the shortcuts rather than performing multi-hop reasoning. After adversarial training, the baseline’s performance improves but is still limited on the adversarial test. Hence, we use a control unit that dynamically attends to the question at different reasoning hops to guide the model’s multi-hop reasoning. We show that our 2-hop model trained on the regular data is more robust to the adversaries than the baseline. After adversarial training, it not only achieves significant improvements over its counterpart trained on regular data, but also outperforms the adversarially-trained baseline significantly. Finally, we sanity-check that these improvements are not obtained by exploiting potential new shortcuts in the adversarial data, but indeed due to robust multi-hop reasoning skills of the models.",,,,ACL
263,2019,Exploiting Explicit Paths for Multi-hop Reading Comprehension,"Souvik Kundu, Tushar Khot, Ashish Sabharwal, Peter Clark","We propose a novel, path-based reasoning approach for the multi-hop reading comprehension task where a system needs to combine facts from multiple passages to answer a question. Although inspired by multi-hop reasoning over knowledge graphs, our proposed approach operates directly over unstructured text. It generates potential paths through passages and scores them without any direct path supervision. The proposed model, named PathNet, attempts to extract implicit relations from text through entity pair representations, and compose them to encode each path. To capture additional context, PathNet also composes the passage representations along each path to compute a passage-based representation. Unlike previous approaches, our model is then able to explain its reasoning via these explicit paths through the passages. We show that our approach outperforms prior models on the multi-hop Wikihop dataset, and also can be generalized to apply to the OpenBookQA dataset, matching state-of-the-art performance.",,,,ACL
264,2019,Sentence Mover’s Similarity: Automatic Evaluation for Multi-Sentence Texts,"Elizabeth Clark, Asli Celikyilmaz, Noah A. Smith","For evaluating machine-generated texts, automatic methods hold the promise of avoiding collection of human judgments, which can be expensive and time-consuming. The most common automatic metrics, like BLEU and ROUGE, depend on exact word matching, an inflexible approach for measuring semantic similarity. We introduce methods based on sentence mover’s similarity; our automatic metrics evaluate text in a continuous space using word and sentence embeddings. We find that sentence-based metrics correlate with human judgments significantly better than ROUGE, both on machine-generated summaries (average length of 3.4 sentences) and human-authored essays (average length of 7.5). We also show that sentence mover’s similarity can be used as a reward when learning a generation model via reinforcement learning; we present both automatic and human evaluations of summaries learned in this way, finding that our approach outperforms ROUGE.",,,,ACL
265,2019,Analysis of Automatic Annotation Suggestions for Hard Discourse-Level Tasks in Expert Domains,"Claudia Schulz, Christian M. Meyer, Jan Kiesewetter, Michael Sailer, Elisabeth Bauer","Many complex discourse-level tasks can aid domain experts in their work but require costly expert annotations for data creation. To speed up and ease annotations, we investigate the viability of automatically generated annotation suggestions for such tasks. As an example, we choose a task that is particularly hard for both humans and machines: the segmentation and classification of epistemic activities in diagnostic reasoning texts. We create and publish a new dataset covering two domains and carefully analyse the suggested annotations. We find that suggestions have positive effects on annotation speed and performance, while not introducing noteworthy biases. Envisioning suggestion models that improve with newly annotated texts, we contrast methods for continuous model adjustment and suggest the most effective setup for suggestions in future expert tasks.",,,,ACL
266,2019,Deep Dominance - How to Properly Compare Deep Neural Models,"Rotem Dror, Segev Shlomov, Roi Reichart","Comparing between Deep Neural Network (DNN) models based on their performance on unseen data is crucial for the progress of the NLP field. However, these models have a large number of hyper-parameters and, being non-convex, their convergence point depends on the random values chosen at initialization and during training. Proper DNN comparison hence requires a comparison between their empirical score distributions on unseen data, rather than between single evaluation scores as is standard for more simple, convex models. In this paper, we propose to adapt to this problem a recently proposed test for the Almost Stochastic Dominance relation between two distributions. We define the criteria for a high quality comparison method between DNNs, and show, both theoretically and through analysis of extensive experimental results with leading DNN models for sequence tagging tasks, that the proposed test meets all criteria while previously proposed methods fail to do so. We hope the test we propose here will set a new working practice in the NLP community.",,,,ACL
267,2019,We Need to Talk about Standard Splits,"Kyle Gorman, Steven Bedrick","It is standard practice in speech & language technology to rank systems according to their performance on a test set held out for evaluation. However, few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018, each of which claimed state-of-the-art performance on a widely-used “standard split”. While we replicate results on the standard split, we fail to reliably reproduce some rankings when we repeat this analysis with randomly generated training-testing splits. We argue that randomly generated splits should be used in system evaluation.",,,,ACL
268,2019,Aiming beyond the Obvious: Identifying Non-Obvious Cases in Semantic Similarity Datasets,"Nicole Peinelt, Maria Liakata, Dong Nguyen",Existing datasets for scoring text pairs in terms of semantic similarity contain instances whose resolution differs according to the degree of difficulty. This paper proposes to distinguish obvious from non-obvious text pairs based on superficial lexical overlap and ground-truth labels. We characterise existing datasets in terms of containing difficult cases and find that recently proposed models struggle to capture the non-obvious cases of semantic similarity. We describe metrics that emphasise cases of similarity which require more complex inference and propose that these are used for evaluating systems for semantic similarity.,,,,ACL
269,2019,Putting Evaluation in Context: Contextual Embeddings Improve Machine Translation Evaluation,"Nitika Mathur, Timothy Baldwin, Trevor Cohn","Accurate, automatic evaluation of machine translation is critical for system tuning, and evaluating progress in the field. We proposed a simple unsupervised metric, and additional supervised metrics which rely on contextual word embeddings to encode the translation and reference sentences. We find that these models rival or surpass all existing metrics in the WMT 2017 sentence-level and system-level tracks, and our trained model has a substantially higher correlation with human judgements than all existing metrics on the WMT 2017 to-English sentence level dataset.",,,,ACL
270,2019,Joint Effects of Context and User History for Predicting Online Conversation Re-entries,"Xingshan Zeng, Jing Li, Lu Wang, Kam-Fai Wong","As the online world continues its exponential growth, interpersonal communication has come to play an increasingly central role in opinion formation and change. In order to help users better engage with each other online, we study a challenging problem of re-entry prediction foreseeing whether a user will come back to a conversation they once participated in. We hypothesize that both the context of the ongoing conversations and the users’ previous chatting history will affect their continued interests in future engagement. Specifically, we propose a neural framework with three main layers, each modeling context, user history, and interactions between them, to explore how the conversation context and user chatting history jointly result in their re-entry behavior. We experiment with two large-scale datasets collected from Twitter and Reddit. Results show that our proposed framework with bi-attention achieves an F1 score of 61.1 on Twitter conversations, outperforming the state-of-the-art methods from previous work.",,,,ACL
271,2019,CONAN - COunter NArratives through Nichesourcing: a Multilingual Dataset of Responses to Fight Online Hate Speech,"Yi-Ling Chung, Elizaveta Kuzmenko, Serra Sinem Tekiroglu, Marco Guerini","Although there is an unprecedented effort to provide adequate responses in terms of laws and policies to hate content on social media platforms, dealing with hatred online is still a tough problem. Tackling hate speech in the standard way of content deletion or user suspension may be charged with censorship and overblocking. One alternate strategy, that has received little attention so far by the research community, is to actually oppose hate content with counter-narratives (i.e. informed textual responses). In this paper, we describe the creation of the first large-scale, multilingual, expert-based dataset of hate-speech/counter-narrative pairs. This dataset has been built with the effort of more than 100 operators from three different NGOs that applied their training and expertise to the task. Together with the collected data we also provide additional annotations about expert demographics, hate and response type, and data augmentation through translation and paraphrasing. Finally, we provide initial experiments to assess the quality of our data.",,,,ACL
272,2019,Categorizing and Inferring the Relationship between the Text and Image of Twitter Posts,"Alakananda Vempala, Daniel Preoţiuc-Pietro","Text in social media posts is frequently accompanied by images in order to provide content, supply context, or to express feelings. This paper studies how the meaning of the entire tweet is composed through the relationship between its textual content and its image. We build and release a data set of image tweets annotated with four classes which express whether the text or the image provides additional information to the other modality. We show that by combining the text and image information, we can build a machine learning approach that accurately distinguishes between the relationship types. Further, we derive insights into how these relationships are materialized through text and image content analysis and how they are impacted by user demographic traits. These methods can be used in several downstream applications including pre-training image tagging models, collecting distantly supervised data for image captioning, and can be directly used in end-user applications to optimize screen estate.",,,,ACL
273,2019,Who Sides with Whom? Towards Computational Construction of Discourse Networks for Political Debates,"Sebastian Padó, Andre Blessing, Nico Blokker, Erenay Dayanik, Sebastian Haunss","Understanding the structures of political debates (which actors make what claims) is essential for understanding democratic political decision making. The vision of computational construction of such discourse networks from newspaper reports brings together political science and natural language processing. This paper presents three contributions towards this goal: (a) a requirements analysis, linking the task to knowledge base population; (b) an annotated pilot corpus of migration claims based on German newspaper reports; (c) initial modeling results.",,,,ACL
274,2019,Analyzing Linguistic Differences between Owner and Staff Attributed Tweets,"Daniel Preoţiuc-Pietro, Rita Devlin Marier","Research on social media has to date assumed that all posts from an account are authored by the same person. In this study, we challenge this assumption and study the linguistic differences between posts signed by the account owner or attributed to their staff. We introduce a novel data set of tweets posted by U.S. politicians who self-reported their tweets using a signature. We analyze the linguistic topics and style features that distinguish the two types of tweets. Predictive results show that we are able to predict owner and staff attributed tweets with good accuracy, even when not using any training data from that account.",,,,ACL
275,2019,Exploring Author Context for Detecting Intended vs Perceived Sarcasm,"Silviu Oprea, Walid Magdy","We investigate the impact of using author context on textual sarcasm detection. We define author context as the embedded representation of their historical posts on Twitter and suggest neural models that extract these representations. We experiment with two tweet datasets, one labelled manually for sarcasm, and the other via tag-based distant supervision. We achieve state-of-the-art performance on the second dataset, but not on the one labelled manually, indicating a difference between intended sarcasm, captured by distant supervision, and perceived sarcasm, captured by manual labelling.",,,,ACL
276,2019,Open Domain Event Extraction Using Neural Latent Variable Models,"Xiao Liu, Heyan Huang, Yue Zhang","We consider open domain event extraction, the task of extracting unconstraint types of events from news clusters. A novel latent variable neural model is constructed, which is scalable to very large corpus. A dataset is collected and manually annotated, with task-specific evaluation metrics being designed. Results show that the proposed unsupervised model gives better performance compared to the state-of-the-art method for event schema induction.",,,,ACL
277,2019,Multi-Level Matching and Aggregation Network for Few-Shot Relation Classification,"Zhi-Xiu Ye, Zhen-Hua Ling","This paper presents a multi-level matching and aggregation network (MLMAN) for few-shot relation classification. Previous studies on this topic adopt prototypical networks, which calculate the embedding vector of a query instance and the prototype vector of the support set for each relation candidate independently. On the contrary, our proposed MLMAN model encodes the query instance and each support set in an interactive way by considering their matching information at both local and instance levels. The final class prototype for each support set is obtained by attentive aggregation over the representations of support instances, where the weights are calculated using the query instance. Experimental results demonstrate the effectiveness of our proposed methods, which achieve a new state-of-the-art performance on the FewRel dataset.",,,,ACL
278,2019,Quantifying Similarity between Relations with Fact Distribution,"Weize Chen, Hao Zhu, Xu Han, Zhiyuan Liu, Maosong Sun","We introduce a conceptually simple and effective method to quantify the similarity between relations in knowledge bases. Specifically, our approach is based on the divergence between the conditional probability distributions over entity pairs. In this paper, these distributions are parameterized by a very simple neural network. Although computing the exact similarity is in-tractable, we provide a sampling-based method to get a good approximation. We empirically show the outputs of our approach significantly correlate with human judgments. By applying our method to various tasks, we also find that (1) our approach could effectively detect redundant relations extracted by open information extraction (Open IE) models, that (2) even the most competitive models for relational classification still make mistakes among very similar relations, and that (3) our approach could be incorporated into negative sampling and softmax classification to alleviate these mistakes.",,,,ACL
279,2019,Matching the Blanks: Distributional Similarity for Relation Learning,"Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, Tom Kwiatkowski","General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris’ distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task’s training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED",,,,ACL
280,2019,Fine-Grained Temporal Relation Extraction,"Siddharth Vashishtha, Benjamin Van Durme, Aaron Steven White","We present a novel semantic framework for modeling temporal relations and event durations that maps pairs of events to real-valued scales. We use this framework to construct the largest temporal relations dataset to date, covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to train models for jointly predicting fine-grained temporal relations and event durations. We report strong results on our data and show the efficacy of a transfer-learning approach for predicting categorical relations.",,,,ACL
281,2019,FIESTA: Fast IdEntification of State-of-The-Art models using adaptive bandit algorithms,"Henry Moss, Andrew Moore, David Leslie, Paul Rayson","We present FIESTA, a model selection approach that significantly reduces the computational resources required to reliably identify state-of-the-art performance from large collections of candidate models. Despite being known to produce unreliable comparisons, it is still common practice to compare model evaluations based on single choices of random seeds. We show that reliable model selection also requires evaluations based on multiple train-test splits (contrary to common practice in many shared tasks). Using bandit theory from the statistics literature, we are able to adaptively determine appropriate numbers of data splits and random seeds used to evaluate each model, focusing computational resources on the evaluation of promising models whilst avoiding wasting evaluations on models with lower performance. Furthermore, our user-friendly Python implementation produces confidence guarantees of correctly selecting the optimal model. We evaluate our algorithms by selecting between 8 target-dependent sentiment analysis methods using dramatically fewer model evaluations than current model selection approaches.",,,,ACL
282,2019,Is Attention Interpretable?,"Sofia Serrano, Noah A. Smith","Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components’ representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components’ overall importance to a model, it is by no means a fail-safe indicator.",,,,ACL
283,2019,Correlating Neural and Symbolic Representations of Language,"Grzegorz Chrupała, Afra Alishahi","Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in NLP. Here we present two methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees. We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics, and show that they exhibit the expected pattern of results. We then our methods to correlate neural representations of English sentences with their constituency parse trees.",,,,ACL
284,2019,Interpretable Neural Predictions with Differentiable Binary Variables,"Joost Bastings, Wilker Aziz, Ivan Titov","The success of neural networks comes hand in hand with a desire for more interpretability. We focus on text classifiers and make them more interpretable by having them provide a justification–a rationale–for their predictions. We approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classifier that learns from the words in the rationale alone. Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as L0 regularisation. We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE. In our formulation, we can tractably compute the expected value of penalties such as L0, which allows us to directly optimise the model towards a pre-specified text selection rate. We show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms.",,,,ACL
285,2019,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le","Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",,,,ACL
286,2019,Domain Adaptation of Neural Machine Translation by Lexicon Induction,"Junjie Hu, Mengzhou Xia, Graham Neubig, Jaime Carbonell","It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.",,,,ACL
287,2019,Reference Network for Neural Machine Translation,"Han Fu, Chenghao Liu, Jianling Sun","Neural Machine Translation (NMT) has achieved notable success in recent years. Such a framework usually generates translations in isolation. In contrast, human translators often refer to reference data, either rephrasing the intricate sentence fragments with common terms in source language, or just accessing to the golden translation directly. In this paper, we propose a Reference Network to incorporate referring process into translation decoding of NMT. To construct a reference book, an intuitive way is to store the detailed translation history with extra memory, which is computationally expensive. Instead, we employ Local Coordinates Coding (LCC) to obtain global context vectors containing monolingual and bilingual contextual information for NMT decoding. Experimental results on Chinese-English and English-German tasks demonstrate that our proposed model is effective in improving the translation quality with lightweight computation cost.",,,,ACL
288,2019,Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation,"Chenze Shao, Yang Feng, Jinchao Zhang, Fandong Meng, Xilin Chen","Non-Autoregressive Transformer (NAT) aims to accelerate the Transformer model through discarding the autoregressive mechanism and generating target words independently, which fails to exploit the target sequential information. Over-translation and under-translation errors often occur for the above reason, especially in the long sentence translation scenario. In this paper, we propose two approaches to retrieve the target sequential information for NAT to enhance its translation ability while preserving the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. Experimental results on three translation tasks show that the Reinforce-NAT surpasses the baseline NAT system by a significant margin on BLEU without decelerating the decoding speed and the FS-decoder achieves comparable translation performance to the autoregressive Transformer with considerable speedup.",,,,ACL
289,2019,STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework,"Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu","Simultaneous translation, which translates sentences before they are finished, is use- ful in many scenarios but is notoriously dif- ficult due to word-order differences. While the conventional seq-to-seq framework is only suitable for full-sentence translation, we pro- pose a novel prefix-to-prefix framework for si- multaneous translation that implicitly learns to anticipate in a single translation model. Within this framework, we present a very sim- ple yet surprisingly effective “wait-k” policy trained to generate the target sentence concur- rently with the source sentence, but always k words behind. Experiments show our strat- egy achieves low latency and reasonable qual- ity (compared to full-sentence translation) on 4 directions: zh↔en and de↔en.",,,,ACL
290,2019,Look Harder: A Neural Machine Translation Model with Hard Attention,"Sathish Reddy Indurthi, Insoo Chung, Sangha Kim","Soft-attention based Neural Machine Translation (NMT) models have achieved promising results on several translation tasks. These models attend all the words in the source sequence for each target token, which makes them ineffective for long sequence translation. In this work, we propose a hard-attention based NMT model which selects a subset of source tokens for each target token to effectively handle long sequence translation. Due to the discrete nature of the hard-attention mechanism, we design a reinforcement learning algorithm coupled with reward shaping strategy to efficiently train it. Experimental results show that the proposed model performs better on long sequences and thereby achieves significant BLEU score improvement on English-German (EN-DE) and English-French (ENFR) translation tasks compared to the soft attention based NMT.",,,,ACL
291,2019,Robust Neural Machine Translation with Joint Textual and Phonetic Embedding,"Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, Zhongjun He","Neural machine translation (NMT) is notoriously sensitive to noises, but noises are almost inevitable in practice. One special kind of noise is the homophone noise, where words are replaced by other words with similar pronunciations. We propose to improve the robustness of NMT to homophone noises by 1) jointly embedding both textual and phonetic information of source sentences, and 2) augmenting the training dataset with homophone noises. Interestingly, to achieve better translation quality and more robustness, we found that most (though not all) weights should be put on the phonetic rather than textual information. Experiments show that our method not only significantly improves the robustness of NMT to homophone noises, but also surprisingly improves the translation quality on some clean test sets.",,,,ACL
292,2019,A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning,"Gonçalo M. Correia, André F. T. Martins","Automatic post-editing (APE) seeks to automatically refine the output of a black-box machine translation (MT) system through human post-edits. APE systems are usually trained by complementing human post-edited data with large, artificial data generated through back-translations, a time-consuming process often no easier than training a MT system from scratch. in this paper, we propose an alternative where we fine-tune pre-trained BERT models on both the encoder and decoder of an APE system, exploring several parameter sharing strategies. By only training on a dataset of 23K sentences for 3 hours on a single GPU we obtain results that are competitive with systems that were trained on 5M artificial sentences. When we add this artificial data our method obtains state-of-the-art results.",,,,ACL
293,2019,Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation,"Nima Pourdamghani, Nada Aldarrab, Marjan Ghazvininejad, Kevin Knight, Jonathan May","Given a rough, word-by-word gloss of a source language sentence, target language natives can uncover the latent, fully-fluent rendering of the translation. In this work we explore this intuition by breaking translation into a two step process: generating a rough gloss by means of a dictionary and then ‘translating’ the resulting pseudo-translation, or ‘Translationese’ into a fully fluent translation. We build our Translationese decoder once from a mish-mash of parallel data that has the target language in common and then can build dictionaries on demand using unsupervised techniques, resulting in rapidly generated unsupervised neural MT systems for many source languages. We apply this process to 14 test languages, obtaining better or comparable translation results on high-resource languages than previously published unsupervised MT studies, and obtaining good quality results for low-resource languages that have never been used in an unsupervised MT scenario.",,,,ACL
294,2019,Training Neural Machine Translation to Apply Terminology Constraints,"Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan","This paper proposes a novel method to inject custom terminology into neural machine translation at run time. Previous works have mainly proposed modifications to the decoding algorithm in order to constrain the output to include run-time-provided target terms. While being effective, these constrained decoding methods add, however, significant computational overhead to the inference step, and, as we show in this paper, can be brittle when tested in realistic conditions. In this paper we approach the problem by training a neural MT system to learn how to use custom terminology when provided with the input. Comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding, but is also as fast as constraint-free decoding.",,,,ACL
295,2019,Leveraging Local and Global Patterns for Self-Attention Networks,"Mingzhou Xu, Derek F. Wong, Baosong Yang, Yue Zhang, Lidia S. Chao","Self-attention networks have received increasing research attention. By default, the hidden states of each word are hierarchically calculated by attending to all words in the sentence, which assembles global information. However, several studies pointed out that taking all signals into account may lead to overlooking neighboring information (e.g. phrase pattern). To address this argument, we propose a hybrid attention mechanism to dynamically leverage both of the local and global information. Specifically, our approach uses a gating scalar for integrating both sources of the information, which is also convenient for quantifying their contributions. Experiments on various neural machine translation tasks demonstrate the effectiveness of the proposed method. The extensive analyses verify that the two types of contexts are complementary to each other, and our method gives highly effective improvements in their integration.",,,,ACL
296,2019,Sentence-Level Agreement for Neural Machine Translation,"Mingming Yang, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita","The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references. In NMT, there is a natural correspondence between the source sentence and the target sentence. However, this relationship has only been represented using the entire neural network and the training objective is computed in word-level. In this paper, we propose a sentence-level agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance.",,,,ACL
297,2019,Multilingual Unsupervised NMT using Shared Encoder and Language-Specific Decoders,"Sukanta Sen, Kamal Kumar Gupta, Asif Ekbal, Pushpak Bhattacharyya","In this paper, we propose a multilingual unsupervised NMT scheme which jointly trains multiple languages with a shared encoder and multiple decoders. Our approach is based on denoising autoencoding of each language and back-translating between English and multiple non-English languages. This results in a universal encoder which can encode any language participating in training into an inter-lingual representation, and language-specific decoders. Our experiments using only monolingual corpora show that multilingual unsupervised model performs better than the separately trained bilingual models achieving improvement of up to 1.48 BLEU points on WMT test sets. We also observe that even if we do not train the network for all possible translation directions, the network is still able to translate in a many-to-many fashion leveraging encoder’s ability to generate interlingual representation.",,,,ACL
298,2019,Lattice-Based Transformer Encoder for Neural Machine Translation,"Fengshun Xiao, Jiangtong Li, Hai Zhao, Rui Wang, Kehai Chen","Neural machine translation (NMT) takes deterministic sequences for source representations. However, either word-level or subword-level segmentations have multiple choices to split a source sequence with different word segmentors or different subword vocabulary sizes. We hypothesize that the diversity in segmentations may affect the NMT performance. To integrate different segmentations with the state-of-the-art NMT model, Transformer, we propose lattice-based encoders to explore effective word or subword representation in an automatic way during training. We propose two methods: 1) lattice positional encoding and 2) lattice-aware self-attention. These two methods can be used together and show complementary to each other to further improve translation performance. Experiment results show superiorities of lattice-based encoders in word-level and subword-level representations over conventional Transformer encoder.",,,,ACL
299,2019,Multi-Source Cross-Lingual Model Transfer: Learning What to Share,"Xilun Chen, Ahmed Hassan Awadallah, Hany Hassan, Wei Wang, Claire Cardie","Modern NLP applications have enjoyed a great boost utilizing neural networks models. Such deep neural models, however, are not applicable to most human languages due to the lack of annotated training data for various NLP tasks. Cross-lingual transfer learning (CLTL) is a viable method for building NLP models for a low-resource target language by leveraging labeled data from other (source) languages. In this work, we focus on the multilingual transfer setting where training data in multiple source languages is leveraged to further boost target language performance. Unlike most existing methods that rely only on language-invariant features for CLTL, our approach coherently utilizes both language-invariant and language-specific features at instance level. Our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target language and each individual source language. This enables our model to learn effectively what to share between various languages in the multilingual setup. Moreover, when coupled with unsupervised multilingual embeddings, our model can operate in a zero-resource setting where neither target language training data nor cross-lingual resources are available. Our model achieves significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classification and sequence tagging tasks including a large-scale industry dataset.",,,,ACL
300,2019,Unsupervised Multilingual Word Embedding with Limited Resources using Neural Language Models,"Takashi Wada, Tomoharu Iwata, Yuji Matsumoto","Recently, a variety of unsupervised methods have been proposed that map pre-trained word embeddings of different languages into the same space without any parallel data. These methods aim to find a linear transformation based on the assumption that monolingual word embeddings are approximately isomorphic between languages. However, it has been demonstrated that this assumption holds true only on specific conditions, and with limited resources, the performance of these methods decreases drastically. To overcome this problem, we propose a new unsupervised multilingual embedding method that does not rely on such assumption and performs well under resource-poor scenarios, namely when only a small amount of monolingual data (i.e., 50k sentences) are available, or when the domains of monolingual data are different across languages. Our proposed model, which we call ‘Multilingual Neural Language Models’, shares some of the network parameters among multiple languages, and encodes sentences of multiple languages into the same space. The model jointly learns word embeddings of different languages in the same space, and generates multilingual embeddings without any parallel data or pre-training. Our experiments on word alignment tasks have demonstrated that, on the low-resource condition, our model substantially outperforms existing unsupervised and even supervised methods trained with 500 bilingual pairs of words. Our model also outperforms unsupervised methods given different-domain corpora across languages. Our code is publicly available.",,,,ACL
301,2019,Choosing Transfer Languages for Cross-Lingual Learning,"Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang","Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (NLP) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method.",,,,ACL
302,2019,CogNet: A Large-Scale Cognate Database,"Khuyagbaatar Batsuren, Gabor Bella, Fausto Giunchiglia","This paper introduces CogNet, a new, large-scale lexical database that provides cognates -words of common origin and meaning- across languages. The database currently contains 3.1 million cognate pairs across 338 languages using 35 writing systems. The paper also describes the automated method by which cognates were computed from publicly available wordnets, with an accuracy evaluated to 94%. Finally, it presents statistics about the cognate data and some initial insights into it, hinting at a possible future exploitation of the resource by various fields of lingustics.",,,,ACL
303,2019,Neural Decipherment via Minimum-Cost Flow: From Ugaritic to Linear B,"Jiaming Luo, Yuan Cao, Regina Barzilay","In this paper we propose a novel neural approach for automatic decipherment of lost languages. To compensate for the lack of strong supervision signal, our model design is informed by patterns in language change documented in historical linguistics. The model utilizes an expressive sequence-to-sequence model to capture character-level correspondences between cognates. To effectively train the model in unsupervised manner, we innovate the training procedure by formalizing it as a minimum-cost flow problem. When applied to decipherment of Ugaritic, we achieve 5% absolute improvement over state-of-the-art results. We also report first automatic results in deciphering Linear B, a syllabic language related to ancient Greek, where our model correctly translates 67.3% of cognates.",,,,ACL
304,2019,Cross-lingual Knowledge Graph Alignment via Graph Matching Neural Network,"Kun Xu, Liwei Wang, Mo Yu, Yansong Feng, Yan Song","Previous cross-lingual knowledge graph (KG) alignment studies rely on entity embeddings derived only from monolingual KG structural information, which may fail at matching entities that have different facts in two KGs. In this paper, we introduce the topic entity graph, a local sub-graph of an entity, to represent entities with their contextual information in KG. From this view, the KB-alignment task can be formulated as a graph matching problem; and we further propose a graph-attention based solution, which first matches all entities in two topic entity graphs, and then jointly model the local matching information to derive a graph-level matching vector. Experiments show that our model outperforms previous state-of-the-art methods by a large margin.",,,,ACL
305,2019,Zero-Shot Cross-Lingual Abstractive Sentence Summarization through Teaching Generation and Attention,"Xiangyu Duan, Mingming Yin, Min Zhang, Boxing Chen, Weihua Luo","Abstractive Sentence Summarization (ASSUM) targets at grasping the core idea of the source sentence and presenting it as the summary. It is extensively studied using statistical models or neural models based on the large-scale monolingual source-summary parallel corpus. But there is no cross-lingual parallel corpus, whose source sentence language is different to the summary language, to directly train a cross-lingual ASSUM system. We propose to solve this zero-shot problem by using resource-rich monolingual ASSUM system to teach zero-shot cross-lingual ASSUM system on both summary word generation and attention. This teaching process is along with a back-translation process which simulates source-summary pairs. Experiments on cross-lingual ASSUM task show that our proposed method is significantly better than pipeline baselines and previous works, and greatly enhances the cross-lingual performances closer to the monolingual performances.",,,,ACL
306,2019,Improving Low-Resource Cross-lingual Document Retrieval by Reranking with Deep Bilingual Representations,"Rui Zhang, Caitlin Westerfield, Sungrok Shim, Garrett Bingham, Alexander Fabbri","In this paper, we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations. We match queries and documents in both source and target languages with four components, each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input. By including query likelihood scores as extra features, our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs. Due to the shared cross-lingual word embedding space, the model can also be directly applied to another language pair without any training label. Experimental results on the Material dataset show that our model outperforms the competitive translation-based baselines on English-Swahili, English-Tagalog, and English-Somali cross-lingual information retrieval tasks.",,,,ACL
307,2019,Are Girls Neko or Shōjo? Cross-Lingual Alignment of Non-Isomorphic Embeddings with Iterative Normalization,"Mozhi Zhang, Keyulu Xu, Ken-ichi Kawarabayashi, Stefanie Jegelka, Jordan Boyd-Graber","Cross-lingual word embeddings (CLWE) underlie many multilingual natural language processing systems, often through orthogonal transformations of pre-trained monolingual embeddings. However, orthogonal mapping only works on language pairs whose embeddings are naturally isomorphic. For non-isomorphic pairs, our method (Iterative Normalization) transforms monolingual embeddings to make orthogonal alignment easier by simultaneously enforcing that (1) individual word vectors are unit length, and (2) each language’s average vector is zero. Iterative Normalization consistently improves word translation accuracy of three CLWE methods, with the largest improvement observed on English-Japanese (from 2% to 44% test accuracy).",,,,ACL
308,2019,MAAM: A Morphology-Aware Alignment Model for Unsupervised Bilingual Lexicon Induction,"Pengcheng Yang, Fuli Luo, Peng Chen, Tianyu Liu, Xu Sun","The task of unsupervised bilingual lexicon induction (UBLI) aims to induce word translations from monolingual corpora in two languages. Previous work has shown that morphological variation is an intractable challenge for the UBLI task, where the induced translation in failure case is usually morphologically related to the correct translation. To tackle this challenge, we propose a morphology-aware alignment model for the UBLI task. The proposed model aims to alleviate the adverse effect of morphological variation by introducing grammatical information learned by the pre-trained denoising language model. Results show that our approach can substantially outperform several state-of-the-art unsupervised systems, and even achieves competitive performance compared to supervised methods.",,,,ACL
309,2019,Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings,"Mikel Artetxe, Holger Schwenk","Machine translation is highly sensitive to the size and quality of the training data, which has led to an increasing interest in collecting and filtering large parallel corpora. In this paper, we propose a new method for this task based on multilingual sentence embeddings. In contrast to previous approaches, which rely on nearest neighbor retrieval with a hard threshold over cosine similarity, our proposed method accounts for the scale inconsistencies of this measure, considering the margin between a given sentence pair and its closest candidates instead. Our experiments show large improvements over existing methods. We outperform the best published results on the BUCC mining task and the UN reconstruction task by more than 10 F1 and 30 precision points, respectively. Filtering the English-German ParaCrawl corpus with our approach, we obtain 31.2 BLEU points on newstest2014, an improvement of more than one point over the best official filtered version.",,,,ACL
310,2019,JW300: A Wide-Coverage Parallel Corpus for Low-Resource Languages,"Željko Agić, Ivan Vulić","Viable cross-lingual transfer critically depends on the availability of parallel texts. Shortage of such resources imposes a development and evaluation bottleneck in multilingual processing. We introduce JW300, a parallel corpus of over 300 languages with around 100 thousand parallel sentences per language pair on average. In this paper, we present the resource and showcase its utility in experiments with cross-lingual word embedding induction and multi-source part-of-speech projection.",,,,ACL
311,2019,Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of Invertible Projections,"Junxian He, Zhisong Zhang, Taylor Berg-Kirkpatrick, Graham Neubig","Cross-lingual transfer is an effective way to build syntactic analysis tools in low-resource languages. However, transfer is difficult when transferring to typologically distant languages, especially when neither annotated target data nor parallel corpora are available. In this paper, we focus on methods for cross-lingual transfer to distant languages and propose to learn a generative model with a structured prior that utilizes labeled source data and unlabeled target data jointly. The parameters of source model and target model are softly shared through a regularized log likelihood objective. An invertible projection is employed to learn a new interlingual latent embedding space that compensates for imperfect cross-lingual word embedding input. We evaluate our method on two syntactic tasks: part-of-speech (POS) tagging and dependency parsing. On the Universal Dependency Treebanks, we use English as the only source corpus and transfer to a wide range of target languages. On the 10 languages in this dataset that are distant from English, our method yields an average of 5.2% absolute improvement on POS tagging and 8.3% absolute improvement on dependency parsing over a direct transfer method using state-of-the-art discriminative models.",,,,ACL
312,2019,Unsupervised Joint Training of Bilingual Word Embeddings,"Benjamin Marie, Atsushi Fujita","State-of-the-art methods for unsupervised bilingual word embeddings (BWE) train a mapping function that maps pre-trained monolingual word embeddings into a bilingual space. Despite its remarkable results, unsupervised mapping is also well-known to be limited by the original dissimilarity between the word embedding spaces to be mapped. In this work, we propose a new approach that trains unsupervised BWE jointly on synthetic parallel data generated through unsupervised machine translation. We demonstrate that existing algorithms that jointly train BWE are very robust to noisy training data and show that unsupervised BWE jointly trained significantly outperform unsupervised mapped BWE in several cross-lingual NLP tasks.",,,,ACL
313,2019,Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings,"Matthew Le, Stephen Roller, Laetitia Papaxanthos, Douwe Kiela, Maximilian Nickel","We consider the task of inferring “is-a” relationships from large text corpora. For this purpose, we propose a new method combining hyperbolic embeddings and Hearst patterns. This approach allows us to set appropriate constraints for inferring concept hierarchies from distributional contexts while also being able to predict missing “is-a”-relationships and to correct wrong extractions. Moreover – and in contrast with other methods – the hierarchical nature of hyperbolic space allows us to learn highly efficient representations and to improve the taxonomic consistency of the inferred hierarchies. Experimentally, we show that our approach achieves state-of-the-art performance on several commonly-used benchmarks.",,,,ACL
314,2019,Is Word Segmentation Necessary for Deep Learning of Chinese Representations?,"Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan","Segmenting a chunk of text into words is usually the first step of processing Chinese text, but its necessity has rarely been explored. In this paper, we ask the fundamental question of whether Chinese word segmentation (CWS) is necessary for deep learning-based Chinese Natural Language Processing. We benchmark neural word-based models which rely on word segmentation against neural char-based models which do not involve word segmentation in four end-to-end NLP benchmark tasks: language modeling, machine translation, sentence matching/paraphrase and text classification. Through direct comparisons between these two types of models, we find that char-based models consistently outperform word-based models. Based on these observations, we conduct comprehensive experiments to study why word-based models underperform char-based models in these deep learning-based NLP tasks. We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting. We hope this paper could encourage researchers in the community to rethink the necessity of word segmentation in deep learning-based Chinese Natural Language Processing.",,,,ACL
315,2019,Towards Understanding Linear Word Analogies,"Kawin Ethayarajh, David Duvenaud, Graeme Hirst","A surprising property of word vectors is that word analogies can often be solved with vector arithmetic. However, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, justifying its use in capturing word dissimilarity.",,,,ACL
316,2019,On the Compositionality Prediction of Noun Phrases using Poincaré Embeddings,"Abhik Jana, Dima Puzyrev, Alexander Panchenko, Pawan Goyal, Chris Biemann","The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations. Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models. We introduce a novel technique to blend hierarchical information with distributional information for predicting compositionality. In particular, we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced Poincaré embeddings in addition to the distributional information to detect compositionality for noun phrases. Using a weighted average of the distributional similarity and a Poincaré similarity function, we obtain consistent and substantial, statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only. Unlike traditional approaches that solely use an unsupervised setting, we have also framed the problem as a supervised task, obtaining comparable improvements. Further, we publicly release our Poincaré embeddings, which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus.",,,,ACL
317,2019,Robust Representation Learning of Biomedical Names,"Minh C. Phan, Aixin Sun, Yi Tay","Biomedical concepts are often mentioned in medical documents under different name variations (synonyms). This mismatch between surface forms is problematic, resulting in difficulties pertaining to learning effective representations. Consequently, this has tremendous implications such as rendering downstream applications inefficacious and/or potentially unreliable. This paper proposes a new framework for learning robust representations of biomedical names and terms. The idea behind our approach is to consider and encode contextual meaning, conceptual meaning, and the similarity between synonyms during the representation learning process. Via extensive experiments, we show that our proposed method outperforms other baselines on a battery of retrieval, similarity and relatedness benchmarks. Moreover, our proposed method is also able to compute meaningful representations for unseen names, resulting in high practical utility in real-world applications.",,,,ACL
318,2019,Relational Word Embeddings,"Jose Camacho-Collados, Luis Espinosa Anke, Steven Schockaert","While word embeddings have been shown to implicitly encode various forms of attributional knowledge, the extent to which they capture relational information is far more limited. In previous work, this limitation has been addressed by incorporating relational knowledge from external knowledge bases when learning the word embedding. Such strategies may not be optimal, however, as they are limited by the coverage of available resources and conflate similarity with other forms of relatedness. As an alternative, in this paper we propose to encode relational knowledge in a separate word embedding, which is aimed to be complementary to a given standard word embedding. This relational word embedding is still learned from co-occurrence statistics, and can thus be used even when no external knowledge base is available. Our analysis shows that relational word vectors do indeed capture information that is complementary to what is encoded in standard word embeddings.",,,,ACL
319,2019,Unraveling Antonym’s Word Vectors through a Siamese-like Network,"Mathias Etcheverry, Dina Wonsever","Discriminating antonyms and synonyms is an important NLP task that has the difficulty that both, antonyms and synonyms, contains similar distributional information. Consequently, pairs of antonyms and synonyms may have similar word vectors. We present an approach to unravel antonymy and synonymy from word vectors based on a siamese network inspired approach. The model consists of a two-phase training of the same base network: a pre-training phase according to a siamese model supervised by synonyms and a training phase on antonyms through a siamese-like model that supports the antitransitivity present in antonymy. The approach makes use of the claim that the antonyms in common of a word tend to be synonyms. We show that our approach outperforms distributional and pattern-based approaches, relaying on a simple feed forward network as base network of the training phases.",,,,ACL
320,2019,Incorporating Syntactic and Semantic Information in Word Embeddings using Graph Convolutional Networks,"Shikhar Vashishth, Manik Bhandari, Prateek Yadav, Piyush Rai, Chiranjib Bhattacharyya","Word embeddings have been widely adopted across several NLP applications. Most existing word embedding methods utilize sequential context of a word to learn its embedding. While there have been some attempts at utilizing syntactic context of a word, such methods result in an explosion of the vocabulary size. In this paper, we overcome this problem by proposing SynGCN, a flexible Graph Convolution based method for learning word embeddings. SynGCN utilizes the dependency context of a word without increasing the vocabulary size. Word embeddings learned by SynGCN outperform existing methods on various intrinsic and extrinsic tasks and provide an advantage when used with ELMo. We also propose SemGCN, an effective framework for incorporating diverse semantic knowledge for further enhancing learned word representations. We make the source code of both models available to encourage reproducible research.",,,,ACL
321,2019,Word and Document Embedding with vMF-Mixture Priors on Context Word Vectors,"Shoaib Jameel, Steven Schockaert","Word embedding models typically learn two types of vectors: target word vectors and context word vectors. These vectors are normally learned such that they are predictive of some word co-occurrence statistic, but they are otherwise unconstrained. However, the words from a given language can be organized in various natural groupings, such as syntactic word classes (e.g. nouns, adjectives, verbs) and semantic themes (e.g. sports, politics, sentiment). Our hypothesis in this paper is that embedding models can be improved by explicitly imposing a cluster structure on the set of context word vectors. To this end, our model relies on the assumption that context word vectors are drawn from a mixture of von Mises-Fisher (vMF) distributions, where the parameters of this mixture distribution are jointly optimized with the word vectors. We show that this results in word vectors which are qualitatively different from those obtained with existing word embedding models. We furthermore show that our embedding model can also be used to learn high-quality document representations.",,,,ACL
322,2019,Delta Embedding Learning,"Xiao Zhang, Ji Wu, Dejing Dou","Unsupervised word embeddings have become a popular approach of word representation in NLP tasks. However there are limitations to the semantics represented by unsupervised embeddings, and inadequate fine-tuning of embeddings can lead to suboptimal performance. We propose a novel learning technique called Delta Embedding Learning, which can be applied to general NLP tasks to improve performance by optimized tuning of the word embeddings. A structured regularization is applied to the embeddings to ensure they are tuned in an incremental way. As a result, the tuned word embeddings become better word representations by absorbing semantic information from supervision without “forgetting.” We apply the method to various NLP tasks and see a consistent improvement in performance. Evaluation also confirms the tuned word embeddings have better semantic properties.",,,,ACL
323,2019,Annotation and Automatic Classification of Aspectual Categories,"Markus Egg, Helena Prepens, Will Roberts","We present the first annotated resource for the aspectual classification of German verb tokens in their clausal context. We use aspectual features compatible with the plurality of aspectual classifications in previous work and treat aspectual ambiguity systematically. We evaluate our corpus by using it to train supervised classifiers to automatically assign aspectual categories to verbs in context, permitting favourable comparisons to previous work.",,,,ACL
324,2019,Putting Words in Context: LSTM Language Models and Lexical Ambiguity,"Laura Aina, Kristina Gulordava, Gemma Boleda","In neural network models of language, words are commonly represented using context-invariant representations (word embeddings) which are then put in context in the hidden layers. Since words are often ambiguous, representing the contextually relevant information is not trivial. We investigate how an LSTM language model deals with lexical ambiguity in English, designing a method to probe its hidden representations for lexical and contextual information about words. We find that both types of information are represented to a large extent, but also that there is room for improvement for contextual information.",,,,ACL
325,2019,Making Fast Graph-based Algorithms with Graph Metric Embeddings,"Andrey Kutuzov, Mohammad Dorgham, Oleksiy Oliynyk, Chris Biemann, Alexander Panchenko","Graph measures, such as node distances, are inefficient to compute. We explore dense vector representations as an effective way to approximate the same information. We introduce a simple yet efficient and effective approach for learning graph embeddings. Instead of directly operating on the graph structure, our method takes structural measures of pairwise node similarities into account and learns dense node representations reflecting user-defined graph distance measures, such as e.g. the shortest path distance or distance measures that take information beyond the graph structure into account. We demonstrate a speed-up of several orders of magnitude when predicting word similarity by vector operations on our embeddings as opposed to directly computing the respective path-based measures, while outperforming various other graph embeddings on semantic similarity and word sense disambiguation tasks.",,,,ACL
326,2019,Embedding Imputation with Grounded Language Information,"Ziyi Yang, Chenguang Zhu, Vin Sachidananda, Eric Darve","Due to the ubiquitous use of embeddings as input representations for a wide range of natural language tasks, imputation of embeddings for rare and unseen words is a critical problem in language processing. Embedding imputation involves learning representations for rare or unseen words during the training of an embedding model, often in a post-hoc manner. In this paper, we propose an approach for embedding imputation which uses grounded information in the form of a knowledge graph. This is in contrast to existing approaches which typically make use of vector space properties or subword information. We propose an online method to construct a graph from grounded information and design an algorithm to map from the resulting graphical structure to the space of the pre-trained embeddings. Finally, we evaluate our approach on a range of rare and unseen word tasks across various domains and show that our model can learn better representations. For example, on the Card-660 task our method improves Pearson’s and Spearman’s correlation coefficients upon the state-of-the-art by 11% and 17.8% respectively using GloVe embeddings.",,,,ACL
327,2019,The Effectiveness of Simple Hybrid Systems for Hypernym Discovery,"William Held, Nizar Habash","Hypernymy modeling has largely been separated according to two paradigms, pattern-based methods and distributional methods. However, recent works utilizing a mix of these strategies have yielded state-of-the-art results. This paper evaluates the contribution of both paradigms to hybrid success by evaluating the benefits of hybrid treatment of baseline models from each paradigm. Even with a simple methodology for each individual system, utilizing a hybrid approach establishes new state-of-the-art results on two domain-specific English hypernym discovery tasks and outperforms all non-hybrid approaches in a general English hypernym discovery task.",,,,ACL
328,2019,BERT-based Lexical Substitution,"Wangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, Ming Zhou","Previous studies on lexical substitution tend to obtain substitute candidates by finding the target word’s synonyms from lexical resources (e.g., WordNet) and then rank the candidates based on its contexts. These approaches have two limitations: (1) They are likely to overlook good substitute candidates that are not the synonyms of the target words in the lexical resources; (2) They fail to take into account the substitution’s influence on the global context of the sentence. To address these issues, we propose an end-to-end BERT-based lexical substitution approach which can propose and validate substitute candidates without using any annotated data or manually curated resources. Our approach first applies dropout to the target word’s embedding for partially masking the word, allowing BERT to take balanced consideration of the target word’s semantics and contexts for proposing substitute candidates, and then validates the candidates based on their substitution’s influence on the global contextualized representation of the sentence. Experiments show our approach performs well in both proposing and ranking substitute candidates, achieving the state-of-the-art results in both LS07 and LS14 benchmarks.",,,,ACL
329,2019,Exploring Numeracy in Word Embeddings,"Aakanksha Naik, Abhilasha Ravichander, Carolyn Rose, Eduard Hovy","Word embeddings are now pervasive across NLP subfields as the de-facto method of forming text representataions. In this work, we show that existing embedding models are inadequate at constructing representations that capture salient aspects of mathematical meaning for numbers, which is important for language understanding. Numbers are ubiquitous and frequently appear in text. Inspired by cognitive studies on how humans perceive numbers, we develop an analysis framework to test how well word embeddings capture two essential properties of numbers: magnitude (e.g. 3<4) and numeration (e.g. 3=three). Our experiments reveal that most models capture an approximate notion of magnitude, but are inadequate at capturing numeration. We hope that our observations provide a starting point for the development of methods which better capture numeracy in NLP systems.",,,,ACL
330,2019,HighRES: Highlight-based Reference-less Evaluation of Summarization,"Hardy Hardy, Shashi Narayan, Andreas Vlachos","There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",,,,ACL
331,2019,EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing,"Yue Dong, Zichao Li, Mehdi Rezagholizadeh, Jackie Chi Kit Cheung","We present the first sentence simplification model that learns explicit edit operations (ADD, DELETE, and KEEP) via a neural programmer-interpreter approach. Most current neural sentence simplification systems are variants of sequence-to-sequence models adopted from machine translation. These methods learn to simplify sentences as a byproduct of the fact that they are trained on complex-simple sentence pairs. By contrast, our neural programmer-interpreter is directly trained to predict explicit edit operations on targeted parts of the input sentence, resembling the way that humans perform simplification and revision. Our model outperforms previous state-of-the-art neural sentence simplification models (without external knowledge) by large margins on three benchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89 WikiSmall, +1.41 Newsela), and is judged by humans to produce overall better and simpler output sentences.",,,,ACL
332,2019,Decomposable Neural Paraphrase Generation,"Zichao Li, Xin Jiang, Lifeng Shang, Qun Liu","Paraphrasing exists at different granularity levels, such as lexical level, phrasal level and sentential level. This paper presents Decomposable Neural Paraphrase Generator (DNPG), a Transformer-based model that can learn and generate paraphrases of a sentence at different levels of granularity in a disentangled way. Specifically, the model is composed of multiple encoders and decoders with different structures, each of which corresponds to a specific granularity. The empirical study shows that the decomposition mechanism of DNPG makes paraphrase generation more interpretable and controllable. Based on DNPG, we further develop an unsupervised domain adaptation method for paraphrase generation. Experimental results show that the proposed model achieves competitive in-domain performance compared to state-of-the-art neural models, and significantly better performance when adapting to a new domain.",,,,ACL
333,2019,Transforming Complex Sentences into a Semantic Hierarchy,"Christina Niklaus, Matthias Cetto, André Freitas, Siegfried Handschuh","We present an approach for recursively splitting and rephrasing complex English sentences into a novel semantic hierarchy of simplified sentences, with each of them presenting a more regular structure that may facilitate a wide variety of artificial intelligence tasks, such as machine translation (MT) or information extraction (IE). Using a set of hand-crafted transformation rules, input sentences are recursively transformed into a two-layered hierarchical representation in the form of core sentences and accompanying contexts that are linked via rhetorical relations. In this way, the semantic relationship of the decomposed constituents is preserved in the output, maintaining its interpretability for downstream applications. Both a thorough manual analysis and automatic evaluation across three datasets from two different domains demonstrate that the proposed syntactic simplification approach outperforms the state of the art in structural text simplification. Moreover, an extrinsic evaluation shows that when applying our framework as a preprocessing step the performance of state-of-the-art Open IE systems can be improved by up to 346% in precision and 52% in recall. To enable reproducible research, all code is provided online.",,,,ACL
334,2019,Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference,"Tom McCoy, Ellie Pavlick, Tal Linzen","A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",,,,ACL
335,2019,Zero-Shot Entity Linking by Reading Entity Descriptions,"Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin","We present the zero-shot entity linking task, where mentions must be linked to unseen entities without in-domain labeled data. The goal is to enable robust transfer to highly specialized domains, and so no metadata or alias tables are assumed. In this setting, entities are only identified by text descriptions, and models must rely strictly on language understanding to resolve the new entities. First, we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities. Second, we propose a simple and effective adaptive pre-training strategy, which we term domain-adaptive pre-training (DAP), to address the domain shift problem associated with linking unseen entities in a new domain. We present experiments on a new dataset that we construct for this task and show that DAP improves over strong pre-training baselines, including BERT. The data and code are available at https://github.com/lajanugen/zeshel.",,,,ACL
336,2019,Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition,"Joey Tianyi Zhou, Hao Zhang, Di Jin, Hongyuan Zhu, Meng Fang","We propose a new neural transfer method termed Dual Adversarial Transfer Network (DATNet) for addressing low-resource Named Entity Recognition (NER). Specifically, two variants of DATNet, i.e., DATNet-F and DATNet-P, are investigated to explore effective feature fusion between high and low resource. To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD). Additionally, adversarial training is adopted to boost model generalization. In experiments, we examine the effects of different components in DATNet across domains and languages and show that significant improvement can be obtained especially for low-resource data, without augmenting any additional hand-crafted features and pre-trained language model.",,,,ACL
337,2019,Scalable Syntax-Aware Language Models Using Knowledge Distillation,"Adhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen Clark, Phil Blunsom","Prior work has shown that, on small amounts of training data, syntactic neural language models learn structurally sensitive generalisations more successfully than sequential language models. However, their computational complexity renders scaling difficult, and it remains an open question whether structural biases are still necessary when sequential models have access to ever larger amounts of training data. To answer this question, we introduce an efficient knowledge distillation (KD) technique that transfers knowledge from a syntactic language model trained on a small corpus to an LSTM language model, hence enabling the LSTM to develop a more structurally sensitive representation of the larger training data it learns from. On targeted syntactic evaluations, we find that, while sequential LSTMs perform much better than previously reported, our proposed technique substantially improves on this baseline, yielding a new state of the art. Our findings and analysis affirm the importance of structural biases, even in models that learn from large amounts of data.",,,,ACL
338,2019,An Imitation Learning Approach to Unsupervised Parsing,"Bowen Li, Lili Mou, Frank Keller","Recently, there has been an increasing interest in unsupervised parsers that optimize semantically oriented objectives, typically using reinforcement learning. Unfortunately, the learned trees often do not match actual syntax trees well. Shen et al. (2018) propose a structured attention mechanism for language modeling (PRPN), which induces better syntactic structures but relies on ad hoc heuristics. Also, their model lacks interpretability as it is not grounded in parsing actions. In our work, we propose an imitation learning approach to unsupervised parsing, where we transfer the syntactic knowledge induced by PRPN to a Tree-LSTM model with discrete parsing actions. Its policy is then refined by Gumbel-Softmax training towards a semantically oriented objective. We evaluate our approach on the All Natural Language Inference dataset and show that it achieves a new state of the art in terms of parsing F-score, outperforming our base models, including PRPN.",,,,ACL
339,2019,Women’s Syntactic Resilience and Men’s Grammatical Luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing,"Aparna Garimella, Carmen Banea, Dirk Hovy, Rada Mihalcea","Several linguistic studies have shown the prevalence of various lexical and grammatical patterns in texts authored by a person of a particular gender, but models for part-of-speech tagging and dependency parsing have still not adapted to account for these differences. To address this, we annotate the Wall Street Journal part of the Penn Treebank with the gender information of the articles’ authors, and build taggers and parsers trained on this data that show performance differences in text written by men and women. Further analyses reveal numerous part-of-speech tags and syntactic relations whose prediction performances benefit from the prevalence of a specific gender in the training data. The results underscore the importance of accounting for gendered differences in syntactic tasks, and outline future venues for developing more accurate taggers and parsers. We release our data to the research community.",,,,ACL
340,2019,Multilingual Constituency Parsing with Self-Attention and Pre-Training,"Nikita Kitaev, Steven Cao, Dan Klein","We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2% relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).",,,,ACL
341,2019,A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction,"Mengjie Zhao, Hinrich Schütze","We present a new method for sentiment lexicon induction that is designed to be applicable to the entire range of typological diversity of the world’s languages. We evaluate our method on Parallel Bible Corpus+ (PBC+), a parallel corpus of 1593 languages. The key idea is to use Byte Pair Encodings (BPEs) as basic units for multilingual embeddings. Through zero-shot transfer from English sentiment, we learn a seed lexicon for each language in the domain of PBC+. Through domain adaptation, we then generalize the domain-specific lexicon to a general one. We show – across typologically diverse languages in PBC+ – good quality of seed and general-domain sentiment lexicons by intrinsic and extrinsic and by automatic and human evaluation. We make freely available our code, seed sentiment lexicons for all 1593 languages and induced general-domain sentiment lexicons for 200 languages.",,,,ACL
342,2019,Tree Communication Models for Sentiment Analysis,"Yuan Zhang, Yue Zhang","Tree-LSTMs have been used for tree-based sentiment analysis over Stanford Sentiment Treebank, which allows the sentiment signals over hierarchical phrase structures to be calculated simultaneously. However, traditional tree-LSTMs capture only the bottom-up dependencies between constituents. In this paper, we propose a tree communication model using graph convolutional neural network and graph recurrent neural network, which allows rich information exchange between phrases constituent tree. Experiments show that our model outperforms existing work on bidirectional tree-LSTMs in both accuracy and efficiency, providing more consistent predictions on phrase-level sentiments.",,,,ACL
343,2019,Improved Sentiment Detection via Label Transfer from Monolingual to Synthetic Code-Switched Text,"Bidisha Samanta, Niloy Ganguly, Soumen Chakrabarti","Multilingual writers and speakers often alternate between two languages in a single discourse. This practice is called “code-switching”. Existing sentiment detection methods are usually trained on sentiment-labeled monolingual text. Manually labeled code-switched text, especially involving minority languages, is extremely rare. Consequently, the best monolingual methods perform relatively poorly on code-switched text. We present an effective technique for synthesizing labeled code-switched text from labeled monolingual text, which is relatively readily available. The idea is to replace carefully selected subtrees of constituency parses of sentences in the resource-rich language with suitable token spans selected from automatic translations to the resource-poor language. By augmenting the scarce labeled code-switched text with plentiful synthetic labeled code-switched text, we achieve significant improvements in sentiment labeling accuracy (1.5%, 5.11% 7.20%) for three different language pairs (English-Hindi, English-Spanish and English-Bengali). The improvement is even significant in hatespeech detection whereby we achieve a 4% improvement using only synthetic code-switched data (6% with data augmentation).",,,,ACL
344,2019,Exploring Sequence-to-Sequence Learning in Aspect Term Extraction,"Dehong Ma, Sujian Li, Fangzhao Wu, Xing Xie, Houfeng Wang","Aspect term extraction (ATE) aims at identifying all aspect terms in a sentence and is usually modeled as a sequence labeling problem. However, sequence labeling based methods cannot make full use of the overall meaning of the whole sentence and have the limitation in processing dependencies between labels. To tackle these problems, we first explore to formalize ATE as a sequence-to-sequence (Seq2Seq) learning task where the source sequence and target sequence are composed of words and labels respectively. At the same time, to make Seq2Seq learning suit to ATE where labels correspond to words one by one, we design the gated unit networks to incorporate corresponding word representation into the decoder, and position-aware attention to pay more attention to the adjacent words of a target word. The experimental results on two datasets show that Seq2Seq learning is effective in ATE accompanied with our proposed gated unit networks and position-aware attention mechanism.",,,,ACL
345,2019,Aspect Sentiment Classification Towards Question-Answering with Reinforced Bidirectional Attention Network,"Jingjing Wang, Changlong Sun, Shoushan Li, Xiaozhong Liu, Luo Si","In the literature, existing studies on aspect sentiment classification (ASC) focus on individual non-interactive reviews. This paper extends the research to interactive reviews and proposes a new research task, namely Aspect Sentiment Classification towards Question-Answering (ASC-QA), for real-world applications. This new task aims to predict sentiment polarities for specific aspects from interactive QA style reviews. In particular, a high-quality annotated corpus is constructed for ASC-QA to facilitate corresponding research. On this basis, a Reinforced Bidirectional Attention Network (RBAN) approach is proposed to address two inherent challenges in ASC-QA, i.e., semantic matching between question and answer, and data noise. Experimental results demonstrate the great advantage of the proposed approach to ASC-QA against several state-of-the-art baselines.",,,,ACL
346,2019,ELI5: Long Form Question Answering,"Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston","We introduce the first large-scale corpus for long form question answering, a task requiring elaborate and in-depth answers to open-ended questions. The dataset comprises 270K threads from the Reddit forum “Explain Like I’m Five” (ELI5) where an online community provides answers to questions which are comprehensible by five year olds. Compared to existing datasets, ELI5 comprises diverse questions requiring multi-sentence answers. We provide a large set of web documents to help answer the question. Automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq, language modeling, as well as a strong extractive baseline.However, our best model is still far from human performance since raters prefer gold responses in over 86% of cases, leaving ample opportunity for future improvement.",,,,ACL
347,2019,Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension,"Daesik Kim, Seonhoon Kim, Nojun Kwak","In this work, we introduce a novel algorithm for solving the textbook question answering (TQA) task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with analysis of the TQA dataset. First, solving the TQA problems requires to comprehend multi-modal contexts in complicated input data. To tackle this issue of extracting knowledge features from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f-GCN based on graph convolutional networks (GCN). Second, scientific terms are not spread over the chapters and subjects are split in the TQA dataset. To overcome this so called ‘out-of-domain’ issue, before learning QA problems, we introduce a novel self-supervised open-set learning process without any annotations. The experimental results show that our model significantly outperforms prior state-of-the-art methods. Moreover, ablation studies validate that both methods of incorporating f-GCN for extracting knowledge from multi-modal contexts and our newly proposed self-supervised learning process are effective for TQA problems.",,,,ACL
348,2019,Generating Question Relevant Captions to Aid Visual Question Answering,"Jialin Wu, Zeyuan Hu, Raymond Mooney",Visual question answering (VQA) and image captioning require a shared body of general knowledge connecting language and vision. We present a novel approach to better VQA performance that exploits this connection by jointly generating captions that are targeted to help answer a specific visual question. The model is trained using an existing caption dataset by automatically determining question-relevant captions using an online gradient-based method. Experimental results on the VQA v2 challenge demonstrates that our approach obtains state-of-the-art VQA performance (e.g. 68.4% in the Test-standard set using a single model) by simultaneously generating question-relevant captions.,,,,ACL
349,2019,Multi-grained Attention with Object-level Grounding for Visual Question Answering,"Pingping Huang, Jianhui Huang, Yuqing Guo, Min Qiao, Yong Zhu","Attention mechanisms are widely used in Visual Question Answering (VQA) to search for visual clues related to the question. Most approaches train attention models from a coarse-grained association between sentences and images, which tends to fail on small objects or uncommon concepts. To address this problem, this paper proposes a multi-grained attention method. It learns explicit word-object correspondence by two types of word-level attention complementary to the sentence-image association. Evaluated on the VQA benchmark, the multi-grained attention model achieves competitive performance with state-of-the-art models. And the visualized attention maps demonstrate that addition of object-level groundings leads to a better understanding of the images and locates the attended objects more precisely.",,,,ACL
350,2019,Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering,"Claudio Greco, Barbara Plank, Raquel Fernández, Raffaella Bernardi","We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. Our results show that dramatic forgetting is at play and that task difficulty and order matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.",,,,ACL
351,2019,Improving Visual Question Answering by Referring to Generated Paragraph Captions,"Hyounghun Kim, Mohit Bansal","Paragraph-style image captions describe diverse aspects of an image as opposed to the more common single-sentence captions that only provide an abstract description of the image. These paragraph captions can hence contain substantial information of the image for tasks such as visual question answering. Moreover, this textual information is complementary with visual information present in the image because it can discuss both more abstract concepts and more explicit, intermediate symbolic information about objects, events, and scenes that can directly be matched with the textual question and copied into the textual answer (i.e., via easier modality match). Hence, we propose a combined Visual and Textual Question Answering (VTQA) model which takes as input a paragraph caption as well as the corresponding image, and answers the given question based on both inputs. In our model, the inputs are fused to extract related information by cross-attention (early fusion), then fused again in the form of consensus (late fusion), and finally expected answers are given an extra score to enhance the chance of selection (later fusion). Empirical results show that paragraph captions, even when automatically generated (via an RL-based encoder-decoder model), help correctly answer more visual questions. Overall, our joint model, when trained on the Visual Genome dataset, significantly improves the VQA performance over a strong baseline model.",,,,ACL
352,2019,Shared-Private Bilingual Word Embeddings for Neural Machine Translation,"Xuebo Liu, Derek F. Wong, Yang Liu, Lidia S. Chao, Tong Xiao","Word embedding is central to neural machine translation (NMT), which has attracted intensive research interest in recent years. In NMT, the source embedding plays the role of the entrance while the target embedding acts as the terminal. These layers occupy most of the model parameters for representation learning. Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated. In this paper, we propose shared-private bilingual word embeddings, which give a closer relationship between the source and target embeddings, and which also reduce the number of model parameters. For similar source and target words, their embeddings tend to share a part of the features and they cooperatively learn these common representation units. Experiments on 5 language pairs belonging to 6 different language families and written in 5 different alphabets demonstrate that the proposed model provides a significant performance boost over the strong baselines with dramatically fewer model parameters.",,,,ACL
353,2019,Literary Event Detection,"Matthew Sims, Jong Ho Park, David Bamman","In this work we present a new dataset of literary events—events that are depicted as taking place within the imagined space of a novel. While previous work has focused on event detection in the domain of contemporary news, literature poses a number of complications for existing systems, including complex narration, the depiction of a broad array of mental states, and a strong emphasis on figurative language. We outline the annotation decisions of this new dataset and compare several models for predicting events; the best performing model, a bidirectional LSTM with BERT token representations, achieves an F1 score of 73.9. We then apply this model to a corpus of novels split across two dimensions—prestige and popularity—and demonstrate that there are statistically significant differences in the distribution of events for prestige.",,,,ACL
354,2019,Assessing the Ability of Self-Attention Networks to Learn Word Order,"Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, Zhaopeng Tu","Self-attention networks (SAN) have attracted a lot of interests due to their high parallelization and strong performance on a variety of NLP tasks, e.g. machine translation. Due to the lack of recurrence structure such as recurrent neural networks (RNN), SAN is ascribed to be weak at learning positional information of words for sequence modeling. However, neither this speculation has been empirically confirmed, nor explanations for their strong performances on machine translation tasks when “lacking positional information” have been explored. To this end, we propose a novel word reordering detection task to quantify how well the word order information learned by SAN and RNN. Specifically, we randomly move one word to another position, and examine whether a trained model can detect both the original and inserted positions. Experimental results reveal that: 1) SAN trained on word reordering detection indeed has difficulty learning the positional information even with the position embedding; and 2) SAN trained on machine translation learns better positional information than its RNN counterpart, in which position embedding plays a critical role. Although recurrence structure make the model more universally-effective on learning word order, learning objectives matter more in the downstream tasks such as machine translation.",,,,ACL
355,2019,Energy and Policy Considerations for Deep Learning in NLP,"Emma Strubell, Ananya Ganesh, Andrew McCallum","Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",,,,ACL
356,2019,What Does BERT Learn about the Structure of Language?,"Ganesh Jawahar, Benoît Sagot, Djamé Seddah","BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT’s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.",,,,ACL
357,2019,A Just and Comprehensive Strategy for Using NLP to Address Online Abuse,"David Jurgens, Libby Hemphill, Eshwar Chandrasekharan","Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities.",,,,ACL
358,2019,"Learning from Dialogue after Deployment: Feed Yourself, Chatbot!","Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, Jason Weston","The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving a vast store of potential training signal untapped. In this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in. As our agent engages in conversation, it also estimates user satisfaction in its responses. When the conversation appears to be going well, the user’s responses become new training examples to imitate. When the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot’s dialogue abilities further. On the PersonaChat chit-chat dataset with over 131k training examples, we find that learning from dialogue with a self-feeding chatbot significantly improves performance, regardless of the amount of traditional supervision.",,,,ACL
359,2019,Generating Responses with a Specific Emotion in Dialog,"Zhenqiao Song, Xiaoqing Zheng, Lu Liu, Mu Xu, Xuanjing Huang","It is desirable for dialog systems to have capability to express specific emotions during a conversation, which has a direct, quantifiable impact on improvement of their usability and user satisfaction. After a careful investigation of real-life conversation data, we found that there are at least two ways to express emotions with language. One is to describe emotional states by explicitly using strong emotional words; another is to increase the intensity of the emotional experiences by implicitly combining neutral words in distinct ways. We propose an emotional dialogue system (EmoDS) that can generate the meaningful responses with a coherent structure for a post, and meanwhile express the desired emotion explicitly or implicitly within a unified framework. Experimental results showed EmoDS performed better than the baselines in BLEU, diversity and the quality of emotional expression.",,,,ACL
360,2019,Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention,"Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan, William Yang Wang","Semantically controlled neural response generation on limited-domain has achieved great performance. However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics.",,,,ACL
361,2019,Incremental Learning from Scratch for Task-Oriented Dialogue Systems,"Weikang Wang, Jiajun Zhang, Qian Li, Mei-Yuh Hwang, Chengqing Zong","Clarifying user needs is essential for existing task-oriented dialogue systems. However, in real-world applications, developers can never guarantee that all possible user demands are taken into account in the design phase. Consequently, existing systems will break down when encountering unconsidered user needs. To address this problem, we propose a novel incremental learning framework to design task-oriented dialogue systems, or for short Incremental Dialogue System (IDS), without pre-defining the exhaustive list of user needs. Specifically, we introduce an uncertainty estimation module to evaluate the confidence of giving correct responses. If there is high confidence, IDS will provide responses to users. Otherwise, humans will be involved in the dialogue process, and IDS can learn from human intervention through an online learning module. To evaluate our method, we propose a new dataset which simulates unanticipated user needs in the deployment stage. Experiments show that IDS is robust to unconsidered user actions, and can update itself online by smartly selecting only the most effective training data, and hence attains better performance with less annotation cost.",,,,ACL
362,2019,ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation,"Hainan Zhang, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng","In multi-turn dialogue generation, response is usually related with only a few contexts. Therefore, an ideal model should be able to detect these relevant contexts and produce a suitable response accordingly. However, the widely used hierarchical recurrent encoder-decoder models just treat all the contexts indiscriminately, which may hurt the following response generation process. Some researchers try to use the cosine similarity or the traditional attention mechanism to find the relevant contexts, but they suffer from either insufficient relevance assumption or position bias problem. In this paper, we propose a new model, named ReCoSa, to tackle this problem. Firstly, a word level LSTM encoder is conducted to obtain the initial representation of each context. Then, the self-attention mechanism is utilized to update both the context and masked response representation. Finally, the attention weights between each context and response representations are computed and used in the further decoding process. Experimental results on both Chinese customer services dataset and English Ubuntu dialogue dataset show that ReCoSa significantly outperforms baseline models, in terms of both metric-based and human evaluations. Further analysis on attention shows that the detected relevant contexts by ReCoSa are highly coherent with human’s understanding, validating the correctness and interpretability of ReCoSa.",,,,ACL
363,2019,Dialogue Natural Language Inference,"Sean Welleck, Jason Weston, Arthur Szlam, Kyunghyun Cho","Consistency is a long standing issue faced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model’s consistency.",,,,ACL
364,2019,Budgeted Policy Learning for Task-Oriented Dialogue Systems,"Zhirui Zhang, Xiujun Li, Jianfeng Gao, Enhong Chen","This paper presents a new approach that extends Deep Dyna-Q (DDQ) by incorporating a Budget-Conscious Scheduling (BCS) to best utilize a fixed, small amount of user interactions (budget) for learning task-oriented dialogue agents. BCS consists of (1) a Poisson-based global scheduler to allocate budget over different stages of training; (2) a controller to decide at each training step whether the agent is trained using real or simulated experiences; (3) a user goal sampling module to generate the experiences that are most effective for policy learning. Experiments on a movie-ticket booking task with simulated and real users show that our approach leads to significant improvements in success rate over the state-of-the-art baselines given the fixed budget.",,,,ACL
365,2019,Comparison of Diverse Decoding Methods from Conditional Language Models,"Daphne Ippolito, Reno Kriz, Joao Sedoc, Maria Kustikova, Chris Callison-Burch","While conditional language models have greatly improved in their ability to output high quality natural language, many NLP applications benefit from being able to generate a diverse set of candidate sequences. Diverse decoding strategies aim to, within a given-sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that rerank and combine candidate outputs. Standard decoding methods, such as beam search, optimize for generating high likelihood sequences rather than diverse ones, though recent work has focused on increasing diversity in these methods. In this work, we perform an extensive survey of decoding-time strategies for generating diverse outputs from a conditional language model. In addition, we present a novel method where we over-sample candidates, then use clustering to remove similar sequences, thus achieving high diversity without sacrificing quality.",,,,ACL
366,2019,Retrieval-Enhanced Adversarial Training for Neural Response Generation,"Qingfu Zhu, Lei Cui, Wei-Nan Zhang, Furu Wei, Ting Liu","Dialogue systems are usually built on either generation-based or retrieval-based approaches, yet they do not benefit from the advantages of different models. In this paper, we propose a Retrieval-Enhanced Adversarial Training (REAT) method for neural response generation. Distinct from existing approaches, the REAT method leverages an encoder-decoder framework in terms of an adversarial training paradigm, while taking advantage of N-best response candidates from a retrieval-based system to construct the discriminator. An empirical study on a large scale public available benchmark dataset shows that the REAT method significantly outperforms the vanilla Seq2Seq model as well as the conventional adversarial training approach.",,,,ACL
367,2019,Vocabulary Pyramid Network: Multi-Pass Encoding and Decoding with Multi-Level Vocabularies for Response Generation,"Cao Liu, Shizhu He, Kang Liu, Jun Zhao","We study the task of response generation. Conventional methods employ a fixed vocabulary and one-pass decoding, which not only make them prone to safe and general responses but also lack further refining to the first generated raw sequence. To tackle the above two problems, we present a Vocabulary Pyramid Network (VPN) which is able to incorporate multi-pass encoding and decoding with multi-level vocabularies into response generation. Specifically, the dialogue input and output are represented by multi-level vocabularies which are obtained from hierarchical clustering of raw words. Then, multi-pass encoding and decoding are conducted on the multi-level vocabularies. Since VPN is able to leverage rich encoding and decoding information with multi-level vocabularies, it has the potential to generate better responses. Experiments on English Twitter and Chinese Weibo datasets demonstrate that VPN remarkably outperforms strong baselines.",,,,ACL
368,2019,On-device Structured and Context Partitioned Projection Networks,"Sujith Ravi, Zornitsa Kozareva","A challenging problem in on-device text classification is to build highly accurate neural models that can fit in small memory footprint and have low latency. To address this challenge, we propose an on-device neural network SGNN++ which dynamically learns compact projection vectors from raw text using structured and context-dependent partition projections. We show that this results in accelerated inference and performance improvements. We conduct extensive evaluation on multiple conversational tasks and languages such as English, Japanese, Spanish and French. Our SGNN++ model significantly outperforms all baselines, improves upon existing on-device neural models and even surpasses RNN, CNN and BiLSTM models on dialog act and intent prediction. Through a series of ablation studies we show the impact of the partitioned projections and structured information leading to 10% improvement. We study the impact of the model size on accuracy and introduce quatization-aware training for SGNN++ to further reduce the model size while preserving the same quality. Finally, we show fast inference on mobile phones.",,,,ACL
369,2019,Proactive Human-Machine Conversation with Explicit Conversation Goal,"Wenquan Wu, Zhen Guo, Xiangyang Zhou, Hua Wu, Xiyuan Zhang","Though great progress has been made for human-machine conversation, current dialogue system is still in its infancy: it usually converses passively and utters words more as a matter of response, rather than on its own initiatives. In this paper, we take a radical step towards building a human-like conversational agent: endowing it with the ability of proactively leading the conversation (introducing a new topic or maintaining the current topic). To facilitate the development of such conversation systems, we create a new dataset named Konv where one acts as a conversation leader and the other acts as the follower. The leader is provided with a knowledge graph and asked to sequentially change the discussion topics, following the given conversation goal, and meanwhile keep the dialogue as natural and engaging as possible. Konv enables a very challenging task as the model needs to both understand dialogue and plan over the given knowledge graph. We establish baseline results on this dataset (about 270K utterances and 30k dialogues) using several state-of-the-art models. Experimental results show that dialogue models that plan over the knowledge graph can make full use of related knowledge to generate more diverse multi-turn conversations. The baseline systems along with the dataset are publicly available.",,,,ACL
370,2019,Learning a Matching Model with Co-teaching for Multi-turn Response Selection in Retrieval-based Dialogue Systems,"Jiazhan Feng, Chongyang Tao, Wei Wu, Yansong Feng, Dongyan Zhao","We study learning of a matching model for response selection in retrieval-based dialogue systems. The problem is equally important with designing the architecture of a model, but is less explored in existing literature. To learn a robust matching model from noisy training data, we propose a general co-teaching framework with three specific teaching strategies that cover both teaching with loss functions and teaching with data curriculum. Under the framework, we simultaneously learn two matching models with independent training sets. In each iteration, one model transfers the knowledge learned from its training set to the other model, and at the same time receives the guide from the other model on how to overcome noise in training. Through being both a teacher and a student, the two models learn from each other and get improved together. Evaluation results on two public data sets indicate that the proposed learning approach can generally and significantly improve the performance of existing matching models.",,,,ACL
371,2019,Learning to Abstract for Memory-augmented Conversational Response Generation,"Zhiliang Tian, Wei Bi, Xiaopeng Li, Nevin L. Zhang","Neural generative models for open-domain chit-chat conversations have become an active area of research in recent years. A critical issue with most existing generative models is that the generated responses lack informativeness and diversity. A few researchers attempt to leverage the results of retrieval models to strengthen the generative models, but these models are limited by the quality of the retrieval results. In this work, we propose a memory-augmented generative model, which learns to abstract from the training corpus and saves the useful information to the memory to assist the response generation. Our model clusters query-response samples, extracts characteristics of each cluster, and learns to utilize these characteristics for response generation. Experimental results show that our model outperforms other competitive baselines.",,,,ACL
372,2019,Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References,"Lisong Qiu, Juntao Li, Wei Bi, Dongyan Zhao, Rui Yan","Due to its potential applications, open-domain dialogue generation has become popular and achieved remarkable progress in recent years, but sometimes suffers from generic responses. Previous models are generally trained based on 1-to-1 mapping from an input query to its response, which actually ignores the nature of 1-to-n mapping in dialogue that there may exist multiple valid responses corresponding to the same query. In this paper, we propose to utilize the multiple references by considering the correlation of different valid responses and modeling the 1-to-n mapping with a novel two-step generation architecture. The first generation phase extracts the common features of different responses which, combined with distinctive features obtained in the second phase, can generate multiple diverse and appropriate responses. Experimental results show that our proposed model can effectively improve the quality of response and outperform existing neural dialogue models on both automatic and human evaluations.",,,,ACL
373,2019,Pretraining Methods for Dialog Context Representation Learning,"Shikib Mehri, Evgeniia Razumovskaia, Tiancheng Zhao, Maxine Eskenazi","This paper examines various unsupervised pretraining objectives for learning dialog context representations. Two novel methods of pretraining dialog context encoders are proposed, and a total of four methods are examined. Each pretraining objective is fine-tuned and evaluated on a set of downstream dialog tasks using the MultiWoz dataset and strong performance improvement is observed. Further evaluation shows that our pretraining objectives result in not only better performance, but also better convergence, models that are less data hungry and have better domain generalizability.",,,,ACL
374,2019,A Large-Scale Corpus for Conversation Disentanglement,"Jonathan K. Kummerfeld, Sai R. Gouravajhala, Joseph J. Peper, Vignesh Athreya, Chulaka Gunasekara","Disentangling conversations mixed together in a single stream of messages is a difficult task, made harder by the lack of large manually annotated datasets. We created a new dataset of 77,563 messages manually annotated with reply-structure graphs that both disentangle conversations and define internal conversation structure. Our data is 16 times larger than all previously released datasets combined, the first to include adjudication of annotation disagreements, and the first to include context. We use our data to re-examine prior work, in particular, finding that 89% of conversations in a widely used dialogue corpus are either missing messages or contain extra messages. Our manually-annotated data presents an opportunity to develop robust data-driven methods for conversation disentanglement, which will help advance dialogue research.",,,,ACL
375,2019,Self-Supervised Dialogue Learning,"Jiawei Wu, Xin Wang, William Yang Wang","The sequential order of utterances is often meaningful in coherent dialogues, and the order changes of utterances could lead to low-quality and incoherent conversations. We consider the order information as a crucial supervised signal for dialogue learning, which, however, has been neglected by many previous dialogue systems. Therefore, in this paper, we introduce a self-supervised learning task, inconsistent order detection, to explicitly capture the flow of conversation in dialogues. Given a sampled utterance pair triple, the task is to predict whether it is ordered or misordered. Then we propose a sampling-based self-supervised network SSN to perform the prediction with sampled triple references from previous dialogue history. Furthermore, we design a joint learning framework where SSN can guide the dialogue systems towards more coherent and relevant dialogue learning through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and task-oriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets.",,,,ACL
376,2019,Are we there yet? Encoder-decoder neural networks as cognitive models of English past tense inflection,"Maria Corkery, Yevgen Matusevych, Sharon Goldwater","The cognitive mechanisms needed to account for the English past tense have long been a subject of debate in linguistics and cognitive science. Neural network models were proposed early on, but were shown to have clear flaws. Recently, however, Kirov and Cotterell (2018) showed that modern encoder-decoder (ED) models overcome many of these flaws. They also presented evidence that ED models demonstrate humanlike performance in a nonce-word task. Here, we look more closely at the behaviour of their model in this task. We find that (1) the model exhibits instability across multiple simulations in terms of its correlation with human data, and (2) even when results are aggregated across simulations (treating each simulation as an individual human participant), the fit to the human data is not strong—worse than an older rule-based model. These findings hold up through several alternative training regimes and evaluation measures. Although other neural architectures might do better, we conclude that there is still insufficient evidence to claim that neural nets are a good cognitive model for this task.",,,,ACL
377,2019,A Spreading Activation Framework for Tracking Conceptual Complexity of Texts,"Ioana Hulpuș, Sanja Štajner, Heiner Stuckenschmidt","We propose an unsupervised approach for assessing conceptual complexity of texts, based on spreading activation. Using DBpedia knowledge graph as a proxy to long-term memory, mentioned concepts become activated and trigger further activation as the text is sequentially traversed. Drawing inspiration from psycholinguistic theories of reading comprehension, we model memory processes such as semantic priming, sentence wrap-up, and forgetting. We show that our models capture various aspects of conceptual text complexity and significantly outperform current state of the art.",,,,ACL
378,2019,End-to-End Sequential Metaphor Identification Inspired by Linguistic Theories,"Rui Mao, Chenghua Lin, Frank Guerin","End-to-end training with Deep Neural Networks (DNN) is a currently popular method for metaphor identification. However, standard sequence tagging models do not explicitly take advantage of linguistic theories of metaphor identification. We experiment with two DNN models which are inspired by two human metaphor identification procedures. By testing on three public datasets, we find that our models achieve state-of-the-art performance in end-to-end metaphor identification.",,,,ACL
379,2019,Diachronic Sense Modeling with Deep Contextualized Word Embeddings: An Ecological View,"Renfen Hu, Shen Li, Shichen Liang","Diachronic word embeddings have been widely used in detecting temporal changes. However, existing methods face the meaning conflation deficiency by representing a word as a single vector at each time period. To address this issue, this paper proposes a sense representation and tracking framework based on deep contextualized embeddings, aiming at answering not only what and when, but also how the word meaning changes. The experiments show that our framework is effective in representing fine-grained word senses, and it brings a significant improvement in word change detection task. Furthermore, we model the word change from an ecological viewpoint, and sketch two interesting sense behaviors in the process of language evolution, i.e. sense competition and sense cooperation.",,,,ACL
380,2019,Miss Tools and Mr Fruit: Emergent Communication in Agents Learning about Object Affordances,"Diane Bouchacourt, Marco Baroni","Recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. We propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential communication. However, the agents develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge.",,,,ACL
381,2019,CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks,"Roberto Dessì, Marco Baroni","Lake and Baroni (2018) introduced the SCAN dataset probing the ability of seq2seq models to capture compositional generalizations, such as inferring the meaning of “jump around” 0-shot from the component words. Recurrent networks (RNNs) were found to completely fail the most challenging generalization cases. We test here a convolutional network (CNN) on these tasks, reporting hugely improved performance with respect to RNNs. Despite the big improvement, the CNN has however not induced systematic rules, suggesting that the difference between compositional and non-compositional behaviour is not clear-cut.",,,,ACL
382,2019,Uncovering Probabilistic Implications in Typological Knowledge Bases,"Johannes Bjerva, Yova Kementchedjhieva, Ryan Cotterell, Isabelle Augenstein","The study of linguistic typology is rooted in the implications we find between linguistic features, such as the fact that languages with object-verb word ordering tend to have postpositions. Uncovering such implications typically amounts to time-consuming manual processing by trained and experienced linguists, which potentially leaves key linguistic universals unexplored. In this paper, we present a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further linguistic investigation. Our approach outperforms baselines previously used for this problem, as well as a strong baseline from knowledge base population.",,,,ACL
383,2019,Is Word Segmentation Child’s Play in All Languages?,"Georgia R. Loukatou, Steven Moran, Damian Blasi, Sabine Stoll, Alejandrina Cristia","When learning language, infants need to break down the flow of input speech into minimal word-like units, a process best described as unsupervised bottom-up segmentation. Proposed strategies include several segmentation algorithms, but only cross-linguistically robust algorithms could be plausible candidates for human word learning, since infants have no initial knowledge of the ambient language. We report on the stability in performance of 11 conceptually diverse algorithms on a selection of 8 typologically distinct languages. The results consist evidence that some segmentation algorithms are cross-linguistically valid, thus could be considered as potential strategies employed by all infants.",,,,ACL
384,2019,On the Distribution of Deep Clausal Embeddings: A Large Cross-linguistic Study,"Damian Blasi, Ryan Cotterell, Lawrence Wolf-Sonkin, Sabine Stoll, Balthasar Bickel","Embedding a clause inside another (“the girl [who likes cars [that run fast]] has arrived”) is a fundamental resource that has been argued to be a key driver of linguistic expressiveness. As such, it plays a central role in fundamental debates on what makes human language unique, and how they might have evolved. Empirical evidence on the prevalence and the limits of embeddings has however been based on either laboratory setups or corpus data of relatively limited size. We introduce here a collection of large, dependency-parsed written corpora in 17 languages, that allow us, for the first time, to capture clausal embedding through dependency graphs and assess their distribution. Our results indicate that there is no evidence for hard constraints on embedding depth: the tail of depth distributions is heavy. Moreover, although deeply embedded clauses tend to be shorter, suggesting processing load issues, complex sentences with many embeddings do not display a bias towards less deep embeddings. Taken together, the results suggest that deep embeddings are not disfavoured in written language. More generally, our study illustrates how resources and methods from latest-generation big-data NLP can provide new perspectives on fundamental questions in theoretical linguistics.",,,,ACL
385,2019,Attention-based Conditioning Methods for External Knowledge Integration,"Katerina Margatina, Christos Baziotis, Alexandros Potamianos","In this paper, we present a novel approach for incorporating external knowledge in Recurrent Neural Networks (RNNs). We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures. This form of conditioning on the attention distribution, enforces the contribution of the most salient words for the task at hand. We introduce three methods, namely attentional concatenation, feature-based gating and affine transformation. Experiments on six benchmark datasets show the effectiveness of our methods. Attentional feature-based gating yields consistent performance improvement across tasks. Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture.",,,,ACL
386,2019,The KnowRef Coreference Corpus: Removing Gender and Number Cues for Difficult Pronominal Anaphora Resolution,"Ali Emami, Paul Trichelair, Adam Trischler, Kaheer Suleman, Hannes Schulz","We introduce a new benchmark for coreference resolution and NLI, KnowRef, that targets common-sense understanding and world knowledge. Previous coreference resolution tasks can largely be solved by exploiting the number and gender of the antecedents, or have been handcrafted and do not reflect the diversity of naturally occurring text. We present a corpus of over 8,000 annotated text passages with ambiguous pronominal anaphora. These instances are both challenging and realistic. We show that various coreference systems, whether rule-based, feature-rich, or neural, perform significantly worse on the task than humans, who display high inter-annotator agreement. To explain this performance gap, we show empirically that state-of-the art models often fail to capture context, instead relying on the gender or number of candidate antecedents to make a decision. We then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in models. Finally, we show that antecedent switching yields promising results on other tasks as well: we use it to achieve state-of-the-art results on the GAP coreference task.",,,,ACL
387,2019,StRE: Self Attentive Edit Quality Prediction in Wikipedia,"Soumya Sarkar, Bhanu Prakash Reddy, Sandipan Sikdar, Animesh Mukherjee","Wikipedia can easily be justified as a behemoth, considering the sheer volume of content that is added or removed every minute to its several projects. This creates an immense scope, in the field of natural language processing toward developing automated tools for content moderation and review. In this paper we propose Self Attentive Revision Encoder (StRE) which leverages orthographic similarity of lexical units toward predicting the quality of new edits. In contrast to existing propositions which primarily employ features like page reputation, editor activity or rule based heuristics, we utilize the textual content of the edits which, we believe contains superior signatures of their quality. More specifically, we deploy deep encoders to generate representations of the edits from its text content, which we then leverage to infer quality. We further contribute a novel dataset containing ∼ 21M revisions across 32K Wikipedia pages and demonstrate that StRE outperforms existing methods by a significant margin – at least 17% and at most 103%. Our pre-trained model achieves such result after retraining on a set as small as 20% of the edits in a wikipage. This, to the best of our knowledge, is also the first attempt towards employing deep language models to the enormous domain of automated content moderation and review in Wikipedia.",,,,ACL
388,2019,How Large Are Lions? Inducing Distributions over Quantitative Attributes,"Yanai Elazar, Abhijit Mahabal, Deepak Ramachandran, Tania Bedrax-Weiss, Dan Roth","Most current NLP systems have little knowledge about quantitative attributes of objects and events. We propose an unsupervised method for collecting quantitative information from large amounts of web data, and use it to create a new, very large resource consisting of distributions over physical quantities associated with objects, adjectives, and verbs which we call Distributions over Quantitative (DoQ). This contrasts with recent work in this area which has focused on making only relative comparisons such as “Is a lion bigger than a wolf?”. Our evaluation shows that DoQ compares favorably with state of the art results on existing datasets for relative comparisons of nouns and adjectives, and on a new dataset we introduce.",,,,ACL
389,2019,Fine-Grained Sentence Functions for Short-Text Conversation,"Wei Bi, Jun Gao, Xiaojiang Liu, Shuming Shi","Sentence function is an important linguistic feature referring to a user’s purpose in uttering a specific sentence. The use of sentence function has shown promising results to improve the performance of conversation models. However, there is no large conversation dataset annotated with sentence functions. In this work, we collect a new Short-Text Conversation dataset with manually annotated SEntence FUNctions (STC-Sefun). Classification models are trained on this dataset to (i) recognize the sentence function of new data in a large corpus of short-text conversations; (ii) estimate a proper sentence function of the response given a test query. We later train conversation models conditioned on the sentence functions, including information retrieval-based and neural generative models. Experimental results demonstrate that the use of sentence functions can help improve the quality of the returned responses.",,,,ACL
390,2019,Give Me More Feedback II: Annotating Thesis Strength and Related Attributes in Student Essays,"Zixuan Ke, Hrishikesh Inamdar, Hui Lin, Vincent Ng","While the vast majority of existing work on automated essay scoring has focused on holistic scoring, researchers have recently begun work on scoring specific dimensions of essay quality. Nevertheless, progress on dimension-specific essay scoring is limited in part by the lack of annotated corpora. To facilitate advances in this area, we design a scoring rubric for scoring a core, yet unexplored dimension of persuasive essay quality, thesis strength, and annotate a corpus of essays with thesis strength scores. We additionally identify the attributes that could impact thesis strength and annotate the essays with the values of these attributes, which, when predicted by computational models, could provide further feedback to students on why her essay receives a particular thesis strength score.",,,,ACL
391,2019,Crowdsourcing and Validating Event-focused Emotion Corpora for German and English,"Enrica Troiano, Sebastian Padó, Roman Klinger","Sentiment analysis has a range of corpora available across multiple languages. For emotion analysis, the situation is more limited, which hinders potential research on crosslingual modeling and the development of predictive models for other languages. In this paper, we fill this gap for German by constructing deISEAR, a corpus designed in analogy to the well-established English ISEAR emotion dataset. Motivated by Scherer’s appraisal theory, we implement a crowdsourcing experiment which consists of two steps. In step 1, participants create descriptions of emotional events for a given emotion. In step 2, five annotators assess the emotion expressed by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not, on average, cause a performance drop.",,,,ACL
392,2019,Pay Attention when you Pay the Bills. A Multilingual Corpus with Dependency-based and Semantic Annotation of Collocations.,"Marcos Garcia, Marcos García Salido, Susana Sotelo, Estela Mosqueira, Margarita Alonso-Ramos","This paper presents a new multilingual corpus with semantic annotation of collocations in English, Portuguese, and Spanish. The whole resource contains 155k tokens and 1,526 collocations labeled in context. The annotated examples belong to three syntactic relations (adjective-noun, verb-object, and nominal compounds), and represent 58 lexical functions in the Meaning-Text Theory (e.g., Oper, Magn, Bon, etc.). Each collocation was annotated by three linguists and the final resource was revised by a team of experts. The resulting corpus can serve as a basis to evaluate different approaches for collocation identification, which in turn can be useful for different NLP tasks such as natural language understanding or natural language generation.",,,,ACL
393,2019,Does it Make Sense? And Why? A Pilot Study for Sense Making and Explanation,"Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, Tian Gao","Introducing common sense to natural language understanding systems has received increasing research attention. It remains a fundamental question on how to evaluate whether a system has the sense-making capability. Existing benchmarks measure common sense knowledge indirectly or without reasoning. In this paper, we release a benchmark to directly test whether a system can differentiate natural language statements that make sense from those that do not make sense. In addition, a system is asked to identify the most crucial reason why a statement does not make sense. We evaluate models trained over large-scale language modeling tasks as well as human performance, showing that there are different challenges for system sense-making.",,,,ACL
394,2019,Large Dataset and Language Model Fun-Tuning for Humor Recognition,"Vladislav Blinov, Valeria Bolotova-Baranova, Pavel Braslavski","The task of humor recognition has attracted a lot of attention recently due to the urge to process large amounts of user-generated texts and rise of conversational agents. We collected a dataset of jokes and funny dialogues in Russian from various online resources and complemented them carefully with unfunny texts with similar lexical properties. The dataset comprises of more than 300,000 short texts, which is significantly larger than any previous humor-related corpus. Manual annotation of 2,000 items proved the reliability of the corpus construction approach. Further, we applied language model fine-tuning for text classification and obtained an F1 score of 0.91 on a test set, which constitutes a considerable gain over baseline methods. The dataset is freely available for research community.",,,,ACL
395,2019,Towards Language Agnostic Universal Representations,"Armen Aghajanyan, Xia Song, Saurabh Tiwary","When a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in, even if the math lessons were only taught in one language. However, current representations in machine learning are language dependent. In this work, we present a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion. We learn these representations by taking inspiration from linguistics, specifically the Universal Grammar hypothesis and learn universal latent representations that are language agnostic. We demonstrate the capabilities of these representations by showing that models trained on a single language using language agnostic representations achieve very similar accuracies in other languages.",,,,ACL
396,2019,Leveraging Meta Information in Short Text Aggregation,"He Zhao, Lan Du, Guanfeng Liu, Wray Buntine","Short texts such as tweets often contain insufficient word co-occurrence information for training conventional topic models. To deal with the insufficiency, we propose a generative model that aggregates short texts into clusters by leveraging the associated meta information. Our model can generate more interpretable topics as well as document clusters. We develop an effective Gibbs sampling algorithm favoured by the fully local conjugacy in the model. Extensive experiments demonstrate that our model achieves better performance in terms of document clustering and topic coherence.",,,,ACL
397,2019,Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning,"Shuai Tang, Virginia R. de Sa","Encoder-decoder models for unsupervised sentence representation learning using the distributional hypothesis effectively constrain the learnt representation of a sentence to only that needed to reproduce the next sentence. While the decoder is important to constrain the representation, these models tend to discard the decoder after training since only the encoder is needed to map the input sentence into a vector representation. However, parameters learnt in the decoder also contain useful information about the language. In order to utilise the decoder after learning, we present two types of decoding functions whose inverse can be easily derived without expensive inverse calculation. Therefore, the inverse of the decoding function serves as another encoder that produces sentence representations. We show that, with careful design of the decoding functions, the model learns good sentence representations, and the ensemble of the representations produced from the encoder and the inverse of the decoder demonstrate even better generalisation ability and solid transferability.",,,,ACL
398,2019,"Self-Attentive, Multi-Context One-Class Classification for Unsupervised Anomaly Detection on Text","Lukas Ruff, Yury Zemlyanskiy, Robert Vandermeulen, Thomas Schnake, Marius Kloft","There exist few text-specific methods for unsupervised anomaly detection, and for those that do exist, none utilize pre-trained models for distributed vector representations of words. In this paper we introduce a new anomaly detection method—Context Vector Data Description (CVDD)—which builds upon word embedding models to learn multiple sentence representations that capture multiple semantic contexts via the self-attention mechanism. Modeling multiple contexts enables us to perform contextual anomaly detection of sentences and phrases with respect to the multiple themes and concepts present in an unlabeled text corpus. These contexts in combination with the self-attention weights make our method highly interpretable. We demonstrate the effectiveness of CVDD quantitatively as well as qualitatively on the well-known Reuters, 20 Newsgroups, and IMDB Movie Reviews datasets.",,,,ACL
399,2019,Hubless Nearest Neighbor Search for Bilingual Lexicon Induction,"Jiaji Huang, Qiang Qiu, Kenneth Church","Bilingual Lexicon Induction (BLI) is the task of translating words from corpora in two languages. Recent advances in BLI work by aligning the two word embedding spaces. Following that, a key step is to retrieve the nearest neighbor (NN) in the target space given the source word. However, a phenomenon called hubness often degrades the accuracy of NN. Hubness appears as some data points, called hubs, being extra-ordinarily close to many of the other data points. Reducing hubness is necessary for retrieval tasks. One successful example is Inverted SoFtmax (ISF), recently proposed to improve NN. This work proposes a new method, Hubless Nearest Neighbor (HNN), to mitigate hubness. HNN differs from NN by imposing an additional equal preference assumption. Moreover, the HNN formulation explains why ISF works as well as it does. Empirical results demonstrate that HNN outperforms NN, ISF and other state-of-the-art. For reproducibility and follow-ups, we have published all code.",,,,ACL
400,2019,Distant Learning for Entity Linking with Automatic Noise Detection,"Phong Le, Ivan Titov","Accurate entity linkers have been produced for domains and languages where annotated data (i.e., texts linked to a knowledge base) is available. However, little progress has been made for the settings where no or very limited amounts of labeled data are present (e.g., legal or most scientific domains). In this work, we show how we can learn to link mentions without having any labeled examples, only a knowledge base and a collection of unannotated texts from the corresponding domain. In order to achieve this, we frame the task as a multi-instance learning problem and rely on surface matching to create initial noisy labels. As the learning signal is weak and our surrogate labels are noisy, we introduce a noise detection component in our model: it lets the model detect and disregard examples which are likely to be noisy. Our method, jointly learning to detect noise and link entities, greatly outperforms the surface matching baseline. For a subset of entity categories, it even approaches the performance of supervised learning.",,,,ACL
401,2019,Learning How to Active Learn by Dreaming,"Thuy-Trang Vu, Ming Liu, Dinh Phung, Gholamreza Haffari",Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary. Recent data-driven AL policy learning methods are also restricted to learn from closely related domains. We introduce a new sample-efficient method that learns the AL policy directly on the target domain of interest by using wake and dream cycles. Our approach interleaves between querying the annotation of the selected datapoints to update the underlying student learner and improving AL policy using simulation where the current student learner acts as an imperfect annotator. We evaluate our method on cross-domain and cross-lingual text classification and named entity recognition tasks. Experimental results show that our dream-based AL policy training strategy is more effective than applying the pretrained policy without further fine-tuning and better than the existing strong baseline methods that use heuristics or reinforcement learning.,,,,ACL
402,2019,Few-Shot Representation Learning for Out-Of-Vocabulary Words,"Ziniu Hu, Ting Chen, Kai-Wei Chang, Yizhou Sun","Existing approaches for learning word embedding often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts. However, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do not appear in training corpus emerge frequently. How to learn accurate representations of these words to augment a pre-trained embedding by only a few observations is a challenging research problem. In this paper, we formulate the learning of OOV embedding as a few-shot regression problem by fitting a representation function to predict an oracle embedding vector (defined as embedding trained with abundant observations) based on limited contexts. Specifically, we propose a novel hierarchical attention network-based embedding framework to serve as the neural regression function, in which the context information of a word is encoded and aggregated from K observations. Furthermore, we propose to use Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new corpus fast and robustly. Experiments show that the proposed approach significantly outperforms existing methods in constructing an accurate embedding for OOV words and improves downstream tasks when the embedding is utilized.",,,,ACL
403,2019,Neural Temporality Adaptation for Document Classification: Diachronic Word Embeddings and Domain Adaptation Models,"Xiaolei Huang, Michael J. Paul","Language usage can change across periods of time, but document classifiers models are usually trained and tested on corpora spanning multiple years without considering temporal variations. This paper describes two complementary ways to adapt classifiers to shifts across time. First, we show that diachronic word embeddings, which were originally developed to study language change, can also improve document classification, and we show a simple method for constructing this type of embedding. Second, we propose a time-driven neural classification model inspired by methods for domain adaptation. Experiments on six corpora show how these methods can make classifiers more robust over time.",,,,ACL
404,2019,Learning Transferable Feature Representations Using Neural Networks,"Himanshu Sharad Bhatt, Shourya Roy, Arun Rajkumar, Sriranjani Ramakrishnan","Learning representations such that the source and target distributions appear as similar as possible has benefited transfer learning tasks across several applications. Generally it requires labeled data from the source and only unlabeled data from the target to learn such representations. While these representations act like a bridge to transfer knowledge learned in the source to the target; they may lead to negative transfer when the source specific characteristics detract their ability to represent the target data. We present a novel neural network architecture to simultaneously learn a two-part representation which is based on the principle of segregating source specific representation from the common representation. The first part captures the source specific characteristics while the second part captures the truly common representation. Our architecture optimizes an objective function which acts adversarial for the source specific part if it contributes towards the cross-domain learning. We empirically show that two parts of the representation, in different arrangements, outperforms existing learning algorithms on the source learning as well as cross-domain tasks on multiple datasets.",,,,ACL
405,2019,"Bayes Test of Precision, Recall, and F1 Measure for Comparison of Two Natural Language Processing Models","Ruibo Wang, Jihong Li","Direct comparison on point estimation of the precision (P), recall (R), and F1 measure of two natural language processing (NLP) models on a common test corpus is unreasonable and results in less replicable conclusions due to a lack of a statistical test. However, the existing t-tests in cross-validation (CV) for model comparison are inappropriate because the distributions of P, R, F1 are skewed and an interval estimation of P, R, and F1 based on a t-test may exceed [0,1]. In this study, we propose to use a block-regularized 3×2 CV (3×2 BCV) in model comparison because it could regularize the difference in certain frequency distributions over linguistic units between training and validation sets and yield stable estimators of P, R, and F1. On the basis of the 3×2 BCV, we calibrate the posterior distributions of P, R, and F1 and derive an accurate interval estimation of P, R, and F1. Furthermore, we formulate the comparison into a hypothesis testing problem and propose a novel Bayes test. The test could directly compute the probabilities of the hypotheses on the basis of the posterior distributions and provide more informative decisions than the existing significance t-tests. Three experiments with regard to NLP chunking tasks are conducted, and the results illustrate the validity of the Bayes test.",,,,ACL
406,2019,TIGS: An Inference Algorithm for Text Infilling with Gradient Search,"Dayiheng Liu, Jie Fu, Pengfei Liu, Jiancheng Lv","Text infilling aims at filling in the missing part of a sentence or paragraph, which has been applied to a variety of real-world natural language generation scenarios. Given a well-trained sequential generative model, it is challenging for its unidirectional decoder to generate missing symbols conditioned on the past and future information around the missing part. In this paper, we propose an iterative inference algorithm based on gradient search, which could be the first inference algorithm that can be broadly applied to any neural sequence generative models for text infilling tasks. Extensive experimental comparisons show the effectiveness and efficiency of the proposed method on three different text infilling tasks with various mask ratios and different mask strategies, comparing with five state-of-the-art methods.",,,,ACL
407,2019,Keeping Notes: Conditional Natural Language Generation with a Scratchpad Encoder,"Ryan Benmalek, Madian Khabsa, Suma Desu, Claire Cardie, Michele Banko","We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.",,,,ACL
408,2019,Using Automatically Extracted Minimum Spans to Disentangle Coreference Evaluation from Boundary Detection,"Nafise Sadat Moosavi, Leo Born, Massimo Poesio, Michael Strube","The common practice in coreference resolution is to identify and evaluate the maximum span of mentions. The use of maximum spans tangles coreference evaluation with the challenges of mention boundary detection like prepositional phrase attachment. To address this problem, minimum spans are manually annotated in smaller corpora. However, this additional annotation is costly and therefore, this solution does not scale to large corpora. In this paper, we propose the MINA algorithm for automatically extracting minimum spans to benefit from minimum span evaluation in all corpora. We show that the extracted minimum spans by MINA are consistent with those that are manually annotated by experts. Our experiments show that using minimum spans is in particular important in cross-dataset coreference evaluation, in which detected mention boundaries are noisier due to domain shift. We have integrated MINA into https://github.com/ns-moosavi/coval for reporting standard coreference scores based on both maximum and automatically detected minimum spans.",,,,ACL
409,2019,Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution,"Shany Barhom, Vered Shwartz, Alon Eirew, Michael Bugert, Nils Reimers","Recognizing coreferring events and entities across multiple texts is crucial for many NLP applications. Despite the task’s importance, research focus was given mostly to within-document entity coreference, with rather little attention to the other variants. We propose a neural architecture for cross-document coreference resolution. Inspired by Lee et al. (2012), we jointly model entity and event coreference. We represent an event (entity) mention using its lexical span, surrounding context, and relation to entity (event) mentions via predicate-arguments structures. Our model outperforms the previous state-of-the-art event coreference model on ECB+, while providing the first entity coreference results on this corpus. Our analysis confirms that all our representation elements, including the mention span itself, its context, and the relation to other mentions contribute to the model’s success.",,,,ACL
410,2019,A Unified Linear-Time Framework for Sentence-Level Discourse Parsing,"Xiang Lin, Shafiq Joty, Prathyusha Jwalapuram, M Saiful Bari","We propose an efficient neural framework for sentence-level discourse analysis in accordance with Rhetorical Structure Theory (RST). Our framework comprises a discourse segmenter to identify the elementary discourse units (EDU) in a text, and a discourse parser that constructs a discourse tree in a top-down fashion. Both the segmenter and the parser are based on Pointer Networks and operate in linear time. Our segmenter yields an F1 score of 95.4%, and our parser achieves an F1 score of 81.7% on the aggregated labeled (relation) metric, surpassing previous approaches by a good margin and approaching human agreement on both tasks (98.3 and 83.0 F1).",,,,ACL
411,2019,Employing the Correspondence of Relations and Connectives to Identify Implicit Discourse Relations via Label Embeddings,"Linh The Nguyen, Linh Van Ngo, Khoat Than, Thien Huu Nguyen","It has been shown that implicit connectives can be exploited to improve the performance of the models for implicit discourse relation recognition (IDRR). An important property of the implicit connectives is that they can be accurately mapped into the discourse relations conveying their functions. In this work, we explore this property in a multi-task learning framework for IDRR in which the relations and the connectives are simultaneously predicted, and the mapping is leveraged to transfer knowledge between the two prediction tasks via the embeddings of relations and connectives. We propose several techniques to enable such knowledge transfer that yield the state-of-the-art performance for IDRR on several settings of the benchmark dataset (i.e., the Penn Discourse Treebank dataset).",,,,ACL
412,2019,Do You Know That Florence Is Packed with Visitors? Evaluating State-of-the-art Models of Speaker Commitment,"Nanjiang Jiang, Marie-Catherine de Marneffe","When a speaker, Mary, asks “Do you know that Florence is packed with visitors?”, we take her to believe that Florence is packed with visitors, but not if she asks “Do you think that Florence is packed with visitors?”. Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. Here, we explore the hypothesis that linguistic deficits drive the error patterns of existing speaker commitment models by analyzing the linguistic correlates of model error on a challenging naturalistic dataset. We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses. The CommitmentBank is annotated with speaker commitment towards the content of the complement (“Florence is packed with visitors” in our example) of clause-embedding verbs (“know”, “think”) under four entailment-canceling environments (negation, modal, question, conditional). A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement.",,,,ACL
413,2019,Multi-Relational Script Learning for Discourse Relations,"I-Ta Lee, Dan Goldwasser","Modeling script knowledge can be useful for a wide range of NLP tasks. Current statistical script learning approaches embed the events, such that their relationships are indicated by their similarity in the embedding. While intuitive, these approaches fall short of representing nuanced relations, needed for downstream tasks. In this paper, we suggest to view learning event embedding as a multi-relational problem, which allows us to capture different aspects of event pairs. We model a rich set of event relations, such as Cause and Contrast, derived from the Penn Discourse Tree Bank. We evaluate our model on three types of tasks, the popular Mutli-Choice Narrative Cloze and its variants, several multi-relational prediction tasks, and a related downstream task—implicit discourse sense classification.",,,,ACL
414,2019,Open-Domain Why-Question Answering with Adversarial Learning to Encode Answer Texts,"Jong-Hoon Oh, Kazuma Kadowaki, Julien Kloetzer, Ryu Iida, Kentaro Torisawa","In this paper, we propose a method for why-question answering (why-QA) that uses an adversarial learning framework. Existing why-QA methods retrieve “answer passages” that usually consist of several sentences. These multi-sentence passages contain not only the reason sought by a why-question and its connection to the why-question, but also redundant and/or unrelated parts. We use our proposed “Adversarial networks for Generating compact-answer Representation” (AGR) to generate from a passage a vector representation of the non-redundant reason sought by a why-question and exploit the representation for judging whether the passage actually answers the why-question. Through a series of experiments using Japanese why-QA datasets, we show that these representations improve the performance of our why-QA neural model as well as that of a BERT-based why-QA model. We show that they also improve a state-of-the-art distantly supervised open-domain QA (DS-QA) method on publicly available English datasets, even though the target task is not a why-QA.",,,,ACL
415,2019,Learning to Ask Unanswerable Questions for Machine Reading Comprehension,"Haichao Zhu, Li Dong, Furu Wei, Wenhui Wang, Bing Qin","Machine reading comprehension with unanswerable questions is a challenging task. In this work, we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable question paired with its corresponding paragraph that contains the answer. We introduce a pair-to-sequence model for unanswerable question generation, which effectively captures the interactions between the question and the paragraph. We also present a way to construct training data for our question generation models by leveraging the existing reading comprehension dataset. Experimental results show that the pair-to-sequence model performs consistently better compared with the sequence-to-sequence baseline. We further use the automatically generated unanswerable questions as a means of data augmentation on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model.",,,,ACL
416,2019,Compositional Questions Do Not Necessitate Multi-hop Reasoning,"Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi","Multi-hop reading comprehension (RC) questions are challenging because they require reading and reasoning over multiple paragraphs. We argue that it can be difficult to construct large multi-hop RC datasets. For example, even highly compositional questions can be answered with a single hop if they target specific entity types, or the facts needed to answer them are redundant. Our analysis is centered on HotpotQA, where we show that single-hop reasoning can solve much more of the dataset than previously thought. We introduce a single-hop BERT-based RC model that achieves 67 F1—comparable to state-of-the-art multi-hop models. We also design an evaluation setting where humans are not shown all of the necessary paragraphs for the intended multi-hop reasoning but can still answer over 80% of questions. Together with detailed error analysis, these results suggest there should be an increasing focus on the role of evidence in multi-hop reasoning and possibly even a shift towards information retrieval style evaluations with large and diverse evidence collections.",,,,ACL
417,2019,Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader,"Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, William Yang Wang","We propose a new end-to-end question answering model, which learns to aggregate answer evidence from an incomplete knowledge base (KB) and a set of retrieved text snippets.Under the assumptions that structured data is easier to query and the acquired knowledge can help the understanding of unstructured text, our model first accumulates knowledge ofKB entities from a question-related KB sub-graph; then reformulates the question in the latent space and reads the text with the accumulated entity knowledge at hand. The evidence from KB and text are finally aggregated to predict answers. On the widely-used KBQA benchmark WebQSP, our model achieves consistent improvements across settings with different extents of KB incompleteness.",,,,ACL
418,2019,AdaNSP: Uncertainty-driven Adaptive Decoding in Neural Semantic Parsing,"Xiang Zhang, Shizhu He, Kang Liu, Jun Zhao","Neural semantic parsers utilize the encoder-decoder framework to learn an end-to-end model for semantic parsing that transduces a natural language sentence to the formal semantic representation. To keep the model aware of the underlying grammar in target sequences, many constrained decoders were devised in a multi-stage paradigm, which decode to the sketches or abstract syntax trees first, and then decode to target semantic tokens. We instead to propose an adaptive decoding method to avoid such intermediate representations. The decoder is guided by model uncertainty and automatically uses deeper computations when necessary. Thus it can predict tokens adaptively. Our model outperforms the state-of-the-art neural models and does not need any expertise like predefined grammar or sketches in the meantime.",,,,ACL
419,2019,The Language of Legal and Illegal Activity on the Darknet,"Leshem Choshen, Dan Eldad, Daniel Hershcovich, Elior Sulem, Omri Abend","The non-indexed parts of the Internet (the Darknet) have become a haven for both legal and illegal anonymous activity. Given the magnitude of these networks, scalably monitoring their activity necessarily relies on automated tools, and notably on NLP tools. However, little is known about what characteristics texts communicated through the Darknet have, and how well do off-the-shelf NLP tools do on this domain. This paper tackles this gap and performs an in-depth investigation of the characteristics of legal and illegal text in the Darknet, comparing it to a clear net website with similar content as a control condition. Taking drugs-related websites as a test case, we find that texts for selling legal and illegal drugs have several linguistic characteristics that distinguish them from one another, as well as from the control condition, among them the distribution of POS tags, and the coverage of their named entities in Wikipedia.",,,,ACL
420,2019,Eliciting Knowledge from Experts: Automatic Transcript Parsing for Cognitive Task Analysis,"Junyi Du, He Jiang, Jiaming Shen, Xiang Ren","Cognitive task analysis (CTA) is a type of analysis in applied psychology aimed at eliciting and representing the knowledge and thought processes of domain experts. In CTA, often heavy human labor is involved to parse the interview transcript into structured knowledge (e.g., flowchart for different actions). To reduce human efforts and scale the process, automated CTA transcript parsing is desirable. However, this task has unique challenges as (1) it requires the understanding of long-range context information in conversational text; and (2) the amount of labeled data is limited and indirect—i.e., context-aware, noisy, and low-resource. In this paper, we propose a weakly-supervised information extraction framework for automated CTA transcript parsing. We partition the parsing process into a sequence labeling task and a text span-pair relation extraction task, with distant supervision from human-curated protocol files. To model long-range context information for extracting sentence relations, neighbor sentences are involved as a part of input. Different types of models for capturing context dependency are then applied. We manually annotate real-world CTA transcripts to facilitate the evaluation of the parsing tasks.",,,,ACL
421,2019,Course Concept Expansion in MOOCs with External Knowledge and Interactive Game,"Jifan Yu, Chenyu Wang, Gan Luo, Lei Hou, Juanzi Li","As Massive Open Online Courses (MOOCs) become increasingly popular, it is promising to automatically provide extracurricular knowledge for MOOC users. Suffering from semantic drifts and lack of knowledge guidance, existing methods can not effectively expand course concepts in complex MOOC environments. In this paper, we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results. In addition, to involve human efforts in our model, we design an interactive optimization mechanism based on a game. Our experiments on the four datasets from Coursera and XuetangX show that the proposed method achieves significant improvements(+0.19 by MAP) over existing methods.",,,,ACL
422,2019,Towards Near-imperceptible Steganographic Text,"Falcon Dai, Zheng Cai","We show that the imperceptibility of several existing linguistic steganographic systems (Fang et al., 2017; Yang et al., 2018) relies on implicit assumptions on statistical behaviors of fluent text. We formally analyze them and empirically evaluate these assumptions. Furthermore, based on these observations, we propose an encoding algorithm called patient-Huffman with improved near-imperceptible guarantees.",,,,ACL
423,2019,Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network,"Sunil Kumar Sahu, Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou","Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.",,,,ACL
424,2019,Neural Legal Judgment Prediction in English,"Ilias Chalkidis, Ion Androutsopoulos, Nikolaos Aletras","Legal judgment prediction is the task of automatically predicting the outcome of a court case, given a text describing the case’s facts. Previous work on using neural models for this task has focused on Chinese; only feature-based models (e.g., using bags of words and topics) have been considered in English. We release a new English legal judgment prediction dataset, containing cases from the European Court of Human Rights. We evaluate a broad variety of neural models on the new dataset, establishing strong baselines that surpass previous feature-based models in three tasks: (1) binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation.",,,,ACL
425,2019,Robust Neural Machine Translation with Doubly Adversarial Inputs,"Yong Cheng, Lu Jiang, Wolfgang Macherey","Neural machine translation (NMT) often suffers from the vulnerability to noisy perturbations in the input. We propose an approach to improving the robustness of NMT models, which consists of two parts: (1) attack the translation model with adversarial source examples; (2) defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs. For the generation of adversarial inputs, we propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs. Experimental results on Chinese-English and English-German translation tasks demonstrate that our approach achieves significant improvements (2.8 and 1.6 BLEU points) over Transformer on standard clean benchmarks as well as exhibiting higher robustness on noisy data.",,,,ACL
426,2019,Bridging the Gap between Training and Inference for Neural Machine Translation,"Wen Zhang, Yang Feng, Fandong Meng, Di You, Qun Liu","Neural Machine Translation (NMT) generates target words sequentially in the way of predicting the next word conditioned on the context words. At training time, it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch. This discrepancy of the fed context leads to error accumulation among the way. Furthermore, word-level training requires strict matching between the generated sequence and the ground truth sequence which leads to overcorrection over different but reasonable translations. In this paper, we address these issues by sampling context words not only from the ground truth sequence but also from the predicted sequence by the model during training, where the predicted sequence is selected with a sentence-level optimum. Experiment results on Chinese->English and WMT’14 English->German translation tasks demonstrate that our approach can achieve significant improvements on multiple datasets.",,,,ACL
427,2019,Beyond BLEU:Training Neural Machine Translation with Semantic Similarity,"John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, Graham Neubig","While most neural machine translation (NMT)systems are still trained using maximum likelihood estimation, recent work has demonstrated that optimizing systems to directly improve evaluation metrics such as BLEU can significantly improve final translation accuracy. However, training with BLEU has some limitations: it doesn’t assign partial credit, it has a limited range of output values, and it can penalize semantically correct hypotheses if they differ lexically from the reference. In this paper, we introduce an alternative reward function for optimizing NMT systems that is based on recent work in semantic similarity. We evaluate on four disparate languages trans-lated to English, and find that training with our proposed metric results in better translations as evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU",,,,ACL
428,2019,AutoML Strategy Based on Grammatical Evolution: A Case Study about Knowledge Discovery from Text,"Suilan Estevez-Velarde, Yoan Gutiérrez, Andrés Montoyo, Yudivián Almeida-Cruz","The process of extracting knowledge from natural language text poses a complex problem that requires both a combination of machine learning techniques and proper feature selection. Recent advances in Automatic Machine Learning (AutoML) provide effective tools to explore large sets of algorithms, hyper-parameters and features to find out the most suitable combination of them. This paper proposes a novel AutoML strategy based on probabilistic grammatical evolution, which is evaluated on the health domain by facing the knowledge discovery challenge in Spanish text documents. Our approach achieves state-of-the-art results and provides interesting insights into the best combination of parameters and algorithms to use when dealing with this challenge. Source code is provided for the research community.",,,,ACL
429,2019,Distilling Discrimination and Generalization Knowledge for Event Detection via Delta-Representation Learning,"Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun","Event detection systems rely on discrimination knowledge to distinguish ambiguous trigger words and generalization knowledge to detect unseen/sparse trigger words. Current neural event detection approaches focus on trigger-centric representations, which work well on distilling discrimination knowledge, but poorly on learning generalization knowledge. To address this problem, this paper proposes a Delta-learning approach to distill discrimination and generalization knowledge by effectively decoupling, incrementally learning and adaptively fusing event representation. Experiments show that our method significantly outperforms previous approaches on unseen/sparse trigger words, and achieves state-of-the-art performance on both ACE2005 and KBP2017 datasets.",,,,ACL
430,2019,Chinese Relation Extraction with Multi-Grained Information and External Linguistic Knowledge,"Ziran Li, Ning Ding, Zhiyuan Liu, Haitao Zheng, Ying Shen","Chinese relation extraction is conducted using neural networks with either character-based or word-based inputs, and most existing methods typically suffer from segmentation errors and ambiguity of polysemy. To address the issues, we propose a multi-grained lattice framework (MG lattice) for Chinese relation extraction to take advantage of multi-grained language information and external linguistic knowledge. In this framework, (1) we incorporate word-level information into character sequence inputs so that segmentation errors can be avoided. (2) We also model multiple senses of polysemous words with the help of external linguistic knowledge, so as to alleviate polysemy ambiguity. Experiments on three real-world datasets in distinct domains show consistent and significant superiority and robustness of our model, as compared with other baselines. We will release the source code of this paper in the future.",,,,ACL
431,2019,A2N: Attending to Neighbors for Knowledge Graph Inference,"Trapit Bansal, Da-Cheng Juan, Sujith Ravi, Andrew McCallum","State-of-the-art models for knowledge graph completion aim at learning a fixed embedding representation of entities in a multi-relational graph which can generalize to infer unseen entity relationships at test time. This can be sub-optimal as it requires memorizing and generalizing to all possible entity relationships using these fixed representations. We thus propose a novel attention-based method to learn query-dependent representation of entities which adaptively combines the relevant graph neighborhood of an entity leading to more accurate KG completion. The proposed method is evaluated on two benchmark datasets for knowledge graph completion, and experimental results show that the proposed model performs competitively or better than existing state-of-the-art, including recent methods for explicit multi-hop reasoning. Qualitative probing offers insight into how the model can reason about facts involving multiple hops in the knowledge graph, through the use of neighborhood attention.",,,,ACL
432,2019,Graph based Neural Networks for Event Factuality Prediction using Syntactic and Semantic Structures,"Amir Pouran Ben Veyseh, Thien Huu Nguyen, Dejing Dou","Event factuality prediction (EFP) is the task of assessing the degree to which an event mentioned in a sentence has happened. For this task, both syntactic and semantic information are crucial to identify the important context words. The previous work for EFP has only combined these information in a simple way that cannot fully exploit their coordination. In this work, we introduce a novel graph-based neural network for EFP that can integrate the semantic and syntactic information more effectively. Our experiments demonstrate the advantage of the proposed model for EFP.",,,,ACL
433,2019,Embedding Time Expressions for Deep Temporal Ordering Models,"Tanya Goyal, Greg Durrett","Data-driven models have demonstrated state-of-the-art performance in inferring the temporal ordering of events in text. However, these models often overlook explicit temporal signals, such as dates and time windows. Rule-based methods can be used to identify the temporal links between these time expressions (timexes), but they fail to capture timexes’ interactions with events and are hard to integrate with the distributed representations of neural net models. In this paper, we introduce a framework to infuse temporal awareness into such models by learning a pre-trained model to embed timexes. We generate synthetic data consisting of pairs of timexes, then train a character LSTM to learn embeddings and classify the timexes’ temporal relation. We evaluate the utility of these embeddings in the context of a strong neural model for event temporal ordering, and show a small increase in performance on the MATRES dataset and more substantial gains on an automatically collected dataset with more frequent event-timex interactions.",,,,ACL
434,2019,Episodic Memory Reader: Learning What to Remember for Question Answering from Streaming Data,"Moonsu Han, Minki Kang, Hyunwoo Jung, Sung Ju Hwang","We consider a novel question answering (QA) task where the machine needs to read from large streaming data (long documents or videos) without knowing when the questions will be given, which is difficult to solve with existing QA methods due to their lack of scalability. To tackle this problem, we propose a novel end-to-end deep network model for reading comprehension, which we refer to as Episodic Memory Reader (EMR) that sequentially reads the input contexts into an external memory, while replacing memories that are less important for answering unseen questions. Specifically, we train an RL agent to replace a memory entry when the memory is full, in order to maximize its QA accuracy at a future timepoint, while encoding the external memory using either the GRU or the Transformer architecture to learn representations that considers relative importance between the memory entries. We validate our model on a synthetic dataset (bAbI) as well as real-world large-scale textual QA (TriviaQA) and video QA (TVQA) datasets, on which it achieves significant improvements over rule based memory scheduling policies or an RL based baseline that independently learns the query-specific importance of each memory.",,,,ACL
435,2019,Selection Bias Explorations and Debias Methods for Natural Language Sentence Matching Datasets,"Guanhua Zhang, Bing Bai, Jian Liang, Kun Bai, Shiyu Chang","Natural Language Sentence Matching (NLSM) has gained substantial attention from both academics and the industry, and rich public datasets contribute a lot to this process. However, biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results. For many NLSM datasets, the providers select some pairs of sentences into the datasets, and this sampling procedure can easily bring unintended pattern, i.e., selection bias. One example is the QuoraQP dataset, where some content-independent naive features are unreasonably predictive. Such features are the reflection of the selection bias and termed as the “leakage features.” In this paper, we investigate the problem of selection bias on six NLSM datasets and find that four out of them are significantly biased. We further propose a training and evaluation framework to alleviate the bias. Experimental results on QuoraQP suggest that the proposed framework can improve the generalization ability of trained models, and give more trustworthy evaluation results for real-world adoptions.",,,,ACL
436,2019,Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index,"Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi","Existing open-domain question answering (QA) models are not suitable for real-time usage because they need to process several long documents on-demand for every input query, which is computationally prohibitive. In this paper, we introduce query-agnostic indexable representations of document phrases that can drastically speed up open-domain QA. In particular, our dense-sparse phrase encoding effectively captures syntactic, semantic, and lexical information of the phrases and eliminates the pipeline filtering of context documents. Leveraging strategies for optimizing training and inference time, our model can be trained and deployed even in a single 4-GPU server. Moreover, by representing phrases as pointers to their start and end tokens, our model indexes phrases in the entire English Wikipedia (up to 60 billion phrases) using under 2TB. Our experiments on SQuAD-Open show that our model is on par with or more accurate than previous models with 6000x reduced computational cost, which translates into at least 68x faster end-to-end inference benchmark on CPUs. Code and demo are available at nlp.cs.washington.edu/denspi",,,,ACL
437,2019,Language Modeling with Shared Grammar,"Yuyu Zhang, Le Song","Sequential recurrent neural networks have achieved superior performance on language modeling, but overlook the structure information in natural language. Recent works on structure-aware models have shown promising results on language modeling. However, how to incorporate structure knowledge on corpus without syntactic annotations remains an open problem. In this work, we propose neural variational language model (NVLM), which enables the sharing of grammar knowledge among different corpora. Experimental results demonstrate the effectiveness of our framework on two popular benchmark datasets. With the help of shared grammar, our language model converges significantly faster to a lower perplexity on new training corpus.",,,,ACL
438,2019,Zero-Shot Semantic Parsing for Instructions,"Ofer Givoli, Roi Reichart","We consider a zero-shot semantic parsing task: parsing instructions into compositional logical forms, in domains that were not seen during training. We present a new dataset with 1,390 examples from 7 application domains (e.g. a calendar or a file manager), each example consisting of a triplet: (a) the application’s initial state, (b) an instruction, to be carried out in the context of that state, and (c) the state of the application after carrying out the instruction. We introduce a new training algorithm that aims to train a semantic parser on examples from a set of source domains, so that it can effectively parse instructions from an unknown target domain. We integrate our algorithm into the floating parser of Pasupat and Liang (2015), and further augment the parser with features and a logical form candidate filtering logic, to support zero-shot adaptation. Our experiments with various zero-shot adaptation setups demonstrate substantial performance gains over a non-adapted parser.",,,,ACL
439,2019,Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling,"Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy","Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning BERT on an intermediate task often negatively impacts downstream transfer. In a more positive trend, we see modest gains from multitask training, suggesting the development of more sophisticated multitask and transfer learning techniques as an avenue for further research.",,,,ACL
440,2019,Complex Question Decomposition for Semantic Parsing,"Haoyu Zhang, Jingjing Cai, Jianjun Xu, Ji Wang","In this work, we focus on complex question semantic parsing and propose a novel Hierarchical Semantic Parsing (HSP) method, which utilizes the decompositionality of complex questions for semantic parsing. Our model is designed within a three-stage parsing architecture based on the idea of decomposition-integration. In the first stage, we propose a question decomposer which decomposes a complex question into a sequence of sub-questions. In the second stage, we design an information extractor to derive the type and predicate information of these questions. In the last stage, we integrate the generated information from previous stages and generate a logical form for the complex question. We conduct experiments on COMPLEXWEBQUESTIONS which is a large scale complex question semantic parsing dataset, results show that our model achieves significant improvement compared to state-of-the-art methods.",,,,ACL
441,2019,Multi-Task Deep Neural Networks for Natural Language Understanding,"Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao","In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.",,,,ACL
442,2019,DisSent: Learning Sentence Representations from Explicit Discourse Relations,"Allen Nie, Erin Bennett, Noah Goodman","Learning effective representations of sentences is one of the core missions of natural language understanding. Existing models either train on a vast amount of text, or require costly, manually curated sentence relation datasets. We show that with dependency parsing and rule-based rubrics, we can curate a high quality sentence relation task by leveraging explicit discourse relations. We show that our curated dataset provides an excellent signal for learning vector representations of sentence meaning, representing relations that can only be determined when the meanings of two sentences are combined. We demonstrate that the automatically curated corpus allows a bidirectional LSTM sentence encoder to yield high quality sentence embeddings and can serve as a supervised fine-tuning dataset for larger models such as BERT. Our fixed sentence embeddings achieve high performance on a variety of transfer tasks, including SentEval, and we achieve state-of-the-art results on Penn Discourse Treebank’s implicit relation prediction task.",,,,ACL
443,2019,SParC: Cross-Domain Semantic Parsing in Context,"Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin","We present SParC, a dataset for cross-domainSemanticParsing inContext that consists of 4,298 coherent question sequences (12k+ individual questions annotated with SQL queries). It is obtained from controlled user interactions with 200 complex databases over 138 domains. We provide an in-depth analysis of SParC and show that it introduces new challenges compared to existing datasets. SParC demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. We experiment with two state-of-the-art text-to-SQL models adapted to the context-dependent, cross-domain setup. The best model obtains an exact match accuracy of 20.2% over all questions and less than10% over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. The dataset, baselines, and leaderboard are released at https://yale-lily.github.io/sparc.",,,,ACL
444,2019,Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation,"Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou","We present a neural approach called IRNet for complex and cross-domain Text-to-SQL. IRNet aims to address two challenges: 1) the mismatch between intents expressed in natural language (NL) and the implementation details in SQL; 2) the challenge in predicting columns caused by the large number of out-of-domain words. Instead of end-to-end synthesizing a SQL query, IRNet decomposes the synthesis process into three phases. In the first phase, IRNet performs a schema linking over a question and a database schema. Then, IRNet adopts a grammar-based neural model to synthesize a SemQL query which is an intermediate representation that we design to bridge NL and SQL. Finally, IRNet deterministically infers a SQL query from the synthesized SemQL query with domain knowledge. On the challenging Text-to-SQL benchmark Spider, IRNet achieves 46.7% accuracy, obtaining 19.5% absolute improvement over previous state-of-the-art approaches. At the time of writing, IRNet achieves the first position on the Spider leaderboard.",,,,ACL
445,2019,EigenSent: Spectral sentence embeddings using higher-order Dynamic Mode Decomposition,"Subhradeep Kayal, George Tsatsaronis","Distributed representation of words, or word embeddings, have motivated methods for calculating semantic representations of word sequences such as phrases, sentences and paragraphs. Most of the existing methods to do so either use algorithms to learn such representations, or improve on calculating weighted averages of the word vectors. In this work, we experiment with spectral methods of signal representation and summarization as mechanisms for constructing such word-sequence embeddings in an unsupervised fashion. In particular, we explore an algorithm rooted in fluid-dynamics, known as higher-order Dynamic Mode Decomposition, which is designed to capture the eigenfrequencies, and hence the fundamental transition dynamics, of periodic and quasi-periodic systems. It is empirically observed that this approach, which we call EigenSent, can summarize transitions in a sequence of words and generate an embedding that can represent well the sequence itself. To the best of the authors’ knowledge, this is the first application of a spectral decomposition and signal summarization technique on text, to create sentence embeddings. We test the efficacy of this algorithm in creating sentence embeddings on three public datasets, where it performs appreciably well. Moreover it is also shown that, due to the positive combination of their complementary properties, concatenating the embeddings generated by EigenSent with simple word vector averaging achieves state-of-the-art results.",,,,ACL
446,2019,SemBleu: A Robust Metric for AMR Parsing Evaluation,"Linfeng Song, Daniel Gildea","Evaluating AMR parsing accuracy involves comparing pairs of AMR graphs. The major evaluation metric, SMATCH (Cai and Knight, 2013), searches for one-to-one mappings between the nodes of two AMRs with a greedy hill-climbing algorithm, which leads to search errors. We propose SEMBLEU, a robust metric that extends BLEU (Papineni et al., 2002) to AMRs. It does not suffer from search errors and considers non-local correspondences in addition to local ones. SEMBLEU is fully content-driven and punishes situations where a system’s output does not preserve most information from the input. Preliminary experiments on both sentence and corpus levels show that SEMBLEU has slightly higher consistency with human judgments than SMATCH. Our code is available at http://github.com/ freesunshine0316/sembleu.",,,,ACL
447,2019,Reranking for Neural Semantic Parsing,"Pengcheng Yin, Graham Neubig","Semantic parsing considers the task of transducing natural language (NL) utterances into machine executable meaning representations (MRs). While neural network-based semantic parsers have achieved impressive improvements over previous methods, results are still far from perfect, and cursory manual inspection can easily identify obvious problems such as lack of adequacy or coherence of the generated MRs. This paper presents a simple approach to quickly iterate and improve the performance of an existing neural semantic parser by reranking an n-best list of predicted MRs, using features that are designed to fix observed problems with baseline models. We implement our reranker in a competitive neural semantic parser and test on four semantic parsing (GEO, ATIS) and Python code generation (Django, CoNaLa) tasks, improving the strong baseline parser by up to 5.7% absolute in BLEU (CoNaLa) and 2.9% in accuracy (Django), outperforming the best published neural parser results on all four datasets.",,,,ACL
448,2019,Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing,"Ben Bogin, Jonathan Berant, Matt Gardner","Research on parsing language to SQL has largely ignored the structure of the database (DB) schema, either because the DB was very simple, or because it was observed at both training and test time. In spider, a recently-released text-to-SQL dataset, new and complex DBs are given at test time, and so the structure of the DB schema can inform the predicted SQL query. In this paper, we present an encoder-decoder semantic parser, where the structure of the DB schema is encoded with a graph neural network, and this representation is later used at both encoding and decoding time. Evaluation shows that encoding the schema structure improves our parser accuracy from 33.8% to 39.4%, dramatically above the current state of the art, which is at 19.7%.",,,,ACL
449,2019,Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark,"Nikita Nangia, Samuel R. Bowman","The GLUE benchmark (Wang et al., 2019b) is a suite of language understanding tasks which has seen dramatic progress in the past year, with average performance moving from 70.0 at launch to 83.9, state of the art at the time of writing (May 24, 2019). Here, we measure human performance on the benchmark, in order to learn whether significant headroom remains for further progress. We provide a conservative estimate of human performance on the benchmark through crowdsourcing: Our annotators are non-experts who must learn each task from a brief set of instructions and 20 examples. In spite of limited training, these annotators robustly outperform the state of the art on six of the nine GLUE tasks and achieve an average score of 87.1. Given the fast pace of progress however, the headroom we observe is quite limited. To reproduce the data-poor setting that our annotators must learn in, we also train the BERT model (Devlin et al., 2019) in limited-data regimes, and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding.",,,,ACL
450,2019,Compositional Semantic Parsing across Graphbanks,"Matthias Lindemann, Jonas Groschwitz, Alexander Koller","Most semantic parsers that map sentences to graph-based meaning representations are hand-designed for specific graphbanks. We present a compositional neural semantic parser which achieves, for the first time, competitive accuracies across a diverse range of graphbanks. Incorporating BERT embeddings and multi-task learning improves the accuracy further, setting new states of the art on DM, PAS, PSD, AMR 2015 and EDS.",,,,ACL
451,2019,Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning,"Tahira Naseem, Abhishek Shah, Hui Wan, Radu Florian, Salim Roukos","Our work involves enriching the Stack-LSTM transition-based AMR parser (Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs. In addition, we also combined several AMR-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification, named entities and contextualized embeddings. We achieve a highly competitive performance that is comparable to the best published results. We show an in-depth study ablating each of the new components of the parser.",,,,ACL
452,2019,BERT Rediscovers the Classical NLP Pipeline,"Ian Tenney, Dipanjan Das, Ellie Pavlick","Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",,,,ACL
453,2019,Simple and Effective Paraphrastic Similarity from Parallel Translations,"John Wieting, Kevin Gimpel, Graham Neubig, Taylor Berg-Kirkpatrick","We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the time-consuming intermediate step of creating para-phrase corpora. Further, we show that the resulting model can be applied to cross lingual tasks where it both outperforms and is orders of magnitude faster than more complex state-of-the-art baselines.",,,,ACL
454,2019,Second-Order Semantic Dependency Parsing with End-to-End Neural Networks,"Xinyu Wang, Jingxian Huang, Kewei Tu","Semantic dependency parsing aims to identify semantic relationships between words in a sentence that form a graph. In this paper, we propose a second-order semantic dependency parser, which takes into consideration not only individual dependency edges but also interactions between pairs of edges. We show that second-order parsing can be approximated using mean field (MF) variational inference or loopy belief propagation (LBP). We can unfold both algorithms as recurrent layers of a neural network and therefore can train the parser in an end-to-end manner. Our experiments show that our approach achieves state-of-the-art performance.",,,,ACL
455,2019,Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper),"Santiago Castro, Devamanyu Hazarika, Verónica Pérez-Rosas, Roger Zimmermann, Rada Mihalcea","Sarcasm is often expressed through several verbal and non-verbal cues, e.g., a change of tone, overemphasis in a word, a drawn-out syllable, or a straight looking face. Most of the recent work in sarcasm detection has been carried out on textual data. In this paper, we argue that incorporating multimodal cues can improve the automatic classification of sarcasm. As a first step towards enabling the development of multimodal approaches for sarcasm detection, we propose a new sarcasm dataset, Multimodal Sarcasm Detection Dataset (MUStARD), compiled from popular TV shows. MUStARD consists of audiovisual utterances annotated with sarcasm labels. Each utterance is accompanied by its context of historical utterances in the dialogue, which provides additional information on the scenario where the utterance occurs. Our initial results show that the use of multimodal information can reduce the relative error rate of sarcasm detection by up to 12.9% in F-score when compared to the use of individual modalities. The full dataset is publicly available for use at https://github.com/soujanyaporia/MUStARD.",,,,ACL
456,2019,Determining Relative Argument Specificity and Stance for Complex Argumentative Structures,"Esin Durmus, Faisal Ladhak, Claire Cardie","Systems for automatic argument generation and debate require the ability to (1) determine the stance of any claims employed in the argument and (2) assess the specificity of each claim relative to the argument context. Existing work on understanding claim specificity and stance, however, has been limited to the study of argumentative structures that are relatively shallow, most often consisting of a single claim that directly supports or opposes the argument thesis. In this paper, we tackle these tasks in the context of complex arguments on a diverse set of topics. In particular, our dataset consists of manually curated argument trees for 741 controversial topics covering 95,312 unique claims; lines of argument are generally of depth 2 to 6. We find that as the distance between a pair of claims increases along the argument path, determining the relative specificity of a pair of claims becomes easier and determining their relative stance becomes harder.",,,,ACL
457,2019,Latent Variable Sentiment Grammar,"Liwen Zhang, Kewei Tu, Yue Zhang","Neural models have been investigated for sentiment classification over constituent trees. They learn phrase composition automatically by encoding tree structures but do not explicitly model sentiment composition, which requires to encode sentiment class labels. To this end, we investigate two formalisms with deep sentiment representations that capture sentiment subtype expressions by latent variables and Gaussian mixture vectors, respectively. Experiments on Stanford Sentiment Treebank (SST) show the effectiveness of sentiment grammar over vanilla neural encoders. Using ELMo embeddings, our method gives the best results on this benchmark.",,,,ACL
458,2019,An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese,"Enkhbold Bataa, Joshua Wu","Text classification approaches have usually required task-specific model architectures and huge labeled datasets. Recently, thanks to the rise of text-based transfer learning techniques, it is possible to pre-train a language model in an unsupervised manner and leverage them to perform effective on downstream tasks. In this work we focus on Japanese and show the potential use of transfer learning techniques in text classification. Specifically, we perform binary and multi-class sentiment classification on the Rakuten product review and Yahoo movie review datasets. We show that transfer learning-based approaches perform better than task-specific models trained on 3 times as much data. Furthermore, these approaches perform just as well for language modeling pre-trained on only 1/30 of the data. We release our pre-trained models and code as open source.",,,,ACL
459,2019,Probing Neural Network Comprehension of Natural Language Arguments,"Timothy Niven, Hung-Yu Kao","We are surprised to find that BERT’s peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.",,,,ACL
460,2019,Recognising Agreement and Disagreement between Stances with Reason Comparing Networks,"Chang Xu, Cecile Paris, Surya Nepal, Ross Sparks","We identify agreement and disagreement between utterances that express stances towards a topic of discussion. Existing methods focus mainly on conversational settings, where dialogic features are used for (dis)agreement inference. We extend this scope and seek to detect stance (dis)agreement in a broader setting, where independent stance-bearing utterances, which prevail in many stance corpora and real-world scenarios, are compared. To cope with such non-dialogic utterances, we find that the reasons uttered to back up a specific stance can help predict stance (dis)agreements. We propose a reason comparing network (RCN) to leverage reason information for stance comparison. Empirical results on a well-known stance corpus show that our method can discover useful reason information, enabling it to outperform several baselines in stance (dis)agreement detection.",,,,ACL
461,2019,Toward Comprehensive Understanding of a Sentiment Based on Human Motives,"Naoki Otani, Eduard Hovy","In sentiment detection, the natural language processing community has focused on determining holders, facets, and valences, but has paid little attention to the reasons for sentiment decisions. Our work considers human motives as the driver for human sentiments and addresses the problem of motive detection as the first step. Following a study in psychology, we define six basic motives that cover a wide range of topics appearing in review texts, annotate 1,600 texts in restaurant and laptop domains with the motives, and report the performance of baseline methods on this new dataset. We also show that cross-domain transfer learning boosts detection performance, which indicates that these universal motives exist across different domains.",,,,ACL
462,2019,Context-aware Embedding for Targeted Aspect-based Sentiment Analysis,"Bin Liang, Jiachen Du, Ruifeng Xu, Binyang Li, Hejiao Huang","Attention-based neural models were employed to detect the different aspects and sentiment polarities of the same target in targeted aspect-based sentiment analysis (TABSA). However, existing methods do not specifically pre-train reasonable embeddings for targets and aspects in TABSA. This may result in targets or aspects having the same vector representations in different contexts and losing the context-dependent information. To address this problem, we propose a novel method to refine the embeddings of targets and aspects. Such pivotal embedding refinement utilizes a sparse coefficient vector to adjust the embeddings of target and aspect from the context. Hence the embeddings of targets and aspects can be refined from the highly correlative words instead of using context-independent or randomly initialized vectors. Experiment results on two benchmark datasets show that our approach yields the state-of-the-art performance in TABSA task.",,,,ACL
463,2019,"Yes, we can! Mining Arguments in 50 Years of US Presidential Campaign Debates","Shohreh Haddadan, Elena Cabrio, Serena Villata","Political debates offer a rare opportunity for citizens to compare the candidates’ positions on the most controversial topics of the campaign. Thus they represent a natural application scenario for Argument Mining. As existing research lacks solid empirical investigation of the typology of argument components in political debates, we fill this gap by proposing an Argument Mining approach to political debates. We address this task in an empirical manner by annotating 39 political debates from the last 50 years of US presidential campaigns, creating a new corpus of 29k argument components, labeled as premises and claims. We then propose two tasks: (1) identifying the argumentative components in such debates, and (2) classifying them as premises and claims. We show that feature-rich SVM learners and Neural Network architectures outperform standard baselines in Argument Mining over such complex data. We release the new corpus USElecDeb60To16 and the accompanying software under free licenses to the research community.",,,,ACL
464,2019,An Empirical Study of Span Representations in Argumentation Structure Parsing,"Tatsuki Kuribayashi, Hiroki Ouchi, Naoya Inoue, Paul Reisert, Toshinori Miyoshi","For several natural language processing (NLP) tasks, span representation design is attracting considerable attention as a promising new technique; a common basis for an effective design has been established. With such basis, exploring task-dependent extensions for argumentation structure parsing (ASP) becomes an interesting research direction. This study investigates (i) span representation originally developed for other NLP tasks and (ii) a simple task-dependent extension for ASP. Our extensive experiments and analysis show that these representations yield high performance for ASP and provide some challenging types of instances to be parsed.",,,,ACL
465,2019,Simple and Effective Text Matching with Richer Alignment Features,"Runqi Yang, Jianhai Zhang, Xing Gao, Feng Ji, Haiqing Chen","In this paper, we present a fast and strong neural approach for general purpose text matching applications. We explore what is sufficient to build a fast and well-performed text matching model and propose to keep three key features available for inter-sequence alignment: original point-wise features, previous aligned features, and contextual features while simplifying all the remaining components. We conduct experiments on four well-studied benchmark datasets across tasks of natural language inference, paraphrase identification and answer selection. The performance of our model is on par with the state-of-the-art on all datasets with much fewer parameters and the inference speed is at least 6 times faster compared with similarly performed ones.",,,,ACL
466,2019,Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs,"Deepak Nathani, Jatin Chauhan, Charu Sharma, Manohar Kaul","The recent proliferation of knowledge graphs (KGs) coupled with incomplete or partial information, in the form of missing relations (links) between entities, has fueled a lot of research on knowledge base completion (also known as relation prediction). Several recent works suggest that convolutional neural network (CNN) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction. However, we observe that these KG embeddings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighborhood surrounding a triple. To this effect, our paper proposes a novel attention-based feature embedding that captures both entity and relation features in any given entity’s neighborhood. Additionally, we also encapsulate relation clusters and multi-hop relations in our model. Our empirical study offers insights into the efficacy of our attention-based model and we show marked performance gains in comparison to state-of-the-art methods on all datasets.",,,,ACL
467,2019,Neural Network Alignment for Sentential Paraphrases,"Jessica Ouyang, Kathy McKeown","We present a monolingual alignment system for long, sentence- or clause-level alignments, and demonstrate that systems designed for word- or short phrase-based alignment are ill-suited for these longer alignments. Our system is capable of aligning semantically similar spans of arbitrary length. We achieve significantly higher recall on aligning phrases of four or more words and outperform state-of-the- art aligners on the long alignments in the MSR RTE corpus.",,,,ACL
468,2019,Duality of Link Prediction and Entailment Graph Induction,"Mohammad Javad Hosseini, Shay B. Cohen, Mark Johnson, Mark Steedman","Link prediction and entailment graph induction are often treated as different problems. In this paper, we show that these two problems are actually complementary. We train a link prediction model on a knowledge graph of assertions extracted from raw text. We propose an entailment score that exploits the new facts discovered by the link prediction model, and then form entailment graphs between relations. We further use the learned entailments to predict improved link prediction scores. Our results show that the two tasks can benefit from each other. The new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores.",,,,ACL
469,2019,A Cross-Sentence Latent Variable Model for Semi-Supervised Text Sequence Matching,"Jihun Choi, Taeuk Kim, Sang-goo Lee","We present a latent variable model for predicting the relationship between a pair of text sequences. Unlike previous auto-encoding–based approaches that consider each sequence separately, our proposed framework utilizes both sequences within a single model by generating a sequence that has a given relationship with a source sequence. We further extend the cross-sentence generating framework to facilitate semi-supervised training. We also define novel semantic constraints that lead the decoder network to generate semantically plausible and diverse sequences. We demonstrate the effectiveness of the proposed model from quantitative and qualitative experiments, while achieving state-of-the-art results on semi-supervised natural language inference and paraphrase identification.",,,,ACL
470,2019,COMET: Commonsense Transformers for Automatic Knowledge Graph Construction,"Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz","We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",,,,ACL
471,2019,Detecting Subevents using Discourse and Narrative Features,"Mohammed Aldawsari, Mark Finlayson","Recognizing the internal structure of events is a challenging language processing task of great importance for text understanding. We present a supervised model for automatically identifying when one event is a subevent of another. Building on prior work, we introduce several novel features, in particular discourse and narrative features, that significantly improve upon prior state-of-the-art performance. Error analysis further demonstrates the utility of these features. We evaluate our model on the only two annotated corpora with event hierarchies: HiEve and the Intelligence Community corpus. No prior system has been evaluated on both corpora. Our model outperforms previous systems on both corpora, achieving 0.74 BLANC F1 on the Intelligence Community corpus and 0.70 F1 on the HiEve corpus, respectively a 15 and 5 percentage point improvement over previous models.",,,,ACL
472,2019,HellaSwag: Can a Machine Really Finish Your Sentence?,"Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi","Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at a piano,” a machine must select the most likely followup: “She sets her fingers on the keys.” With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical ‘Goldilocks’ zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.",,,,ACL
473,2019,Unified Semantic Parsing with Weak Supervision,"Priyanka Agrawal, Ayushi Dalmia, Parag Jain, Abhishek Bansal, Ashish Mittal","Semantic parsing over multiple knowledge bases enables a parser to exploit structural similarities of programs across the multiple domains. However, the fundamental challenge lies in obtaining high-quality annotations of (utterance, program) pairs across various domains needed for training such models. To overcome this, we propose a novel framework to build a unified multi-domain enabled semantic parser trained only with weak supervision (denotations). Weakly supervised training is particularly arduous as the program search space grows exponentially in a multi-domain setting. To solve this, we incorporate a multi-policy distillation mechanism in which we first train domain-specific semantic parsers (teachers) using weak supervision in the absence of the ground truth programs, followed by training a single unified parser (student) from the domain specific policies obtained from these teachers. The resultant semantic parser is not only compact but also generalizes better, and generates more accurate programs. It further does not require the user to provide a domain label while querying. On the standard Overnight dataset (containing multiple domains), we demonstrate that the proposed model improves performance by 20% in terms of denotation accuracy in comparison to baseline techniques.",,,,ACL
474,2019,Every Child Should Have Parents: A Taxonomy Refinement Algorithm Based on Hyperbolic Term Embeddings,"Rami Aly, Shantanu Acharya, Alexander Ossa, Arne Köhn, Chris Biemann","We introduce the use of Poincaré embeddings to improve existing state-of-the-art approaches to domain-specific taxonomy induction from text as a signal for both relocating wrong hyponym terms within a (pre-induced) taxonomy as well as for attaching disconnected terms in a taxonomy. This method substantially improves previous state-of-the-art results on the SemEval-2016 Task 13 on taxonomy extraction. We demonstrate the superiority of Poincaré embeddings over distributional semantic representations, supporting the hypothesis that they can better capture hierarchical lexical-semantic relationships than embeddings in the Euclidean space.",,,,ACL
475,2019,Learning to Rank for Plausible Plausibility,"Zhongyang Li, Tongfei Chen, Benjamin Van Durme","Researchers illustrate improvements in contextual encoding strategies via resultant performance on a battery of shared Natural Language Understanding (NLU) tasks. Many of these tasks are of a categorical prediction variety: given a conditioning context (e.g., an NLI premise), provide a label based on an associated prompt (e.g., an NLI hypothesis). The categorical nature of these tasks has led to common use of a cross entropy log-loss objective during training. We suggest this loss is intuitively wrong when applied to plausibility tasks, where the prompt by design is neither categorically entailed nor contradictory given the context. Log-loss naturally drives models to assign scores near 0.0 or 1.0, in contrast to our proposed use of a margin-based loss. Following a discussion of our intuition, we describe a confirmation study based on an extreme, synthetically curated task derived from MultiNLI. We find that a margin-based loss leads to a more plausible model of plausibility. Finally, we illustrate improvements on the Choice Of Plausible Alternative (COPA) task through this change in loss.",,,,ACL
476,2019,Generalized Tuning of Distributional Word Vectors for Monolingual and Cross-Lingual Lexical Entailment,"Goran Glavaš, Ivan Vulić","Lexical entailment (LE; also known as hyponymy-hypernymy or is-a relation) is a core asymmetric lexical relation that supports tasks like taxonomy induction and text generation. In this work, we propose a simple and effective method for fine-tuning distributional word vectors for LE. Our Generalized Lexical ENtailment model (GLEN) is decoupled from the word embedding model and applicable to any distributional vector space. Yet – unlike existing retrofitting models – it captures a general specialization function allowing for LE-tuning of the entire distributional space and not only the vectors of words seen in lexical constraints. Coupled with a multilingual embedding space, GLEN seamlessly enables cross-lingual LE detection. We demonstrate the effectiveness of GLEN in graded LE and report large improvements (over 20% in accuracy) over state-of-the-art in cross-lingual LE detection.",,,,ACL
477,2019,Attention Is (not) All You Need for Commonsense Reasoning,"Tassilo Klein, Moin Nabi","The recently introduced BERT model exhibits strong performance on several language understanding benchmarks. In this paper, we describe a simple re-implementation of BERT for commonsense reasoning. We show that the attentions produced by BERT can be directly utilized for tasks such as the Pronoun Disambiguation Problem and Winograd Schema Challenge. Our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful. Experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin. While results suggest that BERT seems to implicitly learn to establish complex relationships between entities, solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora.",,,,ACL
478,2019,A Surprisingly Robust Trick for the Winograd Schema Challenge,"Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, Thomas Lukasiewicz","The Winograd Schema Challenge (WSC) dataset WSC273 and its inference counterpart WNLI are popular benchmarks for natural language understanding and commonsense reasoning. In this paper, we show that the performance of three language models on WSC273 consistently and robustly improves when fine-tuned on a similar pronoun disambiguation problem dataset (denoted WSCR). We additionally generate a large unsupervised WSC-like dataset. By fine-tuning the BERT language model both on the introduced and on the WSCR dataset, we achieve overall accuracies of 72.5% and 74.7% on WSC273 and WNLI, improving the previous state-of-the-art solutions by 8.8% and 9.6%, respectively. Furthermore, our fine-tuned models are also consistently more accurate on the “complex” subsets of WSC273, introduced by Trichelair et al. (2018).",,,,ACL
479,2019,Coherent Comments Generation for Chinese Articles with a Graph-to-Sequence Model,"Wei Li, Jingjing Xu, Yancheng He, ShengLi Yan, Yunfang Wu","Automatic article commenting is helpful in encouraging user engagement on online news platforms. However, the news documents are usually too long for models under traditional encoder-decoder frameworks, which often results in general and irrelevant comments. In this paper, we propose to generate comments with a graph-to-sequence model that models the input news as a topic interaction graph. By organizing the article into graph structure, our model can better understand the internal structure of the article and the connection between topics, which makes it better able to generate coherent and informative comments. We collect and release a large scale news-comment corpus from a popular Chinese online news platform Tencent Kuaibao. Extensive experiment results show that our model can generate much more coherent and informative comments compared with several strong baseline models.",,,,ACL
480,2019,Interconnected Question Generation with Coreference Alignment and Conversation Flow Modeling,"Yifan Gao, Piji Li, Irwin King, Michael R. Lyu","We study the problem of generating interconnected questions in question-answering style conversations. Compared with previous works which generate questions based on a single sentence (or paragraph), this setting is different in two major aspects: (1) Questions are highly conversational. Almost half of them refer back to conversation history using coreferences. (2) In a coherent conversation, questions have smooth transitions between turns. We propose an end-to-end neural model with coreference alignment and conversation flow modeling. The coreference alignment modeling explicitly aligns coreferent mentions in conversation history with corresponding pronominal references in generated questions, which makes generated questions interconnected to conversation history. The conversation flow modeling builds a coherent conversation by starting questioning on the first few sentences in a text passage and smoothly shifting the focus to later parts. Extensive experiments show that our system outperforms several baselines and can generate highly conversational questions. The code implementation is released at https://github.com/Evan-Gao/conversaional-QG.",,,,ACL
481,2019,Cross-Lingual Training for Automatic Question Generation,"Vishwajeet Kumar, Nitish Joshi, Arijit Mukherjee, Ganesh Ramakrishnan, Preethi Jyothi","Automatic question generation (QG) is a challenging problem in natural language understanding. QG systems are typically built assuming access to a large number of training instances where each instance is a question and its corresponding answer. For a new language, such training instances are hard to obtain making the QG problem even more challenging. Using this as our motivation, we study the reuse of an available large QG dataset in a secondary language (e.g. English) to learn a QG model for a primary language (e.g. Hindi) of interest. For the primary language, we assume access to a large amount of monolingual text but only a small QG dataset. We propose a cross-lingual QG model which uses the following training regime: (i) Unsupervised pretraining of language models in both primary and secondary languages and (ii) joint supervised training for QG in both languages. We demonstrate the efficacy of our proposed approach using two different primary languages, Hindi and Chinese. Our proposed framework clearly outperforms a number of baseline models, including a fully-supervised transformer-based model trained on the QG datasets in the primary language. We also create and release a new question answering dataset for Hindi consisting of 6555 sentences.",,,,ACL
482,2019,A Hierarchical Reinforced Sequence Operation Method for Unsupervised Text Style Transfer,"Chen Wu, Xuancheng Ren, Fuli Luo, Xu Sun","Unsupervised text style transfer aims to alter text styles while preserving the content, without aligned data for supervision. Existing seq2seq methods face three challenges: 1) the transfer is weakly interpretable, 2) generated outputs struggle in content preservation, and 3) the trade-off between content and style is intractable. To address these challenges, we propose a hierarchical reinforced sequence operation method, named Point-Then-Operate (PTO), which consists of a high-level agent that proposes operation positions and a low-level agent that alters the sentence. We provide comprehensive training objectives to control the fluency, style, and content of the outputs and a mask-based inference algorithm that allows for multi-step revision based on the single-step trained agents. Experimental results on two text style transfer datasets show that our method significantly outperforms recent methods and effectively addresses the aforementioned challenges.",,,,ACL
483,2019,Handling Divergent Reference Texts when Evaluating Table-to-Text Generation,"Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das","Automatically constructed datasets for generating text from semi-structured data (tables), such as WikiBio, often contain reference texts that diverge from the information in the corresponding semi-structured data. We show that metrics which rely solely on the reference texts, such as BLEU and ROUGE, show poor correlation with human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed by Wiseman et al (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.",,,,ACL
484,2019,Unsupervised Question Answering by Cloze Translation,"Patrick Lewis, Ludovic Denoyer, Sebastian Riedel","Obtaining training data for Question Answering (QA) is time-consuming and resource-intensive, and existing QA datasets are only available for limited domains and languages. In this work, we explore to what extent high quality training data is actually required for Extractive QA, and investigate the possibility of unsupervised Extractive QA. We approach this problem by first learning to generate context, question and answer triples in an unsupervised manner, which we then use to synthesize Extractive QA training data automatically. To generate such triples, we first sample random context paragraphs from a large corpus of documents and then random noun phrases or Named Entity mentions from these paragraphs as answers. Next we convert answers in context to “fill-in-the-blank” cloze questions and finally translate them into natural questions. We propose and compare various unsupervised ways to perform cloze-to-natural question translation, including training an unsupervised NMT model using non-aligned corpora of natural questions and cloze questions as well as a rule-based approach. We find that modern QA models can learn to answer human questions surprisingly well using only synthetic training data. We demonstrate that, without using the SQuAD training data at all, our approach achieves 56.4 F1 on SQuAD v1 (64.5 F1 when the answer is a Named Entity mention), outperforming early supervised models.",,,,ACL
485,2019,MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension,"Alon Talmor, Jonathan Berant","A large number of reading comprehension (RC) datasets has been created recently, but little analysis has been done on whether they generalize to one another, and the extent to which existing datasets can be leveraged for improving performance on new ones. In this paper, we conduct such an investigation over ten RC datasets, training on one or more source RC datasets, and evaluating generalization, as well as transfer to a target RC dataset. We analyze the factors that contribute to generalization, and show that training on a source RC dataset and transferring to a target dataset substantially improves performance, even in the presence of powerful contextual representations from BERT (Devlin et al., 2019). We also find that training on multiple source RC datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new RC dataset. Following our analysis, we propose MultiQA, a BERT-based model, trained on multiple RC datasets, which leads to state-of-the-art performance on five RC datasets. We share our infrastructure for the benefit of the research community.",,,,ACL
486,2019,Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives,"Yi Tay, Shuohang Wang, Anh Tuan Luu, Jie Fu, Minh C. Phan","This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on BLEU-4 and 17% relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components.",,,,ACL
487,2019,Explain Yourself! Leveraging Language Models for Commonsense Reasoning,"Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, Richard Socher","Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.",,,,ACL
488,2019,Interpretable Question Answering on Knowledge Bases and Text,"Alona Sydorova, Nina Poerner, Benjamin Roth","Interpretability of machine learning (ML) models becomes more relevant with their increasing adoption. In this work, we address the interpretability of ML based question answering (QA) models on a combination of knowledge bases (KB) and text documents. We adapt post hoc explanation methods such as LIME and input perturbation (IP) and compare them with the self-explanatory attention mechanism of the model. For this purpose, we propose an automatic evaluation paradigm for explanation methods in the context of QA. We also conduct a study with human annotators to evaluate whether explanations help them identify better QA models. Our results suggest that IP provides better explanations than LIME or attention, according to both automatic and human evaluation. We obtain the same ranking of methods in both experiments, which supports the validity of our automatic evaluation paradigm.",,,,ACL
489,2019,A Resource-Free Evaluation Metric for Cross-Lingual Word Embeddings Based on Graph Modularity,"Yoshinari Fujinuma, Jordan Boyd-Graber, Michael J. Paul","Cross-lingual word embeddings encode the meaning of words from different languages into a shared low-dimensional space. An important requirement for many downstream tasks is that word similarity should be independent of language—i.e., word vectors within one language should not be more similar to each other than to words in another language. We measure this characteristic using modularity, a network measurement that measures the strength of clusters in a graph. Modularity has a moderate to strong correlation with three downstream tasks, even though modularity is based only on the structure of embeddings and does not require any external resources. We show through experiments that modularity can serve as an intrinsic validation metric to improve unsupervised cross-lingual word embeddings, particularly on distant language pairs in low-resource settings.",,,,ACL
490,2019,Multilingual and Cross-Lingual Graded Lexical Entailment,"Ivan Vulić, Simone Paolo Ponzetto, Goran Glavaš","Grounded in cognitive linguistics, graded lexical entailment (GR-LE) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale. In this paper, we present the first work on cross-lingual generalisation of GR-LE relation. Starting from HyperLex, the only available GR-LE dataset in English, we construct new monolingual GR-LE datasets for three other languages, and combine those to create a set of six cross-lingual GR-LE datasets termed CL-HYPERLEX. We next present a novel method dubbed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel) for effectively capturing graded (and binary) LE, both monolingually in different languages as well as across languages (i.e., on CL-HYPERLEX). Coupled with a bilingual dictionary, CLEAR leverages taxonomic LE knowledge in a resource-rich language (e.g., English) and propagates it to other languages. Supported by cross-lingual LE transfer, CLEAR sets competitive baseline performance on three new monolingual GR-LE datasets and six cross-lingual GR-LE datasets. In addition, we show that CLEAR outperforms current state-of-the-art on binary cross-lingual LE detection by a wide margin for diverse language pairs.",,,,ACL
491,2019,What Kind of Language Is Hard to Language-Model?,"Sabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian Roark, Jason Eisner","How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that “translationese” is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample.",,,,ACL
492,2019,Analyzing the Limitations of Cross-lingual Word Embedding Mappings,"Aitor Ormazabal, Mikel Artetxe, Gorka Labaka, Aitor Soroa, Eneko Agirre","Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal.",,,,ACL
493,2019,How Multilingual is Multilingual BERT?,"Telmo Pires, Eva Schlinger, Dan Garrette","In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",,,,ACL
494,2019,Bilingual Lexicon Induction through Unsupervised Machine Translation,"Mikel Artetxe, Gorka Labaka, Eneko Agirre","A recent research line has obtained strong results on bilingual lexicon induction by aligning independently trained word embeddings in two languages and using the resulting cross-lingual embeddings to induce word translation pairs through nearest neighbor or related retrieval methods. In this paper, we propose an alternative approach to this problem that builds on the recent work on unsupervised machine translation. This way, instead of directly inducing a bilingual lexicon from cross-lingual embeddings, we use them to build a phrase-table, combine it with a language model, and use the resulting machine translation system to generate a synthetic parallel corpus, from which we extract the bilingual lexicon using statistical word alignment techniques. As such, our method can work with any word embedding and cross-lingual mapping technique, and it does not require any additional resource besides the monolingual corpus used to train the embeddings. When evaluated on the exact same cross-lingual embeddings, our proposed method obtains an average improvement of 6 accuracy points over nearest neighbor and 4 points over CSLS retrieval, establishing a new state-of-the-art in the standard MUSE dataset.",,,,ACL
495,2019,Automatically Identifying Complaints in Social Media,"Daniel Preoţiuc-Pietro, Mihaela Gaman, Nikolaos Aletras","Complaining is a basic speech act regularly used in human and computer mediated communication to express a negative mismatch between reality and expectations in a particular situation. Automatically identifying complaints in social media is of utmost importance for organizations or brands to improve the customer experience or in developing dialogue systems for handling and responding to complaints. In this paper, we introduce the first systematic analysis of complaints in computational linguistics. We collect a new annotated data set of written complaints expressed on Twitter. We present an extensive linguistic analysis of complaining as a speech act in social media and train strong feature-based and neural models of complaints across nine domains achieving a predictive performance of up to 79 F1 using distant supervision.",,,,ACL
496,2019,TWEETQA: A Social Media Focused Question Answering Dataset,"Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulkarni, Mo Yu","With social media becoming increasingly popular on which lots of news and real-time events are reported, developing automated question answering systems is critical to the effective-ness of many applications that rely on real-time knowledge. While previous datasets have concentrated on question answering (QA) for formal text like news and Wikipedia, we present the first large-scale dataset for QA over social media data. To ensure that the tweets we collected are useful, we only gather tweets used by journalists to write news articles. We then ask human annotators to write questions and answers upon these tweets. Unlike otherQA datasets like SQuAD in which the answers are extractive, we allow the answers to be abstractive. We show that two recently proposed neural models that perform well on formal texts are limited in their performance when applied to our dataset. In addition, even the fine-tuned BERT model is still lagging behind human performance with a large margin. Our results thus point to the need of improved QA systems targeting social media text.",,,,ACL
497,2019,"Asking the Crowd: Question Analysis, Evaluation and Generation for Open Discussion on Online Forums","Zi Chai, Xinyu Xing, Xiaojun Wan, Bo Huang","Teaching machines to ask questions is an important yet challenging task. Most prior work focused on generating questions with fixed answers. As contents are highly limited by given answers, these questions are often not worth discussing. In this paper, we take the first step on teaching machines to ask open-answered questions from real-world news for open discussion (openQG). To generate high-qualified questions, effective ways for question evaluation are required. We take the perspective that the more answers a question receives, the better it is for open discussion, and analyze how language use affects the number of answers. Compared with other factors, e.g. topic and post time, linguistic factors keep our evaluation from being domain-specific. We carefully perform variable control on 11.5M questions from online forums to get a dataset, OQRanD, and further perform question analysis. Based on these conclusions, several models are built for question evaluation. For openQG task, we construct OQGenD, the first dataset as far as we know, and propose a model based on conditional generative adversarial networks and our question evaluation model. Experiments show that our model can generate questions with higher quality compared with commonly-used text generation methods.",,,,ACL
498,2019,Tree LSTMs with Convolution Units to Predict Stance and Rumor Veracity in Social Media Conversations,"Sumeet Kumar, Kathleen Carley","Learning from social-media conversations has gained significant attention recently because of its applications in areas like rumor detection. In this research, we propose a new way to represent social-media conversations as binarized constituency trees that allows comparing features in source-posts and their replies effectively. Moreover, we propose to use convolution units in Tree LSTMs that are better at learning patterns in features obtained from the source and reply posts. Our Tree LSTM models employ multi-task (stance + rumor) learning and propagate the useful stance signal up in the tree for rumor classification at the root node. The proposed models achieve state-of-the-art performance, outperforming the current best model by 12% and 15% on F1-macro for rumor-veracity classification and stance classification tasks respectively.",,,,ACL
499,2019,HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization,"Xingxing Zhang, Furu Wei, Ming Zhou","Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. Training the hierarchical encoder with these inaccurate labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders (Devlin et al., 2018), we propose Hibert (as shorthand for HIerachical Bidirectional Encoder Representations from Transformers) for document encoding and a method to pre-train it using unlabeled data. We apply the pre-trained Hibert to our summarization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset. We also achieve the state-of-the-art performance on these two datasets.",,,,ACL
500,2019,Hierarchical Transformers for Multi-Document Summarization,"Yang Liu, Mirella Lapata","In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.",,,,ACL
501,2019,Abstractive Text Summarization Based on Deep Learning and Semantic Content Generalization,"Panagiotis Kouris, Georgios Alexandridis, Andreas Stafylopatis","This work proposes a novel framework for enhancing abstractive text summarization based on the combination of deep learning techniques along with semantic data transformations. Initially, a theoretical model for semantic-based text generalization is introduced and used in conjunction with a deep encoder-decoder architecture in order to produce a summary in generalized form. Subsequently, a methodology is proposed which transforms the aforementioned generalized summary into human-readable form, retaining at the same time important informational aspects of the original text and addressing the problem of out-of-vocabulary or rare words. The overall approach is evaluated on two popular datasets with encouraging results.",,,,ACL
502,2019,Studying Summarization Evaluation Metrics in the Appropriate Scoring Range,Maxime Peyrard,"In summarization, automatic evaluation metrics are usually compared based on their ability to correlate with human judgments. Unfortunately, the few existing human judgment datasets have been created as by-products of the manual evaluations performed during the DUC/TAC shared tasks. However, modern systems are typically better than the best systems submitted at the time of these shared tasks. We show that, surprisingly, evaluation metrics which behave similarly on these datasets (average-scoring range) strongly disagree in the higher-scoring range in which current systems now operate. It is problematic because metrics disagree yet we can’t decide which one to trust. This is a call for collecting human judgments for high-scoring summaries as this would resolve the debate over which metrics to trust. This would also be greatly beneficial to further improve summarization systems and metrics alike.",,,,ACL
503,2019,Simple Unsupervised Summarization by Contextual Matching,"Jiawei Zhou, Alexander Rush","We propose an unsupervised method for sentence summarization using only language modeling. The approach employs two language models, one that is generic (i.e. pretrained), and the other that is specific to the target domain. We show that by using a product-of-experts criteria these are enough for maintaining continuous contextual matching while maintaining output fluency. Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data.",,,,ACL
504,2019,Generating Summaries with Topic Templates and Structured Convolutional Decoders,"Laura Perez-Beltrachini, Yang Liu, Mirella Lapata",Existing neural generation approaches create multi-sentence text as a single sequence. In this paper we propose a structured convolutional decoder that is guided by the content structure of target summaries. We compare our model with existing sequential decoders on three data sets representing different domains. Automatic and human evaluation demonstrate that our summaries have better content coverage.,,,,ACL
505,2019,Morphological Irregularity Correlates with Frequency,"Shijie Wu, Ryan Cotterell, Timothy O’Donnell","We present a study of morphological irregularity. Following recent work, we define an information-theoretic measure of irregularity based on the predictability of forms in a language. Using a neural transduction model, we estimate this quantity for the forms in 28 languages. We first present several validatory and exploratory analyses of irregularity. We then show that our analyses provide evidence for a correlation between irregularity and frequency: higher frequency items are more likely to be irregular and irregular items are more likely be highly frequent. To our knowledge, this result is the first of its breadth and confirms longstanding proposals from the linguistics literature. The correlation is more robust when aggregated at the level of whole paradigms—providing support for models of linguistic structure in which inflected forms are unified by abstract underlying stems or lexemes.",,,,ACL
506,2019,Like a Baby: Visually Situated Neural Language Acquisition,"Alexander Ororbia, Ankur Mali, Matthew Kelly, David Reitter","We examine the benefits of visual context in training neural language models to perform next-word prediction. A multi-modal neural architecture is introduced that outperform its equivalent trained on language alone with a 2% decrease in perplexity, even when no visual context is available at test. Fine-tuning the embeddings of a pre-trained state-of-the-art bidirectional language model (BERT) in the language modeling framework yields a 3.5% improvement. The advantage for training with visual context when testing without is robust across different languages (English, German and Spanish) and different models (GRU, LSTM, Delta-RNN, as well as those that use BERT embeddings). Thus, language models perform better when they learn like a baby, i.e, in a multi-modal environment. This finding is compatible with the theory of situated cognition: language is inseparable from its physical context.",,,,ACL
507,2019,Relating Simple Sentence Representations in Deep Neural Networks and the Brain,"Sharmistha Jat, Hao Tang, Partha Talukdar, Tom Mitchell","What is the relationship between sentence representations learned by deep recurrent models against those encoded by the brain? Is there any correspondence between hidden layers of these recurrent models and brain regions when processing sentences? Can these deep models be used to synthesize brain data which can then be utilized in other extrinsic tasks? We investigate these questions using sentences with simple syntax and semantics (e.g., The bone was eaten by the dog.). We consider multiple neural network architectures, including recently proposed ELMo and BERT. We use magnetoencephalography (MEG) brain recording data collected from human subjects when they were reading these simple sentences. Overall, we find that BERT’s activations correlate the best with MEG brain data. We also find that the deep network representation can be used to generate brain data from new sentences to augment existing brain data. To the best of our knowledge, this is the first work showing that the MEG brain recording when reading a word in a sentence can be used to distinguish earlier words in the sentence. Our exploration is also the first to use deep neural network representations to generate synthetic brain data and to show that it helps in improving subsequent stimuli decoding task accuracy.",,,,ACL
508,2019,Modeling Affirmative and Negated Action Processing in the Brain with Lexical and Compositional Semantic Models,"Vesna Djokic, Jean Maillard, Luana Bulat, Ekaterina Shutova","Recent work shows that distributional semantic models can be used to decode patterns of brain activity associated with individual words and sentence meanings. However, it is yet unclear to what extent such models can be used to study and decode fMRI patterns associated with specific aspects of semantic composition such as the negation function. In this paper, we apply lexical and compositional semantic models to decode fMRI patterns associated with negated and affirmative sentences containing hand-action verbs. Our results show reduced decoding (correlation) of sentences where the verb is in the negated context, as compared to the affirmative one, within brain regions implicated in action-semantic processing. This supports behavioral and brain imaging studies, suggesting that negation involves reduced access to aspects of the affirmative mental representation. The results pave the way for testing alternate semantic models of negation against human semantic processing in the brain.",,,,ACL
509,2019,Word-order Biases in Deep-agent Emergent Communication,"Rahma Chaabouni, Eugene Kharitonov, Alessandro Lazaric, Emmanuel Dupoux, Marco Baroni","Sequence-processing neural networks led to remarkable progress on many NLP tasks. As a consequence, there has been increasing interest in understanding to what extent they process language as humans do. We aim here to uncover which biases such models display with respect to “natural” word-order constraints. We train models to communicate about paths in a simple gridworld, using miniature languages that reflect or violate various natural language trends, such as the tendency to avoid redundancy or to minimize long-distance dependencies. We study how the controlled characteristics of our miniature languages affect individual learning and their stability across multiple network generations. The results draw a mixed picture. On the one hand, neural networks show a strong tendency to avoid long-distance dependencies. On the other hand, there is no clear preference for the efficient, non-redundant encoding of information that is widely attested in natural language. We thus suggest inoculating a notion of “effort” into neural networks, as a possible way to make their linguistic behavior more human-like.",,,,ACL
510,2019,NNE: A Dataset for Nested Named Entity Recognition in English Newswire,"Nicky Ringland, Xiang Dai, Ben Hachey, Sarvnaz Karimi, Cecile Paris","Named entity recognition (NER) is widely used in natural language processing applications and downstream tasks. However, most NER tools target flat annotation from popular datasets, eschewing the semantic information available in nested entity mentions. We describe NNE—a fine-grained, nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank (PTB). Our annotation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting. We hope the public release of this large dataset for English newswire will encourage development of new techniques for nested NER.",,,,ACL
511,2019,Sequence-to-Nuggets: Nested Entity Mention Detection via Anchor-Region Networks,"Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun","Sequential labeling-based NER approaches restrict each word belonging to at most one entity mention, which will face a serious problem when recognizing nested entity mentions. In this paper, we propose to resolve this problem by modeling and leveraging the head-driven phrase structures of entity mentions, i.e., although a mention can nest other mentions, they will not share the same head word. Specifically, we propose Anchor-Region Networks (ARNs), a sequence-to-nuggets architecture for nested mention detection. ARNs first identify anchor words (i.e., possible head words) of all mentions, and then recognize the mention boundaries for each anchor word by exploiting regular phrase structures. Furthermore, we also design Bag Loss, an objective function which can train ARNs in an end-to-end manner without using any anchor word annotation. Experiments show that ARNs achieve the state-of-the-art performance on three standard nested entity mention detection benchmarks.",,,,ACL
512,2019,Improving Textual Network Embedding with Global Attention via Optimal Transport,"Liqun Chen, Guoyin Wang, Chenyang Tao, Dinghan Shen, Pengyu Cheng","Constituting highly informative network embeddings is an essential tool for network analysis. It encodes network topology, along with other useful side information, into low dimensional node-based feature representations that can be exploited by statistical modeling. This work focuses on learning context-aware network embeddings augmented with text data. We reformulate the network embedding problem, and present two novel strategies to improve over traditional attention mechanisms: (i) a content-aware sparse attention module based on optimal transport; and (ii) a high-level attention parsing module. Our approach yields naturally sparse and self-normalized relational inference. It can capture long-term interactions between sequences, thus addressing the challenges faced by existing textual network embedding schemes. Extensive experiments are conducted to demonstrate our model can consistently outperform alternative state-of-the-art methods.",,,,ACL
513,2019,"Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction","Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly","While the fast-paced inception of novel tasks and new datasets helps foster active research in a community towards interesting directions, keeping track of the abundance of research activity in different areas on different datasets is likely to become increasingly difficult. The community could greatly benefit from an automatic system able to summarize scientific results, e.g., in the form of a leaderboard. In this paper we build two datasets and develop a framework (TDMS-IE) aimed at automatically extracting task, dataset, metric and score from NLP papers, towards the automatic construction of leaderboards. Experiments show that our model outperforms several baselines by a large margin. Our model is a first step towards automatic leaderboard construction, e.g., in the NLP domain.",,,,ACL
514,2019,Scaling up Open Tagging from Tens to Thousands: Comprehension Empowered Attribute Value Extraction from Product Title,"Huimin Xu, Wenting Wang, Xin Mao, Xinyu Jiang, Man Lan","Supplementing product information by extracting attribute values from title is a crucial task in e-Commerce domain. Previous studies treat each attribute only as an entity type and build one set of NER tags (e.g., BIO) for each of them, leading to a scalability issue which unfits to the large sized attribute system in real world e-Commerce. In this work, we propose a novel approach to support value extraction scaling up to thousands of attributes without losing performance: (1) We propose to regard attribute as a query and adopt only one global set of BIO tags for any attributes to reduce the burden of attribute tag or model explosion; (2) We explicitly model the semantic representations for attribute and title, and develop an attention mechanism to capture the interactive semantic relations in-between to enforce our framework to be attribute comprehensive. We conduct extensive experiments in real-life datasets. The results show that our model not only outperforms existing state-of-the-art NER tagging models, but also is robust and generates promising results for up to 8,906 attributes.",,,,ACL
515,2019,Incorporating Linguistic Constraints into Keyphrase Generation,"Jing Zhao, Yuxiang Zhang","Keyphrases, that concisely describe the high-level topics discussed in a document, are very useful for a wide range of natural language processing tasks. Though existing keyphrase generation methods have achieved remarkable performance on this task, they generate many overlapping phrases (including sub-phrases or super-phrases) of keyphrases. In this paper, we propose the parallel Seq2Seq network with the coverage attention to alleviate the overlapping phrase problem. Specifically, we integrate the linguistic constraints of keyphrase into the basic Seq2Seq network on the source side, and employ the multi-task learning framework on the target side. In addition, in order to prevent from generating overlapping phrases of keyphrases with correct syntax, we introduce the coverage vector to keep track of the attention history and to decide whether the parts of source text have been covered by existing generated keyphrases. Experimental results show that our method can outperform the state-of-the-art CopyRNN on scientific datasets, and is also more effective in news domain.",,,,ACL
516,2019,A Unified Multi-task Adversarial Learning Framework for Pharmacovigilance Mining,"Shweta Yadav, Asif Ekbal, Sriparna Saha, Pushpak Bhattacharyya","The mining of adverse drug reaction (ADR) has a crucial role in the pharmacovigilance. The traditional ways of identifying ADR are reliable but time-consuming, non-scalable and offer a very limited amount of ADR relevant information. With the unprecedented growth of information sources in the forms of social media texts (Twitter, Blogs, Reviews etc.), biomedical literature, and Electronic Medical Records (EMR), it has become crucial to extract the most pertinent ADR related information from these free-form texts. In this paper, we propose a neural network inspired multi- task learning framework that can simultaneously extract ADRs from various sources. We adopt a novel adversarial learning-based approach to learn features across multiple ADR information sources. Unlike the other existing techniques, our approach is capable to extracting fine-grained information (such as ‘Indications’, ‘Symptoms’, ‘Finding’, ‘Disease’, ‘Drug’) which provide important cues in pharmacovigilance. We evaluate our proposed approach on three publicly available real- world benchmark pharmacovigilance datasets, a Twitter dataset from PSB 2016 Social Me- dia Shared Task, CADEC corpus and Medline ADR corpus. Experiments show that our unified framework achieves state-of-the-art performance on individual tasks associated with the different benchmark datasets. This establishes the fact that our proposed approach is generic, which enables it to achieve high performance on the diverse datasets.",,,,ACL
517,2019,Quantity Tagger: A Latent-Variable Sequence Labeling Approach to Solving Addition-Subtraction Word Problems,"Yanyan Zou, Wei Lu","An arithmetic word problem typically includes a textual description containing several constant quantities. The key to solving the problem is to reveal the underlying mathematical relations (such as addition and subtraction) among quantities, and then generate equations to find solutions. This work presents a novel approach, Quantity Tagger, that automatically discovers such hidden relations by tagging each quantity with a sign corresponding to one type of mathematical operation. For each quantity, we assume there exists a latent, variable-sized quantity span surrounding the quantity token in the text, which conveys information useful for determining its sign. Empirical results show that our method achieves 5 and 8 points of accuracy gains on two datasets respectively, compared to prior approaches.",,,,ACL
518,2019,A Deep Reinforced Sequence-to-Set Model for Multi-Label Classification,"Pengcheng Yang, Fuli Luo, Shuming Ma, Junyang Lin, Xu Sun","Multi-label classification (MLC) aims to predict a set of labels for a given instance. Based on a pre-defined label order, the sequence-to-sequence (Seq2Seq) model trained via maximum likelihood estimation method has been successfully applied to the MLC task and shows powerful ability to capture high-order correlations between labels. However, the output labels are essentially an unordered set rather than an ordered sequence. This inconsistency tends to result in some intractable problems, e.g., sensitivity to the label order. To remedy this, we propose a simple but effective sequence-to-set model. The proposed model is trained via reinforcement learning, where reward feedback is designed to be independent of the label order. In this way, we can reduce the dependence of the model on the label order, as well as capture high-order correlations between labels. Extensive experiments show that our approach can substantially outperform competitive baselines, as well as effectively reduce the sensitivity to the label order.",,,,ACL
519,2019,Joint Slot Filling and Intent Detection via Capsule Neural Networks,"Chenwei Zhang, Yaliang Li, Nan Du, Wei Fan, Philip Yu","Being able to recognize words as slots and detect the intent of an utterance has been a keen issue in natural language understanding. The existing works either treat slot filling and intent detection separately in a pipeline manner, or adopt joint models which sequentially label slots while summarizing the utterance-level intent without explicitly preserving the hierarchical relationship among words, slots, and intents. To exploit the semantic hierarchy for effective modeling, we propose a capsule-based neural network model which accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema. A re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation. Experiments on two real-world datasets show the effectiveness of our model when compared with other alternative model architectures, as well as existing natural language understanding services.",,,,ACL
520,2019,Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision,"Hongliang Dai, Yangqiu Song","Lack of labeled training data is a major bottleneck for neural network based aspect and opinion term extraction on product reviews. To alleviate this problem, we first propose an algorithm to automatically mine extraction rules from existing training examples based on dependency parsing results. The mined rules are then applied to label a large amount of auxiliary data. Finally, we study training procedures to train a neural model which can learn from both the data automatically labeled by the rules and a small amount of data accurately annotated by human. Experimental results show that although the mined rules themselves do not perform well due to their limited flexibility, the combination of human annotated data and rule labeled auxiliary data can improve the neural model and allow it to achieve performance better than or comparable with the current state-of-the-art.",,,,ACL
521,2019,Cost-sensitive Regularization for Label Confusion-aware Event Detection,"Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun","In supervised event detection, most of the mislabeling occurs between a small number of confusing type pairs, including trigger-NIL pairs and sibling sub-types of the same coarse type. To address this label confusion problem, this paper proposes cost-sensitive regularization, which can force the training procedure to concentrate more on optimizing confusing type pairs. Specifically, we introduce a cost-weighted term into the training loss, which penalizes more on mislabeling between confusing label pairs. Furthermore, we also propose two estimators which can effectively measure such label confusion based on instance-level or population-level statistics. Experiments on TAC-KBP 2017 datasets demonstrate that the proposed method can significantly improve the performances of different models in both English and Chinese event detection.",,,,ACL
522,2019,Exploring Pre-trained Language Models for Event Extraction and Generation,"Sen Yang, Dawei Feng, Linbo Qiao, Zhigang Kan, Dongsheng Li","Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",,,,ACL
523,2019,Improving Open Information Extraction via Iterative Rank-Aware Learning,"Zhengbao Jiang, Pengcheng Yin, Graham Neubig","Open information extraction (IE) is the task of extracting open-domain assertions from natural language sentences. A key step in open IE is confidence modeling, ranking the extractions based on their estimated quality to adjust precision and recall of extracted assertions. We found that the extraction likelihood, a confidence measure used by current supervised open IE systems, is not well calibrated when comparing the quality of assertions extracted from different sentences. We propose an additional binary classification loss to calibrate the likelihood to make it more globally comparable, and an iterative learning process, where extractions generated by the open IE model are incrementally included as training samples to help the model learn from trial and error. Experiments on OIE2016 demonstrate the effectiveness of our method. Code and data are available at https://github.com/jzbjyb/oie_rank.",,,,ACL
524,2019,Towards Improving Neural Named Entity Recognition with Gazetteers,"Tianyu Liu, Jin-Ge Yao, Chin-Yew Lin","Most of the recently proposed neural models for named entity recognition have been purely data-driven, with a strong emphasis on getting rid of the efforts for collecting external resources or designing hand-crafted features. This could increase the chance of overfitting since the models cannot access any supervision signal beyond the small amount of annotated data, limiting their power to generalize beyond the annotated entities. In this work, we show that properly utilizing external gazetteers could benefit segmental neural NER models. We add a simple module on the recently proposed hybrid semi-Markov CRF architecture and observe some promising results.",,,,ACL
525,2019,Span-Level Model for Relation Extraction,"Kalpit Dixit, Yaser Al-Onaizan","Relation Extraction is the task of identifying entity mention spans in raw text and then identifying relations between pairs of the entity mentions. Recent approaches for this span-level task have been token-level models which have inherent limitations. They cannot easily define and implement span-level features, cannot model overlapping entity mentions and have cascading errors due to the use of sequential decoding. To address these concerns, we present a model which directly models all possible spans and performs joint entity mention detection and relation extraction. We report a new state-of-the-art performance of 62.83 F1 (prev best was 60.49) on the ACE2005 dataset.",,,,ACL
526,2019,Enhancing Unsupervised Generative Dependency Parser with Contextual Information,"Wenjuan Han, Yong Jiang, Kewei Tu","Most of the unsupervised dependency parsers are based on probabilistic generative models that learn the joint distribution of the given sentence and its parse. Probabilistic generative models usually explicit decompose the desired dependency tree into factorized grammar rules, which lack the global features of the entire sentence. In this paper, we propose a novel probabilistic model called discriminative neural dependency model with valence (D-NDMV) that generates a sentence and its parse from a continuous latent representation, which encodes global contextual information of the generated sentence. We propose two approaches to model the latent representation: the first deterministically summarizes the representation from the sentence and the second probabilistically models the representation conditioned on the sentence. Our approach can be regarded as a new type of autoencoder model to unsupervised dependency parsing that combines the benefits of both generative and discriminative techniques. In particular, our approach breaks the context-free independence assumption in previous generative approaches and therefore becomes more expressive. Our extensive experimental results on seventeen datasets from various sources show that our approach achieves competitive accuracy compared with both generative and discriminative state-of-the-art unsupervised dependency parsers.",,,,ACL
527,2019,Neural Architectures for Nested NER through Linearization,"Jana Straková, Milan Straka, Jan Hajic","We propose two neural network architectures for nested named entity recognition (NER), a setting in which named entities may overlap and also be labeled with more than one label. We encode the nested labels using a linearized scheme. In our first proposed approach, the nested labels are modeled as multilabels corresponding to the Cartesian product of the nested labels in a standard LSTM-CRF architecture. In the second one, the nested NER is viewed as a sequence-to-sequence problem, in which the input sequence consists of the tokens and output sequence of the labels, using hard attention on the word whose label is being predicted. The proposed methods outperform the nested NER state of the art on four corpora: ACE-2004, ACE-2005, GENIA and Czech CNEC. We also enrich our architectures with the recently published contextual embeddings: ELMo, BERT and Flair, reaching further improvements for the four nested entity corpora. In addition, we report flat NER state-of-the-art results for CoNLL-2002 Dutch and Spanish and for CoNLL-2003 English.",,,,ACL
528,2019,Online Infix Probability Computation for Probabilistic Finite Automata,"Marco Cognetta, Yo-Sub Han, Soon Chan Kwon","Probabilistic finite automata (PFAs) are com- mon statistical language model in natural lan- guage and speech processing. A typical task for PFAs is to compute the probability of all strings that match a query pattern. An impor- tant special case of this problem is computing the probability of a string appearing as a pre- fix, suffix, or infix. These problems find use in many natural language processing tasks such word prediction and text error correction. Recently, we gave the first incremental algorithm to efficiently compute the infix probabilities of each prefix of a string (Cognetta et al., 2018). We develop an asymptotic improvement of that algorithm and solve the open problem of computing the infix probabilities of PFAs from streaming data, which is crucial when process- ing queries online and is the ultimate goal of the incremental approach.",,,,ACL
529,2019,How to Best Use Syntax in Semantic Role Labelling,"Yufei Wang, Mark Johnson, Stephen Wan, Yifang Sun, Wei Wang","There are many different ways in which external information might be used in a NLP task. This paper investigates how external syntactic information can be used most effectively in the Semantic Role Labeling (SRL) task. We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model. We show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL’05 and CoNLL’12 benchmarks.",,,,ACL
530,2019,PTB Graph Parsing with Tree Approximation,"Yoshihide Kato, Shigeki Matsubara","The Penn Treebank (PTB) represents syntactic structures as graphs due to nonlocal dependencies. This paper proposes a method that approximates PTB graph-structured representations by trees. By our approximation method, we can reduce nonlocal dependency identification and constituency parsing into single tree-based parsing. An experimental result demonstrates that our approximation method with an off-the-shelf tree-based constituency parser significantly outperforms the previous methods in nonlocal dependency identification.",,,,ACL
531,2019,Sequence Labeling Parsing by Learning across Representations,"Michalina Strzyz, David Vilares, Carlos Gómez-Rodríguez","We use parsing as sequence labeling as a common framework to learn across constituency and dependency syntactic abstractions.To do so, we cast the problem as multitask learning (MTL). First, we show that adding a parsing paradigm as an auxiliary loss consistently improves the performance on the other paradigm. Secondly, we explore an MTL sequence labeling model that parses both representations, at almost no cost in terms of performance and speed. The results across the board show that on average MTL models with auxiliary losses for constituency parsing outperform single-task ones by 1.05 F1 points, and for dependency parsing by 0.62 UAS points.",,,,ACL
532,2019,A Prism Module for Semantic Disentanglement in Name Entity Recognition,"Kun Liu, Shen Li, Daqi Zheng, Zhengdong Lu, Sheng Gao","Natural Language Processing has been perplexed for many years by the problem that multiple semantics are mixed inside a word, even with the help of context. To solve this problem, we propose a prism module to disentangle the semantic aspects of words and reduce noise at the input layer of a model. In the prism module, some words are selectively replaced with task-related semantic aspects, then these denoised word representations can be fed into downstream tasks to make them easier. Besides, we also introduce a structure to train this module jointly with the downstream model without additional data. This module can be easily integrated into the downstream model and significantly improve the performance of baselines on named entity recognition (NER) task. The ablation analysis demonstrates the rationality of the method. As a side effect, the proposed method also provides a way to visualize the contribution of each word.",,,,ACL
533,2019,Label-Agnostic Sequence Labeling by Copying Nearest Neighbors,"Sam Wiseman, Karl Stratos","Retrieve-and-edit based approaches to structured prediction, where structures associated with retrieved neighbors are edited to form new structures, have recently attracted increased interest. However, much recent work merely conditions on retrieved structures (e.g., in a sequence-to-sequence framework), rather than explicitly manipulating them. We show we can perform accurate sequence labeling by explicitly (and only) copying labels from retrieved neighbors. Moreover, because this copying is label-agnostic, we can achieve impressive performance in zero-shot sequence-labeling tasks. We additionally consider a dynamic programming approach to sequence labeling in the presence of retrieved neighbors, which allows for controlling the number of distinct (copied) segments used to form a prediction, and leads to both more interpretable and accurate predictions.",,,,ACL
534,2019,Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset,"Hannah Rashkin, Eric Michael Smith, Margaret Li, Y-Lan Boureau","One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill. While it is straightforward for humans to recognize and acknowledge others’ feelings in a conversation, this is a significant challenge for AI systems due to the paucity of suitable publicly-available datasets for training and evaluation. This work proposes a new benchmark for empathetic dialogue generation and EmpatheticDialogues, a novel dataset of 25k conversations grounded in emotional situations. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, compared to models merely trained on large-scale Internet conversation data. We also present empirical comparisons of dialogue model adaptations for empathetic responding, leveraging existing models or datasets without requiring lengthy re-training of the full model.",,,,ACL
535,2019,Know More about Each Other: Evolving Dialogue Strategy via Compound Assessment,"Siqi Bao, Huang He, Fan Wang, Rongzhong Lian, Hua Wu","In this paper, a novel Generation-Evaluation framework is developed for multi-turn conversations with the objective of letting both participants know more about each other. For the sake of rational knowledge utilization and coherent conversation flow, a dialogue strategy which controls knowledge selection is instantiated and continuously adapted via reinforcement learning. Under the deployed strategy, knowledge grounded conversations are conducted with two dialogue agents. The generated dialogues are comprehensively evaluated on aspects like informativeness and coherence, which are aligned with our objective and human instinct. These assessments are integrated as a compound reward to guide the evolution of dialogue strategy via policy gradient. Comprehensive experiments have been carried out on the publicly available dataset, demonstrating that the proposed method outperforms the other state-of-the-art approaches significantly.",,,,ACL
536,2019,Training Neural Response Selection for Task-Oriented Dialogue Systems,"Matthew Henderson, Ivan Vulić, Daniela Gerz, Iñigo Casanueva, Paweł Budzianowski","Despite their popularity in the chatbot literature, retrieval-based models have had modest impact on task-oriented dialogue systems, with the main obstacle to their application being the low-data regime of most task-oriented dialogue tasks. Inspired by the recent success of pretraining in language modelling, we propose an effective method for deploying response selection in task-oriented dialogue. To train response selection models for task-oriented dialogue tasks, we propose a novel method which: 1) pretrains the response selection model on large general-domain conversational corpora; and then 2) fine-tunes the pretrained model for the target dialogue domain, relying only on the small in-domain dataset to capture the nuances of the given dialogue domain. Our evaluation on five diverse application domains, ranging from e-commerce to banking, demonstrates the effectiveness of the proposed training method.",,,,ACL
537,2019,Collaborative Dialogue in Minecraft,"Anjali Narayan-Chen, Prashant Jayannavar, Julia Hockenmaier","We wish to develop interactive agents that can communicate with humans to collaboratively solve tasks in grounded scenarios. Since computer games allow us to simulate such tasks without the need for physical robots, we define a Minecraft-based collaborative building task in which one player (A, the Architect) is shown a target structure and needs to instruct the other player (B, the Builder) to build this structure. Both players interact via a chat interface. A can observe B but cannot place blocks. We present the Minecraft Dialogue Corpus, a collection of 509 conversations and game logs. As a first step towards our goal of developing fully interactive agents for this task, we consider the subtask of Architect utterance generation, and show how challenging it is.",,,,ACL
538,2019,Neural Response Generation with Meta-words,"Can Xu, Wei Wu, Chongyang Tao, Huang Hu, Matt Schuerman","We present open domain dialogue generation with meta-words. A meta-word is a structured record that describes attributes of a response, and thus allows us to explicitly model the one-to-many relationship within open domain dialogues and perform response generation in an explainable and controllable manner. To incorporate meta-words into generation, we propose a novel goal-tracking memory network that formalizes meta-word expression as a goal in response generation and manages the generation process to achieve the goal with a state memory panel and a state controller. Experimental results from both automatic evaluation and human judgment on two large-scale data sets indicate that our model can significantly outperform state-of-the-art generation models in terms of response relevance, response diversity, and accuracy of meta-word expression.",,,,ACL
539,2019,Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading,"Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong Liu, Xiang Gao","Although neural conversational models are effective in learning how to produce fluent responses, their primary challenge lies in knowing what to say to make the conversation contentful and non-vacuous. We present a new end-to-end approach to contentful neural conversation that jointly models response generation and on-demand machine reading. The key idea is to provide the conversation model with relevant long-form text on the fly as a source of external knowledge. The model performs QA-style reading comprehension on this text in response to each conversational turn, thereby allowing for more focused integration of external knowledge than has been possible in prior approaches. To support further research on knowledge-grounded conversation, we introduce a new large-scale conversation dataset grounded in external web pages (2.8M turns, 7.4M sentences of grounding). Both human evaluation and automated metrics show that our approach results in more contentful responses compared to a variety of previous methods, improving both the informativeness and diversity of generated output.",,,,ACL
540,2019,Ordinal and Attribute Aware Response Generation in a Multimodal Dialogue System,"Hardik Chauhan, Mauajama Firdaus, Asif Ekbal, Pushpak Bhattacharyya","Multimodal dialogue systems have opened new frontiers in the traditional goal-oriented dialogue systems. The state-of-the-art dialogue systems are primarily based on unimodal sources, predominantly the text, and hence cannot capture the information present in the other sources such as videos, audios, images etc. With the availability of large scale multimodal dialogue dataset (MMD) (Saha et al., 2018) on the fashion domain, the visual appearance of the products is essential for understanding the intention of the user. Without capturing the information from both the text and image, the system will be incapable of generating correct and desirable responses. In this paper, we propose a novel position and attribute aware attention mechanism to learn enhanced image representation conditioned on the user utterance. Our evaluation shows that the proposed model can generate appropriate responses while preserving the position and attribute information. Experimental results also prove that our proposed approach attains superior performance compared to the baseline models, and outperforms the state-of-the-art approaches on text similarity based evaluation metrics.",,,,ACL
541,2019,Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference,"He Bai, Yu Zhou, Jiajun Zhang, Chengqing Zong","Dialogue contexts are proven helpful in the spoken language understanding (SLU) system and they are typically encoded with explicit memory representations. However, most of the previous models learn the context memory with only one objective to maximizing the SLU performance, leaving the context memory under-exploited. In this paper, we propose a new dialogue logistic inference (DLI) task to consolidate the context memory jointly with SLU in the multi-task framework. DLI is defined as sorting a shuffled dialogue session into its original logical order and shares the same memory encoder and retrieval mechanism as the SLU model. Our experimental results show that various popular contextual SLU models can benefit from our approach, and improvements are quite impressive, especially in slot filling.",,,,ACL
542,2019,Personalizing Dialogue Agents via Meta-Learning,"Andrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, Pascale Fung","Existing personalized dialogue models use human designed persona descriptions to improve dialogue consistency. Collecting such descriptions from existing dialogues is expensive and requires hand-crafted feature designs. In this paper, we propose to extend Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) to personalized dialogue learning without using any persona descriptions. Our model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions. Empirical results on Persona-chat dataset (Zhang et al., 2018) indicate that our solution outperforms non-meta-learning baselines using automatic evaluation metrics, and in terms of human-evaluated fluency and consistency.",,,,ACL
543,2019,Reading Turn by Turn: Hierarchical Attention Architecture for Spoken Dialogue Comprehension,"Zhengyuan Liu, Nancy Chen","Comprehending multi-turn spoken conversations is an emerging research area, presenting challenges different from reading comprehension of passages due to the interactive nature of information exchange from at least two speakers. Unlike passages, where sentences are often the default semantic modeling unit, in multi-turn conversations, a turn is a topically coherent unit embodied with immediately relevant context, making it a linguistically intuitive segment for computationally modeling verbal interactions. Therefore, in this work, we propose a hierarchical attention neural network architecture, combining turn-level and word-level attention mechanisms, to improve spoken dialogue comprehension performance. Experiments are conducted on a multi-turn conversation dataset, where nurses inquire and discuss symptom information with patients. We empirically show that the proposed approach outperforms standard attention baselines, achieves more efficient learning outcomes, and is more robust to lengthy and out-of-distribution test samples.",,,,ACL
544,2019,A Novel Bi-directional Interrelated Model for Joint Intent Detection and Slot Filling,"Haihong E, Peiqing Niu, Zhongfu Chen, Meina Song","A spoken language understanding (SLU) system includes two main tasks, slot filling (SF) and intent detection (ID). The joint model for the two tasks is becoming a tendency in SLU. But the bi-directional interrelated connections between the intent and slots are not established in the existing joint models. In this paper, we propose a novel bi-directional interrelated model for joint intent detection and slot filling. We introduce an SF-ID network to establish direct connections for the two tasks to help them promote each other mutually. Besides, we design an entirely new iteration mechanism inside the SF-ID network to enhance the bi-directional interrelated connections. The experimental results show that the relative improvement in the sentence-level semantic frame accuracy of our model is 3.79% and 5.42% on ATIS and Snips datasets, respectively, compared to the state-of-the-art model.",,,,ACL
545,2019,Dual Supervised Learning for Natural Language Understanding and Generation,"Shang-Yu Su, Chao-Wei Huang, Yun-Nung Chen","Natural language understanding (NLU) and natural language generation (NLG) are both critical research topics in the NLP and dialogue fields. Natural language understanding is to extract the core semantic meaning from the given utterances, while natural language generation is opposite, of which the goal is to construct corresponding sentences based on the given semantics. However, such dual relationship has not been investigated in literature. This paper proposes a novel learning framework for natural language understanding and generation on top of dual supervised learning, providing a way to exploit the duality. The preliminary experiments show that the proposed approach boosts the performance for both tasks, demonstrating the effectiveness of the dual relationship.",,,,ACL
546,2019,SUMBT: Slot-Utterance Matching for Universal and Scalable Belief Tracking,"Hwaran Lee, Jinsik Lee, Tae-Yoon Kim","In goal-oriented dialog systems, belief trackers estimate the probability distribution of slot-values at every dialog turn. Previous neural approaches have modeled domain- and slot-dependent belief trackers, and have difficulty in adding new slot-values, resulting in lack of flexibility of domain ontology configurations. In this paper, we propose a new approach to universal and scalable belief tracker, called slot-utterance matching belief tracker (SUMBT). The model learns the relations between domain-slot-types and slot-values appearing in utterances through attention mechanisms based on contextual semantic vectors. Furthermore, the model predicts slot-value labels in a non-parametric way. From our experiments on two dialog corpora, WOZ 2.0 and MultiWOZ, the proposed model showed performance improvement in comparison with slot-dependent methods and achieved the state-of-the-art joint accuracy.",,,,ACL
547,2019,Robust Zero-Shot Cross-Domain Slot Filling with Example Values,"Darsh Shah, Raghav Gupta, Amir Fayazi, Dilek Hakkani-Tur","Task-oriented dialog systems increasingly rely on deep learning-based slot filling models, usually needing extensive labeled training data for target domains. Often, however, little to no target domain training data may be available, or the training and target domain schemas may be misaligned, as is common for web forms on similar websites. Prior zero-shot slot filling models use slot descriptions to learn concepts, but are not robust to misaligned schemas. We propose utilizing both the slot description and a small number of examples of slot values, which may be easily available, to learn semantic representations of slots which are transferable across domains and robust to misaligned schemas. Our approach outperforms state-of-the-art models on two multi-domain datasets, especially in the low-data setting.",,,,ACL
548,2019,Deep Unknown Intent Detection with Margin Loss,"Ting-En Lin, Hua Xu","Identifying the unknown (novel) user intents that have never appeared in the training set is a challenging task in the dialogue system. In this paper, we present a two-stage method for detecting unknown intents. We use bidirectional long short-term memory (BiLSTM) network with the margin loss as the feature extractor. With margin loss, we can learn discriminative deep features by forcing the network to maximize inter-class variance and to minimize intra-class variance. Then, we feed the feature vectors to the density-based novelty detection algorithm, local outlier factor (LOF), to detect unknown intents. Experiments on two benchmark datasets show that our method can yield consistent improvements compared with the baseline methods.",,,,ACL
549,2019,Modeling Semantic Relationship in Multi-turn Conversations with Hierarchical Latent Variables,"Lei Shen, Yang Feng, Haolan Zhan","Multi-turn conversations consist of complex semantic structures, and it is still a challenge to generate coherent and diverse responses given previous utterances. It’s practical that a conversation takes place under a background, meanwhile, the query and response are usually most related and they are consistent in topic but also different in content. However, little work focuses on such hierarchical relationship among utterances. To address this problem, we propose a Conversational Semantic Relationship RNN (CSRR) model to construct the dependency explicitly. The model contains latent variables in three hierarchies. The discourse-level one captures the global background, the pair-level one stands for the common topic information between query and response, and the utterance-level ones try to represent differences in content. Experimental results show that our model significantly improves the quality of responses in terms of fluency, coherence, and diversity compared to baseline methods.",,,,ACL
550,2019,Rationally Reappraising ATIS-based Dialogue Systems,"Jingcheng Niu, Gerald Penn","The Air Travel Information Service (ATIS) corpus has been the most common benchmark for evaluating Spoken Language Understanding (SLU) tasks for more than three decades since it was released. Recent state-of-the-art neural models have obtained F1-scores near 98% on the task of slot filling. We developed a rule-based grammar for the ATIS domain that achieves a 95.82% F1-score on our evaluation set. In the process, we furthermore discovered numerous shortcomings in the ATIS corpus annotation, which we have fixed. This paper presents a detailed account of these shortcomings, our proposed repairs, our rule-based grammar and the neural slot-filling architectures associated with ATIS. We also rationally reappraise the motivations for choosing a neural architecture in view of this account. Fixing the annotation errors results in a relative error reduction of between 19.4 and 52% across all architectures. We nevertheless argue that neural models must play a different role in ATIS dialogues because of the latter’s lack of variety.",,,,ACL
551,2019,Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic Programming,"Caio Corro, Ivan Titov","We treat projective dependency trees as latent variables in our probabilistic model and induce them in such a way as to be beneficial for a downstream task, without relying on any direct tree supervision. Our approach relies on Gumbel perturbations and differentiable dynamic programming. Unlike previous approaches to latent tree learning, we stochastically sample global structures and our parser is fully differentiable. We illustrate its effectiveness on sentiment analysis and natural language inference tasks. We also study its properties on a synthetic structure induction task. Ablation studies emphasize the importance of both stochasticity and constraining latent structures to be projective trees.",,,,ACL
552,2019,Neural-based Chinese Idiom Recommendation for Enhancing Elegance in Essay Writing,"Yuanchao Liu, Bo Pang, Bingquan Liu","Although the proper use of idioms can enhance the elegance of writing, the active use of various expressions is a challenge because remembering idioms is difficult. In this study, we address the problem of idiom recommendation by leveraging a neural machine translation framework, in which we suppose that idioms are written with one pseudo target language. Two types of real-life datasets are collected to support this study. Experimental results show that the proposed approach achieves promising performance compared with other baseline methods.",,,,ACL
553,2019,Better Exploiting Latent Variables in Text Modeling,Canasai Kruengkrai,"We show that sampling latent variables multiple times at a gradient step helps in improving a variational autoencoder and propose a simple and effective method to better exploit these latent variables through hidden state averaging. Consistent gains in performance on two different datasets, Penn Treebank and Yahoo, indicate the generalizability of our method.",,,,ACL
554,2019,Misleading Failures of Partial-input Baselines,"Shi Feng, Eric Wallace, Jordan Boyd-Graber","Recent work establishes dataset difficulty and removes annotation artifacts via partial-input baselines (e.g., hypothesis-only model for SNLI or question-only model for VQA). A successful partial-input baseline indicates that the dataset is cheatable. But the converse is not necessarily true: failures of partial-input baselines do not mean the dataset is free of artifacts. We first design artificial datasets to illustrate how the trivial patterns that are only visible in the full input can evade any partial-input baseline. Next, we identify such artifacts in the SNLI dataset—a hypothesis-only model augmented with trivial patterns in the premise can solve 15% of previously-thought “hard” examples. Our work provides a caveat for the use and creation of partial-input baselines for datasets.",,,,ACL
555,2019,Soft Contextual Data Augmentation for Neural Machine Translation,"Fei Gao, Jinhua Zhu, Lijun Wu, Yingce Xia, Tao Qin","While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is still very limited. In this paper, we present a novel data augmentation method for neural machine translation.Different from previous augmentation methods that randomly drop, swap or replace words with other words in a sentence, we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words. More accurately, we replace the one-hot representation of a word by a distribution (provided by a language model) over the vocabulary, i.e., replacing the embedding of this word by a weighted combination of multiple semantically similar words. Since the weights of those words depend on the contextual information of the word to be replaced,the newly generated sentences capture much richer information than previous augmentation methods. Experimental results on both small scale and large scale machine translation data sets demonstrate the superiority of our method over strong baselines.",,,,ACL
556,2019,Reversing Gradients in Adversarial Domain Adaptation for Question Deduplication and Textual Entailment Tasks,"Anush Kamath, Sparsh Gupta, Vitor Carvalho","Adversarial domain adaptation has been recently proposed as an effective technique for textual matching tasks, such as question deduplication. Here we investigate the use of gradient reversal on adversarial domain adaptation to explicitly learn both shared and unshared (domain specific) representations between two textual domains. In doing so, gradient reversal learns features that explicitly compensate for domain mismatch, while still distilling domain specific knowledge that can improve target domain accuracy. We evaluate reversing gradients for adversarial adaptation on multiple domains, and demonstrate that it significantly outperforms other methods on question deduplication as well as on recognizing textual entailment (RTE) tasks, achieving up to 7% absolute boost in base model accuracy on some datasets.",,,,ACL
557,2019,Towards Integration of Statistical Hypothesis Tests into Deep Neural Networks,"Ahmad Aghaebrahimian, Mark Cieliebak","We report our ongoing work about a new deep architecture working in tandem with a statistical test procedure for jointly training texts and their label descriptions for multi-label and multi-class classification tasks. A statistical hypothesis testing method is used to extract the most informative words for each given class. These words are used as a class description for more label-aware text classification. Intuition is to help the model to concentrate on more informative words rather than more frequent ones. The model leverages the use of label descriptions in addition to the input text to enhance text classification performance. Our method is entirely data-driven, has no dependency on other sources of information than the training data, and is adaptable to different classification problems by providing appropriate training data without major hyper-parameter tuning. We trained and tested our system on several publicly available datasets, where we managed to improve the state-of-the-art on one set with a high margin and to obtain competitive results on all other ones.",,,,ACL
558,2019,Depth Growing for Neural Machine Translation,"Lijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei Gao","While very deep neural networks have shown effectiveness for computer vision and text classification applications, how to increase the network depth of the neural machine translation (NMT) models for better translation quality remains a challenging problem. Directly stacking more blocks to the NMT model results in no improvement and even drop in performance. In this work, we propose an effective two-stage approach with three specially designed components to construct deeper NMT models, which result in significant improvements over the strong Transformer baselines on WMT14 English→German and English→French translation tasks.",,,,ACL
559,2019,Generating Fluent Adversarial Examples for Natural Languages,"Huangzhao Zhang, Hao Zhou, Ning Miao, Lei Li","Efficiently building an adversarial attacker for natural language processing (NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it is difficult to make small perturbations along the direction of gradients. Secondly, the fluency of the generated examples cannot be guaranteed. In this paper, we propose MHA, which addresses both problems by performing Metropolis-Hastings sampling, whose proposal is designed with the guidance of gradients. Experiments on IMDB and SNLI show that our proposed MHAoutperforms the baseline model on attacking capability. Adversarial training with MHA also leads to better robustness and performance.",,,,ACL
560,2019,Towards Explainable NLP: A Generative Explanation Framework for Text Classification,"Hui Liu, Qingyu Yin, William Yang Wang","Building explainable systems is a critical problem in the field of Natural Language Processing (NLP), since most machine learning models provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. However, the fine-grained information (e.g. textual explanations for the labels) is often ignored, and the systems do not explicitly generate the human-readable explanations. To solve this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and fine-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time.",,,,ACL
561,2019,Combating Adversarial Misspellings with Robust Word Recognition,"Danish Pruthi, Bhuwan Dhingra, Zachary C. Lipton","To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. Our word recognition models build upon the RNN semi-character architecture, introducing several new backoff strategies for handling rare and unseen words. Trained to recognize words corrupted by random adds, drops, swaps, and keyboard mistakes, our method achieves 32% relative (and 3.3% absolute) error reduction over the vanilla semi-character model. Notably, our pipeline confers robustness on the downstream classifier, outperforming both adversarial training and off-the-shelf spell checkers. Against a BERT model fine-tuned for sentiment analysis, a single adversarially-chosen character attack lowers accuracy from 90.3% to 45.8%. Our defense restores accuracy to 75%. Surprisingly, better word recognition does not always entail greater robustness. Our analysis reveals that robustness also depends upon a quantity that we denote the sensitivity.",,,,ACL
562,2019,An Empirical Investigation of Structured Output Modeling for Graph-based Neural Dependency Parsing,"Zhisong Zhang, Xuezhe Ma, Eduard Hovy","In this paper, we investigate the aspect of structured output modeling for the state-of-the-art graph-based neural dependency parser (Dozat and Manning, 2017). With evaluations on 14 treebanks, we empirically show that global output-structured models can generally obtain better performance, especially on the metric of sentence-level Complete Match. However, probably because neural models already learn good global views of the inputs, the improvement brought by structured output modeling is modest.",,,,ACL
563,2019,Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes,"Jie Cao, Michael Tanana, Zac Imel, Eric Poitras, David Atkins","Automatically analyzing dialogue can help understand and guide behavior in domains such as counseling, where interactions are largely mediated by conversation. In this paper, we study modeling behavioral codes used to asses a psychotherapy treatment style called Motivational Interviewing (MI), which is effective for addressing substance abuse and related problems. Specifically, we address the problem of providing real-time guidance to therapists with a dialogue observer that (1) categorizes therapist and client MI behavioral codes and, (2) forecasts codes for upcoming utterances to help guide the conversation and potentially alert the therapist. For both tasks, we define neural network models that build upon recent successes in dialogue modeling. Our experiments demonstrate that our models can outperform several baselines for both tasks. We also report the results of a careful analysis that reveals the impact of the various network design tradeoffs for modeling therapy dialogue.",,,,ACL
564,2019,Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems,"Hung Le, Doyen Sahoo, Nancy Chen, Steven Hoi","Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is conducted based on visual and audio aspects of a given video, is significantly more challenging than traditional image or text-grounded dialogue systems because (1) feature space of videos span across multiple picture frames, making it difficult to obtain semantic information; and (2) a dialogue agent must perceive and process information from different modalities (audio, video, caption, etc.) to obtain a comprehensive understanding. Most existing work is based on RNNs and sequence-to-sequence architectures, which are not very effective for capturing complex long-term dependencies (like in videos). To overcome this, we propose Multimodal Transformer Networks (MTN) to encode videos and incorporate information from different modalities. We also propose query-aware attention through an auto-encoder to extract query-aware features from non-text modalities. We develop a training procedure to simulate token-level decoding to improve the quality of generated responses during inference. We get state of the art performance on Dialogue System Technology Challenge 7 (DSTC7). Our model also generalizes to another multimodal visual-grounded dialogue task, and obtains promising performance.",,,,ACL
565,2019,Target-Guided Open-Domain Conversation,"Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric Xing","Many real-world open-domain conversation applications have specific goals to achieve during open-ended chats, such as recommendation, psychotherapy, education, etc. We study the problem of imposing conversational goals on open-domain chat agents. In particular, we want a conversational system to chat naturally with human and proactively guide the conversation to a designated target subject. The problem is challenging as no public data is available for learning such a target-guided strategy. We propose a structured approach that introduces coarse-grained keywords to control the intended content of system responses. We then attain smooth conversation transition through turn-level supervised learning, and drive the conversation towards the target with discourse-level constraints. We further derive a keyword-augmented conversation dataset for the study. Quantitative and human evaluations show our system can produce meaningful and effective conversations, significantly improving over other approaches",,,,ACL
566,2019,Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good,"Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang","Developing intelligent persuasive conversational agents to change people’s opinions and actions for social good is the frontier in advancing the ethical development of automated dialogue systems. To do so, the first step is to understand the intricate organization of strategic disclosures and appeals employed in human persuasion conversations. We designed an online persuasion task where one participant was asked to persuade the other to donate to a specific charity. We collected a large dataset with 1,017 dialogues and annotated emerging persuasion strategies from a subset. Based on the annotation, we built a baseline classifier with context information and sentence-level features to predict the 10 persuasion strategies used in the corpus. Furthermore, to develop an understanding of personalized persuasion processes, we analyzed the relationships between individuals’ demographic and psychological backgrounds including personality, morality, value systems, and their willingness for donation. Then, we analyzed which types of persuasion strategies led to a greater amount of donation depending on the individuals’ personal backgrounds. This work lays the ground for developing a personalized persuasive dialogue system.",,,,ACL
567,2019,Improving Neural Conversational Models with Entropy-Based Data Filtering,"Richárd Csáky, Patrik Purgai, Gábor Recski","Current neural network-based conversational models lack diversity and generate boring responses to open-ended utterances. Priors such as persona, emotion, or topic provide additional information to dialog models to aid response generation, but annotating a dataset with priors is expensive and such annotations are rarely available. While previous methods for improving the quality of open-domain response generation focused on either the underlying model or the training objective, we present a method of filtering dialog datasets by removing generic utterances from training data using a simple entropy-based approach that does not require human supervision. We conduct extensive experiments with different variations of our method, and compare dialog models across 17 evaluation metrics to show that training on datasets filtered this way results in better conversational quality as chatbots learn to output more diverse responses.",,,,ACL
568,2019,Zero-shot Word Sense Disambiguation using Sense Definition Embeddings,"Sawan Kumar, Sharmistha Jat, Karan Saxena, Partha Talukdar","Word Sense Disambiguation (WSD) is a long-standing but open problem in Natural Language Processing (NLP). WSD corpora are typically small in size, owing to an expensive annotation process. Current supervised WSD methods treat senses as discrete labels and also resort to predicting the Most-Frequent-Sense (MFS) for words unseen during training. This leads to poor performance on rare and unseen senses. To overcome this challenge, we propose Extended WSD Incorporating Sense Embeddings (EWISE), a supervised model to perform WSD by predicting over a continuous sense embedding space as opposed to a discrete label space. This allows EWISE to generalize over both seen and unseen senses, thus achieving generalized zero-shot learning. To obtain target sense embeddings, EWISE utilizes sense definitions. EWISE learns a novel sentence encoder for sense definitions by using WordNet relations and also ConvE, a recently proposed knowledge graph embedding method. We also compare EWISE against other sentence encoders pretrained on large corpora to generate definition embeddings. EWISE achieves new state-of-the-art WSD performance.",,,,ACL
569,2019,Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation,"Daniel Loureiro, Alípio Jorge","Contextual embeddings represent a new generation of semantic representations learned from Neural Language Modelling (NLM) that addresses the issue of meaning conflation hampering traditional word embeddings. In this work, we show that contextual embeddings can be used to achieve unprecedented gains in Word Sense Disambiguation (WSD) tasks. Our approach focuses on creating sense-level embeddings with full-coverage of WordNet, and without recourse to explicit knowledge of sense distributions or task-specific modelling. As a result, a simple Nearest Neighbors (k-NN) method using our representations is able to consistently surpass the performance of previous systems using powerful neural sequencing models. We also analyse the robustness of our approach when ignoring part-of-speech and lemma features, requiring disambiguation against the full sense inventory, and revealing shortcomings to be improved. Finally, we explore applications of our sense embeddings for concept-level analyses of contextual embeddings and their respective NLMs.",,,,ACL
570,2019,Word2Sense: Sparse Interpretable Word Embeddings,"Abhishek Panigrahi, Harsha Vardhan Simhadri, Chiranjib Bhattacharyya","We present an unsupervised method to generate Word2Sense word embeddings that are interpretable — each dimension of the embedding space corresponds to a fine-grained sense, and the non-negative value of the embedding along the j-th dimension represents the relevance of the j-th sense to the word. The underlying LDA-based generative model can be extended to refine the representation of a polysemous word in a short context, allowing us to use the embedings in contextual tasks. On computational NLP tasks, Word2Sense embeddings compare well with other word embeddings generated by unsupervised methods. Across tasks such as word similarity, entailment, sense induction, and contextual interpretation, Word2Sense is competitive with the state-of-the-art method for that task. Word2Sense embeddings are at least as sparse and fast to compute as prior art.",,,,ACL
571,2019,Modeling Semantic Compositionality with Sememe Knowledge,"Fanchao Qi, Junjie Huang, Chenghao Yang, Zhiyuan Liu, Xiao Chen","Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in models. In this paper, we verify the effectiveness of sememes, the minimum semantic units of human languages, in modeling SC by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe knowledge into SC models, and employ the sememe-incorporated models in learning representations of multiword expressions, a typical task of SC. In experiments, we implement our models by incorporating knowledge from a famous sememe knowledge base HowNet and perform both intrinsic and extrinsic evaluations. Experimental results show that our models achieve significant performance boost as compared to the baseline methods without considering sememe knowledge. We further conduct quantitative analysis and case studies to demonstrate the effectiveness of applying sememe knowledge in modeling SC.All the code and data of this paper can be obtained on https://github.com/thunlp/Sememe-SC.",,,,ACL
572,2019,Predicting Humorousness and Metaphor Novelty with Gaussian Process Preference Learning,"Edwin Simpson, Erik-Lân Do Dinh, Tristan Miller, Iryna Gurevych","The inability to quantify key aspects of creative language is a frequent obstacle to natural language understanding. To address this, we introduce novel tasks for evaluating the creativeness of language—namely, scoring and ranking text by humorousness and metaphor novelty. To sidestep the difficulty of assigning discrete labels or numeric scores, we learn from pairwise comparisons between texts. We introduce a Bayesian approach for predicting humorousness and metaphor novelty using Gaussian process preference learning (GPPL), which achieves a Spearman’s ρ of 0.56 against gold using word embeddings and linguistic features. Our experiments show that given sparse, crowdsourced annotation data, ranking using GPPL outperforms best–worst scaling. We release a new dataset for evaluating humour containing 28,210 pairwise comparisons of 4,030 texts, and make our software freely available.",,,,ACL
573,2019,Empirical Linguistic Study of Sentence Embeddings,"Katarzyna Krasnowska-Kieraś, Alina Wróblewska","The purpose of the research is to answer the question whether linguistic information is retained in vector representations of sentences. We introduce a method of analysing the content of sentence embeddings based on universal probing tasks, along with the classification datasets for two contrasting languages. We perform a series of probing and downstream experiments with different types of sentence embeddings, followed by a thorough analysis of the experimental results. Aside from dependency parser-based embeddings, linguistic information is retained best in the recently proposed LASER sentence embeddings.",,,,ACL
574,2019,Probing for Semantic Classes: Diagnosing the Meaning Content of Word Embeddings,"Yadollah Yaghoobzadeh, Katharina Kann, T. J. Hazen, Eneko Agirre, Hinrich Schütze","Word embeddings typically represent different meanings of a word in a single conflated vector. Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. We present a large dataset based on manual Wikipedia annotations and word senses, where word senses from different words are related by semantic classes. This is the basis for novel diagnostic tests for an embedding’s content: we probe word embeddings for semantic classes and analyze the embedding space by classifying embeddings into semantic classes. Our main findings are: (i) Information about a sense is generally represented well in a single-vector embedding – if the sense is frequent. (ii) A classifier can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) Although rare senses are not well represented in single-vector embeddings, this does not have negative impact on an NLP application whose performance depends on frequent senses.",,,,ACL
575,2019,Deep Neural Model Inspection and Comparison via Functional Neuron Pathways,"James Fiacco, Samridhi Choudhary, Carolyn Rose","We introduce a general method for the interpretation and comparison of neural models. The method is used to factor a complex neural model into its functional components, which are comprised of sets of co-firing neurons that cut across layers of the network architecture, and which we call neural pathways. The function of these pathways can be understood by identifying correlated task level and linguistic heuristics in such a way that this knowledge acts as a lens for approximating what the network has learned to apply to its intended task. As a case study for investigating the utility of these pathways, we present an examination of pathways identified in models trained for two standard tasks, namely Named Entity Recognition and Recognizing Textual Entailment.",,,,ACL
576,2019,Collocation Classification with Unsupervised Relation Vectors,"Luis Espinosa Anke, Steven Schockaert, Leo Wanner","Lexical relation classification is the task of predicting whether a certain relation holds between a given pair of words. In this paper, we explore to which extent the current distributional landscape based on word embeddings provides a suitable basis for classification of collocations, i.e., pairs of words between which idiosyncratic lexical relations hold. First, we introduce a novel dataset with collocations categorized according to lexical functions. Second, we conduct experiments on a subset of this benchmark, comparing it in particular to the well known DiffVec dataset. In these experiments, in addition to simple word vector arithmetic operations, we also investigate the role of unsupervised relation vectors as a complementary input. While these relation vectors indeed help, we also show that lexical function classification poses a greater challenge than the syntactic and semantic relations that are typically used for benchmarks in the literature.",,,,ACL
577,2019,Corpus-based Check-up for Thesaurus,Natalia Loukachevitch,"In this paper we discuss the usefulness of applying a checking procedure to existing thesauri. The procedure is based on the analysis of discrepancies of corpus-based and thesaurus-based word similarities. We applied the procedure to more than 30 thousand words of the Russian wordnet and found some serious errors in word sense description, including inaccurate relationships and missing senses of ambiguous words.",,,,ACL
578,2019,Confusionset-guided Pointer Networks for Chinese Spelling Check,"Dingmin Wang, Yi Tay, Li Zhong","This paper proposes Confusionset-guided Pointer Networks for Chinese Spell Check (CSC) task. More concretely, our approach utilizes the off-the-shelf confusionset for guiding the character generation. To this end, our novel Seq2Seq model jointly learns to copy a correct character from an input sentence through a pointer network, or generate a character from the confusionset rather than the entire vocabulary. We conduct experiments on three human-annotated datasets, and results demonstrate that our proposed generative model outperforms all competitor models by a large margin of up to 20% F1 score, achieving state-of-the-art performance on three datasets.",,,,ACL
579,2019,Generalized Data Augmentation for Low-Resource Translation,"Mengzhou Xia, Xiang Kong, Antonios Anastasopoulos, Graham Neubig","Low-resource language pairs with a paucity of parallel data pose challenges for machine translation in terms of both adequacy and fluency. Data augmentation utilizing a large amount of monolingual data is regarded as an effective way to alleviate the problem. In this paper, we propose a general framework of data augmentation for low-resource machine translation not only using target-side monolingual data, but also by pivoting through a related high-resource language. Specifically, we experiment with a two-step pivoting method to convert high-resource data to the low-resource language, making best use of available resources to better approximate the true distribution of the low-resource language. First, we inject low-resource words into high-resource sentences through an induced bilingual dictionary. Second, we further edit the high-resource data injected with low-resource words using a modified unsupervised machine translation framework. Extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to 1.5 to 8 BLEU points compared to supervised back-translation baselines.",,,,ACL
580,2019,"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned","Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov","Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.",,,,ACL
581,2019,Better OOV Translation with Bilingual Terminology Mining,"Matthias Huck, Viktor Hangya, Alexander Fraser","Unseen words, also called out-of-vocabulary words (OOVs), are difficult for machine translation. In neural machine translation, byte-pair encoding can be used to represent OOVs, but they are still often incorrectly translated. We improve the translation of OOVs in NMT using easy-to-obtain monolingual data. We look for OOVs in the text to be translated and translate them using simple-to-construct bilingual word embeddings (BWEs). In our MT experiments we take the 5-best candidates, which is motivated by intrinsic mining experiments. Using all five of the proposed target language words as queries we mine target-language sentences. We then back-translate, forcing the back-translation of each of the five proposed target-language OOV-translation-candidates to be the original source-language OOV. We show that by using this synthetic data to fine-tune our system the translation of OOVs can be dramatically improved. In our experiments we use a system trained on Europarl and mine sentences containing medical terms from monolingual data.",,,,ACL
582,2019,Simultaneous Translation with Flexible Policy via Restricted Imitation Learning,"Baigong Zheng, Renjie Zheng, Mingbo Ma, Liang Huang","Simultaneous translation is widely useful but remains one of the most difficult tasks in NLP. Previous work either uses fixed-latency policies, or train a complicated two-staged model using reinforcement learning. We propose a much simpler single model that adds a “delay” token to the target vocabulary, and design a restricted dynamic oracle to greatly simplify training. Experiments on Chinese <-> English simultaneous translation show that our work leads to flexible policies that achieve better BLEU scores and lower latencies compared to both fixed and RL-learned policies.",,,,ACL
583,2019,Target Conditioned Sampling: Optimizing Data Selection for Multilingual Neural Machine Translation,"Xinyi Wang, Graham Neubig","To improve low-resource Neural Machine Translation (NMT) with multilingual corpus, training on the most related high-resource language only is generally more effective than us- ing all data available (Neubig and Hu, 2018). However, it remains a question whether a smart data selection strategy can further improve low-resource NMT with data from other auxiliary languages. In this paper, we seek to construct a sampling distribution over all multilingual data, so that it minimizes the training loss of the low-resource language. Based on this formulation, we propose and efficient algorithm, (TCS), which first samples a target sentence, and then conditionally samples its source sentence. Experiments show TCS brings significant gains of up to 2 BLEU improvements on three of four languages we test, with minimal training overhead.",,,,ACL
584,2019,Adversarial Learning of Privacy-Preserving Text Representations for De-Identification of Medical Records,"Max Friedrich, Arne Köhn, Gregor Wiedemann, Chris Biemann","De-identification is the task of detecting protected health information (PHI) in medical text. It is a critical step in sanitizing electronic health records (EHR) to be shared for research. Automatic de-identification classifiers can significantly speed up the sanitization process. However, obtaining a large and diverse dataset to train such a classifier that works well across many types of medical text poses a challenge as privacy laws prohibit the sharing of raw medical records. We introduce a method to create privacy-preserving shareable representations of medical text (i.e. they contain no PHI) that does not require expensive manual pseudonymization. These representations can be shared between organizations to create unified datasets for training de-identification models. Our representation allows training a simple LSTM-CRF de-identification model to an F1 score of 97.4%, which is comparable to a strong baseline that exposes private information in its representation. A robust, widely available de-identification classifier based on our representation could potentially enable studies for which de-identification would otherwise be too costly.",,,,ACL
585,2019,Merge and Label: A Novel Neural Network Architecture for Nested NER,"Joseph Fisher, Andreas Vlachos","Named entity recognition (NER) is one of the best studied tasks in natural language processing. However, most approaches are not capable of handling nested structures which are common in many applications. In this paper we introduce a novel neural network architecture that first merges tokens and/or entities into entities forming nested structures, and then labels each of them independently. Unlike previous work, our merge and label approach predicts real-valued instead of discrete segmentation structures, which allow it to combine word and nested entity embeddings while maintaining differentiability. We evaluate our approach using the ACE 2005 Corpus, where it achieves state-of-the-art F1 of 74.6, further improved with contextual embeddings (BERT) to 82.4, an overall improvement of close to 8 F1 points over previous approaches trained on the same data. Additionally we compare it against BiLSTM-CRFs, the dominant approach for flat NER structures, demonstrating that its ability to predict nested structures does not impact performance in simpler cases.",,,,ACL
586,2019,Low-resource Deep Entity Resolution with Transfer and Active Learning,"Jungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li, Lucian Popa","Entity resolution (ER) is the task of identifying different representations of the same real-world entities across databases. It is a key step for knowledge base creation and text mining. Recent adaptation of deep learning methods for ER mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records. While these methods achieve state-of-the-art performance over benchmark data, they require large amounts of labeled data, which are typically unavailable in realistic ER applications. In this paper, we develop a deep learning-based method that targets low-resource settings for ER through a novel combination of transfer learning and active learning. We design an architecture that allows us to learn a transferable model from a high-resource setting to a low-resource one. To further adapt to the target dataset, we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model. Empirical evaluation demonstrates that our method achieves comparable, if not better, performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels.",,,,ACL
587,2019,A Semi-Markov Structured Support Vector Machine Model for High-Precision Named Entity Recognition,"Ravneet Arora, Chen-Tse Tsai, Ketevan Tsereteli, Prabhanjan Kambadur, Yi Yang","Named entity recognition (NER) is the backbone of many NLP solutions. F1 score, the harmonic mean of precision and recall, is often used to select/evaluate the best models. However, when precision needs to be prioritized over recall, a state-of-the-art model might not be the best choice. There is little in literature that directly addresses training-time modifications to achieve higher precision information extraction. In this paper, we propose a neural semi-Markov structured support vector machine model that controls the precision-recall trade-off by assigning weights to different types of errors in the loss-augmented inference during training. The semi-Markov property provides more accurate phrase-level predictions, thereby improving performance. We empirically demonstrate the advantage of our model when high precision is required by comparing against strong baselines based on CRF. In our experiments with the CoNLL 2003 dataset, our model achieves a better precision-recall trade-off at various precision levels.",,,,ACL
588,2019,Using Human Attention to Extract Keyphrase from Microblog Post,"Yingyi Zhang, Chengzhi Zhang","This paper studies automatic keyphrase extraction on social media. Previous works have achieved promising results on it, but they neglect human reading behavior during keyphrase annotating. The human attention is a crucial element of human reading behavior. It reveals the relevance of words to the main topics of the target text. Thus, this paper aims to integrate human attention into keyphrase extraction models. First, human attention is represented by the reading duration estimated from eye-tracking corpus. Then, we merge human attention with neural network models by an attention mechanism. In addition, we also integrate human attention into unsupervised models. To the best of our knowledge, we are the first to utilize human attention on keyphrase extraction tasks. The experimental results show that our models have significant improvements on two Twitter datasets.",,,,ACL
589,2019,Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision,"Abiola Obamuyide, Andreas Vlachos","In this paper we frame the task of supervised relation classification as an instance of meta-learning. We propose a model-agnostic meta-learning protocol for training relation classifiers to achieve enhanced predictive performance in limited supervision settings. During training, we aim to not only learn good parameters for classifying relations with sufficient supervision, but also learn model parameters that can be fine-tuned to enhance predictive performance for relations with limited supervision. In experiments conducted on two relation classification datasets, we demonstrate that the proposed meta-learning approach improves the predictive performance of two state-of-the-art supervised relation classification models.",,,,ACL
590,2019,Variational Pretraining for Semi-supervised Text Classification,"Suchin Gururangan, Tam Dang, Dallas Card, Noah A. Smith","We introduce VAMPIRE, a lightweight pretraining framework for effective text classification when data and computing resources are limited. We pretrain a unigram document model as a variational autoencoder on in-domain, unlabeled data and use its internal states as features in a downstream classifier. Empirically, we show the relative strength of VAMPIRE against computationally expensive contextual embeddings and other popular semi-supervised baselines under low resource settings. We also find that fine-tuning to in-domain data is crucial to achieving decent performance from contextual embeddings when working with limited supervision. We accompany this paper with code to pretrain and use VAMPIRE embeddings in downstream tasks.",,,,ACL
591,2019,Task Refinement Learning for Improved Accuracy and Stability of Unsupervised Domain Adaptation,"Yftah Ziser, Roi Reichart","Pivot Based Language Modeling (PBLM) (Ziser and Reichart, 2018a), combining LSTMs with pivot-based methods, has yielded significant progress in unsupervised domain adaptation. However, this approach is still challenged by the large pivot detection problem that should be solved, and by the inherent instability of LSTMs. In this paper we propose a Task Refinement Learning (TRL) approach, in order to solve these problems. Our algorithms iteratively train the PBLM model, gradually increasing the information exposed about each pivot. TRL-PBLM achieves stateof- the-art accuracy in six domain adaptation setups for sentiment classification. Moreover, it is much more stable than plain PBLM across model configurations, making the model much better fitted for practical use.",,,,ACL
592,2019,Optimal Transport-based Alignment of Learned Character Representations for String Similarity,"Derek Tam, Nicholas Monath, Ari Kobren, Aaron Traylor, Rajarshi Das","String similarity models are vital for record linkage, entity resolution, and search. In this work, we present STANCE–a learned model for computing the similarity of two strings. Our approach encodes the characters of each string, aligns the encodings using Sinkhorn Iteration (alignment is posed as an instance of optimal transport) and scores the alignment with a convolutional neural network. We evaluate STANCE’s ability to detect whether two strings can refer to the same entity–a task we term alias detection. We construct five new alias detection datasets (and make them publicly available). We show that STANCE (or one of its variants) outperforms both state-of-the-art and classic, parameter-free similarity models on four of the five datasets. We also demonstrate STANCE’s ability to improve downstream tasks by applying it to an instance of cross-document coreference and show that it leads to a 2.8 point improvement in Bˆ3 F1 over the previous state-of-the-art approach.",,,,ACL
593,2019,The Referential Reader: A Recurrent Entity Network for Anaphora Resolution,"Fei Liu, Luke Zettlemoyer, Jacob Eisenstein","We present a new architecture for storing and accessing entity mentions during online text processing. While reading the text, entity references are identified, and may be stored by either updating or overwriting a cell in a fixed-length memory. The update operation implies coreference with the other mentions that are stored in the same cell; the overwrite operation causes these mentions to be forgotten. By encoding the memory operations as differentiable gates, it is possible to train the model end-to-end, using both a supervised anaphora resolution objective as well as a supplementary language modeling objective. Evaluation on a dataset of pronoun-name anaphora demonstrates strong performance with purely incremental text processing.",,,,ACL
594,2019,Interpolated Spectral NGram Language Models,"Ariadna Quattoni, Xavier Carreras","Spectral models for learning weighted non-deterministic automata have nice theoretical and algorithmic properties. Despite this, it has been challenging to obtain competitive results in language modeling tasks, for two main reasons. First, in order to capture long-range dependencies of the data, the method must use statistics from long substrings, which results in very large matrices that are difficult to decompose. The second is that the loss function behind spectral learning, based on moment matching, differs from the probabilistic metrics used to evaluate language models. In this work we employ a technique for scaling up spectral learning, and use interpolated predictions that are optimized to maximize perplexity. Our experiments in character-based language modeling show that our method matches the performance of state-of-the-art ngram models, while being very fast to train.",,,,ACL
595,2019,BAM! Born-Again Multi-Task Networks for Natural Language Understanding,"Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, Quoc V. Le","It can be challenging to train multi-task neural networks that outperform or even match their single-task counterparts. To help address this, we propose using knowledge distillation where single-task models teach a multi-task model. We enhance this training with teacher annealing, a novel method that gradually transitions the model from distillation to supervised learning, helping the multi-task model surpass its single-task teachers. We evaluate our approach by multi-task fine-tuning BERT on the GLUE benchmark. Our method consistently improves over standard single-task and multi-task training.",,,,ACL
596,2019,Curate and Generate: A Corpus and Method for Joint Control of Semantics and Style in Neural NLG,"Shereen Oraby, Vrindavan Harrison, Abteen Ebrahimi, Marilyn Walker","Neural natural language generation (NNLG) from structured meaning representations has become increasingly popular in recent years. While we have seen progress with generating syntactically correct utterances that preserve semantics, various shortcomings of NNLG systems are clear: new tasks require new training data which is not available or straightforward to acquire, and model outputs are simple and may be dull and repetitive. This paper addresses these two critical challenges in NNLG by: (1) scalably (and at no cost) creating training datasets of parallel meaning representations and reference texts with rich style markup by using data from freely available and naturally descriptive user reviews, and (2) systematically exploring how the style markup enables joint control of semantic and stylistic aspects of neural model output. We present YelpNLG, a corpus of 300,000 rich, parallel meaning representations and highly stylistically varied reference texts spanning different restaurant attributes, and describe a novel methodology that can be scalably reused to generate NLG datasets for other domains. The experiments show that the models control important aspects, including lexical choice of adjectives, output length, and sentiment, allowing the models to successfully hit multiple style targets without sacrificing semantics.",,,,ACL
597,2019,Automated Chess Commentator Powered by Neural Chess Engine,"Hongyu Zang, Zhiwei Yu, Xiaojun Wan","In this paper, we explore a new approach for automated chess commentary generation, which aims to generate chess commentary texts in different categories (e.g., description, comparison, planning, etc.). We introduce a neural chess engine into text generation models to help with encoding boards, predicting moves, and analyzing situations. By jointly training the neural chess engine and the generation models for different categories, the models become more effective. We conduct experiments on 5 categories in a benchmark Chess Commentary dataset and achieve inspiring results in both automatic and human evaluations.",,,,ACL
598,2019,Barack’s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling,"Robert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, Sameer Singh","Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark. In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language model’s ability to complete sentences requiring factual knowledge, showing that the KGLM outperforms even very large language models in generating facts.",,,,ACL
599,2019,Controllable Paraphrase Generation with a Syntactic Exemplar,"Mingda Chen, Qingming Tang, Sam Wiseman, Kevin Gimpel","Prior work on controllable text generation usually assumes that the controlled attribute can take on one of a small set of values known a priori. In this work, we propose a novel task, where the syntax of a generated sentence is controlled rather by a sentential exemplar. To evaluate quantitatively with standard metrics, we create a novel dataset with human annotations. We also develop a variational model with a neural module specifically designed for capturing syntactic knowledge and several multitask training objectives to promote disentangled representation learning. Empirically, the proposed model is observed to achieve improvements over baselines and learn to capture desirable characteristics.",,,,ACL
600,2019,Towards Comprehensive Description Generation from Factual Attribute-value Tables,"Tianyu Liu, Fuli Luo, Pengcheng Yang, Wei Wu, Baobao Chang","The comprehensive descriptions for factual attribute-value tables, which should be accurate, informative and loyal, can be very helpful for end users to understand the structured data in this form. However previous neural generators might suffer from key attributes missing, less informative and groundless information problems, which impede the generation of high-quality comprehensive descriptions for tables. To relieve these problems, we first propose force attention (FA) method to encourage the generator to pay more attention to the uncovered attributes to avoid potential key attributes missing. Furthermore, we propose reinforcement learning for information richness to generate more informative as well as more loyal descriptions for tables. In our experiments, we utilize the widely used WIKIBIO dataset as a benchmark. Besides, we create WB-filter based on WIKIBIO to test our model in the simulated user-oriented scenarios, in which the generated descriptions should accord with particular user interests. Experimental results show that our model outperforms the state-of-the-art baselines on both automatic and human evaluation.",,,,ACL
601,2019,Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation,"Ning Dai, Jianze Liang, Xipeng Qiu, Xuanjing Huang","Disentangling the content and style in the latent space is prevalent in unpaired text style transfer. However, two major issues exist in most of the current neural models. 1) It is difficult to completely strip the style information from the semantics for a sentence. 2) The recurrent neural network (RNN) based encoder and decoder, mediated by the latent representation, cannot well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content. In this paper, we propose the Style Transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation.",,,,ACL
602,2019,Generating Sentences from Disentangled Syntactic and Semantic Spaces,"Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou","Variational auto-encoders (VAEs) are widely used in natural language generation due to the regularization of the latent space. However, generating sentences from the continuous latent space does not explicitly model the syntactic information. In this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. Our proposed method explicitly models syntactic information in the VAE’s latent space by using the linearized tree sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntax transfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work.",,,,ACL
603,2019,Learning to Control the Fine-grained Sentiment for Story Ending Generation,"Fuli Luo, Damai Dai, Pengcheng Yang, Tianyu Liu, Baobao Chang","Automatic story ending generation is an interesting and challenging task in natural language generation. Previous studies are mainly limited to generate coherent, reasonable and diversified story endings, and few works focus on controlling the sentiment of story endings. This paper focuses on generating a story ending which meets the given fine-grained sentiment intensity. There are two major challenges to this task. First is the lack of story corpus which has fine-grained sentiment labels. Second is the difficulty of explicitly controlling sentiment intensity when generating endings. Therefore, we propose a generic and novel framework which consists of a sentiment analyzer and a sentimental generator, respectively addressing the two challenges. The sentiment analyzer adopts a series of methods to acquire sentiment intensities of the story dataset. The sentimental generator introduces the sentiment intensity into decoder via a Gaussian Kernel Layer to control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.",,,,ACL
604,2019,Self-Attention Architectures for Answer-Agnostic Neural Question Generation,"Thomas Scialom, Benjamin Piwowarski, Jacopo Staiano","Neural architectures based on self-attention, such as Transformers, recently attracted interest from the research community, and obtained significant improvements over the state of the art in several tasks. We explore how Transformers can be adapted to the task of Neural Question Generation without constraining the model to focus on a specific answer passage. We study the effect of several strategies to deal with out-of-vocabulary words such as copy mechanisms, placeholders, and contextual word embeddings. We report improvements obtained over the state-of-the-art on the SQuAD dataset according to automated metrics (BLEU, ROUGE), as well as qualitative human assessments of the system outputs.",,,,ACL
605,2019,Unsupervised Paraphrasing without Translation,"Aurko Roy, David Grangier","Paraphrasing is an important task demonstrating the ability to abstract semantic content from its surface form. Recent literature on automatic paraphrasing is dominated by methods leveraging machine translation as an intermediate step. This contrasts with humans, who can paraphrase without necessarily being bilingual. This work proposes to learn paraphrasing models only from a monolingual corpus. To that end, we propose a residual variant of vector-quantized variational auto-encoder. Our experiments consider paraphrase identification, and paraphrasing for training set augmentation, comparing to supervised and unsupervised translation-based approaches. Monolingual paraphrasing is shown to outperform unsupervised translation in all contexts. The comparison with supervised MT is more mixed: monolingual paraphrasing is interesting for identification and augmentation but supervised MT is superior for generation.",,,,ACL
606,2019,Storyboarding of Recipes: Grounded Contextual Generation,"Khyathi Chandu, Eric Nyberg, Alan W Black","Information need of humans is essentially multimodal in nature, enabling maximum exploitation of situated context. We introduce a dataset for sequential procedural (how-to) text generation from images in cooking domain. The dataset consists of 16,441 cooking recipes with 160,479 photos associated with different steps. We setup a baseline motivated by the best performing model in terms of human evaluation for the Visual Story Telling (ViST) task. In addition, we introduce two models to incorporate high level structure learnt by a Finite State Machine (FSM) in neural sequential generation process by: (1) Scaffolding Structure in Decoder (SSiD) (2) Scaffolding Structure in Loss (SSiL). Our best performing model (SSiL) achieves a METEOR score of 0.31, which is an improvement of 0.6 over the baseline model. We also conducted human evaluation of the generated grounded recipes, which reveal that 61% found that our proposed (SSiL) model is better than the baseline model in terms of overall recipes. We also discuss analysis of the output highlighting key important NLP issues for prospective directions.",,,,ACL
607,2019,Negative Lexically Constrained Decoding for Paraphrase Generation,Tomoyuki Kajiwara,"Paraphrase generation can be regarded as monolingual translation. Unlike bilingual machine translation, paraphrase generation rewrites only a limited portion of an input sentence. Hence, previous methods based on machine translation often perform conservatively to fail to make necessary rewrites. To solve this problem, we propose a neural model for paraphrase generation that first identifies words in the source sentence that should be paraphrased. Then, these words are paraphrased by the negative lexically constrained decoding that avoids outputting these words as they are. Experiments on text simplification and formality transfer show that our model improves the quality of paraphrasing by making necessary rewrites to an input sentence.",,,,ACL
608,2019,Large-Scale Transfer Learning for Natural Language Generation,"Sergey Golovanov, Rauf Kurbanov, Sergey Nikolenko, Kyryl Truskovskyi, Alexander Tselousov","Large-scale pretrained language models define state of the art in natural language processing, achieving outstanding performance on a variety of tasks. We study how these architectures can be applied and adapted for natural language generation, comparing a number of architectural and training schemes. We focus in particular on open-domain dialog as a typical high entropy generation task, presenting and comparing different architectures for adapting pretrained models with state of the art results.",,,,ACL
609,2019,Automatic Grammatical Error Correction for Sequence-to-sequence Text Generation: An Empirical Study,"Tao Ge, Xingxing Zhang, Furu Wei, Ming Zhou","Sequence-to-sequence (seq2seq) models have achieved tremendous success in text generation tasks. However, there is no guarantee that they can always generate sentences without grammatical errors. In this paper, we present a preliminary empirical study on whether and how much automatic grammatical error correction can help improve seq2seq text generation. We conduct experiments across various seq2seq text generation tasks including machine translation, formality style transfer, sentence compression and simplification. Experiments show the state-of-the-art grammatical error correction system can improve the grammaticality of generated text and can bring task-oriented improvements in the tasks where target sentences are in a formal style.",,,,ACL
610,2019,Improving the Robustness of Question Answering Systems to Question Paraphrasing,"Wee Chung Gan, Hwee Tou Ng","Despite the advancement of question answering (QA) systems and rapid improvements on held-out test sets, their generalizability is a topic of concern. We explore the robustness of QA models to question paraphrasing by creating two test sets consisting of paraphrased SQuAD questions. Paraphrased questions from the first test set are very similar to the original questions designed to test QA models’ over-sensitivity, while questions from the second test set are paraphrased using context words near an incorrect answer candidate in an attempt to confuse QA models. We show that both paraphrased test sets lead to significant decrease in performance on multiple state-of-the-art QA models. Using a neural paraphrasing model trained to generate multiple paraphrased questions for a given source question and a set of paraphrase suggestions, we propose a data augmentation approach that requires no human intervention to re-train the models for improved robustness to question paraphrasing.",,,,ACL
611,2019,RankQA: Neural Question Answering with Answer Re-Ranking,"Bernhard Kratzwald, Anna Eigenmann, Stefan Feuerriegel","The conventional paradigm in neural question answering (QA) for narrative content is limited to a two-stage process: first, relevant text passages are retrieved and, subsequently, a neural network for machine comprehension extracts the likeliest answer. However, both stages are largely isolated in the status quo and, hence, information from the two phases is never properly fused. In contrast, this work proposes RankQA: RankQA extends the conventional two-stage process in neural QA with a third stage that performs an additional answer re-ranking. The re-ranking leverages different features that are directly extracted from the QA pipeline, i.e., a combination of retrieval and comprehension features. While our intentionally simple design allows for an efficient, data-sparse estimation, it nevertheless outperforms more complex QA systems by a significant margin: in fact, RankQA achieves state-of-the-art performance on 3 out of 4 benchmark datasets. Furthermore, its performance is especially superior in settings where the size of the corpus is dynamic. Here the answer re-ranking provides an effective remedy against the underlying noise-information trade-off due to a variable corpus size. As a consequence, RankQA represents a novel, powerful, and thus challenging baseline for future research in content-based QA.",,,,ACL
612,2019,Latent Retrieval for Weakly Supervised Open Domain Question Answering,"Kenton Lee, Ming-Wei Chang, Kristina Toutanova","Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.",,,,ACL
613,2019,Multi-hop Reading Comprehension through Question Decomposition and Rescoring,"Sewon Min, Victor Zhong, Luke Zettlemoyer, Hannaneh Hajishirzi","Multi-hop Reading Comprehension (RC) requires reasoning and aggregation across several paragraphs. We propose a system for multi-hop RC that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models. Since annotations for such decomposition are expensive, we recast subquestion generation as a span prediction problem and show that our method, trained using only 400 labeled examples, generates sub-questions that are as effective as human-authored sub-questions. We also introduce a new global rescoring approach that considers each decomposition (i.e. the sub-questions and their answers) to select the best final answer, greatly improving overall performance. Our experiments on HotpotQA show that this approach achieves the state-of-the-art results, while providing explainable evidence for its decision making in the form of sub-questions.",,,,ACL
614,2019,Combining Knowledge Hunting and Neural Language Models to Solve the Winograd Schema Challenge,"Ashok Prakash, Arpit Sharma, Arindam Mitra, Chitta Baral","Winograd Schema Challenge (WSC) is a pronoun resolution task which seems to require reasoning with commonsense knowledge. The needed knowledge is not present in the given text. Automatic extraction of the needed knowledge is a bottleneck in solving the challenge. The existing state-of-the-art approach uses the knowledge embedded in their pre-trained language model. However, the language models only embed part of the knowledge, the ones related to frequently co-existing concepts. This limits the performance of such models on the WSC problems. In this work, we build-up on the language model based methods and augment them with a commonsense knowledge hunting (using automatic extraction from text) module and an explicit reasoning module. Our end-to-end system built in such a manner improves on the accuracy of two of the available language model based approaches by 5.53% and 7.7% respectively. Overall our system achieves the state-of-the-art accuracy of 71.06% on the WSC dataset, an improvement of 7.36% over the previous best.",,,,ACL
615,2019,Careful Selection of Knowledge to Solve Open Book Question Answering,"Pratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra, Chitta Baral","Open book question answering is a type of natural language based QA (NLQA) where questions are expected to be answered with respect to a given set of open book facts, and common knowledge about a topic. Recently a challenge involving such QA, OpenBookQA, has been proposed. Unlike most other NLQA that focus on linguistic understanding, OpenBookQA requires deeper reasoning involving linguistic understanding as well as reasoning with common knowledge. In this paper we address QA with respect to the OpenBookQA dataset and combine state of the art language models with abductive information retrieval (IR), information gain based re-ranking, passage selection and weighted scoring to achieve 72.0% accuracy, an 11.6% improvement over the current state of the art.",,,,ACL
616,2019,Learning Representation Mapping for Relation Detection in Knowledge Base Question Answering,"Peng Wu, Shujian Huang, Rongxiang Weng, Zaixiang Zheng, Jianbing Zhang","Relation detection is a core step in many natural language process applications including knowledge base question answering. Previous efforts show that single-fact questions could be answered with high accuracy. However, one critical problem is that current approaches only get high accuracy for questions whose relations have been seen in the training data. But for unseen relations, the performance will drop rapidly. The main reason for this problem is that the representations for unseen relations are missing. In this paper, we propose a simple mapping method, named representation adapter, to learn the representation mapping for both seen and unseen relations based on previously learned relation embedding. We employ the adversarial objective and the reconstruction objective to improve the mapping performance. We re-organize the popular SimpleQuestion dataset to reveal and evaluate the problem of detecting unseen relations. Experiments show that our method can greatly improve the performance of unseen relations while the performance for those seen part is kept comparable to the state-of-the-art.",,,,ACL
617,2019,Dynamically Fused Graph Network for Multi-hop Reasoning,"Lin Qiu, Yunxuan Xiao, Yanru Qu, Hao Zhou, Lei Li","Text-based question answering (TBQA) has been studied extensively in recent years. Most existing approaches focus on finding the answer to a question within a single paragraph. However, many difficult questions require multiple supporting evidence from scattered text among two or more documents. In this paper, we propose Dynamically Fused Graph Network (DFGN), a novel method to answer those questions requiring multiple scattered evidence and reasoning over them. Inspired by human’s step-by-step reasoning behavior, DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query, explores along the entity graph dynamically built from the text, and gradually finds relevant supporting entities from the given documents. We evaluate DFGN on HotpotQA, a public TBQA dataset requiring multi-hop reasoning. DFGN achieves competitive results on the public board. Furthermore, our analysis shows DFGN produces interpretable reasoning chains.",,,,ACL
618,2019,NLProlog: Reasoning with Weak Unification for Question Answering in Natural Language,"Leon Weber, Pasquale Minervini, Jannes Münchmeyer, Ulf Leser, Tim Rocktäschel","Rule-based models are attractive for various tasks because they inherently lead to interpretable and explainable decisions and can easily incorporate prior knowledge. However, such systems are difficult to apply to problems involving natural language, due to its large linguistic variability. In contrast, neural models can cope very well with ambiguity by learning distributed representations of words and their composition from data, but lead to models that are difficult to interpret. In this paper, we describe a model combining neural networks with logic programming in a novel manner for solving multi-hop reasoning tasks over natural language. Specifically, we propose to use an Prolog prover which we extend to utilize a similarity function over pretrained sentence encoders. We fine-tune the representations for the similarity function via backpropagation. This leads to a system that can apply rule-based reasoning to natural language, and induce domain-specific natural language rules from training data. We evaluate the proposed system on two different question answering tasks, showing that it outperforms two baselines – BiDAF (Seo et al., 2016a) and FastQA( Weissenborn et al., 2017) on a subset of the WikiHop corpus and achieves competitive results on the MedHop data set (Welbl et al., 2017).",,,,ACL
619,2019,Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions,"Jierui Li, Lei Wang, Jipeng Zhang, Yan Wang, Bing Tian Dai","Several deep learning models have been proposed for solving math word problems (MWPs) automatically. Although these models have the ability to capture features without manual efforts, their approaches to capturing features are not specifically designed for MWPs. To utilize the merits of deep learning models with simultaneous consideration of MWPs’ specific features, we propose a group attention mechanism to extract global features, quantity-related features, quantity-pair features and question-related features in MWPs respectively. The experimental results show that the proposed approach performs significantly better than previous state-of-the-art methods, and boost performance from 66.9% to 69.5% on Math23K with training-test split, from 65.8% to 66.9% on Math23K with 5-fold cross-validation and from 69.2% to 76.1% on MAWPS.",,,,ACL
620,2019,Synthetic QA Corpora Generation with Roundtrip Consistency,"Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, Michael Collins","We introduce a novel method of generating synthetic question answering corpora by combining models of question generation and answer extraction, and by filtering the results to ensure roundtrip consistency. By pretraining on the resulting corpora we obtain significant improvements on SQuAD2 and NQ, establishing a new state-of-the-art on the latter. Our synthetic data generation models, for both question generation and answer extraction, can be fully reproduced by finetuning a publicly available BERT model on the extractive subsets of SQuAD2 and NQ. We also describe a more powerful variant that does full sequence-to-sequence pretraining for question generation, obtaining exact match and F1 at less than 0.1% and 0.4% from human performance on SQuAD2.",,,,ACL
621,2019,Are Red Roses Red? Evaluating Consistency of Question-Answering Models,"Marco Tulio Ribeiro, Carlos Guestrin, Sameer Singh","Although current evaluation of question-answering systems treats predictions in isolation, we need to consider the relationship between predictions to measure true understanding. A model should be penalized for answering “no” to “Is the rose red?” if it answers “red” to “What color is the rose?”. We propose a method to automatically extract such implications for instances from two QA datasets, VQA and SQuAD, which we then use to evaluate the consistency of models. Human evaluation shows these generated implications are well formed and valid. Consistency evaluation provides crucial insights into gaps in existing models, while retraining with implication-augmented data improves consistency on both synthetic and human-generated implications.",,,,ACL
622,2019,MCˆ2: Multi-perspective Convolutional Cube for Conversational Machine Reading Comprehension,Xuanyu Zhang,"Conversational machine reading comprehension (CMRC) extends traditional single-turn machine reading comprehension (MRC) by multi-turn interactions, which requires machines to consider the history of conversation. Most of models simply combine previous questions for conversation understanding and only employ recurrent neural networks (RNN) for reasoning. To comprehend context profoundly and efficiently from different perspectives, we propose a novel neural network model, Multi-perspective Convolutional Cube (MCˆ2). We regard each conversation as a cube. 1D and 2D convolutions are integrated with RNN in our model. To avoid models previewing the next turn of conversation, we also extend causal convolution partially to 2D. Experiments on the Conversational Question Answering (CoQA) dataset show that our model achieves state-of-the-art results.",,,,ACL
623,2019,Reducing Word Omission Errors in Neural Machine Translation: A Contrastive Learning Approach,"Zonghan Yang, Yong Cheng, Yang Liu, Maosong Sun","While neural machine translation (NMT) has achieved remarkable success, NMT systems are prone to make word omission errors. In this work, we propose a contrastive learning approach to reducing word omission errors in NMT. The basic idea is to enable the NMT model to assign a higher probability to a ground-truth translation and a lower probability to an erroneous translation, which is automatically constructed from the ground-truth translation by omitting words. We design different types of negative examples depending on the number of omitted words, word frequency, and part of speech. Experiments on Chinese-to-English, German-to-English, and Russian-to-English translation tasks show that our approach is effective in reducing word omission errors and achieves better translation performance than three baseline methods.",,,,ACL
624,2019,Exploiting Sentential Context for Neural Machine Translation,"Xing Wang, Zhaopeng Tu, Longyue Wang, Shuming Shi","In this work, we present novel approaches to exploit sentential context for neural machine translation (NMT). Specifically, we show that a shallow sentential context extracted from the top encoder layer only, can improve translation performance via contextualizing the encoding representations of individual words. Next, we introduce a deep sentential context, which aggregates the sentential context representations from all of the internal layers of the encoder to form a more comprehensive context representation. Experimental results on the WMT14 English-German and English-French benchmarks show that our model consistently improves performance over the strong Transformer model, demonstrating the necessity and effectiveness of exploiting sentential context for NMT.",,,,ACL
625,2019,Wetin dey with these comments? Modeling Sociolinguistic Factors Affecting Code-switching Behavior in Nigerian Online Discussions,"Innocent Ndubuisi-Obi, Sayan Ghosh, David Jurgens","Multilingual individuals code switch between languages as a part of a complex communication process. However, most computational studies have examined only one or a handful of contextual factors predictive of switching. Here, we examine Naija-English code switching in a rich contextual environment to understand the social and topical factors eliciting a switch. We introduce a new corpus of 330K articles and accompanying 389K comments labeled for code switching behavior. In modeling whether a comment will switch, we show that topic-driven variation, tribal affiliation, emotional valence, and audience design all play complementary roles in behavior.",,,,ACL
626,2019,Accelerating Sparse Matrix Operations in Neural Networks on Graphics Processing Units,"Arturo Argueta, David Chiang","Graphics Processing Units (GPUs) are commonly used to train and evaluate neural networks efficiently. While previous work in deep learning has focused on accelerating operations on dense matrices/tensors on GPUs, efforts have concentrated on operations involving sparse data structures. Operations using sparse structures are common in natural language models at the input and output layers, because these models operate on sequences over discrete alphabets. We present two new GPU algorithms: one at the input layer, for multiplying a matrix by a few-hot vector (generalizing the more common operation of multiplication by a one-hot vector) and one at the output layer, for a fused softmax and top-N selection (commonly used in beam search). Our methods achieve speedups over state-of-the-art parallel GPU baselines of up to 7x and 50x, respectively. We also illustrate how our methods scale on different GPU architectures.",,,,ACL
627,2019,An Automated Framework for Fast Cognate Detection and Bayesian Phylogenetic Inference in Computational Historical Linguistics,"Taraka Rama, Johann-Mattis List","We present a fully automated workflow for phylogenetic reconstruction on large datasets, consisting of two novel methods, one for fast detection of cognates and one for fast Bayesian phylogenetic inference. Our results show that the methods take less than a few minutes to process language families that have so far required large amounts of time and computational power. Moreover, the cognates and the trees inferred from the method are quite close, both to gold standard cognate judgments and to expert language family trees. Given its speed and ease of application, our framework is specifically useful for the exploration of very large datasets in historical linguistics.",,,,ACL
628,2019,Sentence Centrality Revisited for Unsupervised Summarization,"Hao Zheng, Mirella Lapata","Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a) we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.",,,,ACL
629,2019,Discourse Representation Parsing for Sentences and Documents,"Jiangming Liu, Shay B. Cohen, Mirella Lapata",We introduce a novel semantic parsing task based on Discourse Representation Theory (DRT; Kamp and Reyle 1993). Our model operates over Discourse Representation Tree Structures which we formally define for sentences and documents. We present a general framework for parsing discourse structures of arbitrary length and granularity. We achieve this with a neural model equipped with a supervised hierarchical attention mechanism and a linguistically-motivated copy strategy. Experimental results on sentence- and document-level benchmarks show that our model outperforms competitive baselines by a wide margin.,,,,ACL
630,2019,Inducing Document Structure for Aspect-based Summarization,"Lea Frermann, Alexandre Klementiev","Automatic summarization is typically treated as a 1-to-1 mapping from document to summary. Documents such as news articles, however, are structured and often cover multiple topics or aspects; and readers may be interested in only some of them. We tackle the task of aspect-based summarization, where, given a document and a target aspect, our models generate a summary centered around the aspect. We induce latent document structure jointly with an abstractive summarization objective, and train our models in a scalable synthetic setup. In addition to improvements in summarization over topic-agnostic baselines, we demonstrate the benefit of the learnt document structure: we show that our models (a) learn to accurately segment documents by aspect; (b) can leverage the structure to produce both abstractive and extractive aspect-based summaries; and (c) that structure is particularly advantageous for summarizing long documents. All results transfer from synthetic training documents to natural news articles from CNN/Daily Mail and RCV1.",,,,ACL
631,2019,Incorporating Priors with Feature Attribution on Text Classification,"Frederick Liu, Besim Avci","Feature attribution methods, proposed recently, help users interpret the predictions of complex models. Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building. To demonstrate the effectiveness our technique, we apply it to two tasks: (1) mitigating unintended bias in text classifiers by neutralizing identity terms; (2) improving classifier performance in scarce data setting by forcing model to focus on toxic terms. Our approach adds an L2 distance loss between feature attributions and task-specific prior values to the objective. Our experiments show that i) a classifier trained with our technique reduces undesired model biases without a tradeoff on the original task; ii) incorporating prior helps model performance in scarce data settings.",,,,ACL
632,2019,Matching Article Pairs with Graphical Decomposition and Convolutions,"Bang Liu, Di Niu, Haojie Wei, Jinghong Lin, Yancheng He","Identifying the relationship between two articles, e.g., whether two articles published from different sources describe the same breaking news, is critical to many document understanding tasks. Existing approaches for modeling and matching sentence pairs do not perform well in matching longer documents, which embody more complex interactions between the enclosed entities than a sentence does. To model article pairs, we propose the Concept Interaction Graph to represent an article as a graph of concepts. We then match a pair of articles by comparing the sentences that enclose the same concept vertex through a series of encoding techniques, and aggregate the matching signals through a graph convolutional network. To facilitate the evaluation of long article matching, we have created two datasets, each consisting of about 30K pairs of breaking news articles covering diverse topics in the open domain. Extensive evaluations of the proposed methods on the two datasets demonstrate significant improvements over a wide range of state-of-the-art methods for natural language matching.",,,,ACL
633,2019,Hierarchical Transfer Learning for Multi-label Text Classification,"Siddhartha Banerjee, Cem Akkaya, Francisco Perez-Sorrosal, Kostas Tsioutsiouliklis","Multi-Label Hierarchical Text Classification (MLHTC) is the task of categorizing documents into one or more topics organized in an hierarchical taxonomy. MLHTC can be formulated by combining multiple binary classification problems with an independent classifier for each category. We propose a novel transfer learning based strategy, HTrans, where binary classifiers at lower levels in the hierarchy are initialized using parameters of the parent classifier and fine-tuned on the child category classification task. In HTrans, we use a Gated Recurrent Unit (GRU)-based deep learning architecture coupled with attention. Compared to binary classifiers trained from scratch, our HTrans approach results in significant improvements of 1% on micro-F1 and 3% on macro-F1 on the RCV1 dataset. Our experiments also show that binary classifiers trained from scratch are significantly better than single multi-label models.",,,,ACL
634,2019,Bias Analysis and Mitigation in the Evaluation of Authorship Verification,"Janek Bevendorff, Matthias Hagen, Benno Stein, Martin Potthast","The PAN series of shared tasks is well known for its continuous and high quality research in the field of digital text forensics. Among others, PAN contributions include original corpora, tailored benchmarks, and standardized experimentation platforms. In this paper we review, theoretically and practically, the authorship verification task and conclude that the underlying experiment design cannot guarantee pushing forward the state of the art—in fact, it allows for top benchmarking with a surprisingly straightforward approach. In this regard, we present a “Basic and Fairly Flawed” (BAFF) authorship verifier that is on a par with the best approaches submitted so far, and that illustrates sources of bias that should be eliminated. We pinpoint these sources in the evaluation chain and present a refined authorship corpus as effective countermeasure.",,,,ACL
635,2019,Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments,"Chung-Chi Chen, Hen-Hsen Huang, Hiroya Takamura, Hsin-Hsi Chen","In this paper, we attempt to answer the question of whether neural network models can learn numeracy, which is the ability to predict the magnitude of a numeral at some specific position in a text description. A large benchmark dataset, called Numeracy-600K, is provided for the novel task. We explore several neural network models including CNN, GRU, BiGRU, CRNN, CNN-capsule, GRU-capsule, and BiGRU-capsule in the experiments. The results show that the BiGRU model gets the best micro-averaged F1 score of 80.16%, and the GRU-capsule model gets the best macro-averaged F1 score of 64.71%. Besides discussing the challenges through comprehensive experiments, we also present an important application scenario, i.e., detecting exaggerated information, for the task.",,,,ACL
636,2019,Large-Scale Multi-Label Text Classification on EU Legislation,"Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, Ion Androutsopoulos","We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, annotated with ∼4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass BERT’s maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases.",,,,ACL
637,2019,Why Didn’t You Listen to Me? Comparing User Control of Human-in-the-Loop Topic Models,"Varun Kumar, Alison Smith-Renner, Leah Findlater, Kevin Seppi, Jordan Boyd-Graber","To address the lack of comparative evaluation of Human-in-the-Loop Topic Modeling (HLTM) systems, we implement and evaluate three contrasting HLTM modeling approaches using simulation experiments. These approaches extend previously proposed frameworks, including constraints and informed prior-based methods. Users should have a sense of control in HLTM systems, so we propose a control metric to measure whether refinement operations’ results match users’ expectations. Informed prior-based methods provide better control than constraints, but constraints yield higher quality topics.",,,,ACL
638,2019,Encouraging Paragraph Embeddings to Remember Sentence Identity Improves Classification,"Tu Vu, Mohit Iyyer","While paragraph embedding models are remarkably effective for downstream classification tasks, what they learn and encode into a single vector remains opaque. In this paper, we investigate a state-of-the-art paragraph embedding method proposed by Zhang et al. (2017) and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstruction-based objective of Zhang et al. (2017) with our sentence content probe objective in a semi-supervised setting. Despite its simplicity, our objective improves over paragraph reconstruction in terms of (1) downstream classification accuracies on benchmark datasets, (2) faster training, and (3) better generalization ability.",,,,ACL
639,2019,A Multi-Task Architecture on Relevance-based Neural Query Translation,"Sheikh Muhammad Sarwar, Hamed Bonab, James Allan","We describe a multi-task learning approach to train a Neural Machine Translation (NMT) model with a Relevance-based Auxiliary Task (RAT) for search query translation. The translation process for Cross-lingual Information Retrieval (CLIR) task is usually treated as a black box and it is performed as an independent step. However, an NMT model trained on sentence-level parallel data is not aware of the vocabulary distribution of the retrieval corpus. We address this problem and propose a multi-task learning architecture that achieves 16% improvement over a strong baseline on Italian-English query-document dataset. We show using both quantitative and qualitative analysis that our model generates balanced and precise translations with the regularization effect it achieves from multi-task learning paradigm.",,,,ACL
640,2019,Topic Modeling with Wasserstein Autoencoders,"Feng Nan, Ran Ding, Ramesh Nallapati, Bing Xiang","We propose a novel neural topic model in the Wasserstein autoencoders (WAE) framework. Unlike existing variational autoencoder based models, we directly enforce Dirichlet prior on the latent document-topic vectors. We exploit the structure of the latent space and apply a suitable kernel in minimizing the Maximum Mean Discrepancy (MMD) to perform distribution matching. We discover that MMD performs much better than the Generative Adversarial Network (GAN) in matching high dimensional Dirichlet distribution. We further discover that incorporating randomness in the encoder output during training leads to significantly more coherent topics. To measure the diversity of the produced topics, we propose a simple topic uniqueness metric. Together with the widely used coherence measure NPMI, we offer a more wholistic evaluation of topic quality. Experiments on several real datasets show that our model produces significantly better topics than existing topic models.",,,,ACL
641,2019,Dense Procedure Captioning in Narrated Instructional Videos,"Botian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen","Understanding narrated instructional videos is important for both research and real-world web applications. Motivated by video dense captioning, we propose a model to generate procedure captions from narrated instructional videos which are a sequence of step-wise clips with description. Previous works on video dense captioning learn video segments and generate captions without considering transcripts. We argue that transcripts in narrated instructional videos can enhance video representation by providing fine-grained complimentary and semantic textual information. In this paper, we introduce a framework to (1) extract procedures by a cross-modality module, which fuses video content with the entire transcript; and (2) generate captions by encoding video frames as well as a snippet of transcripts within each extracted procedure. Experiments show that our model can achieve state-of-the-art performance in procedure extraction and captioning, and the ablation studies demonstrate that both the video frames and the transcripts are important for the task.",,,,ACL
642,2019,Latent Variable Model for Multi-modal Translation,"Iacer Calixto, Miguel Rios, Wilker Aziz","In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation (MMT) through a latent variable model. This latent variable can be seen as a multi-modal stochastic embedding of an image and its description in a foreign language. It is used in a target-language decoder and also to predict image features. Importantly, our model formulation utilises visual and textual inputs during training but does not require that images be available at test time. We show that our latent variable MMT formulation improves considerably over strong baselines, including a multi-task learning approach (Elliott and Kadar, 2017) and a conditional variational auto-encoder approach (Toyama et al., 2016). Finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a constraint on the KL term to promote models with non-negligible mutual information between inputs and latent variable, and (iii) by training on additional target-language image descriptions (i.e. synthetic data).",,,,ACL
643,2019,Identifying Visible Actions in Lifestyle Vlogs,"Oana Ignat, Laura Burdick, Jia Deng, Rada Mihalcea","We consider the task of identifying human actions visible in online videos. We focus on the widely spread genre of lifestyle vlogs, which consist of videos of people performing actions while verbally describing them. Our goal is to identify if actions mentioned in the speech description of a video are visually present. We construct a dataset with crowdsourced manual annotations of visible actions, and introduce a multimodal algorithm that leverages information derived from visual and linguistic clues to automatically infer which actions are visible in a video.",,,,ACL
644,2019,A Corpus for Reasoning about Natural Language Grounded in Photographs,"Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai","We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.",,,,ACL
645,2019,"Learning to Discover, Ground and Use Words with Segmental Neural Language Models","Kazuya Kawakami, Chris Dyer, Phil Blunsom","We propose a segmental neural language model that combines the generalization power of neural networks with the ability to discover word-like units that are latent in unsegmented character sequences. In contrast to previous segmentation models that treat word segmentation as an isolated task, our model unifies word discovery, learning how words fit together to form sentences, and, by conditioning the model on visual context, how words’ meanings ground in representations of nonlinguistic modalities. Experiments show that the unconditional model learns predictive distributions better than character LSTM models, discovers words competitively with nonparametric Bayesian word segmentation models, and that modeling language conditional on visual context improves performance on both.",,,,ACL
646,2019,What Should I Ask? Using Conversationally Informative Rewards for Goal-oriented Visual Dialog.,"Pushkar Shukla, Carlos Elmadjian, Richika Sharan, Vivek Kulkarni, Matthew Turk","The ability to engage in goal-oriented conversations has allowed humans to gain knowledge, reduce uncertainty, and perform tasks more efficiently. Artificial agents, however, are still far behind humans in having goal-driven conversations. In this work, we focus on the task of goal-oriented visual dialogue, aiming to automatically generate a series of questions about an image with a single objective. This task is challenging since these questions must not only be consistent with a strategy to achieve a goal, but also consider the contextual information in the image. We propose an end-to-end goal-oriented visual dialogue system, that combines reinforcement learning with regularized information gain. Unlike previous approaches that have been proposed for the task, our work is motivated by the Rational Speech Act framework, which models the process of human inquiry to reach a goal. We test the two versions of our model on the GuessWhat?! dataset, obtaining significant results that outperform the current state-of-the-art models in the task of generating questions to find an undisclosed object in an image.",,,,ACL
647,2019,Symbolic Inductive Bias for Visually Grounded Learning of Spoken Language,Grzegorz Chrupała,"A widespread approach to processing spoken language is to first automatically transcribe it into text. An alternative is to use an end-to-end approach: recent works have proposed to learn semantic embeddings of spoken language from images with spoken captions, without an intermediate transcription step. We propose to use multitask learning to exploit existing transcribed speech within the end-to-end setting. We describe a three-task architecture which combines the objectives of matching spoken captions with corresponding images, speech with text, and text with images. We show that the addition of the speech/text task leads to substantial performance improvements on image retrieval when compared to training the speech/image task in isolation. We conjecture that this is due to a strong inductive bias transcribed speech provides to the model, and offer supporting evidence for this.",,,,ACL
648,2019,Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog,"Zhe Gan, Yu Cheng, Ahmed Kholy, Linjie Li, Jingjing Liu","This paper presents a new model for visual dialog, Recurrent Dual Attention Network (ReDAN), using multi-step reasoning to answer a series of questions about an image. In each question-answering turn of a dialog, ReDAN infers the answer progressively through multiple reasoning steps. In each step of the reasoning process, the semantic representation of the question is updated based on the image and the previous dialog history, and the recurrently-refined representation is used for further reasoning in the subsequent step. On the VisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art of 64.47% NDCG score. Visualization on the reasoning process further demonstrates that ReDAN can locate context-relevant visual and textual clues via iterative refinement, which can lead to the correct answer step-by-step.",,,,ACL
649,2019,Lattice Transformer for Speech Translation,"Pei Zhang, Niyu Ge, Boxing Chen, Kai Fan","Recent advances in sequence modeling have highlighted the strengths of the transformer architecture, especially in achieving state-of-the-art machine translation results. However, depending on the up-stream systems, e.g., speech recognition, or word segmentation, the input to translation system can vary greatly. The goal of this work is to extend the attention mechanism of the transformer to naturally consume the lattice in addition to the traditional sequential input. We first propose a general lattice transformer for speech translation where the input is the output of the automatic speech recognition (ASR) which contains multiple paths and posterior scores. To leverage the extra information from the lattice structure, we develop a novel controllable lattice attention mechanism to obtain latent representations. On the LDC Spanish-English speech translation corpus, our experiments show that lattice transformer generalizes significantly better and outperforms both a transformer baseline and a lattice LSTM. Additionally, we validate our approach on the WMT 2017 Chinese-English translation task with lattice inputs from different BPE segmentations. In this task, we also observe the improvements over strong baselines.",,,,ACL
650,2019,Informative Image Captioning with External Sources of Information,"Sanqiang Zhao, Piyush Sharma, Tomer Levinboim, Radu Soricut","An image caption should fluently present the essential information in a given image, including informative, fine-grained entity mentions and the manner in which these entities interact. However, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important “informativeness” dimension. We present a mechanism for integrating image information together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the image in a fluent and informative manner. We introduce a multimodal, multi-encoder model based on Transformer that ingests both image features and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative.",,,,ACL
651,2019,CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication,"Jin-Hwa Kim, Nikita Kitaev, Xinlei Chen, Marcus Rohrbach, Byoung-Tak Zhang","In this work, we propose a goal-driven collaborative task that combines language, perception, and action. Specifically, we develop a Collaborative image-Drawing game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages exchanged between human players. We define protocols and metrics to evaluate learned agents in this testbed, highlighting the need for a novel “crosstalk” evaluation condition which pairs agents trained independently on disjoint subsets of the training data. We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans.",,,,ACL
652,2019,Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning,"Zhihao Fan, Zhongyu Wei, Siyuan Wang, Xuanjing Huang","Image Captioning aims at generating a short description for an image. Existing research usually employs the architecture of CNN-RNN that views the generation as a sequential decision-making process and the entire dataset vocabulary is used as decoding space. They suffer from generating high frequent n-gram with irrelevant words. To tackle this problem, we propose to construct an image-grounded vocabulary, based on which, captions are generated with limitation and guidance. In specific, a novel hierarchical structure is proposed to construct the vocabulary incorporating both visual information and relations among words. For generation, we propose a word-aware RNN cell incorporating vocabulary information into the decoding process directly. Reinforce algorithm is employed to train the generator using constraint vocabulary as action space. Experimental results on MS COCO and Flickr30k show the effectiveness of our framework compared to some state-of-the-art models.",,,,ACL
653,2019,Distilling Translations with Visual Awareness,"Julia Ive, Pranava Madhyastha, Lucia Specia","Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.",,,,ACL
654,2019,VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions,"Pranava Madhyastha, Josiah Wang, Lucia Specia","We address the task of evaluating image description generation systems. We propose a novel image-aware metric for this task: VIFIDEL. It estimates the faithfulness of a generated caption with respect to the content of the actual image, based on the semantic similarity between labels of objects depicted in images and words in the description. The metric is also able to take into account the relative importance of objects mentioned in human reference descriptions during evaluation. Even if these human reference descriptions are not available, VIFIDEL can still reliably evaluate system descriptions. The metric achieves high correlation with human judgments on two well-known datasets and is competitive with metrics that depend on and rely exclusively on human references.",,,,ACL
655,2019,Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation,"Ronghang Hu, Daniel Fried, Anna Rohrbach, Dan Klein, Trevor Darrell","Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.",,,,ACL
656,2019,Multimodal Transformer for Unaligned Multimodal Language Sequences,"Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency","Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.",,,,ACL
657,2019,"Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports","Baoyu Jing, Zeya Wang, Eric Xing","Chest X-Ray (CXR) images are commonly used for clinical screening and diagnosis. Automatically writing reports for these images can considerably lighten the workload of radiologists for summarizing descriptive findings and conclusive impressions. The complex structures between and within sections of the reports pose a great challenge to the automatic report generation. Specifically, the section Impression is a diagnostic summarization over the section Findings; and the appearance of normality dominates each section over that of abnormality. Existing studies rarely explore and consider this fundamental structure information. In this work, we propose a novel framework which exploits the structure information between and within report sections for generating CXR imaging reports. First, we propose a two-stage strategy that explicitly models the relationship between Findings and Impression. Second, we design a novel co-operative multi-agent system that implicitly captures the imbalanced distribution between abnormality and normality. Experiments on two CXR report datasets show that our method achieves state-of-the-art performance in terms of various evaluation metrics. Our results expose that the proposed approach is able to generate high-quality medical reports through integrating the structure information.",,,,ACL
658,2019,Visual Story Post-Editing,"Ting-Yao Hsu, Chieh-Yang Huang, Yen-Chia Hsu, Ting-Hao Huang","We introduce the first dataset for human edits of machine-generated visual stories and explore how these collected edits may be used for the visual story post-editing task. The dataset ,VIST-Edit, includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions. We establish baselines for the task, showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models. We also discuss the weak correlation between automatic evaluation scores and human ratings, motivating the need for new automatic metrics.",,,,ACL
659,2019,Multimodal Abstractive Summarization for How2 Videos,"Shruti Palaskar, Jindřich Libovický, Spandana Gella, Florian Metze","In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to “compress” text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU.",,,,ACL
660,2019,Learning to Relate from Captions and Bounding Boxes,"Sarthak Garg, Joel Ruben Antony Moniz, Anshu Aviral, Priyatham Bollimpalli","In this work, we propose a novel approach that predicts the relationships between various entities in an image in a weakly supervised manner by relying on image captions and object bounding box annotations as the sole source of supervision. Our proposed approach uses a top-down attention mechanism to align entities in captions to objects in the image, and then leverage the syntactic structure of the captions to align the relations. We use these alignments to train a relation classification network, thereby obtaining both grounded captions and dense relationships. We demonstrate the effectiveness of our model on the Visual Genome dataset by achieving a recall@50 of 15% and recall@100 of 25% on the relationships present in the image. We also show that the model successfully predicts relations that are not present in the corresponding captions.",,,,ACL
1,2020,Learning to Understand Child-directed and Adult-directed Speech,"Lieke Gelderloos,Grzegorz Chrupała,Afra Alishahi","Speech directed to children differs from adult-directed speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.",,,,ACL
2,2020,Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts,"Alex Rinaldi,Jean Fox Tree,Snigdha Chaturvedi","Accurately diagnosing depression is difficult– requiring time-intensive interviews, assessments, and analysis. Hence, automated methods that can assess linguistic patterns in these interviews could help psychiatric professionals make faster, more informed decisions about diagnosis. We propose JLPC, a model that analyzes interview transcripts to identify depression while jointly categorizing interview prompts into latent categories. This latent categorization allows the model to define high-level conversational contexts that influence patterns of language in depressed individuals. We show that the proposed model not only outperforms competitive baselines, but that its latent prompt categories provide psycholinguistic insights about depression.",,,,ACL
3,2020,Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling,"Zihan Liu,Genta Indra Winata,Peng Xu,Pascale Fung","As an essential task in task-oriented dialog systems, slot filling requires extensive training data in a certain domain. However, such data are not always available. Hence, cross-domain slot filling has naturally arisen to cope with this data scarcity problem. In this paper, we propose a Coarse-to-fine approach (Coach) for cross-domain slot filling. Our model first learns the general pattern of slot entities by detecting whether the tokens are slot entities or not. It then predicts the specific types for the slot entities. In addition, we propose a template regularization approach to improve the adaptation robustness by regularizing the representation of utterances based on utterance templates. Experimental results show that our model significantly outperforms state-of-the-art approaches in slot filling. Furthermore, our model can also be applied to the cross-domain named entity recognition task, and it achieves better adaptation performance than other existing baselines. The code is available at https://github.com/zliucr/coach.",,,,ACL
4,2020,Designing Precise and Robust Dialogue Response Evaluators,"Tianyu Zhao,Divesh Lala,Tatsuya Kawahara","Automatic dialogue response evaluator has been proposed as an alternative to automated metrics and human evaluation. However, existing automatic evaluators achieve only moderate correlation with human judgement and they are not robust. In this work, we propose to build a reference-free evaluator and exploit the power of semi-supervised training and pretrained (masked) language models. Experimental results demonstrate that the proposed evaluator achieves a strong correlation (> 0.6) with human judgement and generalizes robustly to diverse responses and corpora. We open-source the code and data in https://github.com/ZHAOTING/dialog-processing.",,,,ACL
5,2020,Dialogue State Tracking with Explicit Slot Connection Modeling,"Yawen Ouyang,Moxin Chen,Xinyu Dai,Yinggong Zhao","Recent proposed approaches have made promising progress in dialogue state tracking (DST). However, in multi-domain scenarios, ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains. To handle these phenomena, we propose a Dialogue State Tracking with Slot Connections (DST-SC) model to explicitly consider slot correlations across different domains. Given a target slot, the slot connecting mechanism in DST-SC can infer its source slot and copy the source slot value directly, thus significantly reducing the difficulty of learning and reasoning. Experimental results verify the benefits of explicit slot connection modeling, and our model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets.",,,,ACL
6,2020,Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy,"Xiexiong Lin,Weiyu Jian,Jianshan He,Taifeng Wang","Knowledge-driven conversation approaches have achieved remarkable research attention recently. However, generating an informative response with multiple relevant knowledge without losing fluency and coherence is still one of the main challenges. To address this issue, this paper proposes a method that uses recurrent knowledge interaction among response decoding steps to incorporate appropriate knowledge. Furthermore, we introduce a knowledge copy mechanism using a knowledge-aware pointer network to copy words from external knowledge according to knowledge attention distribution. Our joint neural conversation model which integrates recurrent Knowledge-Interaction and knowledge Copy (KIC) performs well on generating informative responses. Experiments demonstrate that our model with fewer parameters yields significant improvements over competitive baselines on two datasets Wizard-of-Wikipedia(average Bleu +87%; abs.: 0.034) and DuConv(average Bleu +20%; abs.: 0.047)) with different knowledge formats (textual & structured) and different languages (English & Chinese).",,,,ACL
7,2020,Guiding Variational Response Generator to Exploit Persona,"Bowen Wu,MengYuan Li,Zongsheng Wang,Yifu Chen","Leveraging persona information of users in Neural Response Generators (NRG) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years. Despite of the promising progress achieved by recent studies in this field, persona information tends to be incorporated into neural networks in the form of user embeddings, with the expectation that the persona can be involved via End-to-End learning. This paper proposes to adopt the personality-related characteristics of human conversations into variational response generators, by designing a specific conditional variational autoencoder based deep model with two new regularization terms employed to the loss function, so as to guide the optimization towards the direction of generating both persona-aware and relevant responses. Besides, to reasonably evaluate the performances of various persona modeling approaches, this paper further presents three direct persona-oriented metrics from different perspectives. The experimental results have shown that our proposed methodology can notably improve the performance of persona-aware response generation, and the metrics are reasonable to evaluate the results.",,,,ACL
8,2020,Large Scale Multi-Actor Generative Dialog Modeling,"Alex Boyd,Raul Puri,Mohammad Shoeybi,Mostofa Patwary","Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and engaging conversations with a user; however, they typically exhibit either inconsistent personality across conversations or the average personality of all users. This paper addresses these issues by controlling an agent’s persona upon generation via conditioning on prior conversations of a target actor. In doing so, we are able to utilize more abstract patterns within a person’s speech and better emulate them in generated responses. This work introduces the Generative Conversation Control model, an augmented and fine-tuned GPT-2 language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor’s persona. We introduce an accompanying data collection procedure to obtain 10.3M conversations from 6 months worth of Reddit comments. We demonstrate that scaling model sizes from 117M to 8.3B parameters yields an improvement from 23.14 to 13.14 perplexity on 1.7M held out Reddit conversations. Increasing model scale yielded similar improvements in human evaluations that measure preference of model samples to the held out target distribution in terms of realism (31% increased to 37% preference), style matching (37% to 42%), grammar and content quality (29% to 42%), and conversation coherency (32% to 40%). We find that conditionally modeling past conversations improves perplexity by 0.47 in automatic evaluations. Through human trials we identify positive trends between conditional modeling and style matching and outline steps to further improve persona control.",,,,ACL
9,2020,PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable,"Siqi Bao,Huang He,Fan Wang,Hua Wu","Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.",,,,ACL
10,2020,Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network,"Yangming Li,Kaisheng Yao,Libo Qin,Wanxiang Che","Data-driven approaches using neural networks have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. Prior works refer this to hallucination phenomenon. In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue act (DA) properly generated in output sentences. We propose Iterative Rectification Network (IRN) for improving general NLG systems to produce both correct and fluent responses. It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness.",,,,ACL
11,2020,Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations,"Samuel Coope,Tyler Farghly,Daniela Gerz,Ivan Vulić","We introduce Span-ConveRT, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task. This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as ConveRT (Henderson et al., 2019). We show that leveraging such knowledge in Span-ConveRT is especially useful for few-shot learning scenarios: we report consistent gains over 1) a span extractor that trains representations from scratch in the target domain, and 2) a BERT-based span extractor. In order to inspire more work on span extraction for the slot-filling task, we also release RESTAURANTS-8K, a new challenging data set of 8,198 utterances, compiled from actual conversations in the restaurant booking domain.",,,,ACL
12,2020,Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking,"Giovanni Campagna,Agata Foryciarz,Mehrad Moradshahi,Monica Lam",Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset. We improve the zero-shot learning state of the art on average across domains by 21%.,,,,ACL
13,2020,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"Shyh-Shiun Hung,Hen-Hsen Huang,Hsin-Hsi Chen","This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.",,,,ACL
14,2020,TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition,"Ruifang He,Jian Wang,Fengyu Guo,Yugui Han","Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argument-relation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.",,,,ACL
15,2020,A Study of Non-autoregressive Model for Sequence Generation,"Yi Ren,Jinglin Liu,Xu Tan,Zhou Zhao","Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS). With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others. In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer: (1) Why NAR models can catch up with AR models in some tasks but not all? (2) Why techniques like knowledge distillation and source-target alignment can help NAR models. Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens. To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks. We have several interesting findings: 1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least. 2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models. 3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models.",,,,ACL
16,2020,Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage,"Ashish V. Thapliyal,Radu Soricut","Cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-English languages by the trend of data-hungry models combined with the lack of non-English annotations. We investigate potential solutions for combining existing language-generation annotations in English with translation capabilities in order to create solutions at web-scale in both domain and language coverage. We describe an approach called Pivot-Language Generation Stabilization (PLuGS), which leverages directly at training time both existing English annotations (gold data) as well as their machine-translated versions (silver data); at run-time, it generates first an English caption and then a corresponding target-language caption. We show that PLuGS models outperform other candidate solutions in evaluations performed over 5 different target languages, under a large-domain testset using images from the Open Images dataset. Furthermore, we find an interesting effect where the English captions generated by the PLuGS models are better than the captions generated by the original, monolingual English model.",,,,ACL
17,2020,Fact-based Text Editing,"Hayate Iso,Chao Qiao,Hang Li","We propose a novel text editing task, referred to as fact-based text editing, in which the goal is to revise a given document to better describe the facts in a knowledge base (e.g., several triples). The task is important in practice because reflecting the truth is a common requirement in text editing. First, we propose a method for automatically generating a dataset for research on fact-based text editing, where each instance consists of a draft text, a revised text, and several facts represented in triples. We apply the method into two public table-to-text datasets, obtaining two new datasets consisting of 233k and 37k instances, respectively. Next, we propose a new neural network architecture for fact-based text editing, called FactEditor, which edits a draft text by referring to given facts using a buffer, a stream, and a memory. A straightforward approach to address the problem would be to employ an encoder-decoder model. Our experimental results on the two datasets show that FactEditor outperforms the encoder-decoder approach in terms of fidelity and fluency. The results also show that FactEditor conducts inference faster than the encoder-decoder approach.",,,,ACL
18,2020,Few-Shot NLG with Pre-Trained Language Model,"Zhiyu Chen,Harini Eavani,Wenhu Chen,Yinyin Liu","Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of few-shot natural language generation. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement. Our code and data can be found at https://github.com/czyssrs/Few-Shot-NLG",,,,ACL
19,2020,Fluent Response Generation for Conversational Question Answering,"Ashutosh Baheti,Alan Ritter,Kevin Small","Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our model’s scalability by conducting tests on the CoQA dataset. The code and data are available at https://github.com/abaheti95/QADialogSystem.",,,,ACL
20,2020,Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs,"Dong Bok Lee,Seanie Lee,Woo Tae Jeong,Donghwan Kim","One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation. An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts (e.g. Wikipedia). In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs given unstructured texts as contexts, while maximizing the mutual information between generated QA pairs to ensure their consistency. We validate our Information Maximizing Hierarchical Conditional Variational AutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the performance of the QA model (BERT-base) using only the generated QA pairs (QA-based evaluation) or by using both the generated and human-labeled pairs (semi-supervised learning) for training, against state-of-the-art baseline models. The results show that our model obtains impressive performance gains over all baselines on both tasks, using only a fraction of data for training.",,,,ACL
21,2020,Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction,"Zi Chai,Xiaojun Wan","Traditional Question Generation (TQG) aims to generate a question given an input passage and an answer. When there is a sequence of answers, we can perform Sequential Question Generation (SQG) to produce a series of interconnected questions. Since the frequently occurred information omission and coreference between questions, SQG is rather challenging. Prior works regarded SQG as a dialog generation task and recurrently produced each question. However, they suffered from problems caused by error cascades and could only capture limited context dependencies. To this end, we generate questions in a semi-autoregressive way. Our model divides questions into different groups and generates each group of them in parallel. During this process, it builds two graphs focusing on information from passages, answers respectively and performs dual-graph interaction to get information for generation. Besides, we design an answer-aware attention mechanism and the coarse-to-fine generation scenario. Experiments on our new dataset containing 81.9K questions show that our model substantially outperforms prior works.",,,,ACL
22,2020,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Tanya Goyal,Greg Durrett","Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly “reorder” the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.",,,,ACL
23,2020,Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders,"Yu Duan,Canwen Xu,Jiaxin Pei,Jialong Han","Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents. Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion. When a new condition added, these techniques require full retraining. In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE) towards flexible conditional text generation. PPVAE decouples the text generation module from the condition representation module to allow “one-to-many” conditional generation. When a fresh condition emerges, only a lightweight network needs to be trained and works as a plug-in for PPVAE, which is efficient and desirable for real-world applications. Extensive experiments demonstrate the superiority of PPVAE against the existing alternatives with better conditionality and diversity but less training effort.",,,,ACL
24,2020,Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order,"Yi Liao,Xin Jiang,Qun Liu","Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive permutated language model. One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. Besides, the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks.",,,,ACL
25,2020,Reverse Engineering Configurations of Neural Text Generation Models,"Yi Tay,Dara Bahri,Che Zheng,Clifford Brunk","Recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways. It is therefore desirable to develop a deeper understanding of the fundamental properties of such models. The study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area. To this end, the extent and degree to which these artifacts surface in generated text is still unclear. In the spirit of better understanding generative text models and their artifacts, we propose the new task of distinguishing which of several variants of a given model generated some piece of text. Specifically, we conduct an extensive suite of diagnostic tests to observe whether modeling choices (e.g., sampling methods, top-k probabilities, model architectures, etc.) leave detectable artifacts in the text they generate. Our key finding, which is backed by a rigorous set of experiments, is that such artifacts are present and that different modeling choices can be inferred by looking at generated text alone. This suggests that neural text generators may actually be more sensitive to various modeling choices than previously thought.",,,,ACL
26,2020,Review-based Question Generation with Adaptive Instance Transfer and Augmentation,"Qian Yu,Lidong Bing,Qiong Zhang,Wai Lam","While online reviews of products and services become an important information source, it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need. We propose to explore question generation as a new way of review information exploitation, namely generating questions that can be answered by the corresponding review sentences. One major challenge of this generation task is the lack of training data, i.e. explicit mapping relation between the user-posed questions and review sentences. To obtain proper training instances for the generation model, we propose an iterative learning framework with adaptive instance transfer and augmentation. To generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task.",,,,ACL
27,2020,TAG : Type Auxiliary Guiding for Code Comment Generation,"Ruichu Cai,Zhihao Liang,Boyan Xu,Zijian Li","Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc. However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information. In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node. Specifically, our framework is featured with a Type-associated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code. We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework. Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies.",,,,ACL
28,2020,Unsupervised Paraphrasing by Simulated Annealing,"Xianggen Liu,Lili Mou,Fandong Meng,Hao Zhou","We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing. We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases. UPSA searches the sentence space towards this objective by performing a sequence of local editing. We evaluate our approach on various datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter. Extensive results show that UPSA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations. Further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of UPSA.",,,,ACL
29,2020,A Joint Model for Document Segmentation and Segment Labeling,"Joe Barrow,Rajiv Jain,Vlad Morariu,Varun Manjunatha","Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.",,,,ACL
30,2020,Contextualized Weak Supervision for Text Classification,"Dheeraj Mekala,Jingbo Shang","Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification. Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus. This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner. This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized. Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained.",,,,ACL
31,2020,Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks,"Yufeng Zhang,Xueli Yu,Zeyu Cui,Shu Wu","Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN. We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document. Finally, the word nodes are aggregated as the document embedding. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.",,,,ACL
32,2020,Neural Topic Modeling with Bidirectional Adversarial Training,"Rui Wang,Xuemeng Hu,Deyu Zhou,Yulan He","Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6% is observed in accuracy.",,,,ACL
33,2020,Text Classification with Negative Supervision,"Sora Ohashi,Junya Takayama,Tomoyuki Kajiwara,Chenhui Chu","Advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks. However, the discrepancy between the semantic similarity of texts and labelling standards affects classifiers, i.e. leading to lower performance in cases where classifiers should assign different labels to semantically similar texts. To address this problem, we propose a simple multitask learning model that uses negative supervision. Specifically, our model encourages texts with different labels to have distinct representations. Comprehensive experiments show that our model outperforms the state-of-the-art pre-trained model on both single- and multi-label classifications, sentence and document classifications, and classifications in three different languages.",,,,ACL
34,2020,Content Word Aware Neural Machine Translation,"Kehai Chen,Rui Wang,Masao Utiyama,Eiichiro Sumita","Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word. However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words). To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance. Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT.",,,,ACL
35,2020,Evaluating Explanation Methods for Neural Machine Translation,"Jierui Li,Lemao Liu,Huayang Li,Guanlin Li","Recently many efforts have been devoted to interpreting the black-box NMT models, but little progress has been made on metrics to evaluate explanation methods. Word Alignment Error Rate can be used as such a metric that matches human understanding, however, it can not measure explanation methods on those target words that are not aligned to any source word. This paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint. To this end, it proposes a principled metric based on fidelity in regard to the predictive behavior of the NMT model. As the exact computation for this metric is intractable, we employ an efficient approach as its approximation. On six standard translation tasks, we quantitatively evaluate several explanation methods in terms of the proposed metric and we reveal some valuable findings for these explanation methods in our experiments.",,,,ACL
36,2020,Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation,"Junliang Guo,Linli Xu,Enhong Chen","The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model.",,,,ACL
37,2020,Learning Source Phrase Representations for Neural Machine Translation,"Hongfei Xu,Josef van Genabith,Deyi Xiong,Qiuhui Liu","The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT). Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (Tang et al., 2018). Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation (SMT) approach through the use of larger translation blocks (“phrases”) and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships. In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations. In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships. In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach. Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations.",,,,ACL
38,2020,Lipschitz Constrained Parameter Initialization for Deep Transformers,"Hongfei Xu,Qiuhui Liu,Josef van Genabith,Deyi Xiong","The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers. We then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence. In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization, deep Transformers with the original computation order can converge, and obtain significant BLEU improvements with up to 24 layers. In contrast to previous research which focuses on deep encoders, our approach additionally enables Transformers to also benefit from deep decoders.",,,,ACL
39,2020,Location Attention for Extrapolation to Longer Sequences,"Yann Dubois,Gautier Dagan,Dieuwke Hupkes,Elia Bruni","Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important and how one could hope to tackle it. We then focus on a specific type of extrapolation which is especially useful for natural language processing: generalization to sequences that are longer than the training ones. We hypothesize that models with a separate content- and location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.",,,,ACL
40,2020,Multiscale Collaborative Deep Models for Neural Machine Translation,"Xiangpeng Wei,Heng Yu,Yue Hu,Yue Zhang","Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials.",,,,ACL
41,2020,Norm-Based Curriculum Learning for Neural Machine Translation,"Xuebo Liu,Houtim Lai,Derek F. Wong,Lidia S. Chao","A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT’14 English-German and WMT’17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).",,,,ACL
42,2020,Opportunistic Decoding with Timely Correction for Simultaneous Translation,"Renjie Zheng,Mingbo Ma,Baigong Zheng,Kaibo Liu","Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.",,,,ACL
43,2020,A Formal Hierarchy of RNN Architectures,"William Merrill,Gail Weiss,Yoav Goldberg,Roy Schwartz","We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based on two formal properties: space complexity, which measures the RNN’s memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. We place several RNN variants within this hierarchy. For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016). We also show how these models’ expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions. Our results build on the theory of “saturated” RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy. We provide empirical results to support this conjecture. Experimental findings from training unsaturated networks on formal languages support this conjecture.",,,,ACL
44,2020,A Three-Parameter Rank-Frequency Relation in Natural Languages,"Chenchen Ding,Masao Utiyama,Eiichiro Sumita","We present that, the rank-frequency relation in textual data follows f ∝ r-𝛼(r+𝛾)-𝛽, where f is the token frequency and r is the rank by frequency, with (𝛼, 𝛽, 𝛾) as parameters. The formulation is derived based on the empirical observation that d2 (x+y)/dx2 is a typical impulse function, where (x,y)=(log r, log f). The formulation is the power law when 𝛽=0 and the Zipf–Mandelbrot law when 𝛼=0. We illustrate that 𝛼 is related to the analytic features of syntax and 𝛽+𝛾 to those of morphology in natural languages from an investigation of multilingual corpora.",,,,ACL
45,2020,Dice Loss for Data-imbalanced NLP Tasks,"Xiaoya Li,Xiaofei Sun,Yuxian Meng,Junjun Liang","Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sørensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.",,,,ACL
46,2020,Emergence of Syntax Needs Minimal Supervision,"Raphaël Bailly,Kata Gábor","This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance. Our approach originates in the observable structure of a corpus, which we use to define and isolate grammaticality (syntactic information) and meaning/pragmatics information. We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model.",,,,ACL
47,2020,Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in Japanese,"Tatsuki Kuribayashi,Takumi Ito,Jun Suzuki,Kentaro Inui","We examine a methodology using neural language models (LMs) for analyzing the word order of language. This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods. In this study, we explore whether the LM-based method is valid for analyzing the word order. As a case study, this study focuses on Japanese due to its complex and flexible word order. To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies. Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool. Finally, using the LM-based method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by large-scale experiments.",,,,ACL
48,2020,GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media,"Yi-Ju Lu,Cheng-Te Li","This paper solves the fake news detection problem under a more realistic scenario on social media. Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern. We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal. Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average. In addition, the case studies also show that GCAN can produce reasonable explanations.",,,,ACL
49,2020,Integrating Semantic and Structural Information with Graph Convolutional Network for Controversy Detection,"Lei Zhong,Juan Cao,Qiang Sheng,Junbo Guo","Identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views. However, existing methods fail to 1) effectively incorporate the semantic information from content-related posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set. To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional Network (TPC-GCN), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy detection. As to the third limitation, we extend our model to Disentangled TPC-GCN (DTPC-GCN), to disentangle topic-related and topic-unrelated features and then fuse dynamically. Extensive experiments on two real-world datasets demonstrate that our models outperform existing methods. Analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability.",,,,ACL
50,2020,Predicting the Topical Stance and Political Leaning of Media using Tweets,"Peter Stefanov,Kareem Darwish,Atanas Atanasov,Preslav Nakov","Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers. Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly. In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the target polarizing topic. We evaluate the model by comparing its predictions to gold labels from the Media Bias/Fact Check website, achieving 82.6% accuracy.",,,,ACL
51,2020,"Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora","Hila Gonen,Ganesh Jawahar,Djamé Seddah,Yoav Goldberg","The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science. This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large. However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results. We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word. The method is simple, interpretable and stable. We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew).",,,,ACL
52,2020,CDL: Curriculum Dual Learning for Emotion-Controllable Response Generation,"Lei Shen,Yang Feng","Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process. However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified. Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence. To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively. CDL utilizes two rewards focusing on emotion and content to improve the duality. Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions. Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.",,,,ACL
53,2020,Efficient Dialogue State Tracking by Selectively Overwriting Memory,"Sungdong Kim,Sohee Yang,Gyuwan Kim,Sang-Woo Lee","Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. This enhances the effectiveness of training and DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting. In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.",,,,ACL
54,2020,End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2,"Donghoon Ham,Jeong-Gwan Lee,Youngsoo Jang,Kee-Eung Kim","The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal. The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually. However, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system. On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus. This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response. In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above. In the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8).",,,,ACL
55,2020,Evaluating Dialogue Generation Systems via Response Selection,"Shiki Sato,Reina Akama,Hiroki Ouchi,Jun Suzuki","Existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation. We focus on evaluating response generation systems via response selection. To evaluate systems properly via response selection, we propose a method to construct response selection test sets with well-chosen false candidates. Specifically, we propose to construct test sets filtering out some types of false candidates: (i) those unrelated to the ground-truth response and (ii) those acceptable as appropriate responses. Through experiments, we demonstrate that evaluating systems via response selection with the test set developed by our method correlates more strongly with human evaluation, compared with widely used automatic evaluation metrics such as BLEU.",,,,ACL
56,2020,Gated Convolutional Bidirectional Attention-based Model for Off-topic Spoken Response Detection,"Yefei Zha,Ruobing Li,Hui Lin","Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system. In many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training. In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts. We introduce a new model, Gated Convolutional Bidirectional Attention-based Model (GCBiA), which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses, and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts. Moreover, a new negative sampling method is proposed to augment training data. Experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall, for both seen and unseen prompts.",,,,ACL
57,2020,Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment,"Yinpei Dai,Hangyu Li,Chengguang Tang,Yongbin Li","Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration. We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning. Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.",,,,ACL
58,2020,Learning to Tag OOV Tokens by Integrating Contextual Representation and Background Knowledge,"Keqing He,Yuanmeng Yan,Weiran Xu","Neural-based context-aware models for slot tagging have achieved state-of-the-art performance. However, the presence of OOV(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge. Besides, we use multi-level graph attention to explicitly model lexical relations. The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets.",,,,ACL
59,2020,Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition,"Ryuichi Takanobu,Runze Liang,Minlie Huang","Many studies have applied reinforcement learning to train a dialog policy and show great promise these years. One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. However, modeling a realistic user simulator is challenging. A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents. Two agents interact with each other and are jointly learned simultaneously. The method uses the actor-critic framework to facilitate pretraining and improve scalability. We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog. Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction.",,,,ACL
60,2020,Paraphrase Augmented Task-Oriented Dialog Generation,"Silin Gao,Yichi Zhang,Zhijian Ou,Zhou Yu","Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set. However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings. We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance. We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels. PARG is applicable to various dialog generation models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al., 2019). Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ. PARG also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings.",,,,ACL
61,2020,Response-Anticipated Memory for On-Demand Knowledge Integration in Response Generation,"Zhiliang Tian,Wei Bi,Dongkyu Lee,Lanqing Xue","Neural conversation models are known to generate appropriate but non-informative responses in general. A scenario where informativeness can be significantly enhanced is Conversing by Reading (CbR), where conversations take place with respect to a given external document. In previous work, the external document is utilized by (1) creating a context-aware document memory that integrates information from the document and the conversational context, and then (2) generating responses referring to the memory. In this paper, we propose to create the document memory with some anticipated responses in mind. This is achieved using a teacher-student framework. The teacher is given the external document, the context, and the ground-truth response, and learns how to build a response-aware document memory from three sources of information. The student learns to construct a response-anticipated document memory from the first two sources, and teacher’s insight on memory creation. Empirical results show that our model outperforms the previous state-of-the-art for the CbR task.",,,,ACL
62,2020,Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation,"Xinting Huang,Jianzhong Qi,Yu Sun,Rui Zhang","Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems. This is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogues. To address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards. This approach requires complete state-action annotations of human-to-human dialogues (i.e., expert demonstrations), which is labor intensive. To overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning. The proposed approach learns a dynamics model as the reward function which models dialogue progress (i.e., state-action sequences) based on expert demonstrations, either with or without annotations. The dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations. We further propose to learn action embeddings for a better generalization of the reward function. The proposed approach outperforms competitive policy learning baselines on MultiWOZ, a benchmark multi-domain dataset.",,,,ACL
63,2020,Towards Unsupervised Language Understanding and Generation by Joint Dual Learning,"Shang-Yu Su,Chao-Wei Huang,Yun-Nung Chen","In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations. However, the dual property between understanding and generation has been rarely explored. The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework. However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion. The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG. The source code is available at: https://github.com/MiuLab/DuaLUG.",,,,ACL
64,2020,USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation,"Shikib Mehri,Maxine Eskenazi","The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. Standard language generation metrics have been shown to be ineffective for evaluating dialog models. To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog. USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces interpretable measures for several desirable properties of dialog.",,,,ACL
65,2020,Explicit Semantic Decomposition for Definition Generation,"Jiahuan Li,Yu Bao,Shujian Huang,Xinyu Dai","Definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts. However, previous works hardly consider explicitly modeling the “components” of definitions, leading to under-specific generation results. In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition Generation, which explicitly decomposes the meaning of words into semantic components, and models them with discrete latent variables for definition generation. Experimental results show that achieves top results on WordNet and Oxford benchmarks, outperforming strong previous baselines.",,,,ACL
66,2020,Improved Natural Language Generation via Loss Truncation,"Daniel Kang,Tatsunori Hashimoto","Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss. While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts). Even a small fraction of noisy data can degrade the performance of log loss. As an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references. However, distinguishability has not been used in practice due to challenges in optimization and estimation. We propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability. Empirically, we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task. Furthermore, we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references.",,,,ACL
67,2020,Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks,"Yanbin Zhao,Lu Chen,Zhi Chen,Ruisheng Cao","Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. This work focuses on AMR-to-text generation – A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR). Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1) The message propagation process in AMR graphs is only guided by the first-order adjacency information. 2) The relationships between labeled edges are not fully considered. In this work, we propose a novel graph encoding framework which can effectively explore the edge relations. We also adopt graph attention networks with higher-order neighborhood information to encode the rich structure in AMR graphs. Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets. The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.",,,,ACL
68,2020,Rigid Formats Controlled Text Generation,"Piji Li,Haisong Zhang,Xiaojiang Liu,Shuming Shi","Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.",,,,ACL
69,2020,Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation,"Kaustubh Dhole,Christopher D. Manning","Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form. We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs. We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems. In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules. A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions.",,,,ACL
70,2020,An Online Semantic-enhanced Dirichlet Model for Short Text Stream Clustering,"Jay Kumar,Junming Shao,Salah Uddin,Wazir Ali","Clustering short text streams is a challenging task due to its unique properties: infinite length, sparse data representation and cluster evolution. Existing approaches often exploit short text streams in a batch way. However, determine the optimal batch size is usually a difficult task since we have no priori knowledge when the topics evolve. In addition, traditional independent word representation in graphical model tends to cause “term ambiguity” problem in short text clustering. Therefore, in this paper, we propose an Online Semantic-enhanced Dirichlet Model for short sext stream clustering, called OSDM, which integrates the word-occurance semantic information (i.e., context) into a new graphical model and clusters each arriving short text automatically in an online way. Extensive results have demonstrated that OSDM has better performance compared to many state-of-the-art algorithms on both synthetic and real-world data sets.",,,,ACL
71,2020,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Lin Zheng,Qinliang Su,Dinghan Shen,Changyou Chen","Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.",,,,ACL
72,2020,Interactive Construction of User-Centric Dictionary for Text Analytics,"Ryosuke Kohita,Issei Yoshida,Hiroshi Kanayama,Tetsuya Nasukawa","We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine, which helps the creation of flexible dictionaries with precise granularity required in typical text analysis. This paper introduces the first formulation of interactive dictionary construction to address this issue. To optimize the interaction, we propose a new algorithm that effectively captures an analyst’s intention starting from only a small number of sample terms. Along with the algorithm, we also design an automatic evaluation framework that provides a systematic assessment of any interactive method for the dictionary creation task. Experiments using real scenario based corpora and dictionaries show that our algorithm outperforms baseline methods, and works even with a small number of interactions.",,,,ACL
73,2020,Tree-Structured Neural Topic Model,"Masaru Isonuma,Junichiro Mori,Danushka Bollegala,Ichiro Sakata","This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks. With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010). This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks.",,,,ACL
74,2020,Unsupervised FAQ Retrieval with Question Generation and BERT,"Yosi Mass,Boaz Carmeli,Haggai Roitman,David Konopnicki","We focus on the task of Frequently Asked Questions (FAQ) retrieval. A given user query can be matched against the questions and/or the answers in the FAQ. We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models. The two models match user queries to FAQ answers and questions, respectively. We alleviate the missing labeled data of the latter by automatically generating high-quality question paraphrases. We show that our model is on par and even outperforms supervised models on existing datasets.",,,,ACL
75,2020,“The Boating Store Had Its Best Sail Ever”: Pronunciation-attentive Contextualized Pun Recognition,"Yichao Zhou,Jyun-Yu Jiang,Jieyu Zhao,Kai-Wei Chang","Humor plays an important role in human languages and it is essential to model humor when building intelligence systems. Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity. However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks. In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence. PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols. Extensive experiments are conducted on two benchmark datasets. Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks. In-depth analyses verify the effectiveness and robustness of PCPR.",,,,ACL
76,2020,Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning,"Joongbo Shin,Yoonhyung Lee,Seunghyun Yoon,Kyomin Jung","Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations. To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder (T-TA). The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of BERT. In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the BERT-like model on a reranking task and twelve times faster on a semantic similarity task. Furthermore, the T-TA shows competitive or even better accuracies than those of BERT on the above tasks. Code is available at https://github.com/joongbo/tta.",,,,ACL
77,2020,Fine-grained Interest Matching for Neural News Recommendation,"Heyuan Wang,Fangzhao Wu,Zheng Liu,Xing Xie","Personalized news recommendation is a critical technology to improve users’ online news reading experience. The core of news recommendation is accurate matching between user’s interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Fine-grained Interest Matching method for neural news recommendation. Instead of aggregating user’s all historical browsed news into a unified vector, we hierarchically construct multi-level representations for each news via stacked dilated convolutions. Then we perform fine-grained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.",,,,ACL
78,2020,Interpretable Operational Risk Classification with Semi-Supervised Variational Autoencoder,"Fan Zhou,Shengming Zhang,Yi Yang","Operational risk management is one of the biggest challenges nowadays faced by financial institutions. There are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled/unlabeled data and lacking interpretability. To tackle these challenges, we present a semi-supervised text classification framework that integrates multi-head attention mechanism with Semi-supervised variational inference for Operational Risk Classification (SemiORC). We empirically evaluate the framework on a real-world dataset. The results demonstrate that our method can better utilize unlabeled data and learn visually interpretable document representations. SemiORC also outperforms other baseline methods on operational risk classification.",,,,ACL
79,2020,Interpreting Twitter User Geolocation,"Ting Zhong,Tianliang Wang,Fan Zhou,Goce Trajcevski","Identifying user geolocation in online social networks is an essential task in many location-based applications. Existing methods rely on the similarity of text and network structure, however, they suffer from a lack of interpretability on the corresponding results, which is crucial for understanding model behavior. In this work, we adopt influence functions to interpret the behavior of GNN-based models by identifying the importance of training users when predicting the locations of the testing users. This methodology helps with providing meaningful explanations on prediction results. Furthermore, it also initiates an attempt to uncover the so-called “black-box” GNN-based models by investigating the effect of individual nodes.",,,,ACL
80,2020,Modeling Code-Switch Languages Using Bilingual Parallel Corpus,"Grandee Lee,Haizhou Li","Language modeling is the technique to estimate the probability of a sequence of words. A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages. We propose a bilingual attention language model (BALM) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency. The attention mechanism learns the bilingual context from a parallel corpus. BALM achieves state-of-the-art performance on the SEAME code-switch database by reducing the perplexity of 20.5% over the best-reported result. We also apply BALM in bilingual lexicon induction, and language normalization tasks to validate the idea.",,,,ACL
81,2020,SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check,"Xingyi Cheng,Weidi Xu,Kunlong Chen,Shaohua Jiang","Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language. Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters. However, they take the similarity knowledge as either an external input resource or just heuristic rules. This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN). The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers. These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable. Experiments are conducted on three human-annotated datasets. Our method achieves superior performance against previous models by a large margin.",,,,ACL
82,2020,Spelling Error Correction with Soft-Masked BERT,"Shaohua Zhang,Haoran Huang,Jicong Liu,Hang Li","Spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability. Without loss of generality we consider Chinese spelling error correction (CSC) in this paper. A state-of-the-art method for the task selects a character from a list of candidates for correction (including non-correction) at each position of the sentence on the basis of BERT, the language representation model. The accuracy of the method can be sub-optimal, however, because BERT does not have sufficient capability to detect whether there is an error at each position, apparently due to the way of pre-training it using mask language modeling. In this work, we propose a novel neural architecture to address the aforementioned issue, which consists of a network for error detection and a network for error correction based on BERT, with the former being connected to the latter with what we call soft-masking technique. Our method of using ‘Soft-Masked BERT’ is general, and it may be employed in other language detection-correction problems. Experimental results on two datasets, including one large dataset which we create and plan to release, demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT.",,,,ACL
83,2020,A Frame-based Sentence Representation for Machine Reading Comprehension,"Shaoru Guo,Ru Li,Hongye Tan,Xiaoli Li","Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC). MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge. To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling. Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models, which utilize both LUs in frame and Frame-to-Frame (F-to-F) relations, are designed to model frames and sentences with attention schema. Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations. Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task.",,,,ACL
84,2020,A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation,"Jan Deriu,Katsiaryna Mlynchyk,Philippe Schläpfer,Alvaro Rodrigo","In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT). This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of the tokens to the operations. Thus, we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens. We compare our corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance.",,,,ACL
85,2020,Contextualized Sparse Representations for Real-Time Open-Domain Question Answering,"Jinhyuk Lee,Minjoon Seo,Hannaneh Hajishirzi,Jaewoo Kang","Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models. In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space. By augmenting the previous phrase retrieval model (Seo et al., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open. Our CuratedTREC score is even better than the best known retrieve & read model with at least 45x faster inference speed.",,,,ACL
86,2020,Dynamic Sampling Strategies for Multi-Task Reading Comprehension,"Ananth Gottumukkala,Dheeru Dua,Sameer Singh,Matt Gardner","Building general reading comprehension systems, capable of solving multiple datasets at the same time, is a recent aspirational goal in the research community. Prior work has focused on model architecture or generalization to held out datasets, and largely passed over the particulars of the multi-task learning set up. We show that a simple dynamic sampling strategy, selecting instances for training proportional to the multi-task model’s current performance on a dataset relative to its single task performance, gives substantive gains over prior multi-task sampling strategies, mitigating the catastrophic forgetting that is common in multi-task learning. We also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear benefit in multitask performance over forcing task homogeneity at the epoch or batch level. Our final model shows greatly increased performance over the best model on ORB, a recently-released multitask reading comprehension benchmark.",,,,ACL
87,2020,Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension,"Fei Yuan,Linjun Shou,Xuanyu Bai,Ming Gong","Multilingual pre-trained models could leverage the training data from a rich source language (such as English) to improve performance on low resource languages. However, the transfer quality for multilingual Machine Reading Comprehension (MRC) is significantly worse than sentence classification tasks mainly due to the requirement of MRC to detect the word level answer boundary. In this paper, we propose two auxiliary tasks in the fine-tuning stage to create additional phrase boundary supervision: (1) A mixed MRC task, which translates the question or passage to other languages and builds cross-lingual question-passage pairs; (2) A language-agnostic knowledge masking task by leveraging knowledge phrases mined from web. Besides, extensive experiments on two cross-lingual MRC datasets show the effectiveness of our proposed approach.",,,,ACL
88,2020,Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading,"Yifan Gao,Chien-Sheng Wu,Shafiq Joty,Caiming Xiong","The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions. Existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them. In this paper, we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker (EMT) to track whether conditions listed in the rule text have already been satisfied to make a decision. Moreover, our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy, utilizing sentence-level entailment scores to weight token-level distributions. On the ShARC benchmark (blind, held-out) testset, EMT achieves new state-of-the-art results of 74.6% micro-averaged decision accuracy and 49.5 BLEU4. We also show that EMT is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows. Code and models are released at https://github.com/Yifan-Gao/explicit_memory_tracker.",,,,ACL
89,2020,Injecting Numerical Reasoning Skills into Language Models,"Mor Geva,Ankit Gupta,Jonathan Berant","Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 –> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.",,,,ACL
90,2020,Learning to Identify Follow-Up Questions in Conversational Question Answering,"Souvik Kundu,Qian Lin,Hwee Tou Ng","Despite recent progress in conversational question answering, most prior work does not focus on follow-up questions. Practical conversational question answering systems often receive follow-up questions in an ongoing conversation, and it is crucial for a system to be able to determine whether a question is a follow-up question of the current conversation, for more effective answer finding subsequently. In this paper, we introduce a new follow-up question identification task. We propose a three-way attentive pooling network that determines the suitability of a follow-up question by capturing pair-wise interactions between the associated passage, the conversation history, and a candidate follow-up question. It enables the model to capture topic continuity and topic shift while scoring a particular candidate follow-up question. Experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins.",,,,ACL
91,2020,Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases,"Yunshi Lan,Jing Jiang","Previous work on answering complex questions from knowledge bases usually separately addresses two types of complexity: questions with constraints and questions with multiple hops of relations. In this paper, we handle both types of complexity at the same time. Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs. Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets.",,,,ACL
92,2020,A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers,"Shen-yun Miao,Chao-Chun Liang,Keh-Yih Su","We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers. Existing MWP corpora for studying AI progress remain limited either in language usage patterns or in problem types. We thus present a new English MWP corpus with 2,305 MWPs that cover more text patterns and most problem types taught in elementary school. Each MWP is annotated with its problem type and grade level (for indicating the level of difficulty). Furthermore, we propose a metric to measure the lexicon usage diversity of a given MWP corpus, and demonstrate that ASDiv is more diverse than existing corpora. Experiments show that our proposed corpus reflects the true capability of MWP solvers more faithfully.",,,,ACL
93,2020,Improving Image Captioning Evaluation by Considering Inter References Variance,"Yanzhi Yi,Hangyu Deng,Jinglu Hu","Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image. Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions. It usually leads to over-penalization and thus a bad correlation to human judgment. Recently, the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance. In this paper, we propose a novel metric based on BERTScore that could handle such a challenge and extend BERTScore with a few new features appropriately for image captioning evaluation. The experimental results show that our metric achieves state-of-the-art human judgment correlation.",,,,ACL
94,2020,Revisiting the Context Window for Cross-lingual Word Embeddings,"Ryokan Ri,Yoshimasa Tsuruoka","Existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar. The structures of embedding spaces largely depend on the co-occurrence statistics of each word, which the choice of context window determines. Despite this obvious connection between the context window and mapping-based cross-lingual embeddings, their relationship has been underexplored in prior work. In this work, we provide a thorough evaluation, in various languages, domains, and tasks, of bilingual embeddings trained with different context windows. The highlight of our findings is that increasing the size of both the source and target window sizes improves the performance of bilingual lexicon induction, especially the performance on frequent nouns.",,,,ACL
95,2020,Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders,"Terra Blevins,Luke Zettlemoyer","A major obstacle in Word Sense Disambiguation (WSD) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training. We propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and (2) the dictionary definition, or gloss, of each sense. The encoders are jointly optimized in the same representation space, so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding. Our system outperforms previous state-of-the-art models on English all-words WSD; these gains predominantly come from improved performance on rare senses, leading to a 31.1% error reduction on less frequent senses over prior work. This demonstrates that rare senses can be more effectively disambiguated by modeling their definitions.",,,,ACL
96,2020,"Code-Switching Patterns Can Be an Effective Route to Improve Performance of Downstream NLP Applications: A Case Study of Humour, Sarcasm and Hate Speech Detection","Srijan Bansal,Vishal Garimella,Ayush Suhane,Jasabanta Patro","In this paper, we demonstrate how code-switching patterns can be utilised to improve various downstream NLP applications. In particular, we encode various switching features to improve humour, sarcasm and hate speech detection tasks. We believe that this simple linguistic observation can also be potentially helpful in improving other similar NLP applications.",,,,ACL
97,2020,DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim Verification,"Lianwei Wu,Yuan Rao,Yongqiang Zhao,Hao Liang","Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized. However, in these methods, the discovery process of evidence is nontransparent and unexplained. Simultaneously, the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims. In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification. Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way. Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim. Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by more than 3.11%, 2.41%, respectively.",,,,ACL
98,2020,Towards Conversational Recommendation over Multi-Type Dialogs,"Zeming Liu,Haifeng Wang,Zheng-Yu Niu,Hua Wu","We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user’s interests and feedback. To facilitate the study of this task, we create a human-to-human Chinese dialog dataset DuRecDial (about 10k dialogs, 156k utterances), where there are multiple sequential dialogs for a pair of a recommendation seeker (user) and a recommender (bot). In each dialog, the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. Finally we establish baseline results on DuRecDial for future studies.",,,,ACL
99,2020,Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification,"Guangfeng Yan,Lu Fan,Qimai Li,Han Liu","User intent classification plays a vital role in dialogue systems. Since user intent may frequently change over time in many realistic scenarios, unknown (new) intent detection has become an essential problem, where the study has just begun. This paper proposes a semantic-enhanced Gaussian mixture model (SEG) for unknown intent detection. In particular, we model utterance embeddings with a Gaussian mixture distribution and inject dynamic class semantic information into Gaussian means, which enables learning more class-concentrated embeddings that help to facilitate downstream outlier detection. Coupled with a density-based outlier detection algorithm, SEG achieves competitive results on three real task-oriented dialogue datasets in two languages for unknown intent detection. On top of that, we propose to integrate SEG as an unknown intent identifier into existing generalized zero-shot intent classification models to improve their performance. A case study on a state-of-the-art method, ReCapsNet, shows that SEG can push the classification performance to a significantly higher level.",,,,ACL
100,2020,Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen,"Yixin Cao,Ruihao Shui,Liangming Pan,Min-Yen Kan","The curse of knowledge can impede communication between experts and laymen. We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. Solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words. This is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures. We establish the benchmark performance of five state-of-the-art models for style transfer and text simplification. The results demonstrate a significant gap between machine and human performance. We also discuss the challenges of automatic evaluation, to provide insights into future research directions. The dataset is publicly available at https://srhthu.github.io/expertise-style-transfer/.",,,,ACL
101,2020,Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints,"Zhenyi Wang,Xiaoyang Wang,Bang An,Dong Yu","Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions. Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table. In this paper, for the first time, we propose a novel Transformer-based generation framework to achieve the goal. The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the Transformer model. Furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem. We also provide detailed analysis on each component of our model in our experiments. Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin.",,,,ACL
102,2020,Dynamic Memory Induction Networks for Few-Shot Text Classification,"Ruiying Geng,Binhua Li,Yongbin Li,Jian Sun","This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification. The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification. The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning. The proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the miniRCV1 and ODIC datasets. Detailed analysis is further performed to show how the proposed network achieves the new performance.",,,,ACL
103,2020,Exclusive Hierarchical Decoding for Deep Keyphrase Generation,"Wang Chen,Hou Pong Chan,Piji Li,Irwin King","Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases. A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce. Previous work in this setting employs a sequential decoding process to generate keyphrases. However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document. Moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources. To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism. The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set. Both the soft and the hard exclusion mechanisms keep track of previously-predicted keyphrases within a window size to enhance the diversity of the generated keyphrases. Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases.",,,,ACL
104,2020,Hierarchy-Aware Global Model for Hierarchical Text Classification,"Jie Zhou,Chunping Ma,Dingkun Long,Guangwei Xu","Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy. Existing methods have difficulties in modeling the hierarchical label structure in a global view. Furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space. In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies. Based on the hierarchy encoder, we propose a novel end-to-end hierarchy-aware global model (HiAGM) with two variants. A multi-label attention variant (HiAGM-LA) learns hierarchy-aware label embeddings through the hierarchy encoder and conducts inductive fusion of label-aware text features. A text feature propagation model (HiAGM-TP) is proposed as the deductive variant that directly feeds text features into hierarchy encoders. Compared with previous works, both HiAGM-LA and HiAGM-TP achieve significant and consistent improvements on three benchmark datasets.",,,,ACL
105,2020,Keyphrase Generation for Scientific Document Retrieval,"Florian Boudin,Ygor Gallina,Akiko Aizawa","Sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval. This study provides empirical evidence that such models can significantly improve retrieval performance, and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models. Using this framework, we point out and discuss the difficulties encountered with supplementing documents with -not present in text- keyphrases, and generalizing models across domains. Our code is available at https://github.com/boudinfl/ir-using-kg",,,,ACL
106,2020,A Graph Auto-encoder Model of Derivational Morphology,"Valentin Hofmann,Hinrich Schütze,Janet Pierrehumbert","There has been little work on modeling the morphological well-formedness (MWF) of derivatives, a problem judged to be complex and difficult in linguistics. We present a graph auto-encoder that learns embeddings capturing information about the compatibility of affixes and stems in derivation. The auto-encoder models MWF in English surprisingly well by combining syntactic and semantic information with associative information from the mental lexicon.",,,,ACL
107,2020,Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell,"Djamé Seddah,Farah Essaidi,Amal Fethi,Matthieu Futeral","We introduce the first treebank for a romanized user-generated content variety of Algerian, a North-African Arabic dialect known for its frequent usage of code-switching. Made of 1500 sentences, fully annotated in morpho-syntax and Universal Dependency syntax, with full translation at both the word and the sentence levels, this treebank is made freely available. It is supplemented with 50k unlabeled sentences collected from Common Crawl and web-crawled data using intensive data-mining techniques. Preliminary experiments demonstrate its usefulness for POS tagging and dependency parsing. We believe that what we present in this paper is useful beyond the low-resource language community. This is the first time that enough unlabeled and annotated data is provided for an emerging user-generated content dialectal language with rich morphology and code switching, making it an challenging test-bed for most recent NLP approaches.",,,,ACL
108,2020,Crawling and Preprocessing Mailing Lists At Scale for Dialog Analysis,"Janek Bevendorff,Khalid Al Khatib,Martin Potthast,Benno Stein","This paper introduces the Webis Gmane Email Corpus 2019, the largest publicly available and fully preprocessed email corpus to date. We crawled more than 153 million emails from 14,699 mailing lists and segmented them into semantically consistent components using a new neural segmentation model. With 96% accuracy on 15 classes of email segments, our model achieves state-of-the-art performance while being more efficient to train than previous ones. All data, code, and trained models are made freely available alongside the paper.",,,,ACL
109,2020,Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences,"Dmitry Nikolaev,Ofir Arviv,Taelin Karidi,Neta Kenneth","The patterns in which the syntax of different languages converges and diverges are often used to inform work on cross-lingual transfer. Nevertheless, little empirical work has been done on quantifying the prevalence of different syntactic divergences across language pairs. We propose a framework for extracting divergence patterns for any language pair from a parallel corpus, building on Universal Dependencies. We show that our framework provides a detailed picture of cross-language divergences, generalizes previous approaches, and lends itself to full automation. We further present a novel dataset, a manually word-aligned subset of the Parallel UD corpus in five languages, and use it to perform a detailed corpus study. We demonstrate the usefulness of the resulting analysis by showing that it can help account for performance patterns of a cross-lingual parser.",,,,ACL
110,2020,Generating Counter Narratives against Online Hate Speech: Data and Strategies,"Serra Sinem Tekiroğlu,Yi-Ling Chung,Marco Guerini","Recently research has started focusing on avoiding undesired effects that come with content moderation, such as censorship and overblocking, when dealing with hatred online. The core idea is to directly intervene in the discussion with textual responses that are meant to counter the hate content and prevent it from further spreading. Accordingly, automation strategies, such as natural language generation, are beginning to be investigated. Still, they suffer from the lack of sufficient amount of quality data and tend to produce generic/repetitive responses. Being aware of the aforementioned limitations, we present a study on how to collect responses to hate effectively, employing large scale unsupervised language models such as GPT-2 for the generation of silver data, and the best annotation strategies/neural architectures that can be used for data filtering before expert validation/post-editing.",,,,ACL
111,2020,KLEJ: Comprehensive Benchmark for Polish Language Understanding,"Piotr Rybak,Robert Mroczkowski,Janusz Tracz,Ireneusz Gawlik","In recent years, a series of Transformer-based models unlocked major improvements in general natural language understanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which allow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of languages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others. We also introduce a new sentiment analysis task for the e-commerce domain, named Allegro Reviews (AR). To ensure a common evaluation scheme and promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language, which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based models.",,,,ACL
112,2020,Learning and Evaluating Emotion Lexicons for 91 Languages,"Sven Buechel,Susanna Rücker,Udo Hahn","Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901.",,,,ACL
113,2020,Multi-Hypothesis Machine Translation Evaluation,"Marina Fomicheva,Lucia Specia,Francisco Guzmán","Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references. The latter has been shown to significantly improve the performance of evaluation metrics. However, collecting multiple references is expensive and in practice a single reference is generally used. In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether. We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%.",,,,ACL
114,2020,Multimodal Quality Estimation for Machine Translation,"Shu Okabe,Frédéric Blain,Lucia Specia","We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE. We compare various multimodality integration and fusion strategies. For both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality.",,,,ACL
115,2020,PuzzLing Machines: A Challenge on Learning From Small Data,"Gözde Gül Şahin,Yova Kementchedjhieva,Phillip Rust,Iryna Gurevych","Deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ML and NLP benchmarks. They struggle to achieve human-like thinking, however, because they lack the skill of iterative reasoning upon knowledge. To expose this problem in a new light, we introduce a challenge on learning from small data, PuzzLing Machines, which consists of Rosetta Stone puzzles from Linguistic Olympiads for high school students. These puzzles are carefully designed to contain only the minimal amount of parallel text necessary to deduce the form of unseen expressions. Solving them does not require external information (e.g., knowledge bases, visual signals) or linguistic expertise, but meta-linguistic awareness and deductive skills. Our challenge contains around 100 puzzles covering a wide range of linguistic phenomena from 81 languages. We show that both simple statistical algorithms and state-of-the-art deep neural models perform inadequately on this challenge, as expected. We hope that this benchmark, available at https://ukplab.github.io/PuzzLing-Machines/, inspires further efforts towards a new paradigm in NLP—one that is grounded in human-like reasoning and understanding.",,,,ACL
116,2020,The SOFC-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain,"Annemarie Friedrich,Heike Adel,Federico Tomazic,Johannes Hingerl","This paper presents a new challenging information extraction task in the domain of materials science. We develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications, such as involved materials and measurement conditions. With this paper, we publish our annotation guidelines, as well as our SOFC-Exp corpus consisting of 45 open-access scholarly articles annotated by domain experts. A corpus and an inter-annotator agreement study demonstrate the complexity of the suggested named entity recognition and slot filling tasks as well as high annotation quality. We also present strong neural-network based models for a variety of tasks that can be addressed on the basis of our new data set. On all tasks, using BERT embeddings leads to large performance gains, but with increasing task complexity, adding a recurrent neural network on top seems beneficial. Our models will serve as competitive baselines in future work, and analysis of their performance highlights difficult cases when modeling the data and suggests promising research directions.",,,,ACL
117,2020,The TechQA Dataset,"Vittorio Castelli,Rishav Chakravarti,Saswati Dana,Anthony Ferritto","We introduce TECHQA, a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task. Second, it has a real-world size – 600 training, 310 dev, and 490 evaluation question/answer pairs – thus reflecting the cost of creating large labeled datasets with actual data. Hence, TECHQA is meant to stimulate research in domain adaptation rather than as a resource to build QA systems from scratch. TECHQA was obtained by crawling the IBMDeveloper and DeveloperWorks forums for questions with accepted answers provided in an IBM Technote—a technical document that addresses a specific technical issue. We also release a collection of the 801,998 Technotes available on the web as of April 4, 2019 as a companion resource that can be used to learn representations of the IT domain language.",,,,ACL
118,2020,iSarcasm: A Dataset of Intended Sarcasm,"Silviu Oprea,Walid Magdy","We consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection. The former occurs when an utterance is sarcastic from the perspective of its author, while the latter occurs when the utterance is interpreted as sarcastic by the audience. We show the limitations of previous labelling methods in capturing intended sarcasm and introduce the iSarcasm dataset of tweets labeled for sarcasm directly by their authors. Examining the state-of-the-art sarcasm detection models on our dataset showed low performance compared to previously studied datasets, which indicates that these datasets might be biased or obvious and sarcasm could be a phenomenon under-studied computationally thus far. By providing the iSarcasm dataset, we aim to encourage future NLP research to develop methods for detecting sarcasm in text as intended by the authors of the text, not as labeled under assumptions that we demonstrate to be sub-optimal.",,,,ACL
119,2020,AMR Parsing via Graph-Sequence Iterative Inference,"Deng Cai,Wai Lam","We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported Smatch scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).",,,,ACL
120,2020,A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal,"Demian Gholipour Ghalandari,Chris Hokamp,Nghia The Pham,John Glover","Multi-document summarization (MDS) aims to compress the content in large document collections into short summaries and has important applications in story clustering for newsfeeds, presentation of search results, and timeline generation. However, there is a lack of datasets that realistically address such use cases at a scale large enough for training supervised models for this task. This work presents a new dataset for MDS that is large both in the total number of document clusters and in the size of individual clusters. We build this dataset by leveraging the Wikipedia Current Events Portal (WCEP), which provides concise and neutral human-written summaries of news events, with links to external source articles. We also automatically extend these source articles by looking for related articles in the Common Crawl archive. We provide a quantitative analysis of the dataset and empirical results for several state-of-the-art MDS techniques.",,,,ACL
121,2020,"Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization","Junnan Zhu,Yu Zhou,Jiajun Zhang,Chengqing Zong","Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English). In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary. We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary. Specifically, we first employ the encoder-decoder attention distribution to attend to the source words. Second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word. Finally, each summary word is generated either from the neural distribution or from the translation candidates of source words. Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art.",,,,ACL
122,2020,Examining the State-of-the-Art in News Timeline Summarization,"Demian Gholipour Ghalandari,Georgiana Ifrim","Previous work on automatic news timeline summarization (TLS) leaves an unclear picture about how this task can generally be approached and how well it is currently solved. This is mostly due to the focus on individual subtasks, such as date selection and date summarization, and to the previous lack of appropriate evaluation metrics for the full TLS task. In this paper, we compare different TLS strategies using appropriate evaluation frameworks, and propose a simple and effective combination of methods that improves over the stateof-the-art on all tested benchmarks. For a more robust evaluation, we also present a new TLS dataset, which is larger and spans longer time periods than previous datasets.",,,,ACL
123,2020,Improving Truthfulness of Headline Generation,"Kazuki Matsumaru,Sho Takase,Naoaki Okazaki","Most studies on abstractive summarization report ROUGE scores between system and reference summaries. However, we have a concern about the truthfulness of generated summaries: whether all facts of a generated summary are mentioned in the source text. This paper explores improving the truthfulness in headline generation on two popular datasets. Analyzing headlines generated by the state-of-the-art encoder-decoder model, we show that the model sometimes generates untruthful headlines. We conjecture that one of the reasons lies in untruthful supervision data used for training the model. In order to quantify the truthfulness of article-headline pairs, we consider the textual entailment of whether an article entails its headline. After confirming quite a few untruthful instances in the datasets, this study hypothesizes that removing untruthful instances from the supervision data may remedy the problem of the untruthful behaviors of the model. Building a binary classifier that predicts an entailment relation between an article and its headline, we filter out untruthful instances from the supervision data. Experimental results demonstrate that the headline generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines.",,,,ACL
124,2020,SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization,"Yang Gao,Wei Zhao,Steffen Eger","We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18- 39%. Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/acl20-ref-free-eval.",,,,ACL
125,2020,Self-Attention Guided Copy Mechanism for Abstractive Summarization,"Song Xu,Haoran Li,Peng Yuan,Youzheng Wu","Copy module has been widely equipped in the recent abstractive summarization models, which facilitates the decoder to extract words from the source into the summary. Generally, the encoder-decoder attention is served as the copy distribution, while how to guarantee that important words in the source are copied remains a challenge. In this work, we propose a Transformer-based model to enhance the copy mechanism. Specifically, we identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the Transformer. We use the centrality of each source word to guide the copy process explicitly. Experimental results show that the self-attention graph provides useful guidance for the copy distribution. Our proposed models significantly outperform the baseline methods on the CNN/Daily Mail dataset and the Gigaword dataset.",,,,ACL
126,2020,Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation,"Weixin Liang,James Zou,Zhou Yu","Open Domain dialog system evaluation is one of the most important challenges in dialog research. Existing automatic evaluation metrics, such as BLEU are mostly reference-based. They calculate the difference between the generated response and a limited number of available references. Likert-score based self-reported user rating is widely adopted by social conversational systems, such as Amazon Alexa Prize chatbots. However, self-reported user rating suffers from bias and variance among different users. To alleviate this problem, we formulate dialog evaluation as a comparison task. We also propose an automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that automatically cleans self-reported user ratings as it trains on them. Specifically, we first use a self-supervised method to learn better dialog feature representation, and then use KNN and Shapley to remove confusing samples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog comparison task.",,,,ACL
127,2020,Conversational Word Embedding for Retrieval-Based Dialog System,"Wentao Ma,Yiming Cui,Ting Liu,Dong Wang","Human conversations contain many types of information, e.g., knowledge, common sense, and language habits. In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs <post, reply> to learn word embedding. Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply.To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level.We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems.The experiment results show that PR-Embedding can improve the quality of the selected response.",,,,ACL
128,2020,Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network,"Yutai Hou,Wanxiang Che,Yongkui Lai,Zhihan Zhou","In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. In the few-shot setting, the emission score of CRF can be calculated as a word’s similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model – TapNet, by leveraging label name semantics in representing labels. Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.",,,,ACL
129,2020,Learning Dialog Policies from Weak Demonstrations,"Gabriel Gordon-Hall,Philip John Gorinski,Shay B. Cohen","Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems. Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a user’s requests. We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators. We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment. Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on out-of-domain data.",,,,ACL
130,2020,MuTual: A Dataset for Multi-Turn Dialogue Reasoning,"Leyang Cui,Yu Wu,Shujie Liu,Yue Zhang","Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques. Given a context, current systems are able to yield a relevant and fluent response, but sometimes make logical mistakes because of weak reasoning capabilities. To facilitate the conversation reasoning research, we introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning, consisting of 8,860 manually annotated dialogues based on Chinese student English listening comprehension exams. Compared to previous benchmarks for non-task oriented dialogue systems, MuTual is much more challenging since it requires a model that be able to handle various reasoning problems. Empirical results show that state-of-the-art methods only reach 71%, which is far behind human performance of 94%, indicating that there is ample room for improving reasoning ability.",,,,ACL
131,2020,You Impress Me: Dialogue Generation via Mutual Persona Perception,"Qian Liu,Yihong Chen,Bei Chen,Jian-Guang Lou","Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. Motivated by this, we propose Pˆ2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding. Specifically, Pˆ2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation. Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.",,,,ACL
132,2020,Bridging Anaphora Resolution as Question Answering,Yufang Hou,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of “quasi-bridging” training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (Ro ̈siger, 2018)).",,,,ACL
133,2020,Dialogue Coherence Assessment Without Explicit Dialogue Act Labels,"Mohsen Mesgar,Sebastian Bücker,Iryna Gurevych","Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment. We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code.",,,,ACL
134,2020,Fast and Accurate Non-Projective Dependency Tree Linearization,"Xiang Yu,Simon Tannert,Ngoc Thang Vu,Jonas Kuhn","We propose a graph-based method to tackle the dependency tree linearization task. We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding.",,,,ACL
135,2020,Semantic Graphs for Generating Deep Questions,"Liangming Pan,Yuxi Xie,Yansong Feng,Tat-Seng Chua","This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage. In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding. On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance. The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.",,,,ACL
136,2020,A Novel Cascade Binary Tagging Framework for Relational Triple Extraction,"Zhepei Wei,Jianlin Su,Yue Wang,Yuan Tian","Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities. In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (CasRel) derived from a principled problem formulation. Instead of treating relations as discrete labels as in previous works, our new framework models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem. Experiments show that the CasRel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized BERT encoder, showing the power of the new tagging framework. It enjoys further performance boost when employing a pre-trained BERT encoder, outperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score on two public datasets NYT and WebNLG, respectively. In-depth analysis on different scenarios of overlapping triples shows that the method delivers consistent performance gain across all these scenarios. The source code and data are released online.",,,,ACL
137,2020,In Layman’s Terms: Semi-Open Relation Extraction from Scientific Texts,"Ruben Kruiper,Julian Vincent,Jessica Chen-Burger,Marc Desmulliez","Information Extraction (IE) from scientific texts can be used to guide readers to the central information in scientific documents. But narrow IE systems extract only a fraction of the information captured, and Open IE systems do not perform well on the long and complex sentences encountered in scientific texts. In this work we combine the output of both types of systems to achieve Semi-Open Relation Extraction, a new task that we explore in the Biology domain. First, we present the Focused Open Biological Information Extraction (FOBIE) dataset and use FOBIE to train a state-of-the-art narrow scientific IE system to extract trade-off relations and arguments that are central to biology texts. We then run both the narrow IE system and a state-of-the-art Open IE system on a corpus of 10K open-access scientific biological texts. We show that a significant amount (65%) of erroneous and uninformative Open IE extractions can be filtered using narrow IE extractions. Furthermore, we show that the retained extractions are significantly more often informative to a reader.",,,,ACL
138,2020,NAT: Noise-Aware Training for Robust Neural Sequence Labeling,"Marcin Namysl,Sven Behnke,Joachim Köhler","Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs—as these systems often process user-generated text or follow an error-prone upstream component. To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our stability training algorithm encourages the model to create a noise-invariant latent representation. We employ a vanilla noise model at training time. For evaluation, we use both the original data and its variants perturbed with real OCR errors and misspellings. Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models, preserving accuracy on the original input. We make our code and data publicly available for the research community.",,,,ACL
139,2020,Named Entity Recognition without Labelled Data: A Weak Supervision Approach,"Pierre Lison,Jeremy Barnes,Aliaksandr Hubin,Samia Touileb","Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training. When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain. But what should one do when there is no hand-labelled data for the target domain? This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision. The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain. These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions. A sequence labelling model can finally be trained on the basis of this unified annotation. We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F1 scores compared to an out-of-domain neural NER model.",,,,ACL
140,2020,Probing Linguistic Features of Sentence-Level Representations in Neural Relation Extraction,"Christoph Alt,Aleksandra Gabryszak,Leonhard Hennig","Despite the recent progress, little is known about the features captured by state-of-the-art neural relation extraction (RE) models. Common methods encode the source sentence, conditioned on the entity mentions, before classifying the relation. However, the complexity of the task makes it difficult to understand how encoder architecture and supporting linguistic knowledge affect the features learned by the encoder. We introduce 14 probing tasks targeting linguistic properties relevant to RE, and we use them to study representations learned by more than 40 different encoder architecture and linguistic feature combinations trained on two datasets, TACRED and SemEval 2010 Task 8. We find that the bias induced by the architecture and the inclusion of linguistic features are clearly expressed in the probing task performance. For example, adding contextualized word representations greatly increases performance on probing tasks with a focus on named entity and part-of-speech information, and yields better results in RE. In contrast, entity masking improves RE, but considerably lowers performance on entity type related probing tasks.",,,,ACL
141,2020,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Guoshun Nan,Zhijiang Guo,Ivan Sekulic,Wei Lu","Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.",,,,ACL
142,2020,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"Christoph Alt,Aleksandra Gabryszak,Leonhard Hennig","TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.",,,,ACL
143,2020,Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences,"Xiangyu Duan,Baijun Ji,Hao Jia,Min Tan","In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. We propose anchored training (AT) to tackle the task. AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language. Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-by-word translation, dictionary-supervised cross-lingual word embedding transformation, and unsupervised MT. On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences.",,,,ACL
144,2020,Boosting Neural Machine Translation with Similar Translations,"Jitao Xu,Josep Crego,Jean Senellart","This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with “copy” information while translations based on embedding similarities tend to extend the translation “context”. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.",,,,ACL
145,2020,Character-Level Translation with Self-attention,"Yingqiang Gao,Nikola I. Nikolov,Yuhuang Hu,Richard H.R. Hahnloser","We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.",,,,ACL
146,2020,End-to-End Neural Word Alignment Outperforms GIZA++,"Thomas Zenkel,Joern Wuebker,John DeNero","Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models. Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection. While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems. Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training. We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets. Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.",,,,ACL
147,2020,Enhancing Machine Translation with Dependency-Aware Self-Attention,"Emanuele Bugliarello,Naoaki Okazaki","Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks.",,,,ACL
148,2020,Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation,"Biao Zhang,Philip Williams,Ivan Titov,Rico Sennrich","Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.",,,,ACL
149,2020,It’s Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information,"Emanuele Bugliarello,Sabrina J. Mielke,Antonios Anastasopoulos,Ryan Cotterell","The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.",,,,ACL
150,2020,Language-aware Interlingua for Multilingual Neural Machine Translation,"Changfeng Zhu,Heng Yu,Shanbo Cheng,Weihua Luo","Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.",,,,ACL
151,2020,On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation,"Wei Zhao,Goran Glavaš,Maxime Peyrard,Yang Gao","Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish “translationese”, i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.",,,,ACL
152,2020,Parallel Sentence Mining by Constrained Decoding,"Pinzhen Chen,Nikolay Bogoychev,Kenneth Heafield,Faheem Kirefu","We present a novel method to extract parallel sentences from two monolingual corpora, using neural machine translation. Our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus. We argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates pairwise comparison with a modified beam search. When benchmarked on the BUCC shared task, our method achieves results comparable to other submissions.",,,,ACL
153,2020,Self-Attention with Cross-Lingual Position Representation,"Liang Ding,Longyue Wang,Dacheng Tao","Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences. However, in cross-lingual scenarios, machine translation, the PEs of source and target sentences are modeled independently. Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem. In this paper, we augment SANs with cross-lingual position representations to model the bilingually aware latent structure for the input sentence. Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments. Experimental results on WMT’14 English⇒German, WAT’17 Japanese⇒English, and WMT’17 Chinese⇔English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines. Extensive analyses confirm that the performance gains come from the cross-lingual information.",,,,ACL
154,2020,“You Sound Just Like Your Father” Commercial Machine Translation Systems Include Stylistic Biases,"Dirk Hovy,Federico Bianchi,Tommaso Fornaciari","The main goal of machine translation has been to convey the correct content. Stylistic considerations have been at best secondary. We show that as a consequence, the output of three commercial machine translation systems (Bing, DeepL, Google) make demographically diverse samples from five languages “sound” older and more male than the original. Our findings suggest that translation models reflect demographic bias in the training data. This opens up interesting new research avenues in machine translation to take stylistic considerations into account.",,,,ACL
155,2020,MMPE: A Multi-Modal Interface for Post-Editing Machine Translation,"Nico Herbig,Tim Düwel,Santanu Pal,Kalliopi Meladaki","Current advances in machine translation (MT) increase the need for translators to switch from traditional translation to post-editing (PE) of machine-translated text, a process that saves time and reduces errors. This affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Since this paradigm shift offers potential for modalities other than mouse and keyboard, we present MMPE, the first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT. The results of an evaluation with professional translators suggest that pen and touch interaction are suitable for deletion and reordering tasks, while they are of limited use for longer insertions. On the other hand, speech and multi-modal combinations of select & speech are considered suitable for replacements and insertions but offer less potential for deletion and reordering. Overall, participants were enthusiastic about the new modalities and saw them as good extensions to mouse & keyboard, but not as a complete substitute.",,,,ACL
156,2020,A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages,"Pedro Javier Ortiz Suárez,Laurent Romary,Benoît Sagot","We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.",,,,ACL
157,2020,Will-They-Won’t-They: A Very Large Dataset for Stance Detection on Twitter,"Costanza Conforti,Jakob Berndt,Mohammad Taher Pilehvar,Chryssi Giannitsarou","We present a new challenging stance detection dataset, called Will-They-Won’t-They (WT--WT), which contains 51,284 tweets in English, making it by far the largest available dataset of the type. All the annotations are carried out by experts; therefore, the dataset constitutes a high-quality and reliable benchmark for future research in stance detection. Our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain.",,,,ACL
158,2020,A Systematic Assessment of Syntactic Generalization in Neural Language Models,"Jennifer Hu,Jon Gauthier,Peng Qian,Ethan Wilcox","While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.",,,,ACL
159,2020,Inflecting When There’s No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals,"Kate McCurdy,Sharon Goldwater,Adam Lopez","Can artificial neural networks learn to represent inflectional morphology and generalize to new words as human speakers do? Kirov and Cotterell (2018) argue that the answer is yes: modern Encoder-Decoder (ED) architectures learn human-like behavior when inflecting English verbs, such as extending the regular past tense form /-(e)d/ to novel words. However, their work does not address the criticism raised by Marcus et al. (1995): that neural models may learn to extend not the regular, but the most frequent class — and thus fail on tasks like German number inflection, where infrequent suffixes like /-s/ can still be productively generalized. To investigate this question, we first collect a new dataset from German speakers (production and ratings of plural forms for novel nouns) that is designed to avoid sources of information unavailable to the ED model. The speaker data show high variability, and two suffixes evince ‘regular’ behavior, appearing more often with phonologically atypical inputs. Encoder-decoder models do generalize the most frequently produced plural class, but do not show human-like variability or ‘regular’ extension of these other plural markers. We conclude that modern neural models may still struggle with minority-class generalization.",,,,ACL
160,2020,Overestimation of Syntactic Representation in Neural Language Models,"Jordan Kodner,Nitish Gupta","With the advent of powerful neural language models over the last few years, research attention has increasingly focused on what aspects of language they represent that make them so successful. Several testing methodologies have been developed to probe models’ syntactic representations. One popular method for determining a model’s ability to induce syntactic structure trains a model on strings generated according to a template then tests the model’s ability to distinguish such strings from superficially similar ones with different syntax. We illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with two non-syntactic baseline language models: an n-gram model and an LSTM model trained on scrambled inputs.",,,,ACL
161,2020,Modelling Suspense in Short Stories as Uncertainty Reduction over Neural Representation,"David Wilmot,Frank Keller","Suspense is a crucial ingredient of narrative fiction, engaging readers and making stories compelling. While there is a vast theoretical literature on suspense, it is computationally not well understood. We compare two ways for modelling suspense: surprise, a backward-looking measure of how unexpected the current state is given the story so far; and uncertainty reduction, a forward-looking measure of how unexpected the continuation of the story is. Both can be computed either directly over story representations or over their probability distributions. We propose a hierarchical language model that encodes stories and computes surprise and uncertainty reduction. Evaluating against short stories annotated with human suspense judgements, we find that uncertainty reduction over representations is the best predictor, resulting in near human accuracy. We also show that uncertainty reduction can be used to predict suspenseful events in movie synopses.",,,,ACL
162,2020,You Don’t Have Time to Read This: An Exploration of Document Reading Time Prediction,"Orion Weller,Jordan Hildebrandt,Ilya Reznik,Christopher Challis","Predicting reading time has been a subject of much previous work, focusing on how different words affect human processing, measured by reading time. However, previous work has dealt with a limited number of participants as well as word level only predictions (i.e. predicting the time to read a single word). We seek to extend these works by examining whether or not document level predictions are effective, given additional information such as subject matter, font characteristics, and readability metrics. We perform a novel experiment to examine how different features of text contribute to the time it takes to read, distributing and collecting data from over a thousand participants. We then employ a large number of machine learning methods to predict a user’s reading time. We find that despite extensive research showing that word level reading time can be most effectively predicted by neural networks, larger scale text can be easily and most accurately predicted by one factor, the number of words.",,,,ACL
163,2020,A Generative Model for Joint Natural Language Understanding and Generation,"Bo-Hsiang Tseng,Jianpeng Cheng,Yimai Fang,David Vandyke","Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal representations, whereas NLG does the reverse. A key to success in either task is parallel training data which is expensive to obtain at a large scale. In this work, we propose a generative model which couples NLU and NLG through a shared latent variable. This approach allows us to explore both spaces of natural language and formal representations, and facilitates information sharing through the latent space to eventually benefit NLU and NLG. Our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations. We also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance.",,,,ACL
164,2020,Automatic Detection of Generated Text is Easiest when Humans are Fooled,"Daphne Ippolito,Daniel Duckworth,Chris Callison-Burch,Douglas Eck","Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies—top-_k_, nucleus sampling, and untruncated random sampling—and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.",,,,ACL
165,2020,Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing,"Haoming Jiang,Chen Liang,Chong Wang,Tuo Zhao","Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.",,,,ACL
166,2020,Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation,"Jun Xu,Haifeng Wang,Zheng-Yu Niu,Hua Wu","To address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and controllable dialog. To this end, we first construct a conversational graph (CG) from dialog corpora, in which there are vertices to represent “what to say” and “how to say”, and edges to represent natural transition between a message (the last utterance in a dialog context) and its response. We then present a novel CG grounded policy learning framework that conducts dialog flow planning by graph traversal, which learns to identify a what-vertex and a how-vertex from the CG at each turn to guide response generation. In this way, we effectively leverage the CG to facilitate policy learning as follows: (1) it enables more effective long-term reward design, (2) it provides high-quality candidate actions, and (3) it gives us more control over the policy. Results on two benchmark corpora demonstrate the effectiveness of this framework.",,,,ACL
167,2020,GPT-too: A Language-Model-First Approach for AMR-to-Text Generation,"Manuel Mager,Ramón Fernandez Astudillo,Tahira Naseem,Md Arafat Sultan","Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.",,,,ACL
168,2020,Learning to Update Natural Language Comments Based on Code Changes,"Sheena Panthaplackel,Pengyu Nie,Milos Gligoric,Junyi Jessy Li","We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies. We propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications. We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent update to a method and its corresponding comment. We compare our approach against multiple baselines using both automatic metrics and human evaluation. Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits.",,,,ACL
169,2020,Politeness Transfer: A Tag and Generate Approach,"Aman Madaan,Amrith Setlur,Tanmay Parekh,Barnabas Poczos","This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For politeness as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks. The data and code is located at https://github.com/tag-and-generate.",,,,ACL
170,2020,BPE-Dropout: Simple and Effective Subword Regularization,"Ivan Provilkov,Dmitrii Emelianenko,Elena Voita","Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.",,,,ACL
171,2020,Improving Non-autoregressive Neural Machine Translation with Monolingual Data,"Jiawei Zhou,Phillip Keung","Non-autoregressive (NAR) neural machine translation is usually done via knowledge distillation from an autoregressive (AR) model. Under this framework, we leverage large monolingual corpora to improve the NAR model’s performance, with the goal of transferring the AR model’s generalization ability while preventing overfitting. On top of a strong NAR baseline, our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of the NAR model to approach the teacher AR model’s performance, yields comparable or better results than the best non-iterative NAR methods in the literature and helps reduce overfitting in the training process.",,,,ACL
172,2020,Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization,"Sajad Sotudeh Gharebagh,Nazli Goharian,Ross Filice","Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer. Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically significantly boosts state-of-the-art results in terms of ROUGE metrics (with improvements: 2.9% RG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of improvement impacts patients’ welfare.",,,,ACL
173,2020,On Faithfulness and Factuality in Abstractive Summarization,"Joshua Maynez,Shashi Narayan,Bernd Bohnet,Ryan McDonald","It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",,,,ACL
174,2020,Screenplay Summarization Using Latent Narrative Structure,"Pinelopi Papalampidi,Frank Keller,Lea Frermann,Mirella Lapata","Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document. When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models. We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes). Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.",,,,ACL
175,2020,Unsupervised Opinion Summarization with Noising and Denoising,"Reinald Kim Amplayo,Mirella Lapata","The supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced. In this paper we enable the use of supervised learning for the setting where there are only documents available (e.g., product or business reviews) without ground truth summaries. We create a synthetic dataset from a corpus of user reviews by sampling a review, pretending it is a summary, and generating noisy versions thereof which we treat as pseudo-review input. We introduce several linguistically motivated noise generation functions and a summarization model which learns to denoise the input and generate the original review. At test time, the model accepts genuine reviews and generates a summary containing salient opinions, treating those that do not reach consensus as noise. Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines.",,,,ACL
176,2020,A Tale of Two Perplexities: Sensitivity of Neural Language Models to Lexical Retrieval Deficits in Dementia of the Alzheimer’s Type,"Trevor Cohen,Serguei Pakhomov","In recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia, and those from healthy controls. The difference between perplexity estimates from two neural language models (LMs) - one trained on transcripts of speech produced by healthy participants and one trained on those with dementia - as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance. However, little is known about why this approach is effective, and on account of the lack of case/control matching in the most widely-used evaluation set of transcripts (DementiaBank), it is unclear if these approaches are truly diagnostic, or are sensitive to other variables. In this paper, we interrogate neural LMs trained on participants with and without dementia by using synthetic narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency. We find that perplexity of neural LMs is strongly and differentially associated with lexical frequency, and that using a mixture model resulting from interpolating control and dementia LMs improves upon the current state-of-the-art for models trained on transcript text exclusively.",,,,ACL
177,2020,Probing Linguistic Systematicity,"Emily Goodwin,Koustuv Sinha,Timothy J. O’Donnell","Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we perform a series of experiments in the setting of natural language inference (NLI). We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.",,,,ACL
178,2020,Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models,"Maarten Sap,Eric Horvitz,Yejin Choi,Noah A. Smith","We investigate the use of NLP as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events. To facilitate this, we collect and release Hippocorpus, a dataset of 7,000 stories about imagined and recalled events. We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events. Additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory (Tulving, 1972) for imagined and recalled storytelling by comparing the frequency of descriptions of general commonsense events with more specific realis events. Our analyses show that imagined stories have a substantially more linear narrative flow, compared to recalled stories in which adjacent sentences are more disconnected. In addition, while recalled stories rely more on autobiographical events based on episodic memory, imagined stories express more commonsense knowledge based on semantic memory. Finally, our measures reveal the effect of narrativization of memories in stories (e.g., stories about frequently recalled memories flow more linearly; Bartlett, 1932). Our findings highlight the potential of using NLP tools to study the traces of human cognition in language.",,,,ACL
179,2020,Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment,"Forrest Davis,Marten van Schijndel","A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish. Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences. We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.",,,,ACL
180,2020,Speakers enhance contextually confusable words,"Eric Meinhardt,Eric Bakovic,Leon Bergen","Recent work has found evidence that natural languages are shaped by pressures for efficient communication — e.g. the more contextually predictable a word is, the fewer speech sounds or syllables it has (Piantadosi et al. 2011). Research on the degree to which speech and language are shaped by pressures for effective communication — robustness in the face of noise and uncertainty — has been more equivocal. We develop a measure of contextual confusability during word recognition based on psychoacoustic data. Applying this measure to naturalistic speech corpora, we find evidence suggesting that speakers alter their productions to make contextually more confusable words easier to understand.",,,,ACL
181,2020,What determines the order of adjectives in English? Comparing efficiency-based theories using dependency treebanks,"Richard Futrell,William Dyer,Greg Scontras","We take up the scientific question of what determines the preferred order of adjectives in English, in phrases such as big blue box where multiple adjectives modify a following noun. We implement and test four quantitative theories, all of which are theoretically motivated in terms of efficiency in human language production and comprehension. The four theories we test are subjectivity (Scontras et al., 2017), information locality (Futrell, 2019), integration cost (Dyer, 2017), and information gain, which we introduce. We evaluate theories based on their ability to predict orders of unseen adjectives in hand-parsed and automatically-parsed dependency treebanks. We find that subjectivity, information locality, and information gain are all strong predictors, with some evidence for a two-factor account, where subjectivity and information gain reflect a factor involving semantics, and information locality reflects collocational preferences.",,,,ACL
182,2020,“None of the Above”: Measure Uncertainty in Dialog Response Retrieval,"Yulan Feng,Shikib Mehri,Maxine Eskenazi,Tiancheng Zhao","This paper discusses the importance of uncovering uncertainty in end-to-end dialog tasks and presents our experimental results on uncertainty classification on the processed Ubuntu Dialog Corpus. We show that instead of retraining models for this specific purpose, we can capture the original retrieval model’s underlying confidence concerning the best prediction using trivial additional computation.",,,,ACL
183,2020,Can You Put it All Together: Evaluating Conversational Agents’ Ability to Blend Skills,"Eric Michael Smith,Mary Williamson,Kurt Shuster,Jason Weston","Being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent. Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them. But rather than being specialized in one single quality, a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow. In this work, we investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training that encompass several skills at all training stages. We further propose a new dataset, BlendedSkillTalk, to analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes. Our experiments show that multi-tasking over several tasks that focus on particular capabilities results in better blended conversation performance compared to models trained on a single skill, and that both unified or two-stage approaches perform well if they are constructed to avoid unwanted bias in skill selection or are fine-tuned on our new task.",,,,ACL
184,2020,Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs,"Houyu Zhang,Zhenghao Liu,Chenyan Xiong,Zhiyuan Liu","Human conversations naturally evolve around related concepts and hop to distant concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow’s effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/thunlp/ConceptFlow.",,,,ACL
185,2020,Negative Training for Neural Dialogue Response Generation,"Tianxing He,James Glass","Although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation, recent research results have revealed that the trained models have undesirable generation behaviors, such as malicious responses and generic (boring) responses. In this work, we propose a framework named “Negative Training” to minimize such behaviors. Given a trained model, the framework will first find generated samples that exhibit the undesirable behavior, and then use them to feed negative training signals for fine-tuning the model. Our experiments show that negative training can significantly reduce the hit rate of malicious responses, or discourage frequent responses and improve response diversity.",,,,ACL
186,2020,Recursive Template-based Frame Generation for Task Oriented Dialog,"Rashmi Gangadharaiah,Balakrishnan Narayanaswamy","The Natural Language Understanding (NLU) component in task oriented dialog systems processes a user’s request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker (DST). This information is typically represented as a semantic frame that captures the intent and slot-labels provided by the user. We first show that such a shallow representation is insufficient for complex dialog scenarios, because it does not capture the recursive nature inherent in many domains. We propose a recursive, hierarchical frame-based representation and show how to learn it from data. We formulate the frame generation task as a template-based tree decoding task, where the decoder recursively generates a template and then fills slot values into the template. We extend local tree-based loss functions with terms that provide global supervision and show how to optimize them end-to-end. We achieve a small improvement on the widely used ATIS dataset and a much larger improvement on a more complex dataset we describe here.",,,,ACL
187,2020,Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback,"Ahmed Elgohary,Saghar Hosseini,Ahmed Hassan Awadallah","We study the task of semantic parse correction with natural language feedback. Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form. In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance. We focus on natural language to SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback. We compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction. While we estimated human correction accuracy is 81.5%, our best model achieves only 25.1%, which leaves a large gap for improvement in future research. SPLASH is publicly available at https://aka.ms/Splash_dataset.",,,,ACL
188,2020,Calibrating Structured Output Predictors for Natural Language Processing,"Abhyuday Jagannatha,Hong Yu","We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems. We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well.",,,,ACL
189,2020,Active Imitation Learning with Noisy Guidance,"Kianté Brantley,Amr Sharaf,Hal Daumé III","Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies. Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical. To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance. Our algorithm, LEAQI, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary. We apply LEAQI to three sequence labelling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach.",,,,ACL
190,2020,ExpBERT: Representation Engineering with Natural Language Explanations,"Shikhar Murty,Pang Wei Koh,Percy Liang","Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use BERT fine-tuned on MultiNLI to “interpret” these explanations with respect to the input sentence, producing explanation-guided representations of the input. Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3–20x less labeled data and improves on the baseline by 3–10 F1 points with the same amount of labeled data.",,,,ACL
191,2020,GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples,"Danilo Croce,Giuseppe Castellucci,Roberto Basili","Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks. However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples. In many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected. One promising method to enable semi-supervised learning has been proposed in image processing, based on Semi- Supervised Generative Adversarial Networks. In this paper, we propose GAN-BERT that ex- tends the fine-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting. Experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classification tasks.",,,,ACL
192,2020,Generalizing Natural Language Analysis through Span-relation Representations,"Zhengbao Jiang,Wei Xu,Jun Araki,Graham Neubig","Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.",,,,ACL
193,2020,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,"Ouyu Lan,Xiao Huang,Bill Yuchen Lin,He Jiang","Sequence labeling is a fundamental task for a range of natural language processing problems. When used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often costly. In many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible. In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data). It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module. Finally, it leads to a model reflecting the agreement (consensus) among multiple sources. We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation. Extensive experimental results show that our model achieves significant improvements over existing methods in both settings. We also demonstrate that the method can apply to various tasks and cope with different encoders.",,,,ACL
194,2020,MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification,"Jiaao Chen,Zichao Yang,Diyi Yang","This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data. By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks. The improvement is especially prominent when supervision is extremely limited. We have publicly released our code at https://github.com/GT-SALT/MixText.",,,,ACL
195,2020,MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices,"Zhiqing Sun,Hongkun Yu,Xiaodan Song,Renjie Liu","Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).",,,,ACL
196,2020,On Importance Sampling-Based Evaluation of Latent Language Models,"Robert L. Logan IV,Matt Gardner,Sameer Singh","Language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models. However, likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space. Existing works avoid this issue by using importance sampling. Although this approach has asymptotic guarantees, analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates. In this paper, we carry out this analysis for three models: RNNG, EntityNLM, and KGLM. In addition, we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates, as well as provide theoretical results which reinforce the validity of this technique.",,,,ACL
197,2020,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization,"Haoming Jiang,Pengcheng He,Weizhu Chen,Xiaodong Liu","Transfer learning has fundamentally changed the landscape of natural language processing (NLP). Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance. The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating. Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI. Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.",,,,ACL
198,2020,Stolen Probability: A Structural Weakness of Neural Language Models,"David Demeter,Gregory Kimmel,Doug Downey","Neural Network Language Models (NNLMs) generate probability distributions by applying a softmax function to a distance metric formed by taking the dot product of a prediction vector with all word vectors in a high-dimensional embedding space. The dot-product distance metric forms part of the inductive bias of NNLMs. Although NNLMs optimize well with this inductive bias, we show that this results in a sub-optimal ordering of the embedding space that structurally impoverishes some words at the expense of others when assigning probability. We present numerical, theoretical and empirical analyses which show that words on the interior of the convex hull in the embedding space have their probability bounded by the probabilities of the words on the hull.",,,,ACL
199,2020,Taxonomy Construction of Unseen Domains via Graph-based Cross-Domain Knowledge Transfer,"Chao Shang,Sarthak Dash,Md. Faisal Mahbub Chowdhury,Nandana Mihindukulasooriya","Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications. Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks. However, there has been no attempt to exploit GNN to create taxonomies. In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task. Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain. We also propose a novel method of directed acyclic graph (DAG) generation for taxonomy construction. Specifically, our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym hypernym candidate pairs, and a set of taxonomies for some known domains for training. The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain. Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art.",,,,ACL
200,2020,To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks,"Sinong Wang,Madian Khabsa,Hao Ma","Pretraining NLP models with variants of Masked Language Model (MLM) objectives has recently led to a significant improvements on many tasks. This paper examines the benefits of pretrained models as a function of the number of training samples used in the downstream task. On several text classification tasks, we show that as the number of training examples grow into the millions, the accuracy gap between finetuning BERT-based model and training vanilla LSTM from scratch narrows to within 1%. Our findings indicate that MLM-based models might reach a diminishing return point as the supervised data size increases significantly.",,,,ACL
201,2020,Why Overfitting Isn’t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries,"Mozhi Zhang,Yoshinari Fujinuma,Michael J. Paul,Jordan Boyd-Graber","Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.",,,,ACL
202,2020,XtremeDistil: Multi-stage Distillation for Massive Multilingual Models,"Subhabrata Mukherjee,Ahmed Hassan Awadallah","Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks. However, the huge size of these models could be a deterrent to using them in practice. Some recent works use knowledge distillation to compress these huge models into shallow ones. In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition (NER). In particular, we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works. Additionally, we investigate the role of several factors like the amount of unlabeled data, annotation resources, model architecture and inference latency to name a few. We show that our approach leads to massive compression of teacher models like mBERT by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95% of its F1-score for NER over 41 languages.",,,,ACL
203,2020,A Girl Has A Name: Detecting Authorship Obfuscation,"Asad Mahmood,Zubair Shafiq,Padmini Srinivasan","Authorship attribution aims to identify the author of a text based on the stylometric analysis. Authorship obfuscation, on the other hand, aims to protect against authorship attribution by modifying a text’s style. In this paper, we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an adversarial threat model. An obfuscator is stealthy to the extent an adversary finds it challenging to detect whether or not a text modified by the obfuscator is obfuscated – a decision that is key to the adversary interested in authorship attribution. We show that the existing authorship obfuscation methods are not stealthy as their obfuscated texts can be identified with an average F1 score of 0.87. The reason for the lack of stealthiness is that these obfuscators degrade text smoothness, as ascertained by neural language models, in a detectable manner. Our results highlight the need to develop stealthy authorship obfuscation methods that can better protect the identity of an author seeking anonymity.",,,,ACL
204,2020,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Ji Xin,Raphael Tang,Jaejun Lee,Yaoliang Yu","Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT.",,,,ACL
205,2020,Efficient Strategies for Hierarchical Text Classification: External Knowledge and Auxiliary Tasks,"Kervy Rivas Rojas,Gina Bustamante,Arturo Oncevay,Marco Antonio Sobrevilla Cabezudo","In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy. Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model. We first define the task as a sequence-to-sequence problem. Afterwards, we propose an auxiliary synthetic task of bottom-up-classification. Then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy’s layers, and map them into the word vector space. We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search. Whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy. With our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known English datasets.",,,,ACL
206,2020,Investigating the effect of auxiliary objectives for the automated grading of learner English speech transcriptions,"Hannah Craighead,Andrew Caines,Paula Buttery,Helen Yannakoudakis","We address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts. Motivated by recent advances in multi-task learning, we develop neural networks trained in a multi-task fashion that learn to predict the proficiency level of non-native English speakers by taking advantage of inductive transfer between the main task (grading) and auxiliary prediction tasks: morpho-syntactic labeling, language modeling, and native language identification (L1). We encode the transcriptions with both bi-directional recurrent neural networks and with bi-directional representations from transformers, compare against a feature-rich baseline, and analyse performance at different proficiency levels and with transcriptions of varying error rates. Our best performance comes from a transformer encoder with L1 prediction as an auxiliary task. We discuss areas for improvement and potential applications for text-only speech scoring.",,,,ACL
207,2020,SPECTER: Document-level Representation Learning using Citation-informed Transformers,"Arman Cohan,Sergey Feldman,Iz Beltagy,Doug Downey","Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity. We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that Specter outperforms a variety of competitive baselines on the benchmark.",,,,ACL
208,2020,Semantic Scaffolds for Pseudocode-to-Code Generation,"Ruiqi Zhong,Mitchell Stern,Dan Klein","We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases. By using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art. Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.",,,,ACL
209,2020,Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction,"Samuel Broscheit,Kiril Gashteovski,Yanjie Wang,Rainer Gemulla","Open Information Extraction systems extract (“subject text”, “relation text”, “object text”) triples from raw text. Some triples are textual versions of facts, i.e., non-canonicalized mentions of entities and relations. In this paper, we investigate whether it is possible to infer new facts directly from the open knowledge graph without any canonicalization or any supervision from curated knowledge. For this purpose, we propose the open link prediction task,i.e., predicting test facts by completing (“subject text”, “relation text”, ?) questions. An evaluation in such a setup raises the question if a correct prediction is actually a new fact that was induced by reasoning over the open knowledge graph or if it can be trivially explained. For example, facts can appear in different paraphrased textual variants, which can lead to test leakage. To this end, we propose an evaluation protocol and a methodology for creating the open link prediction benchmark OlpBench. We performed experiments with a prototypical knowledge graph embedding model for openlink prediction. While the task is very challenging, our results suggests that it is possible to predict genuinely new facts, which can not be trivially explained.",,,,ACL
210,2020,INFOTABS: Inference on Tables as Semi-structured Data,"Vivek Gupta,Maitrey Mehta,Pegah Nokhiz,Vivek Srikumar","In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them. We argue that such data can prove as a testing ground for understanding how we reason about information. To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes. Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning. Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge.",,,,ACL
211,2020,Interactive Machine Comprehension with Information Seeking Agents,"Xingdi Yuan,Jie Fu,Marc-Alexandre Côté,Yi Tay","Existing machine reading comprehension (MRC) models do not scale effectively to real-world applications like web-level information retrieval and question answering (QA). We argue that this stems from the nature of MRC datasets: most of these are static environments wherein the supporting documents and all necessary information are fully observed. In this paper, we propose a simple method that reframes existing MRC datasets as interactive, partially observable environments. Specifically, we “occlude” the majority of a document’s text and add context-sensitive commands that reveal “glimpses” of the hidden text to a model. We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making. We believe that this setting can contribute in scaling models to web-level QA scenarios.",,,,ACL
212,2020,Syntactic Data Augmentation Increases Robustness to Inference Heuristics,"Junghyun Min,R. Thomas McCoy,Dipanjan Das,Emily Pitler","Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize that this issue is not primarily caused by the pretrained model’s limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage. We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus. The best-performing augmentation method, subject/object inversion, improved BERT’s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.",,,,ACL
213,2020,Improved Speech Representations with Multi-Target Autoregressive Predictive Coding,"Yu-An Chung,James Glass","Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames. The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions. We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task. Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.",,,,ACL
214,2020,Integrating Multimodal Information in Large Pretrained Transformers,"Wasifur Rahman,Md Kamrul Hasan,Sangwu Lee,AmirAli Bagher Zadeh","Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). More specifically, this is due to the fact that pre-trained models don’t have the necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.",,,,ACL
215,2020,MultiQT: Multimodal learning for real-time question tracking in speech,"Jakob D. Havtorn,Jan Latko,Joakim Edin,Lars Maaløe","We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers. We propose a novel multimodal approach to real-time sequence labeling in speech. Our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition. Our results show significant gains of jointly learning from the two modalities when compared to text or audio only, under adverse noise and limited volume of training data. The results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning.",,,,ACL
216,2020,Multimodal and Multiresolution Speech Recognition with Transformers,"Georgios Paraskevopoulos,Srinivas Parthasarathy,Aparna Khare,Shiva Sundaram","This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture. We particularly focus on the scene context provided by the visual information, to ground the ASR. We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions. Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models. Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models. Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.",,,,ACL
217,2020,Phone Features Improve Speech Translation,"Elizabeth Salesky,Alan W Black","End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation. Their performance is often assumed to be superior, though in many conditions this is not yet the case. We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines. Further, we introduce two methods to incorporate phone features into ST models. We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work – by up to 9 BLEU on our low-resource setting.",,,,ACL
218,2020,Grounding Conversations with Improvised Dialogues,"Hyundong Cho,Jonathan May","Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people. Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication. Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality. We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier. We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.",,,,ACL
219,2020,Image-Chat: Engaging Grounded Conversations,"Kurt Shuster,Samuel Humeau,Antoine Bordes,Jason Weston","To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners. Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014). In this work we study large-scale architectures and datasets for this goal. We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components. To test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (Guo et al., 2019). Our dataset, Image-Chat, consists of 202k dialogues over 202k images using 215 possible style traits. Automatic metrics and human evaluations of engagingness show the efficacy of our approach; in particular, we obtain state-of-the-art performance on the existing IGC task, and our best performing model is almost on par with humans on the Image-Chat test set (preferred 47.7% of the time).",,,,ACL
220,2020,Learning an Unreferenced Metric for Online Dialogue Evaluation,"Koustuv Sinha,Prasanna Parthasarathi,Jasmine Wang,Ryan Lowe","Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation. Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them. We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.",,,,ACL
221,2020,Neural Generation of Dialogue Response Timings,"Matthew Roddy,Naomi Harte","The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue. We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn. The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS). We evaluate our models using offline experiments as well as human listening tests. We show that human listeners consider certain response timings to be more natural based on the dialogue context. The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions.",,,,ACL
222,2020,The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents,"Kurt Shuster,Da Ju,Stephen Roller,Emily Dinan","We introduce dodecaDialogue: a set of 12 tasks that measures if a conversational agent can communicate engagingly with personality and empathy, ask questions, answer questions by utilizing knowledge resources, discuss topics and situations, and perceive and converse about images. By multi-tasking on such a broad large-scale set of data, we hope to both move towards and measure progress in producing a single unified agent that can perceive, reason and converse with humans in an open-domain setting. We show that such multi-tasking improves over a BERT pre-trained baseline, largely due to multi-tasking with very large dialogue datasets in a similar domain, and that the multi-tasking in general provides gains to both text and image-based tasks using several metrics in both the fine-tune and task transfer settings. We obtain state-of-the-art results on many of the tasks, providing a strong baseline for this challenge.",,,,ACL
223,2020,Automatic Poetry Generation from Prosaic Text,Tim Van de Cruys,"In the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language. In particular, language models based on neural networks have improved the state of the art with regard to predictive language modeling, while topic models are successful at capturing clear-cut, semantic dimensions. In this paper, we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation. The system is exclusively trained on standard, non-poetic text, and its output is constrained in order to confer a poetic character to the generated verse. The framework is applied to the generation of poems in both English and French, and is equally evaluated for both languages. Even though it only uses standard, non-poetic text as input, the system yields state of the art results for poetry generation.",,,,ACL
224,2020,Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation,"Chao Zhao,Marilyn Walker,Snigdha Chaturvedi","Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.",,,,ACL
225,2020,Enabling Language Models to Fill in the Blanks,"Chris Donahue,Mina Lee,Percy Liang","We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document. While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling—a special case of infilling where text is predicted at the end of a document. In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling. To this end, we train (or fine tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked. We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics. Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.",,,,ACL
226,2020,INSET: Sentence Infilling with INter-SEntential Transformer,"Yichen Huang,Yizhe Zhang,Oussama Elachqar,Yu Cheng","Missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion. This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context. Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation. In this paper, we propose a framework to decouple the challenge and address these three aspects respectively, leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2. We empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context.",,,,ACL
227,2020,Improving Adversarial Text Generation by Modeling the Distant Future,"Ruiyi Zhang,Changyou Chen,Zhe Gan,Wenlin Wang","Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation. Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply. We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues. Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization. Extensive experiments demonstrate that the proposed method leads to improved performance.",,,,ACL
228,2020,Simple and Effective Retrieve-Edit-Rerank Text Generation,"Nabil Hossain,Marjan Ghazvininejad,Luke Zettlemoyer","Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output. We propose to extend this framework with a simple and effective post-generation ranking approach. Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output. We use a standard editing model with simple task-specific re-ranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies. Experiments on two machine translation (MT) datasets show new state-of-art results. We also achieve near state-of-art performance on the Gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work.",,,,ACL
229,2020,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Wang Zhu,Hexiang Hu,Jiacheng Chen,Zhiwei Deng","Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk’s generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page: https://github.com/Sha-Lab/babywalk.",,,,ACL
230,2020,Cross-media Structured Common Space for Multimedia Event Extraction,"Manling Li,Alireza Zareian,Qi Zeng,Spencer Whitehead","We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.",,,,ACL
231,2020,Learning to Segment Actions from Observation and Narration,"Daniel Fried,Jean-Baptiste Alayrac,Phil Blunsom,Chris Dyer","We apply a generative segmental model of task structure, guided by narration, to action segmentation in video. We focus on unsupervised and weakly-supervised settings where no action labels are known during training. Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos. Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality.",,,,ACL
232,2020,Learning to execute instructions in a Minecraft dialogue,"Prashant Jayannavar,Anjali Narayan-Chen,Julia Hockenmaier","The Minecraft Collaborative Building Task is a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated Blocks World Environment. We define the subtask of predicting correct action sequences (block placements and removals) in a given game context, and show that capturing B’s past actions as well as B’s perspective leads to a significant improvement in performance on this challenging language understanding problem.",,,,ACL
233,2020,MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning,"Jie Lei,Liwei Wang,Yelong Shen,Dong Yu","Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph. Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture. The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation. Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events.",,,,ACL
234,2020,What is Learned in Visually Grounded Neural Syntax Acquisition,"Noriyuki Kojima,Hadar Averbuch-Elor,Alexander Rush,Yoav Artzi","Visual features are a promising signal for learning bootstrap textual models. However, blackbox learning models make it difficult to isolate the specific contribution of visual components. In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal. By constructing simplified versions of the model, we isolate the core factors that yield the model’s strong performance. Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better. We also find that a simple lexical signal of noun concreteness plays the main role in the model’s predictions as opposed to more complex syntactic reasoning.",,,,ACL
235,2020,A Batch Normalized Inference Network Keeps the KL Vanishing Away,"Qile Zhu,Wei Bi,Xiaojiang Liu,Xiyao Ma","Variational Autoencoder (VAE) is widely used as a generative model to approximate a model’s posterior on latent variables by combining the amortized variational inference and deep neural networks. However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as “posterior collapse”. Previous approaches consider the Kullback–Leibler divergence (KL) individual for each datapoint. We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL’s distribution positive. Then we propose Batch Normalized-VAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior’s parameters. Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently. We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE). Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.",,,,ACL
236,2020,Contextual Embeddings: When Are They Worth It?,"Simran Arora,Avner May,Jian Zhang,Christopher Ré","We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline—random word embeddings—focusing on the impact of the training set size and the linguistic properties of the task. Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks. Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.",,,,ACL
237,2020,Interactive Classification by Asking Informative Questions,"Lili Yu,Howard Chen,Sida I. Wang,Tao Lei","We study the potential for interaction in natural language classification. We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions. At each turn, our system decides between asking the most informative question or making the final classification pre-diction. The simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on simple crowd-sourcing tasks. We evaluate our approach on two domains, showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction.",,,,ACL
238,2020,Knowledge Graph Embedding Compression,Mrinmaya Sachan,"Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications. With a large KG, the embeddings consume a large amount of storage and memory. This is problematic and prohibits the deployment of these techniques in many real world settings. Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes. The approach can be trained end-to-end with simple modifications to any existing KG embedding technique. We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance. The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.",,,,ACL
239,2020,Low Resource Sequence Tagging using Sentence Reconstruction,"Tal Perl,Sriram Chaudhury,Raja Giryes","This work revisits the task of training sequence tagging models with limited resources using transfer learning. We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings. Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines. We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 F1 score in Dutch using only one sample from it.",,,,ACL
240,2020,Masked Language Model Scoring,"Julian Salazar,Davis Liang,Toan Q. Nguyen,Katrin Kirchhoff","Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model’s WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL’s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.",,,,ACL
241,2020,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Yun Tang,Jing Huang,Guangtao Wang,Xiaodong He","Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.",,,,ACL
242,2020,Posterior Calibrated Training on Sentence Classification Tasks,"Taehee Jung,Dongyeop Kang,Hua Cheng,Lucas Mentch","Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability. In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone. When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications. Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives. Our PosCal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline. We achieved the comparable task performance with 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline. PosCal training can be easily extendable to any types of classification tasks as a form of regularization term. Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets.",,,,ACL
243,2020,Posterior Control of Blackbox Generation,"Xiang Lisa Li,Alexander Rush","Text generation often requires high-precision output that obeys task-specific rules. This fine-grained control is difficult to enforce with off-the-shelf deep learning models. In this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach. Under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model. This approach allows users to ground internal model decisions based on prior knowledge, without sacrificing the representational power of neural generative models. Experiments consider applications of this approach for text generation. We find that this method improves over standard benchmarks, while also providing fine-grained control.",,,,ACL
244,2020,Pretrained Transformers Improve Out-of-Distribution Robustness,"Dan Hendrycks,Xiaoyuan Liu,Eric Wallace,Adam Dziedzic","Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers’ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.",,,,ACL
245,2020,Robust Encodings: A Framework for Combating Adversarial Typos,"Erik Jones,Robin Jia,Aditi Raghunathan,Percy Liang","Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs. Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT. In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture. The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings. Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks. We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity). We instantiate RobEn to defend against a large family of adversarial typos. Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack.",,,,ACL
246,2020,Showing Your Work Doesn’t Always Work,"Raphael Tang,Jaejun Lee,Ji Xin,Xinyu Liu","In natural language processing, a recently popular line of work explores how to best report the experimental results of neural networks. One exemplar publication, titled “Show Your Work: Improved Reporting of Experimental Results” (Dodge et al., 2019), advocates for reporting the expected validation effectiveness of the best-tuned model, with respect to the computational budget. In the present work, we critically examine this paper. As far as statistical generalizability is concerned, we find unspoken pitfalls and caveats with this approach. We analytically show that their estimator is biased and uses error-prone assumptions. We find that the estimator favors negative errors and yields poor bootstrapped confidence intervals. We derive an unbiased alternative and bolster our claims with empirical evidence from statistical simulation. Our codebase is at https://github.com/castorini/meanmax.",,,,ACL
247,2020,Span Selection Pre-training for Question Answering,"Michael Glass,Alfio Gliozzo,Rishav Chakravarti,Anthony Ferritto","BERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA). BERT is pretrained on two auxiliary tasks: Masked Language Model and Next Sentence Prediction. In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding. Span Selection PreTraining (SSPT) poses cloze-like training instances, but rather than draw the answer from the model’s parameters, it is selected from a relevant passage. We find significant and consistent improvements over both BERT-BASE and BERT-LARGE on multiple Machine Reading Comprehension (MRC) datasets. Specifically, our proposed model has strong empirical evidence as it obtains SOTA results on Natural Questions, a new benchmark MRC dataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction. We also show significant impact in HotpotQA, improving answer prediction F1 by 4 points and supporting fact prediction F1 by 1 point and outperforming the previous best system. Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount.",,,,ACL
248,2020,Topological Sort for Sentence Ordering,"Shrimai Prabhumoye,Ruslan Salakhutdinov,Alan W Black","Sentence ordering is the task of arranging the sentences of a given text in the correct order. Recent work using deep neural networks for this task has framed it as a sequence prediction problem. In this paper, we propose a new framing of this task as a constraint solving problem and introduce a new technique to solve it. Additionally, we propose a human evaluation for this task. The results on both automatic and human metrics across four different datasets show that this new technique is better at capturing coherence in documents.",,,,ACL
249,2020,Weight Poisoning Attacks on Pretrained Models,"Keita Kurita,Paul Michel,Graham Neubig","Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct “weight poisoning” attacks where pre-trained weights are injected with vulnerabilities that expose “backdoors” after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks.",,,,ACL
250,2020,schuBERT: Optimizing Elements of BERT,"Ashish Khetan,Zohar Karnin","Transformers have gradually become a key component for many state-of-the-art natural language representation models. A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0. This model however is computationally prohibitive and has a huge number of parameters. In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model. We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency. We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers. In particular, our schuBERT gives 6.6% higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters.",,,,ACL
251,2020,ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation,"Lifu Tu,Richard Yuanzhe Pang,Sam Wiseman,Kevin Gimpel","We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model. In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy. This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.",,,,ACL
252,2020,Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation,"Aditya Siddhant,Ankur Bapna,Yuan Cao,Orhan Firat","Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results: (i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. (ii) Self-supervision improves zero-shot translation quality in multilingual models. (iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.",,,,ACL
253,2020,On The Evaluation of Machine Translation Systems Trained With Back-Translation,"Sergey Edunov,Myle Ott,Marc’Aurelio Ranzato,Michael Auli","Back-translation is a widely used data augmentation technique which leverages target monolingual data. However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese. This is believed to be due to translationese inputs better matching the back-translated training data. In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators. We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs. BLEU cannot capture human preferences because references are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency.",,,,ACL
254,2020,Simultaneous Translation Policies: From Fixed to Adaptive,"Baigong Zheng,Kaibo Liu,Renjie Zheng,Mingbo Ma","Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies. We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. Experiments on Chinese -> English and German -> English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.",,,,ACL
255,2020,Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information,"Michele Bevilacqua,Roberto Navigli","Neural architectures are the current state of the art in Word Sense Disambiguation (WSD). However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB). We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set. As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard all-words English WSD evaluation benchmarks. On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English.",,,,ACL
256,2020,Glyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs,"Hong-You Chen,Sz-Han Yu,Shou-de Lin","Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus. We show that characters’ written form, Glyphs, in ideographic languages could carry rich semantics. We present a multi-modal model, Glyph2Vec, to tackle Chinese out-of-vocabulary word embedding problem. Glyph2Vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding, without the need of accessing any corpus, which is useful for improving Chinese NLP systems, especially for low-resource scenarios. Experiments across different applications show the significant effectiveness of our model.",,,,ACL
257,2020,Multidirectional Associative Optimization of Function-Specific Word Representations,"Daniela Gerz,Ivan Vulić,Marek Rei,Roi Reichart","We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures. Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together. The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure. We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity. The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%.",,,,ACL
258,2020,Predicting Degrees of Technicality in Automatic Terminology Extraction,"Anna Hätty,Dominik Schlechtweg,Michael Dorna,Sabine Schulte im Walde","While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied. We semi-automatically create a German gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction. When defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings. We suggest two novel models to exploit general- vs. domain-specific comparisons: a simple neural network model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally. Both models outperform previous approaches, with the multi-channel model performing best.",,,,ACL
259,2020,Verbal Multiword Expressions for Identification of Metaphor,"Omid Rohanian,Marek Rei,Shiva Taslimipoor,Le An Ha","Metaphor is a linguistic device in which a concept is expressed by mentioning another. Identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics. Multiword Expressions (MWEs), on the other hand, are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models. This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of MWEs. To the best of our knowledge, this is the first “MWE-aware” metaphor identification system paving the way for further experiments on the complex interactions of these phenomena. The results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets.",,,,ACL
260,2020,Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer,"Jieyu Zhao,Subhabrata Mukherjee,Saghar Hosseini,Kai-Wei Chang","Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.",,,,ACL
261,2020,"Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?","Kobi Leins,Jey Han Lau,Timothy Baldwin","As part of growing NLP capabilities, coupled with an awareness of the ethical dimensions of research, questions have been raised about whether particular datasets and tasks should be deemed off-limits for NLP research. We examine this question with respect to a paper on automatic legal sentencing from EMNLP 2019 which was a source of some debate, in asking whether the paper should have been allowed to be published, who should have been charged with making such a decision, and on what basis. We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines.",,,,ACL
262,2020,Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds,Kawin Ethayarajh,"Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). However, manually annotating a large dataset with a protected attribute is slow and expensive. Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias? While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias. In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval. We provide empirical evidence that a 95% confidence interval derived this way consistently bounds the true bias. In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim. Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases. For example, consider a co-reference resolution system that is 5% more accurate on gender-stereotypical sentences – to claim it is biased with 95% confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.",,,,ACL
263,2020,It’s Morphin’ Time! Combating Linguistic Discrimination with Inflectional Perturbations,"Samson Tan,Shafiq Joty,Min-Yen Kan,Richard Socher","Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.",,,,ACL
264,2020,Mitigating Gender Bias Amplification in Distribution by Posterior Regularization,"Shengyu Jia,Tao Meng,Jieyu Zhao,Kai-Wei Chang","Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models’ top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.",,,,ACL
265,2020,Towards Understanding Gender Bias in Relation Extraction,"Andrew Gaut,Tony Sun,Shirlyn Tang,Yuxin Huang","Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction. While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems. In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems. We find that when extracting spouse-of and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different. However, such disparity does not appear when extracting relations such as birthDate or birthPlace. We also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases. Unfortunately, due to NRE models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in NRE.",,,,ACL
266,2020,A Probabilistic Generative Model for Typographical Analysis of Early Modern Printing,"Kartik Goyal,Chris Dyer,Christopher Warren,Maxwell G’Sell","We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents. We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance. Our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables, and then modifies the result by generating a non-interpretable latent vector responsible for inking variations, jitter, noise from the archiving process, and other unforeseen phenomena associated with Early Modern printing. Critically, by introducing an inference network whose input is restricted to the visual residual between the observation and the interpretably-modified template, we are able to control and isolate what the vector-valued latent variable captures. We show that our approach outperforms rigid interpretable clustering baselines (c.f. Ocular) and overly-flexible deep generative models (VAE) alike on the task of completely unsupervised discovery of typefaces in mixed-fonts documents.",,,,ACL
267,2020,Attentive Pooling with Learnable Norms for Text Representation,"Chuhan Wu,Fangzhao Wu,Tao Qi,Xiaohui Cui","Pooling is an important technique for learning text representations in many neural NLP models. In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L∞ norm of input features. However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks. In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited. In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation. Different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks. In addition, we propose two methods to ensure the numerical stability of the model training. The first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion. The second one is re-formulation, which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation. Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling.",,,,ACL
268,2020,Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks,"Fynn Schröder,Chris Biemann","Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks. However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach. We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups. Our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel. Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work. We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance. We provide an efficient, open-source implementation.",,,,ACL
269,2020,How Does Selective Mechanism Improve Self-Attention Networks?,"Xinwei Geng,Longyue Wang,Xing Wang,Bing Qin","Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words. However, the underlying reasons for their strong performance have not been well explained. In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax. Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs. Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence.",,,,ACL
270,2020,Improving Transformer Models by Reordering their Sublayers,"Ofir Press,Noah A. Smith,Omer Levy","Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.",,,,ACL
271,2020,Single Model Ensemble using Pseudo-Tags and Distinct Vectors,"Ryosuke Kuwabara,Jun Suzuki,Hideki Nakayama","Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort. In this study, we propose a novel method that replicates the effects of a model ensemble with a single model. Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors. Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/K-times fewer parameters.",,,,ACL
272,2020,Zero-shot Text Classification via Reinforced Self-training,"Zhiquan Ye,Yuxia Geng,Jiaoyan Chen,Jingmin Chen","Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity. In this situation, transferring from seen classes to unseen classes is extremely hard. To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data. Traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets. We propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection. Experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification",,,,ACL
273,2020,A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation,"Yongjing Yin,Fandong Meng,Jinsong Su,Chulun Zhou","Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.",,,,ACL
274,2020,A Relaxed Matching Procedure for Unsupervised BLI,"Xu Zhao,Zihao Wang,Yong Zhang,Hao Wu","Recently unsupervised Bilingual Lexicon Induction(BLI) without any parallel corpus has attracted much research interest. One of the crucial parts in methods for the BLI task is the matching procedure. Previous works impose a too strong constraint on the matching and lead to many counterintuitive translation pairings. Thus We propose a relaxed matching procedure to find a more precise matching between two languages. We also find that aligning source and target language embedding space bidirectionally will bring significant improvement. We follow the previous iterative framework to conduct experiments. Results on standard benchmark demonstrate the effectiveness of our proposed method, which substantially outperforms previous unsupervised methods.",,,,ACL
275,2020,Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation,"Xuanli He,Gholamreza Haffari,Mohammad Norouzi","This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English <=> (German, Romanian, Estonian, Finnish, Hungarian).",,,,ACL
276,2020,Geometry-aware domain adaptation for unsupervised alignment of word embeddings,"Pratik Jawanpuria,Mayank Meghwanshi,Bamdev Mishra","We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages. Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices. This viewpoint arises from the aim to align the second order information of the two language spaces. The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation. Empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs. The performance improvement is more significant for distant language pairs.",,,,ACL
277,2020,Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation,"Qiu Ran,Yankai Lin,Peng Li,Jie Zhou","Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is predicted token-by-token. By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors. Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model.",,,,ACL
278,2020,On the Inference Calibration of Neural Machine Translation,"Shuo Wang,Zhaopeng Tu,Shuming Shi,Yang Liu","Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models. Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.",,,,ACL
279,2020,Camouflaged Chinese Spam Content Detection with Semi-supervised Generative Active Learning,"Zhuoren Jiang,Zhe Gao,Yu Duan,Yangyang Kang","We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task. A “self-diversity” criterion is proposed for measuring the “worthiness” of a candidate for annotation. A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation. The preliminary experiment demonstrates the proposed SIGNAL model is not only sensitive to spam sample selection, but also can improve the performance of a series of conventional active learning models for Chinese spam detection task. To the best of our knowledge, this is the first work to integrate active learning and semi-supervised generative learning for text spam detection.",,,,ACL
280,2020,Distinguish Confusing Law Articles for Legal Judgment Prediction,"Nuo Xu,Pinghui Wang,Long Chen,Li Pan","Legal Judgement Prediction (LJP) is the task of automatically predicting a law case’s judgment results given a text describing the case’s facts, which has great prospects in judicial assistance systems and handy services for the public. In practice, confusing charges are often presented, because law cases applicable to similar law articles are easily misjudged. To address this issue, existing work relies heavily on domain experts, which hinders its application in different law systems. In this paper, we present an end-to-end model, LADAN, to solve the task of LJP. To distinguish confusing charges, we propose a novel graph neural network, GDL, to automatically learn subtle differences between confusing law articles, and also design a novel attention mechanism that fully exploits the learned differences to attentively extract effective discriminative features from fact descriptions. Experiments conducted on real-world datasets demonstrate the superiority of our LADAN.",,,,ACL
281,2020,Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation,"Liting Liu,Jie Liu,Wenzheng Zhang,Ziming Chi","Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job description. To this end, we propose a novel task of Job Posting Generation (JPG) which is cast as a conditional text generation problem to generate job requirements according to the job descriptions. To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA. Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels. At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results. The proposed approach is evaluated on real-world job posting data. Experimental results clearly demonstrate the effectiveness of the proposed method.",,,,ACL
282,2020,HyperCore: Hyperbolic and Co-graph Representation for Automatic ICD Coding,"Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao","The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code. ICD coding aims to assign proper ICD codes to a medical record. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence. In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem. Specifically, we propose a hyperbolic representation method to leverage the code hierarchy. Moreover, we propose a graph convolutional network to utilize the code co-occurrence. Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-of-the-art methods.",,,,ACL
283,2020,Hyperbolic Capsule Networks for Multi-Label Classification,"Boli Chen,Xin Huang,Lin Xiao,Liping Jing","Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling). Such operations limit the performance. For instance, a multi-label document may contain several concepts. In this case, one vector can not sufficiently capture its salient and discriminative content. Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which have two merits. First, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents. Second, Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks. To efficiently handle large-scale MLC datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing. Extensive experiments are conducted on four benchmark datasets. Compared with the state-of-the-art methods, HyperCaps significantly improves the performance of MLC especially on tail labels.",,,,ACL
284,2020,Improving Segmentation for Technical Support Problems,"Kushal Chauhan,Abhirut Gupta","Technical support problems are often long and complex. They typically contain user descriptions of the problem, the setup, and steps for attempted resolution. Often they also contain various non-natural language text elements like outputs of commands, snippets of code, error messages or stack traces. These elements contain potentially crucial information for problem resolution. However, they cannot be correctly parsed by tools designed for natural language. In this paper, we address the problem of segmentation for technical support questions. We formulate the problem as a sequence labelling task, and study the performance of state of the art approaches. We compare this against an intuitive contextual sentence-level classification baseline, and a state of the art supervised text-segmentation approach. We also introduce a novel component of combining contextual embeddings from multiple language models pre-trained on different data sources, which achieves a marked improvement over using embeddings from a single pre-trained language model. Finally, we also demonstrate the usefulness of such segmentation with improvements on the downstream task of answer retrieval.",,,,ACL
285,2020,MOOCCube: A Large-scale Data Repository for NLP Applications in MOOCs,"Jifan Yu,Gan Luo,Tong Xiao,Qingyang Zhong","The prosperity of Massive Open Online Courses (MOOCs) provides fodder for many NLP and AI research for education applications, e.g., course concept extraction, prerequisite relation discovery, etc. However, the publicly available datasets of MOOC are limited in size with few types of data, which hinders advanced models and novel attempts in related topics. Therefore, we present MOOCCube, a large-scale data repository of over 700 MOOC courses, 100k concepts, 8 million student behaviors with an external resource. Moreover, we conduct a prerequisite discovery task as an example application to show the potential of MOOCCube in facilitating relevant research. The data repository is now available at http://moocdata.cn/data/MOOCCube.",,,,ACL
286,2020,Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs,"Jun Chen,Xiaoya Dai,Quan Yuan,Chao Lu","The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability. In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system. The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks, which is critical for AI-empowered healthcare. The evaluation conducted on the real Electronic Medical Record (EMR) documents from hospitals and annotated by professional doctors proves that, the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable.",,,,ACL
287,2020,Analyzing the Persuasive Effect of Style in News Editorial Argumentation,"Roxanne El Baff,Henning Wachsmuth,Khalid Al Khatib,Benno Stein","News editorials argue about political issues in order to challenge or reinforce the stance of readers with different ideologies. Previous research has investigated such persuasive effects for argumentative content. In contrast, this paper studies how important the style of news editorials is to achieve persuasion. To this end, we first compare content- and style-oriented classifiers on editorials from the liberal NYTimes with ideology-specific effect annotations. We find that conservative readers are resistant to NYTimes style, but on liberals, style even has more impact than content. Focusing on liberals, we then cluster the leads, bodies, and endings of editorials, in order to learn about writing style patterns of effective argumentation.",,,,ACL
288,2020,"ECPE-2D: Emotion-Cause Pair Extraction based on Joint Two-Dimensional Representation, Interaction and Prediction","Zixiang Ding,Rui Xia,Jianfei Yu","In recent years, a new interesting task, called emotion-cause pair extraction (ECPE), has emerged in the area of text emotion analysis. It aims at extracting the potential pairs of emotions and their corresponding causes in a document. To solve this task, the existing research employed a two-step framework, which first extracts individual emotion set and cause set, and then pair the corresponding emotions and causes. However, such a pipeline of two steps contains some inherent flaws: 1) the modeling does not aim at extracting the final emotion-cause pair directly; 2) the errors from the first step will affect the performance of the second step. To address these shortcomings, in this paper we propose a new end-to-end approach, called ECPE-Two-Dimensional (ECPE-2D), to represent the emotion-cause pairs by a 2D representation scheme. A 2D transformer module and two variants, window-constrained and cross-road 2D transformers, are further proposed to model the interactions of different emotion-cause pairs. The 2D representation, interaction, and prediction are integrated into a joint framework. In addition to the advantages of joint modeling, the experimental results on the benchmark emotion cause corpus show that our approach improves the F1 score of the state-of-the-art from 61.28% to 68.89%.",,,,ACL
289,2020,Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction,"Penghui Wei,Jiahao Zhao,Wenji Mao","Emotion-cause pair extraction aims to extract all emotion clauses coupled with their cause clauses from a given document. Previous work employs two-step approaches, in which the first step extracts emotion clauses and cause clauses separately, and the second step trains a classifier to filter out negative pairs. However, such pipeline-style system for emotion-cause pair extraction is suboptimal because it suffers from error propagation and the two steps may not adapt to each other well. In this paper, we tackle emotion-cause pair extraction from a ranking perspective, i.e., ranking clause pair candidates in a document, and propose a one-step neural approach which emphasizes inter-clause modeling to perform end-to-end extraction. It models the interrelations between the clauses in a document to learn clause representations with graph attention, and enhances clause pair representations with kernel-based relative position embedding for effective ranking. Experimental results show that our approach significantly outperforms the current two-step systems, especially in the condition of extracting multiple pairs in one document.",,,,ACL
290,2020,Embarrassingly Simple Unsupervised Aspect Extraction,"Stéphan Tulkens,Andreas van Cranenburgh","We present a simple but effective method for aspect identification in sentiment analysis. Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages. We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable. Previous work relied on syntactic features and complex neural models. We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed. The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat.",,,,ACL
291,2020,Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge,"Bowen Zhang,Min Yang,Xutao Li,Yunming Ye","Stance detection is an important task, which aims to classify the attitude of an opinionated text towards a given target. Remarkable success has been achieved when sufficient labeled training data is available. However, annotating sufficient data is labor-intensive, which establishes significant barriers for generalizing the stance classifier to the data with new targets. In this paper, we proposed a Semantic-Emotion Knowledge Transferring (SEKT) model for cross-target stance detection, which uses the external knowledge (semantic and emotion lexicons) as a bridge to enable knowledge transfer across different targets. Specifically, a semantic-emotion heterogeneous graph is constructed from external semantic and emotion lexicons, which is then fed into a graph convolutional network to learn multi-hop semantic connections between words and emotion tags. Then, the learned semantic-emotion graph representation, which serves as prior knowledge bridging the gap between the source and target domains, is fully integrated into the bidirectional long short-term memory (BiLSTM) stance classifier by adding a novel knowledge-aware memory unit to the BiLSTM cell. Extensive experiments on a large real-world dataset demonstrate the superiority of SEKT against the state-of-the-art baseline methods.",,,,ACL
292,2020,KinGDOM: Knowledge-Guided DOMain Adaptation for Sentiment Analysis,"Deepanway Ghosal,Devamanyu Hazarika,Abhinaba Roy,Navonil Majumder","Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis. In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge. We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts. These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner. Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework.",,,,ACL
293,2020,Modelling Context and Syntactical Features for Aspect-based Sentiment Analysis,"Minh Hieu Phan,Philip O. Ogunbona","The aspect-based sentiment analysis (ABSA) consists of two conceptual tasks, namely an aspect extraction and an aspect sentiment classification. Rather than considering the tasks separately, we build an end-to-end ABSA solution. Previous works in ABSA tasks did not fully leverage the importance of syntactical information. Hence, the aspect extraction model often failed to detect the boundaries of multi-word aspect terms. On the other hand, the aspect sentiment classifier was unable to account for the syntactical correlation between aspect terms and the context words. This paper explores the grammatical aspect of the sentence and employs the self-attention mechanism for syntactical learning. We combine part-of-speech embeddings, dependency-based embeddings and contextualized embeddings (e.g. BERT, RoBERTa) to enhance the performance of the aspect extractor. We also propose the syntactic relative distance to de-emphasize the adverse effects of unrelated words, having weak syntactic connection with the aspect terms. This increases the accuracy of the aspect sentiment classifier. Our solutions outperform the state-of-the-art models on SemEval-2014 dataset in both two subtasks.",,,,ACL
294,2020,Parallel Data Augmentation for Formality Style Transfer,"Yi Zhang,Tao Ge,Xu Sun","The main barrier to progress in the task of Formality Style Transfer is the inadequacy of training data. In this paper, we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems. Experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset.",,,,ACL
295,2020,Relational Graph Attention Network for Aspect-based Sentiment Analysis,"Kai Wang,Weizhou Shen,Yunyi Yang,Xiaojun Quan","Aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews. Most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words. However, due to the complexity of language and the existence of multiple aspects in a single sentence, these models often confuse the connections. In this paper, we address this problem by means of effective encoding of syntax information. Firstly, we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree. Then, we propose a relational graph attention network (R-GAT) to encode the new tree structure for sentiment prediction. Extensive experiments are conducted on the SemEval 2014 and Twitter datasets, and the experimental results confirm that the connections between aspects and opinion words can be better established with our approach, and the performance of the graph attention network (GAT) is significantly improved as a consequence.",,,,ACL
296,2020,SpanMlt: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction,"He Zhao,Longtao Huang,Rong Zhang,Quan Lu","Aspect terms extraction and opinion terms extraction are two key problems of fine-grained Aspect Based Sentiment Analysis (ABSA). The aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems. However, traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms. Although some recent co-extraction methods have been proposed to extract both terms jointly, they fail to extract them as pairs. To this end, this paper proposes an end-to-end method to solve the task of Pair-wise Aspect and Opinion Terms Extraction (PAOTE). Furthermore, this paper treats the problem from a perspective of joint term and relation extraction rather than under the sequence tagging formulation performed in most prior works. We propose a multi-task learning framework based on shared spans, where the terms are extracted under the supervision of span boundaries. Meanwhile, the pair-wise relations are jointly identified using the span representations. Extensive experiments show that our model consistently outperforms state-of-the-art methods.",,,,ACL
297,2020,Syntax-Aware Opinion Role Labeling with Dependency Graph Convolutional Networks,"Bo Zhang,Yue Zhang,Rui Wang,Zhenghua Li","Opinion role labeling (ORL) is a fine-grained opinion analysis task and aims to answer “who expressed what kind of sentiment towards what?”. Due to the scarcity of labeled data, ORL remains challenging for data-driven methods. In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations. We also propose dependency graph convolutional networks (DEPGCN) to encode parser information at different processing levels. In order to compensate for parser inaccuracy and reduce error propagation, we introduce multi-task learning (MTL) to train the parser and the ORL model simultaneously. We verify our methods on the benchmark MPQA corpus. The experimental results show that syntactic information is highly valuable for ORL, and our final MTL model effectively boosts the F1 score by 9.29 over the syntax-agnostic baseline. In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT). Our best model achieves 4.34 higher F1 score than the current state-ofthe-art.",,,,ACL
298,2020,Towards Better Non-Tree Argument Mining: Proposition-Level Biaffine Parsing with Task-Specific Parameterization,"Gaku Morio,Hiroaki Ozaki,Terufumi Morishita,Yuta Koreeda","State-of-the-art argument mining studies have advanced the techniques for predicting argument structures. However, the technology for capturing non-tree-structured arguments is still in its infancy. In this paper, we focus on non-tree argument mining with a neural model. We jointly predict proposition types and edges between propositions. Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can predict a non-tree argument consisting of edges. Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines.",,,,ACL
299,2020,A Span-based Linearization for Constituent Trees,"Yang Wei,Yuanbin Wu,Man Lan","We propose a novel linearization of a constituent tree, together with a new locally normalized model. For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them. Compared with global models, our model is fast and parallelizable. Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models.",,,,ACL
300,2020,An Empirical Comparison of Unsupervised Constituency Parsing Methods,"Jun Li,Yifan Cao,Jiong Cai,Yong Jiang","Unsupervised constituency parsing aims to learn a constituency parser from a training corpus without parse tree annotations. While many methods have been proposed to tackle the problem, including statistical and neural methods, their experimental results are often not directly comparable due to discrepancies in datasets, data preprocessing, lexicalization, and evaluation metrics. In this paper, we first examine experimental settings used in previous work and propose to standardize the settings for better comparability between methods. We then empirically compare several existing methods, including decade-old and newly proposed ones, under the standardized settings on English and Japanese, two languages with different branching tendencies. We find that recent models do not show a clear advantage over decade-old models in our experiments. We hope our work can provide new insights into existing methods and facilitate future empirical evaluation of unsupervised constituency parsing.",,,,ACL
301,2020,Efficient Constituency Parsing by Pointing,"Thanh-Tung Nguyen,Xuan-Phi Nguyen,Shafiq Joty,Xiaoli Li","We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks. Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span. Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference. The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity. Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster. Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing.",,,,ACL
302,2020,Efficient Second-Order TreeCRF for Neural Dependency Parsing,"Yu Zhang,Zhenghua Li,Min Zhang","In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation. As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss. This paper for the first time presents a second-order TreeCRF extension to the biaffine parser. For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF. To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation. Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data. We release our code at https://github.com/yzhangcs/crfpar.",,,,ACL
303,2020,Representations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive LSTMs,"Michael Lepori,Tal Linzen,R. Thomas McCoy","Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks. Such tree-based networks can be provided with a constituency parse, a dependency parse, or both. We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task. We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement. Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking.",,,,ACL
304,2020,Structure-Level Knowledge Distillation For Multilingual Sequence Labeling,"Xinyu Wang,Yong Jiang,Nguyen Bach,Tao Wang","Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages. Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages. However, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations. In this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student). We propose two novel KD methods based on structure-level information: (1) approximately minimizes the distance between the student’s and the teachers’ structure-level probability distributions, (2) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions. Our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models.",,,,ACL
305,2020,Dynamic Online Conversation Recommendation,"Xingshan Zeng,Jing Li,Lu Wang,Zhiming Mao","Trending topics in social media content evolve over time, and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner. Here we study dynamic online conversation recommendation, to help users engage in conversations that satisfy their evolving interests. While most prior work assumes static user interests, our model is able to capture the temporal aspects of user interests, and further handle future conversations that are unseen during training time. Concretely, we propose a neural architecture to exploit changes of user interactions and interests over time, to predict which discussions they are likely to enter. We conduct experiments on large-scale collections of Reddit conversations, and results on three subreddits show that our model significantly outperforms state-of-the-art models that make a static assumption of user interests. We further evaluate on handling “cold start”, and observe consistently better performance by our model when considering various degrees of sparsity of user’s chatting history and conversation contexts. Lastly, analyses on our model outputs indicate user interest change, explaining the advantage and efficacy of our approach.",,,,ACL
306,2020,Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer,"Jianfei Yu,Jing Jiang,Li Yang,Rui Xia","In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts. Existing approaches for MNER mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context. To tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations. To alleviate the visual bias, we further propose to leverage purely text-based entity span detection as an auxiliary module, and design a Unified Multimodal Transformer to guide the final predictions with the entity span predictions. Experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets.",,,,ACL
307,2020,"Stock Embeddings Acquired from News Articles and Price History, and an Application to Portfolio Optimization","Xin Du,Kumiko Tanaka-Ishii","Previous works that integrated news articles to better process stock prices used a variety of neural networks to predict price movements. The textual and price information were both encoded in the neural network, and it is therefore difficult to apply this approach in situations other than the original framework of the notoriously hard problem of price prediction. In contrast, this paper presents a method to encode the influence of news articles through a vector representation of stocks called a stock embedding. The stock embedding is acquired with a deep learning framework using both news articles and price history. Because the embedding takes the operational form of a vector, it is applicable to other financial problems besides price prediction. As one example application, we show the results of portfolio optimization using Reuters & Bloomberg headlines, producing a capital gain 2.8 times larger than that obtained with a baseline method using only stock price data. This suggests that the proposed stock embedding can leverage textual financial semantics to solve financial prediction problems.",,,,ACL
308,2020,What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context,"Ramy Baly,Georgi Karadzhov,Jisun An,Haewoon Kwak","Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction. The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically. Thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content. This makes it possible to detect likely “fake news” the moment they are published, by simply checking the reliability of their source. From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context. Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself in Twitter) vs. (ii) who reads it (i.e., analyzing the target medium’s audience on social media). We further study (iii) what was written about the target medium (in Wikipedia). The evaluation results show that what was written matters most, and we further show that putting all information sources together yields huge improvements over the current state-of-the-art.",,,,ACL
309,2020,An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models,"Hiroshi Noji,Hiroya Takamura","We explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks. Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement. In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model’s robustness on them in English, with a negligible loss of perplexity. The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word. We then provide a detailed analysis of the trained models. One of our findings is the difficulty of object-relative clauses for RNNs. We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause. Augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses. Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.",,,,ACL
310,2020,On the Robustness of Language Encoders against Grammatical Errors,"Fan Yin,Quanyu Long,Tao Meng,Kai-Wei Chang","We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors. Specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data. We use this approach to facilitate debugging models on downstream applications. Results confirm that the performance of all tested models is affected but the degree of impact varies. To interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors. We find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions. We also design a cloze test for BERT and discover that BERT captures the interaction between errors and specific tokens in context. Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors.",,,,ACL
311,2020,Roles and Utilization of Attention Heads in Transformer-based Neural Language Models,"Jae-young Jo,Sung-Hyon Myaeng","Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture. However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model. For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA). Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.",,,,ACL
312,2020,Understanding Attention for Text Classification,"Xiaobing Sun,Wei Lu","Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the token’s significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities can contribute towards model performance.",,,,ACL
313,2020,A Relational Memory-based Embedding Model for Triple Classification and Search Personalization,"Dai Quoc Nguyen,Tu Nguyen,Dinh Phung","Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems. To this end, we introduce a novel embedding model, named R-MeN, that explores a relational memory network to encode potential dependencies in relationship triples. R-MeN considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism. Thus R-MeN encodes new information from interactions between the memory and each input vector to return a corresponding vector. Consequently, R-MeN feeds these 3 returned vectors to a convolutional neural network-based decoder to produce a scalar score for the triple. Experimental results show that our proposed R-MeN obtains state-of-the-art results on SEARCH17 for the search personalization task, and on WN11 and FB13 for the triple classification task.",,,,ACL
314,2020,Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods,"Ning Miao,Yuxuan Song,Hao Zhou,Lei Li","It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data. In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem. In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones. Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach.",,,,ACL
315,2020,Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention,"Yanzeng Li,Bowen Yu,Xue Mengge,Tingwen Liu","Most Chinese pre-trained models take character as the basic unit and learn representation according to character’s external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese. Hence, we propose a novel word-aligned attention to exploit explicit word information, which is complementary to various character-based Chinese pre-trained language models. Specifically, we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion. As a result, word and character information are explicitly integrated at the fine-tuning procedure. Experimental results on five Chinese NLP benchmark tasks demonstrate that our method achieves significant improvements against BERT, ERNIE and BERT-wwm.",,,,ACL
316,2020,On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond,"Chen Wu,Prince Zizhuang Wang,William Yang Wang","Variational autoencoders (VAEs) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling. By tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold. We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them. To this end, we propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching. We apply the proposed Coupled-VAE approach to various VAE models with different regularization, posterior family, decoder structure, and optimization strategy. Experiments on benchmark datasets (i.e., PTB, Yelp, and Yahoo) show consistently improved results in terms of probability estimation and richness of the latent space. We also generalize our method to conditional language modeling and propose Coupled-CVAE, which largely improves the diversity of dialogue generation on the Switchboard dataset.",,,,ACL
317,2020,SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions,"Mao Ye,Chengyue Gong,Qiang Liu","State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution. For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution. In this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties of the ensemble to provably certify the robustness. Our method is simple and structure-free in that it only requires the black-box queries of the model outputs, and hence can be applied to any pre-trained models (such as BERT) and any types of models (world-level or subword-level). Our method significantly outperforms recent state-of-the-art methods for certified robustness on both IMDB and Amazon text classification tasks. To the best of our knowledge, we are the first work to achieve certified robustness on large systems such as BERT with practically meaningful certified accuracy.",,,,ACL
318,2020,A Graph-based Coarse-to-fine Method for Unsupervised Bilingual Lexicon Induction,"Shuo Ren,Shujie Liu,Ming Zhou,Shuai Ma","Unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages. Recent methods are mostly based on unsupervised cross-lingual word embeddings, the key to which is to find initial solutions of word translations, followed by the learning and refinement of mappings between the embedding spaces of two languages. However, previous methods find initial solutions just based on word-level information, which may be (1) limited and inaccurate, and (2) prone to contain some noise introduced by the insufficiently pre-trained embeddings of some words. To deal with those issues, in this paper, we propose a novel graph-based paradigm to induce bilingual lexicons in a coarse-to-fine way. We first build a graph for each language with its vertices representing different words. Then we extract word cliques from the graphs and map the cliques of two languages. Based on that, we induce the initial word translation solution with the central words of the aligned cliques. This coarse-to-fine approach not only leverages clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings. Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons. Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods.",,,,ACL
319,2020,A Reinforced Generation of Adversarial Examples for Neural Machine Translation,"Wei Zou,Shujian Huang,Jun Xie,Xinyu Dai","Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems—fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance. Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning. Our paradigm could expose pitfalls for a given performance metric, e.g., BLEU, and could target any given neural machine translation architecture. We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, RNN-search, and Transformer. The results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples. We also present a qualitative and quantitative analysis for the preference pattern of the attack, demonstrating its capability of pitfall exposure.",,,,ACL
320,2020,A Retrieve-and-Rewrite Initialization Method for Unsupervised Machine Translation,"Shuo Ren,Yu Wu,Shujie Liu,Ming Zhou","The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance. The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models. We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model. The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git.",,,,ACL
321,2020,A Simple and Effective Unified Encoder for Document-Level Machine Translation,"Shuming Ma,Dongdong Zhang,Ming Zhou","Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts are modeled with two separate encoders. Although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single encoder. In this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of our proposed model.",,,,ACL
322,2020,Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation,"Bei Li,Hui Liu,Ziyang Wang,Yufan Jiang","In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. In this paper, we investigate multi-encoder approaches in document-level neural machine translation (NMT). Surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. This makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training. We compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders. Experimental results show that noisy training plays an important role in multi-encoder-based NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT Fr-En task by careful use of noise generation and dropout methods.",,,,ACL
323,2020,Dynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change,"Hongfei Xu,Josef van Genabith,Deyi Xiong,Qiuhui Liu","The choice of hyper-parameters affects the performance of neural models. While much previous research (Sutskever et al., 2013; Duchi et al., 2011; Kingma and Ba, 2015) focuses on accelerating convergence and reducing the effects of the learning rate, comparatively few papers concentrate on the effect of batch size. In this paper, we analyze how increasing batch size affects gradient direction, and propose to evaluate the stability of gradients with their angle change. Based on our observations, the angle change of gradient direction first tends to stabilize (i.e. gradually decrease) while accumulating mini-batches, and then starts to fluctuate. We propose to automatically and dynamically determine batch sizes by accumulating gradients of mini-batches and performing an optimization step at just the time when the direction of gradients starts to fluctuate. To improve the efficiency of our approach for large models, we propose a sampling approach to select gradients of parameters sensitive to the batch size. Our approach dynamically determines proper and efficient batch sizes during training. In our experiments on the WMT 14 English to German and English to French tasks, our approach improves the Transformer with a fixed 25k batch size by +0.73 and +0.82 BLEU respectively.",,,,ACL
324,2020,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Haipeng Sun,Rui Wang,Kehai Chen,Masao Utiyama","Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.",,,,ACL
325,2020,Lexically Constrained Neural Machine Translation with Levenshtein Transformer,"Raymond Hendy Susanto,Shamil Chollampatt,Liling Tan","This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation. Previous work either required re-training existing models with the lexical constraints or incorporating them during beam search decoding with significantly higher computational overheads. Leveraging the flexibility and speed of a recently proposed Levenshtein Transformer model (Gu et al., 2019), our method injects terminology constraints at inference time without any impact on decoding speed. Our method does not require any modification to the training procedure and can be easily applied at runtime with custom dictionaries. Experiments on English-German WMT datasets show that our approach improves an unconstrained baseline and previous approaches.",,,,ACL
326,2020,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","Chaojun Wang,Rico Sennrich","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.",,,,ACL
327,2020,Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model,"Kosuke Takahashi,Katsuhito Sudoh,Satoshi Nakamura","We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. The proposed method evaluates a translation hypothesis in a regression model. The model takes the paired source, reference, and hypothesis sentence all together as an input. A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors. Our experiments show that our proposed method using Cross-lingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance.",,,,ACL
328,2020,ChartDialogs: Plotting from Natural Language Instructions,"Yutong Shao,Ndapa Nakashole","This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types. Extensive experiments show the best-performing method achieving 61% plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task.",,,,ACL
329,2020,GLUECoS: An Evaluation Benchmark for Code-Switched NLP,"Simran Khanuja,Sandipan Dandapat,Anirudh Srinivasan,Sunayana Sitaram","Code-switching is the use of more than one language in the same conversation or utterance. Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks. We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in English-Hindi and English-Spanish. Specifically, our evaluation benchmark includes Language Identification from text, POS tagging, Named Entity Recognition, Sentiment Analysis, Question Answering and a new task for code-switching, Natural Language Inference. We present results on all these tasks using cross-lingual word embedding models and multilingual models. In addition, we fine-tune multilingual models on artificially generated code-switched data. Although multilingual models perform significantly better than cross-lingual models, our results show that in most tasks, across both language pairs, multilingual models fine-tuned on code-switched data perform best, showing that multilingual models can be further optimized for code-switching tasks.",,,,ACL
330,2020,"MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization","Canwen Xu,Jiaxin Pei,Hongtao Wu,Yiyu Liu","Recently, large-scale datasets have vastly facilitated the development in nearly all domains of Natural Language Processing. However, there is currently no cross-task dataset in NLP, which hinders the development of multi-task learning. We propose MATINF, the first jointly labeled large-scale dataset for classification, question answering and summarization. MATINF contains 1.07 million question-answer pairs with human-labeled categories and user-generated question descriptions. Based on such rich information, MATINF is applicable for three major NLP tasks, including classification, question answering, and summarization. We benchmark existing methods and a novel multi-task baseline over MATINF to inspire further research. Our comprehensive comparison and experiments over MATINF and other datasets demonstrate the merits held by MATINF.",,,,ACL
331,2020,MIND: A Large-scale Dataset for News Recommendation,"Fangzhao Wu,Ying Qiao,Jiun-Hung Chen,Chuhan Wu","News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset. In this paper, we present a large-scale dataset named MIND for news recommendation. Constructed from the user click logs of Microsoft News, MIND contains 1 million users and more than 160k English news articles, each of which has rich textual content such as title, abstract and body. We demonstrate MIND a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets. Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling. Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation. The MIND dataset will be available at https://msnews.github.io.",,,,ACL
332,2020,That is a Known Lie: Detecting Previously Fact-Checked Claims,"Shaden Shaar,Nikolay Babulkov,Giovanni Da San Martino,Preslav Nakov","The recent proliferation of ”fake news” has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. Interestingly, despite the importance of the task, it has been largely ignored by the research community so far. Here, we aim to bridge this gap. In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. We further create a specialized dataset, which we release to the research community. Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.",,,,ACL
333,2020,Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation,"Bo Pang,Erik Nijkamp,Wenjuan Han,Linqi Zhou","Open-domain dialogue generation has gained increasing attention in Natural Language Processing. Its evaluation requires a holistic means. Human ratings are deemed as the gold standard. As human evaluation is inefficient and costly, an automated substitute is highly desirable. In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues. Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency. The empirical validity of our metrics is demonstrated by strong correlations with human judgments. We open source the code and relevant materials.",,,,ACL
334,2020,BiRRE: Learning Bidirectional Residual Relation Embeddings for Supervised Hypernymy Detection,"Chengyu Wang,Xiaofeng He","The hypernymy detection task has been addressed under various frameworks. Previously, the design of unsupervised hypernymy scores has been extensively studied. In contrast, supervised classifiers, especially distributional models, leverage the global contexts of terms to make predictions, but are more likely to suffer from “lexical memorization”. In this work, we revisit supervised distributional models for hypernymy detection. Rather than taking embeddings of two terms as classification inputs, we introduce a representation learning framework named Bidirectional Residual Relation Embeddings (BiRRE). In this model, a term pair is represented by a BiRRE vector as features for hypernymy classification, which models the possibility of a term being mapped to another in the embedding space by hypernymy relations. A Latent Projection Model with Negative Regularization (LPMNR) is proposed to simulate how hypernyms and hyponyms are generated by neural language models, and to generate BiRRE vectors based on bidirectional residuals of projections. Experiments verify BiRRE outperforms strong baselines over various evaluation frameworks.",,,,ACL
335,2020,Biomedical Entity Representations with Synonym Marginalization,"Mujeen Sung,Hwisang Jeon,Jinhyuk Lee,Jaewoo Kang","Biomedical named entities often play important roles in many biomedical text mining tools. However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging. In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities. To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates. Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves. In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates. On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset.",,,,ACL
336,2020,Hypernymy Detection for Low-Resource Languages via Meta Learning,"Changlong Yu,Jialong Han,Haisong Zhang,Wilfred Ng","Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks. Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios. This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages. We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue. Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets.",,,,ACL
337,2020,Investigating Word-Class Distributions in Word Vector Spaces,"Ryohei Sasano,Anna Korhonen","This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space. To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model. Specifically, we considered two types of word classes – the semantic class of direct objects of a verb and the semantic class in a thesaurus – and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class. Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution. We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership.",,,,ACL
338,2020,Aspect Sentiment Classification with Document-level Sentiment Preference Modeling,"Xiao Chen,Changlong Sun,Jingjing Wang,Shoushan Li","In the literature, existing studies always consider Aspect Sentiment Classification (ASC) as an independent sentence-level classification problem aspect by aspect, which largely ignore the document-level sentiment preference information, though obviously such information is crucial for alleviating the information deficiency problem in ASC. In this paper, we explore two kinds of sentiment preference information inside a document, i.e., contextual sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment consistency) and contextual sentiment tendency w.r.t. all the related aspects (namely inter-aspect sentiment tendency). On the basis, we propose a Cooperative Graph Attention Networks (CoGAN) approach for cooperatively learning the aspect-related sentence representation. Specifically, two graph attention networks are leveraged to model above two kinds of document-level sentiment preference information respectively, followed by an interactive mechanism to integrate the two-fold preference. Detailed evaluation demonstrates the great advantage of the proposed approach to ASC over the state-of-the-art baselines. This justifies the importance of the document-level sentiment preference information to ASC and the effectiveness of our approach capturing such information.",,,,ACL
339,2020,Don’t Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction,"Zhenkai Wei,Yu Hong,Bowei Zou,Meng Cheng","The current aspect extraction methods suffer from boundary errors. In general, these errors lead to a relatively minor difference between the extracted aspects and the ground-truth. However, they hurt the performance severely. In this paper, we propose to utilize a pointer network for repositioning the boundaries. Recycling mechanism is used, which enables the training data to be collected without manual intervention. We conduct the experiments on the benchmark datasets SE14 of laptop and SE14-16 of restaurant. Experimental results show that our method achieves substantial improvements over the baseline, and outperforms state-of-the-art methods.",,,,ACL
340,2020,Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis,"Zhuang Chen,Tieyun Qian","Aspect-based sentiment analysis (ABSA) involves three subtasks, i.e., aspect term extraction, opinion term extraction, and aspect-level sentiment classification. Most existing studies focused on one of these subtasks only. Several recent researches made successful attempts to solve the complete ABSA problem with a unified framework. However, the interactive relations among three subtasks are still under-exploited. We argue that such relations encode collaborative signals between different subtasks. For example, when the opinion term is “delicious”, the aspect term must be “food” rather than “place”. In order to fully exploit these relations, we propose a Relation-Aware Collaborative Learning (RACL) framework which allows the subtasks to work coordinately via the multi-task learning and relation propagation mechanisms in a stacked multi-layer network. Extensive experiments on three real-world datasets demonstrate that RACL significantly outperforms the state-of-the-art methods for the complete ABSA task.",,,,ACL
341,2020,SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics,"Da Yin,Tao Meng,Kai-Wei Chang","We propose SentiBERT, a variant of BERT that effectively captures compositional sentiment semantics. The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition. Comprehensive experiments demonstrate that SentiBERT achieves competitive performance on phrase-level sentiment classification. We further demonstrate that the sentiment composition learned from the phrase-level annotations on SST can be transferred to other sentiment analysis tasks as well as related tasks, such as emotion classification tasks. Moreover, we conduct ablation studies and design visualization methods to understand SentiBERT. We show that SentiBERT is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics.",,,,ACL
342,2020,Transition-based Directed Graph Construction for Emotion-Cause Pair Extraction,"Chuang Fan,Chaofa Yuan,Jiachen Du,Lin Gui","Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text. Most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation. Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction. The proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions, from which we can recognize emotions with the corresponding causes simultaneously, thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently. Experimental results show that our approach achieves the best performance, outperforming the state-of-the-art methods by 6.71% (p<0.01) in F1 measure.",,,,ACL
343,2020,CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality,"Wenmeng Yu,Hua Xu,Fanyang Meng,Yilin Zhu","Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations. However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities. In this paper, we introduce a Chinese single- and multi-modal sentiment analysis dataset, CH-SIMS, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis.Furthermore, we propose a multi-task learning framework based on late fusion as the baseline. Extensive experiments on the CH-SIMS show that our methods achieve state-of-the-art performance and learn more distinctive unimodal representations. The full dataset and codes are available for use at https://github.com/thuiar/MMSA.",,,,ACL
344,2020,Curriculum Pre-training for End-to-End Speech Translation,"Chengyi Wang,Yu Wu,Shujie Liu,Ming Zhou","End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.",,,,ACL
345,2020,How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems,"Archiki Prasad,Preethi Jyothi","In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system. We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents. We examine the effects of accent on the internal representation using three main probing techniques: a) Gradient-based explanation methods, b) Information-theoretic measures, and c) Outputs of accent and phone classifiers. We find different accents exhibiting similar trends irrespective of the probing technique used. We also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents.",,,,ACL
346,2020,Improving Disfluency Detection by Self-Training a Self-Attentive Model,"Paria Jamshid Lou,Mark Johnson","Self-attentive neural syntactic parsers using contextualized word embeddings (e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts. Since the contextualized word embeddings are pre-trained on a large amount of unlabeled data, using additional unlabeled data to train a neural model might seem redundant. However, we show that self-training — a semi-supervised technique for incorporating unlabeled data — sets a new state-of-the-art for the self-attentive parser on disfluency detection, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized word representations. We also show that ensembling self-trained parsers provides further gains for disfluency detection.",,,,ACL
347,2020,Learning Spoken Language Representations with Neural Lattice Language Modeling,"Chao-Wei Huang,Yun-Nung Chen","Pre-trained language models have achieved huge improvement on many NLP tasks. However, these methods are usually designed for written text, so they do not consider the properties of spoken language. Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems. We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks. The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency. Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs. The code is available at https://github.com/MiuLab/Lattice-ELMo.",,,,ACL
348,2020,Meta-Transfer Learning for Code-Switched Speech Recognition,"Genta Indra Winata,Samuel Cahyawijaya,Zhaojiang Lin,Zihan Liu","An increasing number of people in the world today speak a mixed-language as a result of being multilingual. However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data. We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets. Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data. Based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge.",,,,ACL
349,2020,Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association,"Nan Xu,Zhixiong Zeng,Wenji Mao","Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means. With the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms. In multimodal context, sarcasm is no longer a pure linguistic phenomenon, and due to the nature of social media short text, the opposite is more often manifested via cross-modality expressions. Thus traditional text-based methods are insufficient to detect multimodal sarcasm. To reason with multimodal sarcastic tweets, in this paper, we propose a novel method for modeling cross-modality contrast in the associated context. Our method models both cross-modality contrast and semantic association by constructing the Decomposition and Relation Network (namely D&R Net). The decomposition network represents the commonality and discrepancy between image and text, and the relation network models the semantic association in cross-modality context. Experimental results on a public dataset demonstrate the effectiveness of our model in multimodal sarcasm detection.",,,,ACL
350,2020,SimulSpeech: End-to-End Simultaneous Speech to Text Translation,"Yi Ren,Jinglin Liu,Xu Tan,Chen Zhang","In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.",,,,ACL
351,2020,Towards end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy conversations,"Karan Singla,Zhuohao Chen,David Atkins,Shrikanth Narayanan","Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR). We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding. Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features. This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec. Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels. We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.",,,,ACL
352,2020,Neural Temporal Opinion Modelling for Opinion Prediction on Twitter,"Lixing Zhu,Yulan He,Deyu Zhou","Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context. In this paper, we model users’ tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a user’s historical tweet sequence and tweets posted by their neighbours. We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context. Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines.",,,,ACL
353,2020,"It Takes Two to Lie: One to Lie, and One to Listen","Denis Peskov,Benny Cheng,Ahmed Elgohary,Joe Barrow","Trust is implicit in many online text conversations—striking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.",,,,ACL
354,2020,Learning Implicit Text Generation via Feature Matching,"Inkit Padhi,Pierre Dognin,Ke Bai,Cícero Nogueira dos Santos","Generative feature matching network (GFMN) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks. In this paper, we present new GFMN formulations that are effective for sequential data. Our experimental results show the effectiveness of the proposed method, SeqGFMN, for three distinct generation tasks in English: unconditional text generation, class-conditional text generation, and unsupervised text style transfer. SeqGFMN is stable to train and outperforms various adversarial approaches for text generation and text style transfer.",,,,ACL
355,2020,"Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data","Hamidreza Shahidi,Ming Li,Jimmy Lin","A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures. In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks. In this work, we show that this is also the case for text generation from structured and unstructured data. We consider neural table-to-text generation and neural question generation (NQG) tasks for text generation from structured and unstructured data, respectively. Table-to-text generation aims to generate a description based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models. Experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks. Code is available at https://github.com/h-shahidi/2birds-gen.",,,,ACL
356,2020,Bayesian Hierarchical Words Representation Learning,"Oren Barkan,Idan Rejwan,Avi Caciularu,Noam Koenigstein","This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm. BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors. By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations. Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors. Finally, we further show that BHWR produces better representations for rare words.",,,,ACL
357,2020,Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning,"Alexandre Tamborrino,Nicola Pellicanò,Baptiste Pannier,Pascal Voitot","Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets. By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.",,,,ACL
358,2020,SEEK: Segmented Embedding of Knowledge Graphs,"Wentao Xu,Shun Zheng,Liang He,Bin Shao","In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering. However, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory. To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity. Our framework focuses on the design of scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions; 2) preserving both symmetry and antisymmetry properties of relations. It is noteworthy that owing to the general and elegant design of scoring functions, our framework can incorporate many famous existing methods as special cases. Moreover, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework. Source codes and data can be found at https://github.com/Wentao-Xu/SEEK.",,,,ACL
359,2020,Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation,"Xabier Soto,Dimitar Shterionov,Alberto Poncelas,Andy Way","Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation. Combining backtranslated data from different sources has led to better results than when using such data in isolation. In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT systems has on new MT systems. We use a real-world low-resource use-case (Basque-to-Spanish in the clinical domain) as well as a high-resource language pair (German-to-English) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora. We exploit different data selection strategies in order to reduce the amount of data used, while at the same time maintaining high-quality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance.",,,,ACL
360,2020,Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture,"Christopher Brix,Parnia Bahar,Hermann Ney","Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs. This is relevant both for time-critical and on-device computations using neural networks. The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model. On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity. Furthermore, we confirm that the parameter’s initial sign and not its specific value is the primary factor for successful training, and show that magnitude pruning cannot be used to find winning lottery tickets.",,,,ACL
361,2020,A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction,"Yilin Niu,Fangkai Jiao,Mantong Zhou,Ting Yao","Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor. The former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence. Despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive MRC tasks such as YES/NO question answering and multi-choice MRC. To address this problem, we present a Self-Training method (STM), which supervises the evidence extractor with auto-generated evidence labels in an iterative process. At each iteration, a base MRC model is trained with golden answers and noisy evidence labels. The trained model will predict pseudo evidence labels as extra supervision in the next iteration. We evaluate STM on seven datasets over three MRC tasks. Experimental results demonstrate the improvement on existing MRC models, and we also analyze how and why such a self-training method works in MRC.",,,,ACL
362,2020,Graph-to-Tree Learning for Solving Math Word Problems,"Jipeng Zhang,Lei Wang,Roy Ka-Wei Lee,Yi Bin","While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well. This results in poor quantity representations and incorrect solution expressions. In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions. Included in our Graph2Tree framework are two graphs, namely the Quantity Cell Graph and Quantity Comparison Graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs. We conduct extensive experiments on two available datasets. Our experiment results show that Graph2Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly. We also discuss case studies and empirically examine Graph2Tree’s effectiveness in translating the MWP text into solution expressions.",,,,ACL
363,2020,An Effectiveness Metric for Ordinal Classification: Formal Properties and Experimental Results,"Enrique Amigo,Julio Gonzalo,Stefano Mizzaro,Jorge Carrillo-de-Albornoz","In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as “positive”, “neutral”, “negative” in sentiment analysis. Remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, Mean Average Error assumes absolute distances between classes). In this paper we propose a new metric for Ordinal Classification, Closeness Evaluation Measure, that is rooted on Measurement Theory and Information Theory. Our theoretical analysis and experimental results over both synthetic data and data from NLP shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously. In addition, it generalizes some popular classification (nominal scale) and error minimization (interval scale) metrics, depending on the measurement scale in which it is instantiated.",,,,ACL
364,2020,Adaptive Compression of Word Embeddings,"Yeachan Kim,Kang-Min Kim,SangKeun Lee","Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks. However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g., self-driving cars, mobile devices). In this paper, we propose a novel method to adaptively compress word embeddings. We fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4). However, unlike prior works that assign the same length of codes to all words, we adaptively assign different lengths of codes to each word by learning downstream tasks. The proposed method works in two steps. First, each word directly learns to select its code length in an end-to-end manner by applying the Gumbel-softmax tricks. After selecting the code length, each word learns discrete codes through a neural network with a binary constraint. To showcase the general applicability of the proposed method, we evaluate the performance on four different downstream tasks. Comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy. Moreover, we show that our model assigns word to each code-book by considering the significance of tasks.",,,,ACL
365,2020,Analysing Lexical Semantic Change with Contextualised Word Representations,"Mario Giulianelli,Marco Del Tredici,Raquel Fernández","This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.",,,,ACL
366,2020,Autoencoding Keyword Correlation Graph for Document Clustering,"Billy Chiu,Sunil Kumar Sahu,Derek Thomas,Neha Sengupta","Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global). Existing representation learning models do not fully capture these features. To address this, we present a novel graph-based representation for document clustering that builds a graph autoencoder (GAE) on a Keyword Correlation Graph. The graph is constructed with topical keywords as nodes and multiple local and global features as edges. A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them. Clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes. Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding.",,,,ACL
367,2020,Autoencoding Pixies: Amortised Variational Inference with Graph Convolutions for Functional Distributional Semantics,Guy Emerson,"Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector. However, the large number of latent variables means that inference is computationally expensive, and training a model is therefore slow to converge. In this paper, I introduce the Pixie Autoencoder, which augments the generative model of Functional Distributional Semantics with a graph-convolutional neural network to perform amortised variational inference. This allows the model to be trained more effectively, achieving better results on two tasks (semantic similarity in context and semantic composition), and outperforming BERT, a large pre-trained language model.",,,,ACL
368,2020,BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance,"Timo Schick,Hinrich Schütze","Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Schütze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.",,,,ACL
369,2020,CluBERT: A Cluster-Based Approach for Learning Sense Distributions in Multiple Languages,"Tommaso Pasini,Federico Scozzafava,Bianca Scarlini","Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly. However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary. To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences. Our experiments show that CluBERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches. When used to induce the MFS of a lemma, CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models. Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the MFS on the multilingual WSD tasks. We release our sense distributions in five different languages at https://github.com/SapienzaNLP/clubert.",,,,ACL
370,2020,Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis,"Chunning Du,Haifeng Sun,Jingyu Wang,Qi Qi","Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data. It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation. Due to the pre-training task and corpus, BERT is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge. To tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task. The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way. Based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features. Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin. The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method.",,,,ACL
371,2020,From Arguments to Key Points: Towards Automatic Argument Summarization,"Roy Bar-Haim,Lilach Eden,Roni Friedman,Yoav Kantor","Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem. We propose to represent such summaries as a small set of talking points, termed key points, each scored according to its salience. We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments. Furthermore, we found that a domain expert can often predict these key points in advance. We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task. We report empirical results for an extensive set of experiments with this dataset, showing promising performance.",,,,ACL
372,2020,GoEmotions: A Dataset of Fine-Grained Emotions,"Dorottya Demszky,Dana Movshovitz-Attias,Jeongwoo Ko,Alan Cowen","Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our BERT-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement.",,,,ACL
373,2020,He said “who’s gonna take care of your children when you are at ACL?”: Reported Sexist Acts are Not Sexist,"Patricia Chiril,Véronique Moriceau,Farah Benamara,Alda Mari","In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman. We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet’s vectorial representations (word embeddings, linguistic features, and various generalization strategies). Our results are encouraging and constitute a first step towards offensive content moderation.",,,,ACL
374,2020,SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis,"Hao Tian,Can Gao,Xinyan Xiao,Hao Liu","Recently, sentiment analysis has seen remarkable advance with the help of pre-training approaches. However, sentiment knowledge, such as sentiment words and aspect-sentiment pairs, is ignored in the process of pre-training, despite the fact that they are widely used in traditional sentiment analysis approaches. In this paper, we introduce Sentiment Knowledge Enhanced Pre-training (SKEP) in order to learn a unified sentiment representation for multiple sentiment analysis tasks. With the help of automatically-mined knowledge, SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives, so as to embed sentiment information at the word, polarity and aspect level into pre-trained sentiment representation. In particular, the prediction of aspect-sentiment pairs is converted into multi-label classification, aiming to capture the dependency between words in a pair. Experiments on three kinds of sentiment tasks show that SKEP significantly outperforms strong pre-training baseline, and achieves new state-of-the-art results on most of the test datasets. We release our code at https://github.com/baidu/Senta.",,,,ACL
375,2020,Do Neural Language Models Show Preferences for Syntactic Formalisms?,"Artur Kulmizev,Vinit Ravishankar,Mostafa Abdou,Joakim Nivre","Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages. We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure. We find that both models exhibit a preference for UD over SUD — with interesting variations across languages and layers — and that the strength of this preference is correlated with differences in tree shape.",,,,ACL
376,2020,Enriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing,"Daniel Fernández-González,Carlos Gómez-Rodríguez","Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date. In this paper, we show that these results can be improved by using an in-order linearization instead. Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015)’s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers. Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed.",,,,ACL
377,2020,"Exact yet Efficient Graph Parsing, Bi-directional Locality and the Constructivist Hypothesis","Yajie Ye,Weiwei Sun","A key problem in processing graph-based meaning representations is graph parsing, i.e. computing all possible derivations of a given graph according to a (competence) grammar. We demonstrate, for the first time, that exact graph parsing can be efficient for large graphs and with large Hyperedge Replacement Grammars (HRGs). The advance is achieved by exploiting locality as terminal edge-adjacency in HRG rules. In particular, we highlight the importance of 1) a terminal edge-first parsing strategy, 2) a categorization of a subclass of HRG, i.e. what we call Weakly Regular Graph Grammar, and 3) distributing argument-structures to both lexical and phrasal rules.",,,,ACL
378,2020,Max-Margin Incremental CCG Parsing,"Miloš Stanojević,Mark Steedman","Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT. Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like. A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank. We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias. While known techniques for tackling these biases improve results, they still do not make the parser state of the art. Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation. The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.",,,,ACL
379,2020,Neural Reranking for Dependency Parsing: An Evaluation,"Bich-Ngoc Do,Ines Rehbein","Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser. However, all neural rerankers so far have been evaluated on English and Chinese only, both languages with a configurational word order and poor morphology. In the paper, we re-assess the potential of successful neural reranking models from the literature on English and on two morphologically rich(er) languages, German and Czech. In addition, we introduce a new variation of a discriminative reranker based on graph convolutional networks (GCNs). We show that the GCN not only outperforms previous models on English but is the only model that is able to improve results over the baselines on German and Czech. We explain the differences in reranking performance based on an analysis of a) the gold tree ratio and b) the variety in the k-best lists.",,,,ACL
380,2020,Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting,"Guanhua Zhang,Bing Bai,Junqi Zhang,Kun Bai","With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., “gay”, “black”) are more likely to be abusive in existing abusive language detection datasets. As a result, models trained with these datasets may consider sentences like “She makes me happy to be gay” as abusive simply because of the word “gay.” In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution. Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms. Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models’ generalization ability.",,,,ACL
381,2020,Analyzing analytical methods: The case of phonology in neural models of spoken language,"Grzegorz Chrupała,Bertrand Higy,Afra Alishahi","Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method. As a step in this direction we study the case of representations of phonology in neural network models of spoken language. We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences. We manipulate two factors that can affect the outcome of analysis. First, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models. Second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance. We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.",,,,ACL
382,2020,Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations,"Oana-Maria Camburu,Brendan Shillingford,Pasquale Minervini,Thomas Lukasiewicz","To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ”Because there is a dog in the image.” and ”Because there is no dog in the [same] image.”, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.",,,,ACL
383,2020,Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT,"Zhiyong Wu,Yun Chen,Ben Kao,Qun Liu","By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.",,,,ACL
384,2020,Probing for Referential Information in Language Models,"Ionut-Teodor Sorodoc,Kristina Gulordava,Gemma Boleda","Language models keep track of complex information about the preceding context – including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English. We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus. The Transformer outperforms the LSTM in all analyses. Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world. However, we find traces of the latter aspect, too.",,,,ACL
385,2020,Quantifying Attention Flow in Transformers,"Samira Abnar,Willem Zuidema","In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.",,,,ACL
386,2020,Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?,"Alon Jacovi,Yoav Goldberg","With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is “defined” by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.",,,,ACL
387,2020,Towards Transparent and Explainable Attention Models,"Akash Kumar Mohankumar,Preksha Nema,Sharan Narasimhan,Mitesh M. Khapra","Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model’s predictions. Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model’s prediction. They can be considered a plausible explanation if they provide a human-understandable justification for the model’s predictions. In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model’s predictions. We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model’s predictions. Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model’s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions. To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse. We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model’s predictions (iii) correlate better with gradient-based attribution methods. Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model’s predictions. Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention",,,,ACL
388,2020,Tchebycheff Procedure for Multi-task Text Classification,"Yuren Mao,Shuang Yun,Weiwei Liu,Bo Du","Multi-task Learning methods have achieved great progress in text classification. However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications. To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption. The extensive experiments back up our theoretical analysis and validate the superiority of our proposals.",,,,ACL
389,2020,Modeling Word Formation in English–German Neural Machine Translation,"Marion Weller-Di Marco,Alexander Fraser","This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology. Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation. The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.",,,,ACL
390,2020,Empowering Active Learning to Jointly Optimize System and User Demands,"Ji-Ung Lee,Christian M. Meyer,Iryna Gurevych","Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading. In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances). We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills. We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.",,,,ACL
391,2020,Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction,"Masahiro Kaneko,Masato Mita,Shun Kiyono,Jun Suzuki","This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC). The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC. For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods. Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.",,,,ACL
392,2020,Graph Neural News Recommendation with Unsupervised Preference Disentanglement,"Linmei Hu,Siyong Xu,Chen Li,Cheng Yang","With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents. Most existing methods usually learn the representations of users and news from news contents for recommendation. However, they seldom consider high-order connectivity underlying the user-news interactions. Moreover, existing methods failed to disentangle a user’s latent preference factors which cause her clicks on different news. In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD. Our model can encode high-order relationships into user and news representations by information propagation along the graph. Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability. A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations. Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods.",,,,ACL
393,2020,Identifying Principals and Accessories in a Complex Case based on the Comprehension of Fact Description,"Yakun Hu,Zhunchen Luo,Wenhan Chao","In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case. We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story. We propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework. Experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants’ impacts in a complex case.",,,,ACL
394,2020,Joint Modelling of Emotion and Abusive Language Detection,"Santhosh Rajamanickam,Pushkar Mishra,Helen Yannakoudakis,Ekaterina Shutova","The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.",,,,ACL
395,2020,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"Sebastian Weigelt,Vanessa Steurer,Tobias Hey,Walter F. Tichy","The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.",,,,ACL
396,2020,Toxicity Detection: Does Context Really Matter?,"John Pavlopoulos,Jeffrey Sorensen,Lucas Dixon,Nithum Thain","Moderation is crucial to promoting healthy online discussions. Although several ‘toxicity’ detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently. We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems? We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title. We find that context can both amplify or mitigate the perceived toxicity of posts. Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context. Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware. This points to the need for larger datasets of comments annotated in context. We make our code and data publicly available.",,,,ACL
397,2020,AMR Parsing with Latent Structural Information,"Qiji Zhou,Yue Zhang,Donghong Ji,Hao Tang","Abstract Meaning Representations (AMRs) capture sentence-level semantics structural representations to broad-coverage natural sentences. We investigate parsing AMR with explicit dependency structures and interpretable latent structures. We generate the latent soft structure without additional annotations, and fuse both dependency and latent structure via an extended graph neural networks. The fused structural information helps our experiments results to achieve the best reported results on both AMR 2.0 (77.5% Smatch F1 on LDC2017T10) and AMR 1.0 ((71.8% Smatch F1 on LDC2014T12).",,,,ACL
398,2020,TaPas: Weakly Supervised Table Parsing via Pre-training,"Jonathan Herzig,Pawel Krzysztof Nowak,Thomas Müller,Francesco Piccinno","Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables without generating logical forms. TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TaPas extends BERT’s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.",,,,ACL
399,2020,Target Inference in Argument Conclusion Generation,"Milad Alshomary,Shahbaz Syed,Martin Potthast,Henning Wachsmuth","In argumentation, people state premises to reason towards a conclusion. The conclusion conveys a stance towards some target, such as a concept or statement. Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons. However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation. We thus study the question to what extent an argument’s conclusion can be reconstructed from its premises. In particular, we argue here that a decisive step is to infer a conclusion’s target, and we hypothesize that this target is related to the premises’ targets. We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network. Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines. According to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases.",,,,ACL
400,2020,Multimodal Transformer for Multimodal Machine Translation,"Shaowei Yao,Xiaojun Wan","Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. Equally treating all modalities may encode too much useless information from less important modalities. In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT. The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images. Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.",,,,ACL
401,2020,"Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis","Dushyant Singh Chauhan,Dhanush S R,Asif Ekbal,Pushpak Bhattacharyya","In this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario. We, at first, manually annotate the recently released multi-modal MUStARD sarcasm dataset with sentiment and emotion classes, both implicit and explicit. For multi-tasking, we propose two attention mechanisms, viz. Inter-segment Inter-modal Attention (Ie-Attention) and Intra-segment Inter-modal Attention (Ia-Attention). The main motivation of Ie-Attention is to learn the relationship between the different segments of the sentence across the modalities. In contrast, Ia-Attention focuses within the same segment of the sentence across the modalities. Finally, representations from both the attentions are concatenated and shared across the five classes (i.e., sarcasm, implicit sentiment, explicit sentiment, implicit emotion, explicit emotion) for multi-tasking. Experimental results on the extended version of the MUStARD dataset show the efficacy of our proposed approach for sarcasm detection over the existing state-of-the-art systems. The evaluation also shows that the proposed multi-task framework yields better performance for the primary task, i.e., sarcasm detection, with the help of two secondary tasks, emotion and sentiment analysis.",,,,ACL
402,2020,Towards Emotion-aided Multi-modal Dialogue Act Classification,"Tulika Saha,Aditya Patra,Sriparna Saha,Pushpak Bhattacharyya","The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. Hence, the effect of emotion too on automatic identification of DAs needs to be studied. In this work, we address the role of both multi-modality and emotion recognition (ER) in DAC. DAC and ER help each other by way of multi-task learning. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants.",,,,ACL
403,2020,Analyzing Political Parody in Social Media,"Antonios Maronikolakis,Danae Sánchez Villegas,Daniel Preotiuc-Pietro,Nikolaos Aletras","Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.",,,,ACL
404,2020,Masking Actor Information Leads to Fairer Political Claims Detection,"Erenay Dayanik,Sebastian Padó","A central concern in Computational Social Sciences (CSS) is fairness: where the role of NLP is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties. We analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better. We propose two simple debiasing methods which mask proper names and pronouns during training of the model, thus removing personal information bias. We find that (a) these methods significantly decrease frequency bias while keeping the overall performance stable; and (b) the resulting models improve when evaluated in an out-of-domain setting.",,,,ACL
405,2020,When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?,"Kenneth Joseph,Jonathan Morgan","Social biases are encoded in word embeddings. This presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. We find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. However, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.",,,,ACL
406,2020,"“Who said it, and Why?” Provenance for Natural Language Claims","Yi Zhang,Zachary Ives,Dan Roth","In an era where generating content and publishing it is so easy, we are bombarded with information and are exposed to all kinds of claims, some of which do not always rank high on the truth scale. This paper suggests that the key to a longer-term, holistic, and systematic approach to navigating this information pollution is capturing the provenance of claims. To do that, we develop a formal definition of provenance graph for a given natural language claim, aiming to understand where the claim may come from and how it has evolved. To construct the graph, we model provenance inference, formulated mainly as an information extraction task and addressed via a textual entailment model. We evaluate our approach using two benchmark datasets, showing initial success in capturing the notion of provenance and its effectiveness on the application of claim verification.",,,,ACL
407,2020,Compositionality and Generalization In Emergent Languages,"Rahma Chaabouni,Eugene Kharitonov,Diane Bouchacourt,Emmanuel Dupoux","Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.",,,,ACL
408,2020,ERASER: A Benchmark to Evaluate Rationalized NLP Models,"Jay DeYoung,Sarthak Jain,Nazneen Fatema Rajani,Eric Lehman","State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",,,,ACL
409,2020,Learning to Faithfully Rationalize by Construction,"Sarthak Jain,Sarah Wiegreffe,Yuval Pinter,Byron C. Wallace","In many settings it is important for one to be able to understand why a model made a particular prediction. In NLP this often entails extracting snippets of an input text ‘responsible for’ corresponding model output; when such a snippet comprises tokens that indeed informed the model’s prediction, it is a faithful explanation. In some settings, faithfulness may be critical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules. However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning. We propose a simpler variant of this approach that provides faithful explanations by construction. In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict. An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to ‘end-to-end’ approaches, while being more general and easier to train. Code is available at https://github.com/successar/FRESH.",,,,ACL
410,2020,Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset,"Xiang Yue,Bernal Jimenez Gutierrez,Huan Sun","Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets. In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation. Recently, Pampari et al. (EMNLP’18) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrQA, the first large-scale dataset for question answering (QA) based on clinical notes. In this paper, we provide an in-depth analysis of this dataset and the clinical reading comprehension (CliniRC) task. From our qualitative analysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA questions are often answerable without using domain knowledge. From our quantitative experiments, surprising results include that (iii) using a small sampled subset (5%-20%), we can obtain roughly equal performance compared to the model trained on the entire dataset, (iv) this performance is close to human expert’s performance, and (v) BERT models do not beat the best performing base model. Following our analysis of the emrQA, we further explore two desired aspects of CliniRC systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts. We argue that both should be considered when creating future datasets.",,,,ACL
411,2020,DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering,"Qingqing Cao,Harsh Trivedi,Aruna Balasubramanian,Niranjan Balasubramanian","Transformer-based QA models use input-wide self-attention – i.e. across both the question and the input passage – at all layers, causing them to be slow and memory-intensive. It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers. We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically. Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset. We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy. We open source the code at https://github.com/StonyBrookNLP/deformer.",,,,ACL
412,2020,Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings,"Apoorv Saxena,Aditay Tripathi,Partha Talukdar","Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG. Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer. KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA. Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text, which isn’t always readily available. In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction. Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far. We fill this gap in this paper and propose EmbedKGQA. EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs. EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods. Through extensive experiments on multiple benchmark datasets, we demonstrate EmbedKGQA’s effectiveness over other state-of-the-art baselines.",,,,ACL
413,2020,Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering,"Alexander Fabbri,Patrick Ng,Zhiguo Wang,Ramesh Nallapati","Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows. A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset. This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data. We propose an unsupervised approach to training QA models with generated pseudo-training data. We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships. Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14%, and 20% when the answer is a named entity, achieving state-of-the-art performance on SQuAD for unsupervised QA.",,,,ACL
414,2020,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,"Vikas Yadav,Steven Bethard,Mihai Surdeanu","Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.",,,,ACL
415,2020,A Corpus for Large-Scale Phonetic Typology,"Elizabeth Salesky,Eleanor Chodroff,Tiago Pimentel,Matthew Wiesner","A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions. We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages. However, it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available. We describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings. Our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io.",,,,ACL
416,2020,Dscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing,"Jiangming Liu,Shay B. Cohen,Mirella Lapata","Discourse representation structures (DRSs) are scoped semantic representations for texts of arbitrary length. Evaluating the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance. DRSs are typically visualized as boxes which are not straightforward to process automatically. Counter transforms DRSs to clauses and measures clause overlap by searching for variable mappings between two DRSs. However, this metric is computationally costly (with respect to memory and CPU time) and does not scale with longer texts. We introduce Dscorer, an efficient new metric which converts box-style DRSs to graphs and then measures the overlap of n-grams. Experiments show that Dscorer computes accuracy scores that are correlated with Counter at a fraction of the time.",,,,ACL
417,2020,ParaCrawl: Web-Scale Acquisition of Parallel Corpora,"Marta Bañón,Pinzhen Chen,Barry Haddow,Kenneth Heafield","We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems.",,,,ACL
418,2020,Toward Gender-Inclusive Coreference Resolution,"Yang Trista Cao,Hal Daumé III","Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.",,,,ACL
419,2020,Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?,"Cansu Sen,Thomas Hartvigsen,Biao Yin,Xiangnan Kong","Motivated by human attention, computational attention mechanisms have been designed to help neural networks adjust their focus on specific parts of the input data. While attention mechanisms are claimed to achieve interpretability, little is known about the actual relationships between machine and human attention. In this work, we conduct the first quantitative assessment of human versus computational attention mechanisms for the text classification task. To achieve this, we design and conduct a large-scale crowd-sourcing study to collect human attention maps that encode the parts of a text that humans focus on when conducting text classification. Based on this new resource of human attention dataset for text classification, YELP-HAT, collected on the publicly available YELP dataset, we perform a quantitative comparative analysis of machine attention maps created by deep learning models and human attention maps. Our analysis offers insights into the relationships between human versus machine attention maps along three dimensions: overlap in word selections, distribution over lexical categories, and context-dependency of sentiment polarity. Our findings open promising future research opportunities ranging from supervised attention to the design of human-centric attention-based explanations.",,,,ACL
420,2020,Information-Theoretic Probing for Linguistic Structure,"Tiago Pimentel,Josef Valvoda,Rowan Hall Maudslay,Ran Zmigrod","The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually “know” about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network’s learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research—plus English—totalling eleven languages. Our implementation is available in https://github.com/rycolab/info-theoretic-probing.",,,,ACL
421,2020,On the Cross-lingual Transferability of Monolingual Representations,"Mikel Artetxe,Sebastian Ruder,Dani Yogatama","State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",,,,ACL
422,2020,Similarity Analysis of Contextual Word Representation Models,"John Wu,Yonatan Belinkov,Hassan Sajjad,Nadir Durrani","This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.",,,,ACL
423,2020,SenseBERT: Driving Some Sense into BERT,"Yoav Levine,Barak Lenz,Or Dagan,Ori Ram","The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding. However, existing self-supervision techniques operate at the word form level, which serves as a surrogate for the underlying semantic content. This paper proposes a method to employ weak-supervision directly at the word sense level. Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet supersenses. Accordingly, we attain a lexical-semantic level language model, without the use of human annotation. SenseBERT achieves significantly improved lexical understanding, as we demonstrate by experimenting on SemEval Word Sense Disambiguation, and by attaining a state of the art result on the ‘Word in Context’ task.",,,,ACL
424,2020,ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,"Fernando Alva-Manchego,Louis Martin,Antoine Bordes,Carolina Scarton","In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.",,,,ACL
425,2020,"Fatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts","Agostina Calabrese,Michele Bevilacqua,Roberto Navigli","Thanks to the wealth of high-quality annotated images available in popular repositories such as ImageNet, multimodal language-vision research is in full bloom. However, events, feelings and many other kinds of concepts which can be visually grounded are not well represented in current datasets. Nevertheless, we would expect a wide-coverage language understanding system to be able to classify images depicting recess and remorse, not just cats, dogs and bridges. We fill this gap by presenting BabelPic, a hand-labeled dataset built by cleaning the image-synset association found within the BabelNet Lexical Knowledge Base (LKB). BabelPic explicitly targets non-concrete concepts, thus providing refreshing new data for the community. We also show that pre-trained language-vision systems can be used to further expand the resource by exploiting natural language knowledge available in the LKB. BabelPic is available for download at http://babelpic.org.",,,,ACL
426,2020,Modeling Label Semantics for Predicting Emotional Reactions,"Radhika Gaonkar,Heeyoung Kwon,Mohaddeseh Bastan,Niranjan Balasubramanian","Predicting how events induce emotions in the characters of a story is typically seen as a standard multi-label classification task, which usually treats labels as anonymous classes to predict. They ignore information that may be conveyed by the emotion labels themselves. We propose that the semantics of emotion labels can guide a model’s attention when representing the input story. Further, we observe that the emotions evoked by an event are often related: an event that evokes joy is unlikely to also evoke sadness. In this work, we explicitly model label classes via label embeddings, and add mechanisms that track label-label correlations both during training and inference. We also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data. Our empirical evaluations show that modeling label semantics yields consistent benefits, and we advance the state-of-the-art on an emotion inference task.",,,,ACL
427,2020,CraftAssist Instruction Parsing: Semantic Parsing for a Voxel-World Assistant,"Kavya Srinet,Yacine Jernite,Jonathan Gray,Arthur Szlam","We propose a semantic parsing dataset focused on instruction-driven communication with an agent in the game Minecraft. The dataset consists of 7K human utterances and their corresponding parses. Given proper world state, the parses can be interpreted and executed in game. We report the performance of baseline models, and analyze their successes and failures.",,,,ACL
428,2020,Don’t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training,"Margaret Li,Stephen Roller,Ilia Kulikov,Sean Welleck","Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address. They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws.In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019) to these cases. We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues. For the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability. We demonstrate the efficacy of our approach across several dialogue tasks.",,,,ACL
429,2020,How does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope,"Yiyun Zhao,Steven Bethard","Large pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. We apply this methodology to test BERT and RoBERTa on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. We find that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.",,,,ACL
430,2020,Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models,"Kaiji Lu,Piotr Mardziel,Klas Leino,Matt Fredrikson","LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing (NLP) tasks. Despite their performance, it is unclear whether, or how, LSTMs learn structural features of natural languages such as subject-verb number agreement in English. Lacking this understanding, the generality of LSTM performance on this task and their suitability for related tasks remains uncertain. Further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other exceptional faults. We introduce *influence paths*, a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network. The approach refines the notion of influence (the subject’s grammatical number has influence on the grammatical number of the subsequent verb) into a set of gate or neuron-level paths. The set localizes and segments the concept (e.g., subject-verb agreement), its constituent elements (e.g., the subject), and related or interfering elements (e.g., attractors). We exemplify the methodology on a widely-studied multi-layer LSTM language model, demonstrating its accounting for subject-verb number agreement. The results offer both a finer and a more complete view of an LSTM’s handling of this structural aspect of the English language than prior results based on diagnostic classifiers and ablation.",,,,ACL
431,2020,Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings,"Rishi Bommasani,Kelly Davis,Claire Cardie","Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings — while more diverse and mature than those available for their dynamic counterparts — are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.",,,,ACL
432,2020,Learning to Deceive with Attention-Based Explanations,"Danish Pruthi,Mansi Gupta,Bhuwan Dhingra,Graham Neubig","Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention’s reliability as a tool for auditing algorithms in the context of fairness and accountability.",,,,ACL
433,2020,On the Spontaneous Emergence of Discrete and Compositional Signals,"Nur Geffen Lan,Emmanuel Chemla,Shane Steinert-Threlkeld","We propose a general framework to study language emergence through signaling games with neural agents. Using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge. We explore whether categorical perception effects follow and show that the messages are not compositional.",,,,ACL
434,2020,Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words,"Josef Klafka,Allyson Ettinger","Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words. We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability—but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance. We discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings.",,,,ACL
435,2020,Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA,"Hyounghun Kim,Zineng Tang,Mohit Bansal","Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. Moreover, our model is also comprised of dual-level attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations. We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the state-of-the-art by a large margin (74.09% versus 70.52%). We also present several word, object, and frame level visualization studies.",,,,ACL
436,2020,Shaping Visual Representations with Language for Few-Shot Classification,"Jesse Mu,Percy Liang,Noah Goodman","By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. Existing models for this setting sample new descriptions at test time and use those to classify images. Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language. LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.",,,,ACL
437,2020,Discrete Latent Variable Representations for Low-Resource Text Classification,"Shuning Jin,Sam Wiseman,Karl Stratos,Karen Livescu","While much work on deep latent variable models of text uses continuous latent variables, discrete latent variables are interesting because they are more interpretable and typically more space efficient. We consider several approaches to learning discrete latent variable models for text in the case where exact marginalization over these variables is intractable. We compare the performance of the learned representations as features for low-resource document and sentence classification. Our best models outperform the previous best reported results with continuous representations in these low-resource settings, while learning significantly more compressed representations. Interestingly, we find that an amortized variant of Hard EM performs particularly well in the lowest-resource regimes.",,,,ACL
438,2020,Learning Constraints for Structured Prediction Using Rectifier Networks,"Xingyuan Pan,Maitrey Mehta,Vivek Srikumar","Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.",,,,ACL
439,2020,Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models,"Dan Iter,Kelvin Guu,Larry Lansing,Dan Jurafsky","Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations. We propose Conpono, an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences. Given an anchor sentence, our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and sentences randomly sampled from the corpus. On the discourse representation benchmark DiscoEval, our model improves over the previous state-of-the-art by up to 13% and on average 4% absolute across 7 tasks. Our model is the same size as BERT-Base, but outperforms the much larger BERT-Large model and other more recent approaches that incorporate discourse. We also show that Conpono yields gains of 2%-6% absolute even for tasks that do not explicitly evaluate discourse: textual entailment (RTE), common sense reasoning (COPA) and reading comprehension (ReCoRD).",,,,ACL
440,2020,A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks,"Angela Lin,Sudha Rao,Asli Celikyilmaz,Elnaz Nouri","Many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools. In the cooking domain, the web offers many, partially-overlapping, text and video recipes (i.e. procedures) that describe how to make the same dish (i.e. high-level task). Aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions, providing commonsense insight into how real-world procedures are structured. Learning to align these different instruction sets is challenging because: a) different recipes vary in their order of instructions and use of ingredients; and b) video instructions can be noisy and tend to contain far more information than text instructions. To address these challenges, we use an unsupervised alignment algorithm that learns pairwise alignments between instructions of different recipes for the same dish. We then use a graph algorithm to derive a joint alignment between multiple text and multiple video recipes for the same dish. We release the Microsoft Research Multimodal Aligned Recipe Corpus containing ~150K pairwise alignments between recipes across 4262 dishes with rich commonsense information.",,,,ACL
441,2020,Adversarial NLI: A New Benchmark for Natural Language Understanding,"Yixin Nie,Adina Williams,Emily Dinan,Mohit Bansal","We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",,,,ACL
442,2020,Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,"Marco Tulio Ribeiro,Tongshuang Wu,Carlos Guestrin,Sameer Singh","Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",,,,ACL
443,2020,Code and Named Entity Recognition in StackOverflow,"Jeniya Tabassum,Mounica Maddela,Wei Xu,Alan Ritter","There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet. For example, StackOverflow currently has over 15 million programming related questions written by 8.5 million users. Meanwhile, there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences. In this paper, we introduce a new named entity recognition (NER) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types. We trained in-domain BERT representations (BERTOverflow) on 152 million sentences from StackOverflow, which lead to an absolute increase of +10 F1 score over off-the-shelf BERT. We also present the SoftNER model which achieves an overall 79.10 F-1 score for code and named entity recognition on StackOverflow data. Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERT-based tagging model. Our code and data are available at: https://github.com/jeniyat/StackOverflowNER/",,,,ACL
444,2020,Dialogue-Based Relation Extraction,"Dian Yu,Kai Sun,Claire Cardie,Dong Yu","We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https://dataset.org/dialogre/.",,,,ACL
445,2020,Facet-Aware Evaluation for Extractive Summarization,"Yuning Mao,Liyuan Liu,Qi Zhu,Xiang Ren","Commonly adopted metrics for extractive summarization focus on lexical overlap at the token level. In this paper, we present a facet-aware evaluation setup for better assessment of the information coverage in extracted summaries. Specifically, we treat each sentence in the reference summary as a facet, identify the sentences in the document that express the semantics of each facet as support sentences of the facet, and automatically evaluate extractive summarization methods by comparing the indices of extracted sentences and support sentences of all the facets in the reference summary. To facilitate this new evaluation setup, we construct an extractive version of the CNN/Daily Mail dataset and perform a thorough quantitative investigation, through which we demonstrate that facet-aware evaluation manifests better correlation with human judgment than ROUGE, enables fine-grained evaluation as well as comparative analysis, and reveals valuable insights of state-of-the-art summarization methods. Data can be found at https://github.com/morningmoni/FAR.",,,,ACL
446,2020,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Katherine Stasaski,Grace Hui Yang,Marti A. Hearst","Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpus-level statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpus-level metrics.",,,,ACL
447,2020,S2ORC: The Semantic Scholar Open Research Corpus,"Kyle Lo,Lucy Lu Wang,Mark Neumann,Rodney Kinney","We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.",,,,ACL
448,2020,Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics,"Nitika Mathur,Timothy Baldwin,Trevor Cohn","Automatic metrics are fundamental for the development and evaluation of machine translation systems. Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric’s efficacy. Finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type I versus type II errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected. Together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation.",,,,ACL
449,2020,A Transformer-based Approach for Source Code Summarization,"Wasi Ahmad,Saikat Chakraborty,Baishakhi Ray,Kai-Wei Chang","Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens’ position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.",,,,ACL
450,2020,Asking and Answering Questions to Evaluate the Factual Consistency of Summaries,"Alex Wang,Kyunghyun Cho,Mike Lewis","Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose QAGS (pronounced “kags”), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source. To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets. QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics. Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why. We believe QAGS is a promising tool in automatically generating usable and factually consistent text. Code for QAGS will be available at https://github.com/W4ngatang/qags.",,,,ACL
451,2020,Discourse-Aware Neural Extractive Text Summarization,"Jiacheng Xu,Zhe Gan,Yu Cheng,Jingjing Liu","Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DiscoBert. DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.",,,,ACL
452,2020,Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction,"Raphael Schumann,Lili Mou,Yao Lu,Olga Vechtomova","Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. A good summary is characterized by language fluency and high information overlap with the source sentence. We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics. We search for a high-scoring summary by discrete optimization. Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores. Additionally, we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length. Since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets.",,,,ACL
453,2020,Exploring Content Selection in Summarization of Novel Chapters,"Faisal Ladhak,Bryan Li,Yaser Al-Onaizan,Kathleen McKeown","We present a new summarization task, generating summaries of novel chapters using summary/chapter pairs from online study guides. This is a harder task than the news summarization task, given the chapter length as well as the extreme paraphrasing and generalization found in the summaries. We focus on extractive summarization, which requires the creation of a gold-standard set of extractive summaries. We present a new metric for aligning reference summary sentences with chapter sentences to create gold extracts and also experiment with different alignment methods. Our experiments demonstrate significant improvement over prior alignment approaches for our task as shown through automatic metrics and a crowd-sourced pyramid analysis.",,,,ACL
454,2020,FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization,"Esin Durmus,He He,Mona Diab","Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.",,,,ACL
455,2020,Fact-based Content Weighting for Evaluating Abstractive Summarisation,"Xinnuo Xu,Ondřej Dušek,Jingyi Li,Verena Rieser","Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient. We introduce a new evaluation metric which is based on fact-level content weighting, i.e. relating the facts of the document to the facts of the summary. We fol- low the assumption that a good summary will reflect all relevant facts, i.e. the ones present in the ground truth (human-generated refer- ence summary). We confirm this hypothe- sis by showing that our weightings are highly correlated to human perception and compare favourably to the recent manual highlight- based metric of Hardy et al. (2019).",,,,ACL
456,2020,Hooks in the Headline: Learning to Generate Headlines with Controlled Styles,"Di Jin,Zhijing Jin,Joey Tianyi Zhou,Lisa Orii","Current summarization systems only produce plain, factual headlines, far from the practical needs for the exposure and memorableness of the articles. We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options (humor, romance and clickbait), thus attracting more readers. With no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method TitleStylist generates stylistic headlines by combining the summarization and reconstruction tasks into a multitasking framework. We also introduced a novel parameter sharing scheme to further disentangle the style from text. Through both automatic and human evaluation, we demonstrate that TitleStylist can generate relevant, fluent headlines with three target styles: humor, romance, and clickbait. The attraction score of our model generated headlines outperforms the state-of-the-art summarization model by 9.68%, even outperforming human-written references.",,,,ACL
457,2020,Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward,"Luyang Huang,Lingfei Wu,Lu Wang","Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD. We propose the use of dual encoders—a sequential document encoder and a graph-structured encoder—to maintain the global context and local characteristics of entities, complementing each other. We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets. We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models. Human judges further rate our model outputs as more informative and containing fewer unfaithful errors.",,,,ACL
458,2020,Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports,"Yuhao Zhang,Derek Merck,Emily Tsai,Christopher D. Manning","Neural abstractive summarization models are able to generate summaries which have high overlap with human references. However, existing models are not optimized for factual correctness, a critical metric in real-world applications. In this work, we develop a general framework where we evaluate the factual correctness of a generated summary by fact-checking it automatically against its reference using an information extraction module. We further propose a training strategy which optimizes a neural summarization model with a factual correctness reward via reinforcement learning. We apply the proposed method to the summarization of radiology reports, where factual correctness is a key requirement. On two separate datasets collected from hospitals, we show via both automatic and human evaluation that the proposed approach substantially improves the factual correctness and overall quality of outputs over a competitive neural summarization system, producing radiology summaries that approach the quality of human-authored ones.",,,,ACL
459,2020,Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset,"Revanth Rameshkumar,Peter Bailey","This paper describes the Critical Role Dungeons and Dragons Dataset (CRD3) and related analyses. Critical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an open-ended role-playing game. The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding abstractive summaries collected from the Fandom wiki. The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction. For each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail, and semantic ties to the previous dialogues. In addition, we provide a data augmentation method that produces 34,243 summary-dialogue chunk pairs to support current neural ML approaches, and we provide an abstractive summarization benchmark and evaluation.",,,,ACL
460,2020,The Summary Loop: Learning to Write Abstractive Summaries Without Examples,"Philippe Laban,Andrew Hsi,John Canny,Marti A. Hearst","This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary. A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries. When tested on popular news summarization datasets, the method outperforms previous unsupervised methods by more than 2 R-1 points, and approaches results of competitive supervised methods. Our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work, and learns to compress and merge sentences without supervision.",,,,ACL
461,2020,Unsupervised Opinion Summarization as Copycat-Review Generation,"Arthur Bražinskas,Mirella Lapata,Ivan Titov","Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents, such as product reviews. While the majority of previous work has focused on the extractive setting, i.e., selecting fragments from input reviews to produce a summary, we let the model generate novel sentences and hence produce abstractive summaries. Recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs. Since such training data is expensive to acquire, we instead consider the unsupervised setting, in other words, we do not use any summaries in training. We define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product, we should be able to control the “amount of novelty” going into the new review or, equivalently, vary the extent to which it deviates from the input. At test time, when generating summaries, we force the novelty to be minimal, and produce a text reflecting consensus opinions. We capture this intuition by defining a hierarchical variational autoencoder model. Both individual reviews and the products they correspond to are associated with stochastic latent codes, and the review generator (“decoder”) has direct access to the text of input reviews through the pointer-generator mechanism. Experiments on Amazon and Yelp datasets, show that setting at test time the review’s latent code to its mean, allows the model to produce fluent and coherent summaries reflecting common opinions.",,,,ACL
462,2020,(Re)construing Meaning in NLP,"Sean Trott,Tiago Timponi Torrent,Nancy Chang,Nathan Schneider","Human speakers have an extensive toolkit of ways to express themselves. In this paper, we engage with an idea largely absent from discussions of meaning in natural language understanding—namely, that the way something is expressed reflects different ways of conceptualizing or construing the information being conveyed. We first define this phenomenon more precisely, drawing on considerable prior work in theoretical cognitive semantics and psycholinguistics. We then survey some dimensions of construed meaning and show how insights from construal could inform theoretical and practical work in NLP.",,,,ACL
463,2020,"Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data","Emily M. Bender,Alexander Koller","The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.",,,,ACL
464,2020,Examining Citations of Natural Language Processing Literature,Saif M. Mohammad,"We extracted information from the ACL Anthology (AA) and Google Scholar (GS) to examine trends in citations of NLP papers. We explore questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers from different areas of within NLP? etc. Notably, we show that only about 56% of the papers in AA are cited ten or more times. CL Journal has the most cited papers, but its citation dominance has lessened in recent years. On average, long papers get almost three times as many citations as short papers; and papers on sentiment classification, anaphora resolution, and entity recognition have the highest median citations. The analyses presented here, and the associated dataset of NLP papers mapped to citations, have a number of uses including: understanding how the field is growing and quantifying the impact of different types of papers.",,,,ACL
465,2020,How Can We Accelerate Progress Towards Human-like Linguistic Generalization?,Tal Linzen,"This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",,,,ACL
466,2020,How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence,"Haoxi Zhong,Chaojun Xiao,Cunchao Tu,Tianyang Zhang","Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we introduce the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM.",,,,ACL
467,2020,Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?,"Yada Pruksachatkun,Jason Phang,Haokun Liu,Phu Mon Htut","While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.",,,,ACL
468,2020,Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview,"Deven Santosh Shah,H. Andrew Schwartz,Dirk Hovy","An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.",,,,ACL
469,2020,What Does BERT with Vision Look At?,"Liunian Harold Li,Mark Yatskar,Da Yin,Cho-Jui Hsieh","Pre-trained visually grounded language models such as ViLBERT, LXMERT, and UNITER have achieved significant performance improvement on vision-and-language tasks but what they learn during pre-training remains unclear. In this work, we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions. Specifically, some heads can map entities to image regions, performing the task known as entity grounding. Some heads can even detect the syntactic relations between non-entity words and image regions, tracking, for example, associations between verbs and regions corresponding to their arguments. We denote this ability as syntactic grounding. We verify grounding both quantitatively and qualitatively, using Flickr30K Entities as a testbed.",,,,ACL
470,2020,Balancing Objectives in Counseling Conversations: Advancing Forwards or Looking Backwards,"Justine Zhang,Cristian Danescu-Niculescu-Mizil","Throughout a conversation, participants make choices that can orient the flow of the interaction. Such choices are particularly salient in the consequential domain of crisis counseling, where a difficulty for counselors is balancing between two key objectives: advancing the conversation towards a resolution, and empathetically addressing the crisis situation. In this work, we develop an unsupervised methodology to quantify how counselors manage this balance. Our main intuition is that if an utterance can only receive a narrow range of appropriate replies, then its likely aim is to advance the conversation forwards, towards a target within that range. Likewise, an utterance that can only appropriately follow a narrow range of possible utterances is likely aimed backwards at addressing a specific situation within that range. By applying this intuition, we can map each utterance to a continuous orientation axis that captures the degree to which it is intended to direct the flow of the conversation forwards or backwards. This unsupervised method allows us to characterize counselor behaviors in a large dataset of crisis counseling conversations, where we show that known counseling strategies intuitively align with this axis. We also illustrate how our measure can be indicative of a conversation’s progress, as well as its effectiveness.",,,,ACL
471,2020,Detecting Perceived Emotions in Hurricane Disasters,"Shrey Desai,Cornelia Caragea,Junyi Jessy Li","Natural disasters (e.g., hurricanes) affect millions of people each year, causing widespread destruction in their wake. People have recently taken to social media websites (e.g., Twitter) to share their sentiments and feelings with the larger community. Consequently, these platforms have become instrumental in understanding and perceiving emotions at scale. In this paper, we introduce HurricaneEmo, an emotion dataset of 15,000 English tweets spanning three hurricanes: Harvey, Irma, and Maria. We present a comprehensive study of fine-grained emotions and propose classification tasks to discriminate between coarse-grained emotion groups. Our best BERT model, even after task-guided pre-training which leverages unlabeled Twitter data, achieves only 68% accuracy (averaged across all groups). HurricaneEmo serves not only as a challenging benchmark for models but also as a valuable resource for analyzing emotions in disaster-centric domains.",,,,ACL
472,2020,Hierarchical Modeling for User Personality Prediction: The Role of Message-Level Attention,"Veronica Lynn,Niranjan Balasubramanian,H. Andrew Schwartz","Not all documents are equally important. Language processing is increasingly finding use as a supplement for questionnaires to assess psychological attributes of consenting individuals, but most approaches neglect to consider whether all documents of an individual are equally informative. In this paper, we present a novel model that uses message-level attention to learn the relative weight of users’ social media posts for assessing their five factor personality traits. We demonstrate that models with message-level attention outperform those with word-level attention, and ultimately yield state-of-the-art accuracies for all five traits by using both word and message attention in combination with past approaches (an average increase in Pearson r of 2.5%). In addition, examination of the high-signal posts identified by our model provides insight into the relationship between language and personality, helping to inform future work.",,,,ACL
473,2020,Measuring Forecasting Skill from Text,"Shi Zong,Alan Ritter,Eduard Hovy","People vary in their ability to make accurate predictions about the future. Prior studies have shown that some individuals can predict the outcome of future events with consistently better accuracy. This leads to a natural question: what makes some forecasters better than others? In this paper we explore connections between the language people use to describe their predictions and their forecasting skill. Datasets from two different forecasting domains are explored: (1) geopolitical forecasts from Good Judgment Open, an online prediction forum and (2) a corpus of company earnings forecasts made by financial analysts. We present a number of linguistic metrics which are computed over text associated with people’s predictions about the future including: uncertainty, readability, and emotion. By studying linguistic factors associated with predictions, we are able to shed some light on the approach taken by skilled forecasters. Furthermore, we demonstrate that it is possible to accurately predict forecasting skill using a model that is based solely on language. This could potentially be useful for identifying accurate predictions or potentially skilled forecasters earlier.",,,,ACL
474,2020,Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates,"Katherine Keith,David Jensen,Brendan O’Connor","Many applications of computational social science aim to infer causal conclusions from non-experimental data. Such observational data often contains confounders, variables that influence both potential causes and potential effects. Unmeasured or latent confounders can bias causal estimates, and this has motivated interest in measuring potential confounders from observed text. For example, an individual’s entire history of social media posts or the content of a news article could provide a rich measurement of multiple confounders.Yet, methods and applications for this problem are scattered across different communities and evaluation practices are inconsistent.This review is the first to gather and categorize these examples and provide a guide to data-processing and evaluation decisions. Despite increased attention on adjusting for confounding using text, there are still many open problems, which we highlight in this paper.",,,,ACL
475,2020,Text-Based Ideal Points,"Keyon Vafa,Suresh Naidu,David Blei","Ideal point models analyze lawmakers’ votes to quantify their political positions, or ideal points. But votes are not the only way to express a political position. Lawmakers also give speeches, release press statements, and post tweets. In this paper, we introduce the text-based ideal point model (TBIP), an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors. We demonstrate the TBIP with two types of politicized text data: U.S. Senate speeches and senator tweets. Though the model does not analyze their votes or political affiliations, the TBIP separates lawmakers by party, learns interpretable politicized topics, and infers ideal points close to the classical vote-based ideal points. One benefit of analyzing texts, as opposed to votes, is that the TBIP can estimate ideal points of anyone who authors political texts, including non-voting actors. To this end, we use it to study tweets from the 2020 Democratic presidential candidates. Using only the texts of their tweets, it identifies them along an interpretable progressive-to-moderate spectrum.",,,,ACL
476,2020,Understanding the Language of Political Agreement and Disagreement in Legislative Texts,"Maryam Davoodi,Eric Waltenburg,Dan Goldwasser","While national politics often receive the spotlight, the overwhelming majority of legislation proposed, discussed, and enacted is done at the state level. Despite this fact, there is little awareness of the dynamics that lead to adopting these policies. In this paper, we take the first step towards a better understanding of these processes and the underlying dynamics that shape them, using data-driven methods. We build a new large-scale dataset, from multiple data sources, connecting state bills and legislator information, geographical information about their districts, and donations and donors’ information. We suggest a novel task, predicting the legislative body’s vote breakdown for a given bill, according to different criteria of interest, such as gender, rural-urban and ideological splits. Finally, we suggest a shared relational embedding model, representing the interactions between the text of the bill and the legislative context in which it is presented. Our experiments show that providing this context helps improve the prediction over strong text-based models.",,,,ACL
477,2020,Would you Rather? A New Benchmark for Learning Machine Alignment with Cultural Values and Social Preferences,"Yi Tay,Donovan Ong,Jie Fu,Alvin Chan","Understanding human preferences, along with cultural and social nuances, lives at the heart of natural language understanding. Concretely, we present a new task and corpus for learning alignments between machine and human preferences. Our newly introduced problem is concerned with predicting the preferable options from two sentences describing scenarios that may involve social and cultural situations. Our problem is framed as a natural language inference task with crowd-sourced preference votes by human players, obtained from a gamified voting platform. We benchmark several state-of-the-art neural models, along with BERT and friends on this task. Our experimental results show that current state-of-the-art NLP models still leave much room for improvement.",,,,ACL
478,2020,Discourse as a Function of Event: Profiling Discourse Structure in News Articles around the Main Event,"Prafulla Kumar Choubey,Aaron Lee,Ruihong Huang,Lu Wang","Understanding discourse structures of news articles is vital to effectively contextualize the occurrence of a news event. To enable computational modeling of news structures, we apply an existing theory of functional discourse structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over four domains and three media sources. Next, we propose several document-level neural-network models to automatically construct news content structures. Finally, we demonstrate that incorporating system predicted news structures yields new state-of-the-art performance for event coreference resolution. The news documents we annotated are openly available and the annotations are publicly released for future research.",,,,ACL
479,2020,Harnessing the linguistic signal to predict scalar inferences,"Sebastian Schuster,Yuxing Chen,Judith Degen","Pragmatic inferences often subtly depend on the presence or absence of linguistic features. For example, the presence of a partitive construction (of the) increases the strength of a so-called scalar inference: listeners perceive the inference that Chris did not eat all of the cookies to be stronger after hearing “Chris ate some of the cookies” than after hearing the same utterance without a partitive, “Chris ate some cookies”. In this work, we explore to what extent neural network sentence encoders can learn to predict the strength of scalar inferences. We first show that an LSTM-based sentence encoder trained on an English dataset of human inference strength ratings is able to predict ratings with high accuracy (r = 0.78). We then probe the model’s behavior using manually constructed minimal sentence pairs and corpus data. We first that the model inferred previously established associations between linguistic features and inference strength, suggesting that the model learns to use linguistic features to predict pragmatic inferences.",,,,ACL
480,2020,Implicit Discourse Relation Classification: We Need to Talk about Evaluation,"Najoung Kim,Song Feng,Chulaka Gunasekara,Luis Lastras","Implicit relation classification on Penn Discourse TreeBank (PDTB) 2.0 is a common benchmark task for evaluating the understanding of discourse relations. However, the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature. In this work, we highlight these inconsistencies and propose an improved evaluation protocol. Paired with this protocol, we report strong baseline results from pretrained sentence encoders, which set the new state-of-the-art for PDTB 2.0. Furthermore, this work is the first to explore fine-grained relation classification on PDTB 3.0. We expect our work to serve as a point of comparison for future work, and also as an initiative to discuss models of larger context and possible data augmentations for downstream transferability.",,,,ACL
481,2020,PeTra: A Sparsely Supervised Memory Model for People Tracking,"Shubham Toshniwal,Allyson Ettinger,Kevin Gimpel,Karen Livescu","We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots. PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture. We empirically compare key modeling choices, finding that we can simplify several aspects of the design of the memory module while retaining strong performance. To measure the people tracking capability of memory models, we (a) propose a new diagnostic evaluation based on counting the number of unique entities in text, and (b) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of PeTra relative to a previous approach. PeTra is highly effective in both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation.",,,,ACL
482,2020,ZPR2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and BERT,"Linfeng Song,Kun Xu,Yue Zhang,Jianshu Chen","Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions, respectively. We propose to better explore their interaction by solving both tasks together, while the previous work treats them separately. For zero pronoun resolution, we study this task in a more realistic setting, where no parsing trees or only automatic trees are available, while most previous work assumes gold trees. Experiments on two benchmarks show that joint modeling significantly outperforms our baseline that already beats the previous state of the arts.",,,,ACL
483,2020,Contextualizing Hate Speech Classifiers with Post-hoc Explanation,"Brendan Kennedy,Xisen Jin,Aida Mostafazadeh Davani,Morteza Dehghani","Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like “gay” or “black” are used in offensive or prejudiced ways. Such biases manifest in false positives when these identifiers are present, due to models’ inability to learn the contexts which constitute a hateful usage of identifiers. We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms. Then, we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves. Our approach improved over baselines in limiting false positives on out-of-domain data while maintaining and in cases improving in-domain performance.",,,,ACL
484,2020,Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation,"Tianlu Wang,Xi Victoria Lin,Nazneen Fatema Rajani,Bryan McCann","Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.",,,,ACL
485,2020,Language (Technology) is Power: A Critical Survey of “Bias” in NLP,"Su Lin Blodgett,Solon Barocas,Hal Daumé III,Hanna Wallach","We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers’ proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",,,,ACL
486,2020,Social Bias Frames: Reasoning about Social and Power Implications of Language,"Maarten Sap,Saadia Gabriel,Lianhui Qin,Dan Jurafsky","Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people’s judgments about others. For example, given a statement that “we shouldn’t lower our standards to hire more women,” most listeners will infer the implicature intended by the speaker - that “women (candidates) are less qualified.” Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.",,,,ACL
487,2020,Social Biases in NLP Models as Barriers for Persons with Disabilities,"Ben Hutchinson,Vinodkumar Prabhakaran,Emily Denton,Kellie Webster","Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",,,,ACL
488,2020,Towards Debiasing Sentence Representations,"Paul Pu Liang,Irene Mengze Li,Emily Zheng,Yao Chong Lim","As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.",,,,ACL
489,2020,A Re-evaluation of Knowledge Graph Completion Methods,"Zhiqing Sun,Shikhar Vashishth,Soumya Sanyal,Partha Talukdar","Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available.",,,,ACL
490,2020,Cross-Linguistic Syntactic Evaluation of Word Prediction Models,"Aaron Mueller,Garrett Nicolai,Panayiota Petrou-Zeniou,Natalia Talmina","A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy. However, these studies are based primarily on monolingual evidence from English. To investigate how these models’ ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models. CLAMS includes subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars we develop. We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT. Across languages, monolingual LSTMs achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses. On other constructions, agreement accuracy was generally higher in languages with richer morphology. Multilingual models generally underperformed monolingual models. Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages.",,,,ACL
491,2020,Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?,"Peter Hase,Mohit Bansal","Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.",,,,ACL
492,2020,Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions,"Xiaochuang Han,Byron C. Wallace,Yulia Tsvetkov","Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which ‘saliency maps’ may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.",,,,ACL
493,2020,Finding Universal Grammatical Relations in Multilingual BERT,"Ethan A. Chi,John Hewitt,Christopher D. Manning","Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks’ internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.",,,,ACL
494,2020,Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection,"Hanjie Chen,Guangtao Zheng,Yangfeng Ji","Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness. In natural language processing, existing methods usually provide important features which are words or phrases selected from an input text as an explanation, but ignore the interactions between them. It poses challenges for humans to interpret an explanation and connect it to model prediction. In this work, we build hierarchical explanations by detecting feature interactions. Such explanations visualize how words and phrases are combined at different levels of the hierarchy, which can help users understand the decision-making of black-box models. The proposed method is evaluated with three neural text classifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic and human evaluations. Experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans.",,,,ACL
495,2020,Obtaining Faithful Interpretations from Compositional Neural Networks,"Sanjay Subramanian,Ben Bogin,Nitish Gupta,Tomer Wolfson","Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",,,,ACL
496,2020,Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport,"Kyle Swanson,Lili Yu,Tao Lei","Selecting input features of top relevance has become a popular method for building self-explaining models. In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction. Our approach employs optimal transport (OT) to find a minimal cost alignment between the inputs. However, directly applying OT often produces dense and therefore uninterpretable alignments. To overcome this limitation, we introduce novel constrained variants of the OT problem that result in highly sparse alignments with controllable sparsity. Our model is end-to-end differentiable using the Sinkhorn algorithm for OT and can be trained without any alignment annotations. We evaluate our model on the StackExchange, MultiNews, e-SNLI, and MultiRC datasets. Our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models.",,,,ACL
497,2020,Benefits of Intermediate Annotations in Reading Comprehension,"Dheeru Dua,Sameer Singh,Matt Gardner","Complex compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer. A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task. In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection. We find that these intermediate annotations can provide two-fold benefits. First, we observe that for any collection budget, spending a fraction of it on intermediate annotations results in improved model performance, for two complex compositional datasets: DROP and Quoref. Second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process.",,,,ACL
498,2020,Crossing Variational Autoencoders for Answer Retrieval,"Wenhao Yu,Lingfei Wu,Qingkai Zeng,Shu Tao","Answer retrieval is to find the most aligned answer from a large set of candidates given a question. Learning vector representations of questions/answers is the key factor. Question-answer alignment and question/answer semantics are two important signals for learning the representations. Existing methods learned semantic representations with dual encoders or dual variational auto-encoders. The semantic information was learned from language models or question-to-question (answer-to-answer) generative processes. However, the alignment and semantics were too separate to capture the aligned semantics between question and answer. In this work, we propose to cross variational auto-encoders by generating questions with aligned answers and generating answers with aligned questions. Experiments show that our method outperforms the state-of-the-art answer retrieval method on SQuAD.",,,,ACL
499,2020,Logic-Guided Data Augmentation and Regularization for Consistent Question Answering,"Akari Asai,Hannaneh Hajishirzi","Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events. This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models. Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model. Improving the global consistency of predictions, our approach achieves large improvements over previous methods in a variety of question answering (QA) tasks, including multiple-choice qualitative reasoning, cause-effect reasoning, and extractive machine reading comprehension. In particular, our method significantly improves the performance of RoBERTa-based models by 1-5% across datasets. We advance state of the art by around 5-8% on WIQA and QuaRel and reduce consistency violations by 58% on HotpotQA. We further demonstrate that our approach can learn effectively from limited data.",,,,ACL
500,2020,On the Importance of Diversity in Question Generation for QA,"Md Arafat Sultan,Shubham Chandel,Ramón Fernandez Astudillo,Vittorio Castelli","Automatic question generation (QG) has shown promise as a source of synthetic training data for question answering (QA). In this paper we ask: Is textual diversity in QG beneficial for downstream QA? Using top-p nucleus sampling to derive samples from a transformer-based question generator, we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search. We also show that standard QG evaluation metrics such as BLEU, ROUGE and METEOR are inversely correlated with diversity, and propose a diversity-aware intrinsic measure of overall QG quality that correlates well with extrinsic evaluation on QA.",,,,ACL
501,2020,Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering,"Hao Cheng,Ming-Wei Chang,Kenton Lee,Kristina Toutanova","We address the problem of extractive question answering using document-level distant super-vision, pairing questions and relevant documents with answer strings. We compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans). We show that these assumptions interact, and that different configurations provide complementary benefits. We demonstrate that a multi-objective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation. Our approach outperforms previous state-of-the-art models by 4.3 points in F1 on TriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries.",,,,ACL
502,2020,SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations,"Xiang Kong,Varun Gangal,Eduard Hovy","We introduce SCDE, a dataset to evaluate the performance of computational models through sentence prediction. SCDE is a human created sentence cloze dataset, collected from public school English examinations. Our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers. Experimental results demonstrate that this task requires the use of non-local, discourse-level context beyond the immediate sentence neighborhood. The blanks require joint solving and significantly impair each other’s context. Furthermore, through ablations, we show that the distractors are of high quality and make the task more challenging. Our experiments show that there is a significant performance gap between advanced models (72%) and humans (87%), encouraging future models to bridge this gap.",,,,ACL
503,2020,Selective Question Answering under Domain Shift,"Amita Kamath,Robin Jia,Percy Liang","To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the model’s training data, making errors more likely and thus abstention more critical. In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. Abstention policies based solely on the model’s softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely. Crucially, the calibrator benefits from observing the model’s behavior on out-of-domain data, even if from a different domain than the test data. We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets. Our method answers 56% of questions while maintaining 80% accuracy; in contrast, directly using the model’s probabilities only answers 48% at 80% accuracy.",,,,ACL
504,2020,The Cascade Transformer: an Application for Efficient Answer Sentence Selection,"Luca Soldaini,Alessandro Moschitti","Large transformer-based language models have been shown to be very effective in many classification tasks. However, their computational complexity prevents their use in applications requiring the classification of a large set of candidates. While previous works have investigated approaches to reduce model size, relatively little attention has been paid to techniques to improve batch throughput during inference. In this paper, we introduce the Cascade Transformer, a simple yet effective technique to adapt transformer-based models into a cascade of rankers. Each ranker is used to prune a subset of candidates in a batch, thus dramatically increasing throughput at inference time. Partial encodings from the transformer model are shared among rerankers, providing further speed-up. When compared to a state-of-the-art transformer model, our approach reduces computation by 37% with almost no impact on accuracy, as measured on two English Question Answering datasets.",,,,ACL
505,2020,Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering,"Changmao Li,Jinho D. Choi","We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue. First, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts. Then, multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering (QA). Our approach is evaluated on the FriendsQA dataset and shows improvements of 3.8% and 1.4% over the two state-of-the-art transformer models, BERT and RoBERTa, respectively.",,,,ACL
506,2020,Not All Claims are Created Equal: Choosing the Right Statistical Approach to Assess Hypotheses,"Erfan Sadeqi Azer,Daniel Khashabi,Ashish Sabharwal,Dan Roth","Empirical research in Natural Language Processing (NLP) has adopted a narrow set of principles for assessing hypotheses, relying mainly on p-value computation, which suffers from several known issues. While alternative proposals have been well-debated and adopted in other fields, they remain rarely discussed or used within the NLP community. We address this gap by contrasting various hypothesis assessment techniques, especially those not commonly used in the field (such as evaluations based on Bayesian inference). Since these statistical techniques differ in the hypotheses they can support, we argue that practitioners should first decide their target hypothesis before choosing an assessment method. This is crucial because common fallacies, misconceptions, and misinterpretation surrounding hypothesis assessment methods often stem from a discrepancy between what one would like to claim versus what the method used actually assesses. Our survey reveals that these issues are omnipresent in the NLP research community. As a step forward, we provide best practices and guidelines tailored to NLP research, as well as an easy-to-use package for Bayesian assessment of hypotheses, complementing existing tools.",,,,ACL
507,2020,STARC: Structured Annotations for Reading Comprehension,"Yevgeni Berzak,Jonathan Malmaud,Roger Levy","We present STARC (Structured Annotations for Reading Comprehension), a new annotation framework for assessing reading comprehension with multiple choice questions. Our framework introduces a principled structure for the answer choices and ties them to textual span annotations. The framework is implemented in OneStopQA, a new high-quality dataset for evaluation and analysis of reading comprehension in English. We use this dataset to demonstrate that STARC can be leveraged for a key new application for the development of SAT-like reading comprehension materials: automatic annotation quality probing via span ablation experiments. We further show that it enables in-depth analyses and comparisons between machine and human reading comprehension behavior, including error distributions and guessing ability. Our experiments also reveal that the standard multiple choice dataset in NLP, RACE, is limited in its ability to measure reading comprehension. 47% of its questions can be guessed by machines without accessing the passage, and 18% are unanimously judged by humans as not having a unique correct answer. OneStopQA provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance.",,,,ACL
508,2020,WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge,"Hongming Zhang,Xinran Zhao,Yangqiu Song","In this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge (WSC). For each of the questions, we invite annotators to first provide reasons for making correct decisions and then categorize them into six major knowledge categories. By doing so, we better understand the limitation of existing methods (i.e., what kind of knowledge cannot be effectively represented or inferred with existing methods) and shed some light on the commonsense knowledge that we need to acquire in the future for better commonsense reasoning. Moreover, to investigate whether current WSC models can understand the commonsense or they simply solve the WSC questions based on the statistical bias of the dataset, we leverage the collected reasons to develop a new task called WinoWhy, which requires models to distinguish plausible reasons from very similar but wrong reasons for all WSC questions. Experimental results prove that even though pre-trained language representation models have achieved promising progress on the original WSC dataset, they are still struggling at WinoWhy. Further experiments show that even though supervised models can achieve better performance, the performance of these models can be sensitive to the dataset distribution. WinoWhy and all codes are available at: https://github.com/HKUST-KnowComp/WinoWhy.",,,,ACL
509,2020,Agreement Prediction of Arguments in Cyber Argumentation for Detecting Stance Polarity and Intensity,"Joseph Sirrianni,Xiaoqing Liu,Douglas Adams","In online debates, users express different levels of agreement/disagreement with one another’s arguments and ideas. Often levels of agreement/disagreement are implicit in the text, and must be predicted to analyze collective opinions. Existing stance detection methods predict the polarity of a post’s stance toward a topic or post, but don’t consider the stance’s degree of intensity. We introduce a new research problem, stance polarity and intensity prediction in response relationships between posts. This problem is challenging because differences in stance intensity are often subtle and require nuanced language understanding. Cyber argumentation research has shown that incorporating both stance polarity and intensity data in online debates leads to better discussion analysis. We explore five different learning models: Ridge-M regression, Ridge-S regression, SVR-RF-R, pkudblab-PIP, and T-PAN-PIP for predicting stance polarity and intensity in argumentation. These models are evaluated using a new dataset for stance polarity and intensity prediction collected using a cyber argumentation platform. The SVR-RF-R model performs best for prediction of stance polarity with an accuracy of 70.43% and intensity with RMSE of 0.596. This work is the first to train models for predicting a post’s stance polarity and intensity in one combined value in cyber argumentation with reasonably good accuracy.",,,,ACL
510,2020,Cross-Lingual Unsupervised Sentiment Classification with Multi-View Transfer Learning,"Hongliang Fei,Ping Li","Recent neural network models have achieved impressive performance on sentiment classification in English as well as other languages. Their success heavily depends on the availability of a large amount of labeled data or parallel corpus. In this paper, we investigate an extreme scenario of cross-lingual sentiment classification, in which the low-resource language does not have any labels or parallel corpus. We propose an unsupervised cross-lingual sentiment classification model named multi-view encoder-classifier (MVEC) that leverages an unsupervised machine translation (UMT) system and a language discriminator. Unlike previous language model (LM) based fine-tuning approaches that adjust parameters solely based on the classification error on training data, we employ the encoder-decoder framework of a UMT as a regularization component on the shared network parameters. In particular, the cross-lingual encoder of our model learns a shared representation, which is effective for both reconstructing input sentences of two languages and generating more representative views from the input for classification. Extensive experiments on five language pairs verify that our model significantly outperforms other models for 8/11 sentiment classification tasks.",,,,ACL
511,2020,Efficient Pairwise Annotation of Argument Quality,"Lukas Gienapp,Benno Stein,Matthias Hagen,Martin Potthast","We present an efficient annotation framework for argument quality, a feature difficult to be measured reliably as per previous work. A stochastic transitivity model is combined with an effective sampling strategy to infer high-quality labels with low effort from crowdsourced pairwise judgments. The model’s capabilities are showcased by compiling Webis-ArgQuality-20, an argument quality corpus that comprises scores for rhetorical, logical, dialectical, and overall quality inferred from a total of 41,859 pairwise judgments among 1,271 arguments. With up to 93% cost savings, our approach significantly outperforms existing annotation procedures. Furthermore, novel insight into argument quality is provided through statistical analysis, and a new aggregation method to infer overall quality from individual quality dimensions is proposed.",,,,ACL
512,2020,Entity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification,"Nianzu Ma,Sahisnu Mazumder,Hao Wang,Bing Liu","This paper studies the task of comparative preference classification (CPC). Given two entities in a sentence, our goal is to classify whether the first (or the second) entity is preferred over the other or no comparison is expressed at all between the two entities. Existing works either do not learn entity-aware representations well and fail to deal with sentences involving multiple entity pairs or use sequential modeling approaches that are unable to capture long-range dependencies between the entities. Some also use traditional machine learning approaches that do not generalize well. This paper proposes a novel Entity-aware Dependency-based Deep Graph Attention Network (ED-GAT) that employs a multi-hop graph attention over a dependency graph sentence representation to leverage both the semantic information from word embeddings and the syntactic information from the dependency graph to solve the problem. Empirical evaluation shows that the proposed model achieves the state-of-the-art performance in comparative preference classification.",,,,ACL
513,2020,OpinionDigest: A Simple Framework for Opinion Summarization,"Yoshihiko Suhara,Xiaolan Wang,Stefanos Angelidis,Wang-Chiew Tan","We present OpinionDigest, an abstractive opinion summarization framework, which does not rely on gold-standard summaries for training. The framework uses an Aspect-based Sentiment Analysis model to extract opinion phrases from reviews, and trains a Transformer model to reconstruct the original reviews from these extractions. At summarization time, we merge extractions from multiple reviews and select the most popular ones. The selected opinions are used as input to the trained Transformer model, which verbalizes them into an opinion summary. OpinionDigest can also generate customized summaries, tailored to specific user needs, by filtering the selected opinions according to their aspect and/or sentiment. Automatic evaluation on Yelp data shows that our framework outperforms competitive baselines. Human studies on two corpora verify that OpinionDigest produces informative summaries and shows promising customization capabilities.",,,,ACL
514,2020,A Comprehensive Analysis of Preprocessing for Word Representation Learning in Affective Tasks,"Nastaran Babanejad,Ameeta Agrawal,Aijun An,Manos Papagelis","Affective tasks such as sentiment analysis, emotion classification, and sarcasm detection have been popular in recent years due to an abundance of user-generated data, accurate computational linguistic models, and a broad range of relevant applications in various domains. At the same time, many studies have highlighted the importance of text preprocessing, as an integral step to any natural language processing prediction model and downstream task. While preprocessing in affective systems is well-studied, preprocessing in word vector-based models applied to affective systems, is not. To address this limitation, we conduct a comprehensive analysis of the role of preprocessing techniques in affective analysis based on word vector models. Our analysis is the first of its kind and provides useful insights of the importance of each preprocessing technique when applied at the training phase, commonly ignored in pretrained word vector models, and/or at the downstream task phase.",,,,ACL
515,2020,Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness,"Sixing Wu,Ying Li,Dawei Zhang,Yang Zhou","Generative dialogue systems tend to produce generic responses, which often leads to boring conversations. For alleviating this issue, Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs. While this paradigm works to a certain extent, it usually retrieves knowledge facts only based on the entity word itself, without considering the specific dialogue context. Thus, the introduction of the context-irrelevant knowledge facts can impact the quality of generations. To this end, this paper proposes a novel commonsense knowledge-aware dialogue generation model, ConKADI. We design a Felicitous Fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context; furthermore, two techniques, Context-Knowledge Fusion and Flexible Mode Fusion are proposed to facilitate the integration of the knowledge in the ConKADI. We collect and build a large-scale Chinese dataset aligned with the commonsense knowledge for dialogue generation. Extensive evaluations over both an open-released English dataset and our Chinese dataset demonstrate that our approach ConKADI outperforms the state-of-the-art approach CCM, in most experiments.",,,,ACL
516,2020,"Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation","Haoyu Song,Yan Wang,Wei-Nan Zhang,Xiaojiang Liu","Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines. The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models. Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words. In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one. We carry out evaluations by both human and automatic metrics. Experiments on the Persona-Chat dataset show that our approach achieves good performance.",,,,ACL
517,2020,Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks,"Yiping Song,Zequn Liu,Wei Bi,Rui Yan","Training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems. Existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine-tunes on the target task. However, fine-tuning distinguishes tasks from the parameter perspective but ignores the model-structure perspective, resulting in similar dialogue models for different tasks. In this paper, we propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting. In our approach, each dialogue model consists of a shared module, a gating module, and a private module. The first two modules are shared among all the tasks, while the third one will differentiate into different network structures to better capture the characteristics of the corresponding task. The extensive experiments on two datasets show that our method outperforms all the baselines in terms of task consistency, response quality, and diversity.",,,,ACL
518,2020,Video-Grounded Dialogues with Pretrained Generation Language Models,"Hung Le,Steven C.H. Hoi","Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses. In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns. We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network. Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context. We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from DSTC7, which supports a potential direction in this line of research.",,,,ACL
519,2020,A Unified MRC Framework for Named Entity Recognition,"Xiaoya Li,Jingrong Feng,Yuxian Meng,Qinghong Han","The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not.Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels. In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks. Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (MRC) task. For example, extracting entities with the per label is formalized as extracting answer spans to the question “which person is mentioned in the text"".This formulation naturally tackles the entity overlapping issue in nested NER: the extraction of two overlapping entities with different categories requires answering two independent questions. Additionally, since the query encodes informative prior knowledge, this strategy facilitates the process of entity extraction, leading to better performances for not only nested NER, but flat NER. We conduct experiments on both nested and flat NER datasets.Experiment results demonstrate the effectiveness of the proposed formulation. We are able to achieve a vast amount of performance boost over current SOTA models on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37,respectively on ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets, i.e., +0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA and Chinese OntoNotes 4.0.",,,,ACL
520,2020,An Effective Transition-based Model for Discontinuous NER,"Xiang Dai,Sarvnaz Karimi,Ben Hachey,Cecile Paris","Unlike widely used Named Entity Recognition (NER) data sets in generic domains, biomedical NER data sets often contain mentions consisting of discontinuous spans. Conventional sequence tagging techniques encode Markov assumptions that are efficient but preclude recovery of these mentions. We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER. Through extensive experiments on three biomedical data sets, we show that our model can effectively recognize discontinuous mentions without sacrificing the accuracy on continuous mentions.",,,,ACL
521,2020,IMoJIE: Iterative Memory-Based Joint Open Information Extraction,"Keshav Kolluru,Samarth Aggarwal,Vipul Rathore,Mausam","While traditional systems for Open Information Extraction were statistical and rule-based, recently neural models have been introduced for the task. Our work builds upon CopyAttention, a sequence generation OpenIE model (Cui et. al. 18). Our analysis reveals that CopyAttention produces a constant number of extractions per sentence, and its extracted tuples often express redundant information. We present IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples. This approach overcomes both shortcomings of CopyAttention, resulting in a variable number of diverse extractions per sentence. We train IMoJIE on training data bootstrapped from extractions of several non-neural systems, which have been automatically filtered to reduce redundancy and noise. IMoJIE outperforms CopyAttention by about 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task.",,,,ACL
522,2020,Improving Event Detection via Open-domain Trigger Knowledge,"Meihan Tong,Bin Xu,Shuai Wang,Yixin Cao","Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words. The source code is released on https://github.com/shuaiwa16/ekd.git.",,,,ACL
523,2020,Improving Low-Resource Named Entity Recognition using Joint Sentence and Token Labeling,"Canasai Kruengkrai,Thien Hai Nguyen,Sharifah Mahani Aljunied,Lidong Bing","Exploiting sentence-level labels, which are easy to obtain, is one of the plausible methods to improve low-resource named entity recognition (NER), where token-level labels are costly to annotate. Current models for jointly learning sentence and token labeling are limited to binary classification. We present a joint model that supports multi-class classification and introduce a simple variant of self-attention that allows the model to learn scaling factors. Our model produces 3.78%, 4.20%, 2.08% improvements in F1 over the BiLSTM-CRF baseline on e-commerce product titles in three different low-resource languages: Vietnamese, Thai, and Indonesian, respectively.",,,,ACL
524,2020,Multi-Cell Compositional LSTM for NER Domain Adaptation,"Chen Jia,Yue Zhang","Cross-domain NER is a challenging yet practical problem. Entity mentions can be highly different across domains. However, the correlations between entity types can be relatively more stable across domains. We investigate a multi-cell compositional LSTM structure for multi-task learning, modeling each entity type using a separate cell state. With the help of entity typed units, cross-domain knowledge transfer can be made in an entity type level. Theoretically, the resulting distinct feature distributions for each entity type make it more powerful for cross-domain transfer. Empirically, experiments on four few-shot and zero-shot datasets show our method significantly outperforms a series of multi-task learning methods and achieves the best results.",,,,ACL
525,2020,Pyramid: A Layered Model for Nested Named Entity Recognition,"Jue Wang,Lidan Shou,Ke Chen,Gang Chen","This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER). In our approach, token or text region embeddings are recursively inputted into L flat NER layers, from bottom to top, stacked in a pyramid shape. Each time an embedding passes through a layer of the pyramid, its length is reduced by one. Its hidden state at layer l represents an l-gram in the input text, which is labeled only if its corresponding text region represents a complete entity mention. We also design an inverse pyramid to allow bidirectional interaction between layers. The proposed method achieves state-of-the-art F1 scores in nested NER on ACE-2004, ACE-2005, GENIA, and NNE, which are 80.27, 79.42, 77.78, and 93.70 with conventional embeddings, and 87.74, 86.34, 79.31, and 94.68 with pre-trained contextualized embeddings. In addition, our model can be used for the more general task of Overlapping Named Entity Recognition. A preliminary experiment confirms the effectiveness of our method in overlapping NER.",,,,ACL
526,2020,ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding,"Zhiwen Xie,Guangyou Zhou,Jin Liu,Jimmy Xiangji Huang","The goal of Knowledge graph embedding (KGE) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples. The conventional shallow models are limited to their expressiveness. ConvE (Dettmers et al., 2018) takes advantage of CNN and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings. However, there is no structural information in the embedding space of ConvE, and the performance is still limited by the number of interactions. The recent KBGAT (Nathani et al., 2019) provides another way to learn embeddings by adaptively utilizing structural information. In this paper, we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception network with joint local-global structural information for knowledge graph Embedding (ReInceptionE). Specifically, we first explore the Inception network to learn query embedding, which aims to further increase the interactions between head and relation embeddings. Then, we propose to use a relation-aware attention mechanism to enrich the query embedding with the local neighborhood and global entity information. Experimental results on both WN18RR and FB15k-237 datasets demonstrate that ReInceptionE achieves competitive performance compared with state-of-the-art methods.",,,,ACL
527,2020,Relabel the Noise: Joint Extraction of Entities and Relations via Cooperative Multiagents,"Daoyuan Chen,Yaliang Li,Kai Lei,Ying Shen","Distant supervision based methods for entity and relation extraction have received increasing popularity due to the fact that these methods require light human annotation efforts. In this paper, we consider the problem of shifted label distribution, which is caused by the inconsistency between the noisy-labeled training set subject to external knowledge graph and the human-annotated test set, and exacerbated by the pipelined entity-then-relation extraction manner with noise propagation. We propose a joint extraction approach to address this problem by re-labeling noisy instances with a group of cooperative multiagents. To handle noisy instances in a fine-grained manner, each agent in the cooperative group evaluates the instance by calculating a continuous confidence score from its own perspective; To leverage the correlations between these two extraction tasks, a confidence consensus module is designed to gather the wisdom of all agents and re-distribute the noisy training set with confidence-scored labels. Further, the confidences are used to adjust the training losses of extractors. Experimental results on two real-world datasets verify the benefits of re-labeling noisy instance, and show that the proposed model significantly outperforms the state-of-the-art entity and relation extraction methods.",,,,ACL
528,2020,Simplify the Usage of Lexicon in Chinese NER,"Ruotian Ma,Minlong Peng,Qi Zhang,Zhongyu Wei","Recently, many works have tried to augment the performance of Chinese named entity recognition (NER) using word lexicons. As a representative, Lattice-LSTM has achieved new benchmark results on several public Chinese NER datasets. However, Lattice-LSTM has a complex model architecture. This limits its application in many industrial areas where real-time NER responses are needed. In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations. This method avoids designing a complicated sequence modeling architecture, and for any neural NER model, it requires only subtle adjustment of the character representation layer to introduce the lexicon information. Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-of-the-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT.",,,,ACL
529,2020,AdvAug: Robust Adversarial Augmentation for Neural Machine Translation,"Yong Cheng,Lu Jiang,Wolfgang Macherey,Jacob Eisenstein","In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT). The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning. Experiments on Chinese-English, English-French, and English-German translation benchmarks show that AdvAug achieves significant improvements over theTransformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g.back-translation) without using extra corpora.",,,,ACL
530,2020,Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns,"KayYen Wong,Sameen Maruf,Gholamreza Haffari","The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns. Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation. In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context. Our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic Transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context. We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU.",,,,ACL
531,2020,Improving Neural Machine Translation with Soft Template Prediction,"Jian Yang,Shuming Ma,Dongdong Zhang,Zhoujun Li","Although neural machine translation (NMT) has achieved significant progress in recent years, most previous NMT models only depend on the source text to generate translation. Inspired by the success of template-based and syntax-based approaches in other fields, we propose to use extracted templates from tree structures as soft target templates to guide the translation procedure. In order to learn the syntactic structure of the target sentences, we adopt constituency-based parse tree to generate candidate templates. We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text. Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrates the effectiveness of soft target templates.",,,,ACL
532,2020,Tagged Back-translation Revisited: Why Does It Really Work?,"Benjamin Marie,Raphael Rubino,Atsushi Fujita","In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.",,,,ACL
533,2020,"Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation","Shun-Po Chuang,Tzu-Wei Sung,Alexander H. Liu,Hung-yi Lee","Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.",,,,ACL
534,2020,Neural-DINF: A Neural Network based Framework for Measuring Document Influence,"Jie Tan,Changlin Yang,Ying Li,Siliang Tang","Measuring the scholarly impact of a document without citations is an important and challenging problem. Existing approaches such as Document Influence Model (DIM) are based on dynamic topic models, which only consider the word frequency change. In this paper, we use both frequency changes and word semantic shifts to measure document influence by developing a neural network framework. Our model has three steps. Firstly, we train the word embeddings for different time periods. Subsequently, we propose an unsupervised method to align vectors for different time periods. Finally, we compute the influence value of documents. Our experimental results show that our model outperforms DIM.",,,,ACL
535,2020,Paraphrase Generation by Learning How to Edit from Samples,"Amirhossein Kazemnejad,Mohammadreza Salehi,Mahdieh Soleymani Baghshah","Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation. Despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity. To address these problems, we propose a novel retrieval-based method for paraphrase generation. Our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index. With its novel editor module, the model then paraphrases the input sequence by editing it using the extracted relations between the retrieved pair of sentences. In order to have fine-grained control over the editing process, our model uses the newly introduced concept of Micro Edit Vectors. It both extracts and exploits these vectors using the attention mechanism in the Transformer architecture. Experimental results show the superiority of our paraphrase generation method in terms of both automatic metrics, and human evaluation of relevance, grammaticality, and diversity of generated paraphrases.",,,,ACL
536,2020,Emerging Cross-lingual Structure in Pretrained Language Models,"Alexis Conneau,Shijie Wu,Haoran Li,Luke Zettlemoyer","We study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer. We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains. The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder. To better understand this result, we also show that representations from monolingual BERT models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces. For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process.",,,,ACL
537,2020,FastBERT: a Self-distilling BERT with Adaptive Inference Time,"Weijie Liu,Peng Zhou,Zhiruo Wang,Zhe Zhao","Pre-trained language models like BERT have proven to be highly performant. However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources. To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT with adaptive inference time. The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided. Moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance. Our model achieves promising results in twelve English and Chinese datasets. It is able to speed up by a wide range from 1 to 12 times than BERT if given different speedup thresholds to make a speed-performance tradeoff.",,,,ACL
538,2020,Incorporating External Knowledge through Pre-training for Natural Language to Code Generation,"Frank F. Xu,Zhengbao Jiang,Pengcheng Yin,Bogdan Vasilescu","Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents. Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation. Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute BLEU score on the code generation testbed CoNaLa. The code and resources are available at https://github.com/neulab/external-knowledge-codegen.",,,,ACL
539,2020,LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network,"Wanjun Zhong,Duyu Tang,Zhangyin Feng,Nan Duan","Verifying the correctness of a textual statement requires not only semantic reasoning about the meaning of words, but also symbolic reasoning about logical operations like count, superlative, aggregation, etc. In this work, we propose LogicalFactChecker, a neural network approach capable of leveraging logical operations for fact checking. It achieves the state-of-the-art performance on TABFACT, a large-scale, benchmark dataset built for verifying a textual statement with semi-structured tables. This is achieved by a graph module network built upon the Transformer-based architecture. With a textual statement and a table as the input, LogicalFactChecker automatically derives a program (a.k.a. logical form) of the statement in a semantic parsing manner. A heterogeneous graph is then constructed to capture not only the structures of the table and the program, but also the connections between inputs with different modalities. Such a graph reveals the related contexts of each word in the statement, the table and the program. The graph is used to obtain graph-enhanced contextual representations of words in Transformer-based architecture. After that, a program-driven module network is further introduced to exploit the hierarchical structure of the program, where semantic compositionality is dynamically modeled along the program structure with a set of function-specific modules. Ablation experiments suggest that both the heterogeneous graph and the module network are important to obtain strong results.",,,,ACL
540,2020,Word-level Textual Adversarial Attacking as Combinatorial Optimization,"Yuan Zang,Fanchao Qi,Chenghao Yang,Zhiyuan Liu","Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.",,,,ACL
541,2020,Benchmarking Multimodal Regex Synthesis with Complex Structures,"Xi Ye,Qiaochu Chen,Isil Dillig,Greg Durrett","Existing datasets for regular expression (regex) generation from natural language are limited in complexity; compared to regex tasks that users post on StackOverflow, the regexes in these datasets are simple, and the language used to describe them is not diverse. We introduce StructuredRegex, a new regex synthesis dataset differing from prior ones in three aspects. First, to obtain structurally complex and realistic regexes, we generate the regexes using a probabilistic grammar with pre-defined macros observed from real-world StackOverflow posts. Second, to obtain linguistically diverse natural language descriptions, we show crowdworkers abstract depictions of the underlying regex and ask them to describe the pattern they see, rather than having them paraphrase synthetic language. Third, we augment each regex example with a collection of strings that are and are not matched by the ground truth regex, similar to how real users give examples. Our quantitative and qualitative analysis demonstrates the advantages of StructuredRegex over prior datasets. Further experimental results using various multimodal synthesis techniques highlight the challenge presented by our dataset, including non-local constraints and multi-modal inputs.",,,,ACL
542,2020,Curriculum Learning for Natural Language Understanding,"Benfeng Xu,Licheng Zhang,Zhendong Mao,Quan Wang","With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.",,,,ACL
543,2020,Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?,"Hitomi Yanaka,Koji Mineshima,Daisuke Bekki,Kentaro Inui","Despite the success of language models using neural networks, it remains unclear to what extent neural models have the generalization ability to perform inferences. In this paper, we introduce a method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on composition. We consider four aspects of monotonicity inferences and test whether the models can systematically interpret lexical and logical phenomena on different training/test splits. A series of experiments show that three neural models systematically draw inferences on unseen combinations of lexical and logical phenomena when the syntactic structures of the sentences are similar between the training and test sets. However, the performance of the models significantly decreases when the structures are slightly changed in the test set while retaining all vocabularies and constituents already appearing in the training set. This indicates that the generalization ability of neural models is limited to cases where the syntactic structures are nearly the same as those in the training set.",,,,ACL
544,2020,Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder,"Daya Guo,Duyu Tang,Nan Duan,Jian Yin","Generating inferential texts about an event in different perspectives requires reasoning over different contexts that the event occurs. Existing works usually ignore the context that is not explicitly provided, resulting in a context-independent semantic representation that struggles to support the generation. To address this, we propose an approach that automatically finds evidence for an event from a large text corpus, and leverages the evidence to guide the generation of inferential texts. Our approach works in an encoderdecoder manner and is equipped with Vector Quantised-Variational Autoencoder, where the encoder outputs representations from a distribution over discrete variables. Such discrete representations enable automatically selecting relevant evidence, which not only facilitates evidence-aware generation, but also provides a natural way to uncover rationales behind the generation. Our approach provides state-of-the-art performance on both Event2mind and Atomic datasets. More importantly, we find that with discrete representations, our model selectively uses evidence to generate different inferential texts.",,,,ACL
545,2020,How to Ask Good Questions? Try to Leverage Paraphrases,"Xin Jia,Wenjie Zhou,Xu Sun,Yunfang Wu","Given a sentence and its relevant answer, how to ask good questions is a challenging task, which has many real applications. Inspired by human’s paraphrasing capability to ask questions of the same meaning but with diverse expressions, we propose to incorporate paraphrase knowledge into question generation(QG) to generate human-like questions. Specifically, we present a two-hand hybrid model leveraging a self-built paraphrase resource, which is automatically conducted by a simple back-translation method. On the one hand, we conduct multi-task learning with sentence-level paraphrase generation (PG) as an auxiliary task to supplement paraphrase knowledge to the task-share encoder. On the other hand, we adopt a new loss function for diversity training to introduce more question patterns to QG. Extensive experimental results show that our proposed model obtains obvious performance gain over several strong baselines, and further human evaluation validates that our model can ask questions of high quality by leveraging paraphrase knowledge.",,,,ACL
546,2020,NeuInfer: Knowledge Inference on N-ary Facts,"Saiping Guan,Xiaolong Jin,Jiafeng Guo,Yuanzhuo Wang","Knowledge inference on knowledge graph has attracted extensive attention, which aims to find out connotative valid facts in knowledge graph and is very helpful for improving the performance of many downstream applications. However, researchers have mainly poured attention to knowledge inference on binary facts. The studies on n-ary facts are relatively scarcer, although they are also ubiquitous in the real world. Therefore, this paper addresses knowledge inference on n-ary facts. We represent each n-ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute-value pair(s). We further propose a neural network model, NeuInfer, for knowledge inference on n-ary facts. Besides handling the common task to infer an unknown element in a whole fact, NeuInfer can cope with a new type of task, flexible knowledge inference. It aims to infer an unknown element in a partial fact consisting of the primary triple coupled with any number of its auxiliary description(s). Experimental results demonstrate the remarkable superiority of NeuInfer.",,,,ACL
547,2020,Neural Graph Matching Networks for Chinese Short Text Matching,"Lu Chen,Yanbin Zhao,Boer Lyu,Lesheng Jin","Chinese short text matching usually employs word sequences rather than character sequences to get better performance. However, Chinese word segmentation can be erroneous, ambiguous or inconsistent, which consequently hurts the final matching performance. To address this problem, we propose neural graph matching networks, a novel sentence matching framework capable of dealing with multi-granular input information. Instead of a character sequence or a single word sequence, paired word lattices formed from multiple word segmentation hypotheses are used as input and the model learns a graph representation according to an attentive graph matching mechanism. Experiments on two Chinese datasets show that our models outperform the state-of-the-art short text matching models.",,,,ACL
548,2020,Neural Mixed Counting Models for Dispersed Topic Discovery,"Jiemin Wu,Yanghui Rao,Zusheng Zhang,Haoran Xie","Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics. However, the existing parameter inference method like Monte Carlo sampling is quite time-consuming. In this paper, we propose two efficient neural mixed counting models, i.e., the Negative Binomial-Neural Topic Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for dispersed topic discovery. Neural variational inference algorithms are developed to infer model parameters by using the reparameterization of Gamma distribution and the Gaussian approximation of Poisson distribution. Experiments on real-world datasets indicate that our models outperform state-of-the-art baseline models in terms of perplexity and topic coherence. The results also validate that both NB-NTM and GNB-NTM can produce explainable intermediate variables by generating dispersed proportions of document topics.",,,,ACL
549,2020,Reasoning Over Semantic-Level Graph for Fact Checking,"Wanjun Zhong,Jingjing Xu,Duyu Tang,Zenan Xu","Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a method suitable for reasoning about the semantic-level structure of evidence. Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances. Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph. We evaluate our system on FEVER, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy. Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score.",,,,ACL
550,2020,Automatic Generation of Citation Texts in Scholarly Papers: A Pilot Study,"Xinyu Xing,Xiaosheng Fan,Xiaojun Wan","In this paper, we study the challenging problem of automatic generation of citation texts in scholarly papers. Given the context of a citing paper A and a cited paper B, the task aims to generate a short text to describe B in the given context of A. One big challenge for addressing this task is the lack of training data. Usually, explicit citation texts are easy to extract, but it is not easy to extract implicit citation texts from scholarly papers. We thus first train an implicit citation extraction model based on BERT and leverage the model to construct a large training dataset for the citation text generation task. Then we propose and train a multi-source pointer-generator network with cross attention mechanism for citation text generation. Empirical evaluation results on a manually labeled test dataset verify the efficacy of our model. This pilot study confirms the feasibility of automatically generating citation texts in scholarly papers and the technique has the great potential to help researchers prepare their scientific papers.",,,,ACL
551,2020,Composing Elementary Discourse Units in Abstractive Summarization,"Zhenwen Li,Wenhao Wu,Sujian Li","In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization. To well handle the problem of composing EDUs into an informative and fluent summary, we propose a novel summarization method that first designs an EDU selection model to extract and group informative EDUs and then an EDU fusion model to fuse the EDUs in each group into one sentence. We also design the reinforcement learning mechanism to use EDU fusion results to reward the EDU selection action, boosting the final summarization performance. Experiments on CNN/Daily Mail have demonstrated the effectiveness of our model.",,,,ACL
552,2020,Extractive Summarization as Text Matching,"Ming Zhong,Pengfei Liu,Yiran Chen,Danqing Wang","This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https://github.com/maszhongming/MatchSum.",,,,ACL
553,2020,Heterogeneous Graph Neural Networks for Extractive Document Summarization,"Danqing Wang,Pengfei Liu,Yining Zheng,Xipeng Qiu","As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github.",,,,ACL
554,2020,Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization,"Yue Cao,Hui Liu,Xiaojun Wan","Cross-lingual summarization is the task of generating a summary in one language given a text in a different language. Previous works on cross-lingual summarization mainly focus on using pipeline methods or training an end-to-end model using the translated parallel data. However, it is a big challenge for the model to directly learn cross-lingual summarization as it requires learning to understand different languages and learning how to summarize at the same time. In this paper, we propose to ease the cross-lingual summarization training by jointly learning to align and summarize. We design relevant loss functions to train this framework and propose several methods to enhance the isomorphism and cross-lingual transfer between languages. Experimental results show that our model can outperform competitive models in most cases. In addition, we show that our model even has the ability to generate cross-lingual summaries without access to any cross-lingual corpus.",,,,ACL
555,2020,Leveraging Graph to Improve Abstractive Multi-Document Summarization,"Wei Li,Xinyan Xiao,Jiachen Liu,Hua Wu","Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.",,,,ACL
556,2020,Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization,"Hanqi Jin,Tianming Wang,Xiaojun Wan","In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.",,,,ACL
557,2020,Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference,"Nikita Kitaev,Dan Klein","We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word’s tags in parallel, with minimal task-specific structure. After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies.",,,,ACL
558,2020,Are we Estimating or Guesstimating Translation Quality?,"Shuo Sun,Francisco Guzmán,Lucia Specia","Recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation (QE) for machine translation. A carefully engineered ensemble of such models won the QE shared task at WMT19. Our in-depth analysis, however, shows that the success of using pre-trained language models for QE is over-estimated due to three issues we observed in current QE datasets: (i) The distributions of quality scores are imbalanced and skewed towards good quality scores; (iii) QE models can perform well on these datasets while looking at only source or translated sentences; (iii) They contain statistical artifacts that correlate well with human-annotated QE labels. Our findings suggest that although QE models might capture fluency of translated sentences and complexity of source sentences, they cannot model adequacy of translations effectively.",,,,ACL
559,2020,Language (Re)modelling: Towards Embodied Language Understanding,"Ronen Tamari,Chen Shani,Tom Hope,Miriam R L Petruck","While natural language understanding (NLU) is advancing rapidly, today’s technology differs from human-like language understanding in fundamental ways, notably in its inferior efficiency, interpretability, and generalization. This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL). According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction. This position paper argues that the use of grounding by metaphoric reasoning and simulation will greatly benefit NLU systems, and proposes a system architecture along with a roadmap towards realizing this vision.",,,,ACL
560,2020,The State and Fate of Linguistic Diversity and Inclusion in the NLP World,"Pratik Joshi,Sebastin Santy,Amar Budhiraja,Kalika Bali","Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the “language agnostic” status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.",,,,ACL
561,2020,The Unstoppable Rise of Computational Linguistics in Deep Learning,James Henderson,"In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures. We focus on the importance of variable binding and its instantiation in attention-based models, and argue that Transformer is not a sequence model but an induced-structure model. This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language understanding.",,,,ACL
562,2020,To Boldly Query What No One Has Annotated Before? The Frontiers of Corpus Querying,"Markus Gärtner,Kerstin Jung","Corpus query systems exist to address the multifarious information needs of any person interested in the content of annotated corpora. In this role they play an important part in making those resources usable for a wider audience. Over the past decades, several such query systems and languages have emerged, varying greatly in their expressiveness and technical details. This paper offers a broad overview of the history of corpora and corpus query tools. It focusses strongly on the query side and hints at exciting directions for future development.",,,,ACL
563,2020,A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking,"Yong Shan,Zekang Li,Jinchao Zhang,Fandong Meng","Recent studies in dialogue state tracking (DST) leverage historical information to determine states which are generally represented as slot-value pairs. However, most of them have limitations to efficiently exploit relevant context due to the lack of a powerful mechanism for modeling interactions between the slot and the dialogue history. Besides, existing methods usually ignore the slot imbalance problem and treat all slots indiscriminately, which limits the learning of hard slots and eventually hurts overall performance. In this paper, we propose to enhance the DST through employing a contextual hierarchical attention network to not only discern relevant information at both word level and turn level but also learn contextual representations. We further propose an adaptive objective to alleviate the slot imbalance problem by dynamically adjust weights of different slots during training. Experimental results show that our approach reaches 52.68% and 58.55% joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new state-of-the-art performance with considerable improvements (+1.24% and +5.98%).",,,,ACL
564,2020,Data Manipulation: Towards Effective Instance Learning for Neural Dialogue Generation via Learning to Augment and Reweight,"Hengyi Cai,Hongshen Chen,Yonghao Song,Cheng Zhang","Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm. As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model. However, due to the open-ended nature of human conversations, the quality of user-generated training data varies greatly, and effective training samples are typically insufficient while noisy samples frequently appear. This impedes the learning of those data-driven neural dialogue models. Therefore, effective dialogue learning requires not only more reliable learning samples, but also fewer noisy samples. In this paper, we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously. In particular, the data manipulation model selectively augments the training samples and assigns an importance weight to each instance to reform the training data. Note that, the proposed data manipulation framework is fully data-driven and learnable. It not only manipulates training samples to optimize the dialogue generation model, but also learns to increase its manipulation skills through gradient descent with validation samples. Extensive experiments show that our framework can improve the dialogue generation performance with respect to various automatic evaluation metrics and human judgments.",,,,ACL
565,2020,Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog,"Libo Qin,Xiao Xu,Wanxiang Che,Yue Zhang","Recent studies have shown remarkable success in end-to-end task-oriented dialog system. However, most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling. This makes it difficult to scalable for a new domain with limited labeled data. However, there has been relatively little research on how to effectively use data from all domains to improve the performance of each domain and also unseen domains. To this end, we investigate methods that can make explicit use of domain knowledge and introduce a shared-private network to learn shared and specific knowledge. In addition, we propose a novel Dynamic Fusion Network (DF-Net) which automatically exploit the relevance between the target domain and each domain. Results show that our models outperforms existing methods on multi-domain dialogue, giving the state-of-the-art in the literature. Besides, with little training data, we show its transferability by outperforming prior best model by 13.9% on average.",,,,ACL
566,2020,Learning Efficient Dialogue Policy from Demonstrations through Shaping,"Huimin Wang,Baolin Peng,Kam-Fai Wong","Training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users. Human demonstrations can be used to accelerate learning progress. However, how to effectively leverage demonstrations to learn dialogue policy remains less explored. In this paper, we present Sˆ2Agent that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping. We use an imitation model to distill knowledge from demonstrations, based on which policy shaping estimates feedback on how the agent should act in policy space. Reward shaping is then incorporated to bonus state-actions similar to demonstrations explicitly in value space encouraging better exploration. The effectiveness of the proposed Sˆ2Agentt is demonstrated in three dialogue domains and a challenging domain adaptation task with both user simulator evaluation and human evaluation.",,,,ACL
567,2020,SAS: Dialogue State Tracking via Slot Attention and Slot Information Sharing,"Jiaying Hu,Yan Yang,Chencai Chen,Liang He","Dialogue state tracker is responsible for inferring user intentions through dialogue history. Previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information. We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information’s interference and improve long dialogue context tracking. Specially, we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate them using a slot information sharing module. Our model yields a significantly improved performance compared to previous state-of the-art models on the MultiWOZ dataset.",,,,ACL
568,2020,Speaker Sensitive Response Evaluation Model,"JinYeong Bak,Alice Oh","Automatic evaluation of open-domain dialogue response generation is very challenging because there are many appropriate responses for a given context. Existing evaluation models merely compare the generated response with the ground truth response and rate many of the appropriate responses as inappropriate if they deviate from the ground truth. One approach to resolve this problem is to consider the similarity of the generated response with the conversational context. In this paper, we propose an automatic evaluation model based on that idea and learn the model parameters from an unlabeled conversation corpus. Our approach considers the speakers in defining the different levels of similar context. We use a Twitter conversation corpus that contains many speakers and conversations to test our evaluation model. Experiments show that our model outperforms the other existing evaluation metrics in terms of high correlation with human annotation scores. We also show that our model trained on Twitter can be applied to movie dialogues without any additional training. We provide our code and the learned parameters so that they can be used for automatic evaluation of dialogue response generation models.",,,,ACL
569,2020,A Top-down Neural Architecture towards Text-level Parsing of Discourse Rhetorical Structure,"Longyin Zhang,Yuqing Xing,Fang Kong,Peifeng Li","Due to its great importance in deep natural language understanding and various down-stream applications, text-level parsing of discourse rhetorical structure (DRS) has been drawing more and more attention in recent years. However, all the previous studies on text-level discourse parsing adopt bottom-up approaches, which much limit the DRS determination on local information and fail to well benefit from global information of the overall discourse. In this paper, we justify from both computational and perceptive points-of-view that the top-down architecture is more suitable for text-level DRS parsing. On the basis, we propose a top-down neural architecture toward text-level DRS parsing. In particular, we cast discourse parsing as a recursive split point ranking task, where a split point is classified to different levels according to its rank and the elementary discourse units (EDUs) associated with it are arranged accordingly. In this way, we can determine the complete DRS as a hierarchical tree structure via an encoder-decoder with an internal stack. Experimentation on both the English RST-DT corpus and the Chinese CDTB corpus shows the great effectiveness of our proposed top-down approach towards text-level DRS parsing.",,,,ACL
570,2020,"Amalgamation of protein sequence, structure and textual information for improving protein-protein interaction identification","Pratik Dutta,Sriparna Saha","An in-depth exploration of protein-protein interactions (PPI) is essential to understand the metabolism in addition to the regulations of biological entities like proteins, carbohydrates, and many more. Most of the recent PPI tasks in BioNLP domain have been carried out solely using textual data. In this paper, we argue that incorporating multimodal cues can improve the automatic identification of PPI. As a first step towards enabling the development of multimodal approaches for PPI identification, we have developed two multi-modal datasets which are extensions and multi-modal versions of two popular benchmark PPI corpora (BioInfer and HRPD50). Besides, existing textual modalities, two new modalities, 3D protein structure and underlying genomic sequence, are also added to each instance. Further, a novel deep multi-modal architecture is also implemented to efficiently predict the protein interactions from the developed datasets. A detailed experimental analysis reveals the superiority of the multi-modal approach in comparison to the strong baselines including unimodal approaches and state-of the-art methods over both the generated multi-modal datasets. The developed multi-modal datasets are available for use at https://github.com/sduttap16/MM_PPI_NLP.",,,,ACL
571,2020,Bipartite Flat-Graph Network for Nested Named Entity Recognition,"Ying Luo,Hai Zhao","In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for nested named entity recognition (NER), which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located in inner layers. Bidirectional LSTM (BiLSTM) and graph convolutional network (GCN) are adopted to jointly learn flat entities and their inner dependencies. Different from previous models, which only consider the unidirectional delivery of information from innermost layers to outer ones (or outside-to-inside), our model effectively captures the bidirectional interaction between them. We first use the entities recognized by the flat NER module to construct an entity graph, which is fed to the next graph module. The richer representation learned from graph module carries the dependencies of inner entities and can be exploited to improve outermost entity predictions. Experimental results on three standard nested NER datasets demonstrate that our BiFlaG outperforms previous state-of-the-art models.",,,,ACL
572,2020,Connecting Embeddings for Knowledge Graph Entity Typing,"Yu Zhao,Anxiang Zhang,Ruobing Xie,Kang Liu","Knowledge graph (KG) entity typing aims at inferring possible missing entity type instances in KG, which is a very significant but still under-explored subtask of knowledge graph completion. In this paper, we propose a novel approach for KG entity typing which is trained by jointly utilizing local typing knowledge from existing entity type assertions and global triple knowledge in KGs. Specifically, we present two distinct knowledge-driven effective mechanisms of entity type inference. Accordingly, we build two novel embedding models to realize the mechanisms. Afterward, a joint model via connecting them is used to infer missing entity type instances, which favors inferences that agree with both entity type instances and triple knowledge in KGs. Experimental results on two real-world datasets (Freebase and YAGO) demonstrate the effectiveness of our proposed mechanisms and models for improving KG entity typing. The source code and data of this paper can be obtained from: https://github.com/Adam1679/ConnectE .",,,,ACL
573,2020,Continual Relation Learning via Episodic Memory Activation and Reconsolidation,"Xu Han,Yi Dai,Tianyu Gao,Yankai Lin","Continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations. Some pioneering work has proved that storing a handful of historical relation examples in episodic memory and replaying them in subsequent training is an effective solution for such a challenging problem. However, these memory-based methods usually suffer from overfitting the few memorized examples of old relations, which may gradually cause inevitable confusion among existing relations. Inspired by the mechanism in human long-term memory formation, we introduce episodic memory activation and reconsolidation (EMAR) to continual relation learning. Every time neural models are activated to learn both new and memorized data, EMAR utilizes relation prototypes for memory reconsolidation exercise to keep a stable understanding of old relations. The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models.",,,,ACL
574,2020,Handling Rare Entities for Neural Sequence Labeling,"Yangming Li,Han Li,Kaisheng Yao,Xiaolong Li","One great challenge in neural sequence labeling is the data sparsity problem for rare entity words and phrases. Most of test set entities appear only few times and are even unseen in training corpus, yielding large number of out-of-vocabulary (OOV) and low-frequency (LF) entities during evaluation. In this work, we propose approaches to address this problem. For OOV entities, we introduce local context reconstruction to implicitly incorporate contextual information into their representations. For LF entities, we present delexicalized entity identification to explicitly extract their frequency-agnostic and entity-type-specific representations. Extensive experiments on multiple benchmark datasets show that our model has significantly outperformed all previous methods and achieved new start-of-the-art results. Notably, our methods surpass the model fine-tuned on pre-trained language models without external resource.",,,,ACL
575,2020,Instance-Based Learning of Span Representations: A Case Study through Named Entity Recognition,"Hiroki Ouchi,Jun Suzuki,Sosuke Kobayashi,Sho Yokoi","Interpretable rationales for model predictions play a critical role in practical applications. In this study, we develop models possessing interpretable inference process for structured prediction. Specifically, we present a method of instance-based learning that learns similarities between spans. At inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions. Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance.",,,,ACL
576,2020,MIE: A Medical Information Extractor towards Medical Dialogues,"Yuanzhe Zhang,Zhongtao Jiang,Tao Zhang,Shiwan Liu","Electronic Medical Records (EMRs) have become key components of modern medical care systems. Despite the merits of EMRs, many doctors suffer from writing them, which is time-consuming and tedious. We believe that automatically converting medical dialogues to EMRs can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step. To this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation. We then propose a Medical Information Extractor (MIE) towards medical dialogues. MIE is able to extract mentioned symptoms, surgeries, tests, other information and their corresponding status. To tackle the particular challenges of the task, MIE uses a deep matching architecture, taking dialogue turn-interaction into account. The experimental results demonstrate MIE is a promising solution to extract medical information from doctor-patient dialogues.",,,,ACL
577,2020,Named Entity Recognition as Dependency Parsing,"Juntao Yu,Bernd Bohnet,Massimo Poesio","Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points.",,,,ACL
578,2020,Neighborhood Matching Network for Entity Alignment,"Yuting Wu,Xiao Liu,Yansong Feng,Zheng Wang","Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment. This paper presents Neighborhood Matching Network (NMN), a novel entity alignment framework for tackling the structural heterogeneity challenge. NMN estimates the similarities between entities to capture both the topological structure and the neighborhood difference. It provides two innovative components for better learning representations for entity alignment. It first uses a novel graph sampling method to distill a discriminative neighborhood for each entity. It then adopts a cross-graph neighborhood matching module to jointly encode the neighborhood difference for a given entity pair. Such strategies allow NMN to effectively construct matching-oriented entity representations while ignoring noisy neighbors that have a negative impact on the alignment task. Extensive experiments performed on three entity alignment datasets show that NMN can well estimate the neighborhood similarity in more tough cases and significantly outperforms 12 previous state-of-the-art methods.",,,,ACL
579,2020,Relation Extraction with Explanation,"Hamed Shahbazi,Xiaoli Fern,Reza Ghaeini,Prasad Tadepalli",Recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences. Efforts thus far have focused on improving extraction accuracy but little is known about their explanability. In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models. We demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types not only enhances extraction accuracy but also improves explanation. We also propose to automatically generate “distractor” sentences to augment the bags and train the model to ignore the distractors. Evaluations on the widely used FB-NYT dataset show that our methods achieve new state-of-the-art accuracy while improving model explanability.,,,,ACL
580,2020,Representation Learning for Information Extraction from Form-like Documents,"Bodhisattwa Prasad Majumder,Navneet Potti,Sandeep Tata,James Bradley Wendt","We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases.",,,,ACL
581,2020,Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language,"Qianhui Wu,Zijia Lin,Börje Karlsson,Jian-Guang Lou","To better tackle the named entity recognition (NER) problem on languages with little/no labeled data, cross-lingual NER must effectively leverage knowledge learned from source languages with rich labeled data. Previous works on cross-lingual NER are mostly based on label projection with pairwise texts or direct model transfer. However, such methods either are not applicable if the labeled data in the source languages is unavailable, or do not leverage information contained in unlabeled data in the target language. In this paper, we propose a teacher-student learning method to address such limitations, where NER models in the source languages are used as teachers to train a student model on unlabeled data in the target language. The proposed method works for both single-source and multi-source cross-lingual NER. For the latter, we further propose a similarity measuring method to better weight the supervision from different teacher models. Extensive experiments for 3 target languages on benchmark datasets well demonstrate that our method outperforms existing state-of-the-art methods for both single-source and multi-source cross-lingual NER.",,,,ACL
582,2020,Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction,"Shaowei Chen,Jie Liu,Yu Wang,Wenzheng Zhang","Opinion entity extraction is a fundamental task in fine-grained opinion mining. Related studies generally extract aspects and/or opinion expressions without recognizing the relations between them. However, the relations are crucial for downstream tasks, including sentiment classification, opinion summarization, etc. In this paper, we explore Aspect-Opinion Pair Extraction (AOPE) task, which aims at extracting aspects and opinion expressions in pairs. To deal with this task, we propose Synchronous Double-channel Recurrent Network (SDRN) mainly consisting of an opinion entity extraction unit, a relation detection unit, and a synchronization unit. The opinion entity extraction unit and the relation detection unit are developed as two channels to extract opinion entities and relations simultaneously. Furthermore, within the synchronization unit, we design Entity Synchronization Mechanism (ESM) and Relation Synchronization Mechanism (RSM) to enhance the mutual benefit on the above two channels. To verify the performance of SDRN, we manually build three datasets based on SemEval 2014 and 2015 benchmarks. Extensive experiments demonstrate that SDRN achieves state-of-the-art performances.",,,,ACL
583,2020,Cross-modal Coherence Modeling for Caption Generation,"Malihe Alikhani,Piyush Sharma,Shengjie Li,Radu Soricut","We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning. Using an annotation protocol specifically devised for capturing image–caption coherence relations, we annotate 10,000 instances from publicly-available image–caption pairs. We introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also train coherence-aware, controllable image captioning models. The results show a dramatic improvement in the consistency and quality of the generated captions with respect to information needs specified via coherence relations.",,,,ACL
584,2020,Knowledge Supports Visual Language Grounding: A Case Study on Colour Terms,"Simeon Schüz,Sina Zarrieß","In human cognition, world knowledge supports the perception of object colours: knowing that trees are typically green helps to perceive their colour in certain contexts. We go beyond previous studies on colour terms using isolated colour swatches and study visual grounding of colour terms in realistic objects. Our models integrate processing of visual information and object-specific knowledge via hard-coded (late) or learned (early) fusion. We find that both models consistently outperform a bottom-up baseline that predicts colour terms solely from visual inputs, but show interesting differences when predicting atypical colours of so-called colour diagnostic objects. Our models also achieve promising results when tested on new object categories not seen during training.",,,,ACL
585,2020,Span-based Localizing Network for Natural Language Video Localization,"Hao Zhang,Aixin Sun,Wei Jing,Joey Tianyi Zhou","Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query. Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span. In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage. We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL. The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple and yet effective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to search for matching video span within a highlighted region. Through extensive experiments on three benchmark datasets, we show that the proposed VSLNet outperforms the state-of-the-art methods; and adopting span-based QA framework is a promising direction to solve NLVL.",,,,ACL
586,2020,"Words Aren’t Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions","Arjun Akula,Spandana Gella,Yaser Al-Onaizan,Song-Chun Zhu","Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn’t matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn’t. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv.",,,,ACL
587,2020,A Mixture of h - 1 Heads is Better than h Heads,"Hao Peng,Roy Schwartz,Dianqi Li,Noah A. Smith","Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss. In this work, we instead “reallocate” them—the model learns to activate different heads on different inputs. Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (MAE). MAE is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters. Experiments on machine translation and language modeling show that MAE outperforms strong baselines on both tasks. Particularly, on the WMT14 English to German translation dataset, MAE improves over “transformer-base” by 0.8 BLEU, with a comparable number of parameters. Our analysis shows that our model learns to specialize different experts to different inputs.",,,,ACL
588,2020,Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification,"Hao Tang,Donghong Ji,Chenliang Li,Qiji Zhou","Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect. One sentence may contain various sentiments for different aspects. Many sophisticated methods such as attention mechanism and Convolutional Neural Networks (CNN) have been widely employed for handling this challenge. Recently, semantic dependency tree implemented by Graph Convolutional Networks (GCN) is introduced to describe the inner connection between aspects and the associated emotion words. But the improvement is limited due to the noise and instability of dependency trees. To this end, we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly considering the flat representations learnt from Transformer and graph-based representations learnt from the corresponding dependency graph in an iterative interaction manner. Specifically, a dual-transformer structure is devised in DGEDT to support mutual reinforcement between the flat representation learning and graph-based representation learning. The idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa. The results on five datasets demonstrate that the proposed DGEDT outperforms all state-of-the-art alternatives with a large margin.",,,,ACL
589,2020,Differentiable Window for Dynamic Local Attention,"Thanh-Tung Nguyen,Xuan-Phi Nguyen,Shafiq Joty,Xiaoli Li","We propose Differentiable Window, a new neural module and general purpose component for dynamic window selection. While universally applicable, we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modules by enabling more focused attentions over the input regions. We propose two variants of Differentiable Window, and integrate them within the Transformer architecture in two novel ways. We evaluate our proposed approach on a myriad of NLP tasks, including machine translation, sentiment analysis, subject-verb agreement and language modeling. Our experimental results demonstrate consistent and sizable improvements across all tasks.",,,,ACL
590,2020,Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with Adversarial Examples,"Xiaoqing Zheng,Jiehang Zeng,Yi Zhou,Cho-Jui Hsieh","Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples. Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension. In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings. Our experiments with one of state-of-the-art parsers on the English Penn Treebank (PTB) show that up to 77% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data.",,,,ACL
591,2020,Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach,"Wenyu Du,Zhouhan Lin,Yikang Shen,Timothy J. O’Donnell","It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called “syntactic distances”, where information between these two separate objectives shares the same intermediate representation. Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.",,,,ACL
592,2020,Learning Architectures from an Extended Search Space for Language Modeling,"Yinqiao Li,Chi Hu,Yuhao Zhang,Nuo Xu","Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS). For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously. We implement our model in a differentiable architecture search system. For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB. Moreover, the learned architectures show good transferability to other systems. E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.",,,,ACL
593,2020,The Right Tool for the Job: Matching Model and Instance Complexities,"Roy Schwartz,Gabriel Stanovsky,Swabha Swayamdipta,Jesse Dodge","As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) “exit” from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.",,,,ACL
594,2020,Bootstrapping Techniques for Polysynthetic Morphological Analysis,"William Lane,Steven Bird","Polysynthetic languages have exceptionally large and sparse vocabularies, thanks to the number of morpheme slots and combinations in a word. This complexity, together with a general scarcity of written data, poses a challenge to the development of natural language technologies. To address this challenge, we offer linguistically-informed approaches for bootstrapping a neural morphological analyzer, and demonstrate its application to Kunwinjku, a polysynthetic Australian language. We generate data from a finite state transducer to train an encoder-decoder model. We improve the model by “hallucinating” missing linguistic structure into the training data, and by resampling from a Zipf distribution to simulate a more natural distribution of morphemes. The best model accounts for all instances of reduplication in the test set and achieves an accuracy of 94.7% overall, a 10 percentage point improvement over the FST baseline. This process demonstrates the feasibility of bootstrapping a neural morph analyzer from minimal resources.",,,,ACL
595,2020,Coupling Distant Annotation and Adversarial Training for Cross-Domain Chinese Word Segmentation,"Ning Ding,Dingkun Long,Guangwei Xu,Muhua Zhu","Fully supervised neural approaches have achieved significant progress in the task of Chinese word segmentation (CWS). Nevertheless, the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary (OOV) problem. In order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial training for cross-domain CWS. 1) We rethink the essence of “Chinese words” and design an automatic distant annotation mechanism, which does not need any supervision or pre-defined dictionaries on the target domain. The method could effectively explore domain-specific words and distantly annotate the raw texts for the target domain. 2) We further develop a sentence-level adversarial training procedure to perform noise reduction and maximum utilization of the source domain information. Experiments on multiple real-world datasets across various domains show the superiority and robustness of our model, significantly outperforming previous state-of-the-arts cross-domain CWS methods.",,,,ACL
596,2020,Modeling Morphological Typology for Unsupervised Learning of Language Morphology,"Hongzhi Xu,Jordan Kodner,Mitchell Marcus,Charles Yang","This paper describes a language-independent model for fully unsupervised morphological analysis that exploits a universal framework leveraging morphological typology. By modeling morphological processes including suffixation, prefixation, infixation, and full and partial reduplication with constrained stem change rules, our system effectively constrains the search space and offers a wide coverage in terms of morphological typology. The system is tested on nine typologically and genetically diverse languages, and shows superior performance over leading systems. We also investigate the effect of an oracle that provides only a handful of bits per language to signal morphological type.",,,,ACL
597,2020,Predicting Declension Class from Form and Meaning,"Adina Williams,Tiago Pimentel,Hagen Blix,Arya D. McCarthy","The noun lexica of many natural languages are divided into several declension classes with characteristic morphological properties. Class membership is far from deterministic, but the phonological form of a noun and/or its meaning can often provide imperfect clues. Here, we investigate the strength of those clues. More specifically, we operationalize this by measuring how much information, in bits, we can glean about declension class from knowing the form and/or meaning of nouns. We know that form and meaning are often also indicative of grammatical gender—which, as we quantitatively verify, can itself share information with declension class—so we also control for gender. We find for two Indo-European languages (Czech and German) that form and meaning respectively share significant amounts of information with class (and contribute additional information above and beyond gender). The three-way interaction between class, form, and meaning (given gender) is also significant. Our study is important for two reasons: First, we introduce a new method that provides additional quantitative support for a classic linguistic finding that form and meaning are relevant for the classification of nouns into declensions. Secondly, we show not only that individual declensions classes vary in the strength of their clues within a language, but also that these variations themselves vary across languages.",,,,ACL
598,2020,Unsupervised Morphological Paradigm Completion,"Huiming Jin,Liwei Cai,Yihui Peng,Chen Xia","We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire morphological knowledge. We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. We perform an evaluation on 14 typologically diverse languages. Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems.",,,,ACL
599,2020,Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension,"Bo Zheng,Haoyang Wen,Yaobo Liang,Nan Duan","Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer). Despite the effectiveness of existing methods on this benchmark, they treat these two sub-tasks individually during training while ignoring their dependencies. To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity: documents, paragraphs, sentences, and tokens. We utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously. The long and short answers can be extracted from paragraph-level representation and token-level representation, respectively. In this way, we can model the dependencies between the two-grained answers to provide evidence for each other. We jointly train the two sub-tasks, and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria.",,,,ACL
600,2020,Harvesting and Refining Question-Answer Pairs for Unsupervised QA,"Zhongli Li,Wenhui Wang,Li Dong,Furu Wei","Question Answering (QA) has shown great success thanks to the availability of large-scale datasets and the effectiveness of neural models. Recent research works have attempted to extend these successes to the settings with few or no labeled data available. In this work, we introduce two approaches to improve unsupervised QA. First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as RefQA). Second, we take advantage of the QA model to extract more appropriate answers, which iteratively refines data over RefQA. We conduct experiments on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data. Our approach outperforms previous unsupervised approaches by a large margin, and is competitive with early supervised models. We also show the effectiveness of our approach in the few-shot learning setting.",,,,ACL
601,2020,Low-Resource Generation of Multi-hop Reasoning Questions,"Jianxing Yu,Wei Liu,Shuang Qiu,Qinliang Su","This paper focuses on generating multi-hop reasoning questions from the raw text in a low resource circumstance. Such questions have to be syntactically valid and need to logically correlate with the answers by deducing over multiple relations on several sentences in the text. Specifically, we first build a multi-hop generation model and guide it to satisfy the logical rationality by the reasoning chain extracted from a given text. Since the labeled data is limited and insufficient for training, we propose to learn the model with the help of a large scale of unlabeled data that is much easier to obtain. Such data contains rich expressive forms of the questions with structural patterns on syntax and semantics. These patterns can be estimated by the neural hidden semi-Markov model using latent variables. With latent patterns as a prior, we can regularize the generation model and produce the optimal results. Experimental results on the HotpotQA data set demonstrate the effectiveness of our model. Moreover, we apply the generated results to the task of machine reading comprehension and achieve significant performance improvements.",,,,ACL
602,2020,R4C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason,"Naoya Inoue,Pontus Stenetorp,Kentaro Inui","Recent studies have revealed that reading comprehension (RC) systems learn to exploit annotation artifacts and other biases in current datasets. This prevents the community from reliably measuring the progress of RC systems. To address this issue, we introduce R4C, a new task for evaluating RC systems’ internal reasoning. R4C requires giving not only answers but also derivations: explanations that justify predicted answers. We present a reliable, crowdsourced framework for scalably annotating RC datasets with derivations. We create and publicly release the R4C dataset, the first, quality-assured dataset consisting of 4.6k questions, each of which is annotated with 3 reference derivations (i.e. 13.8k derivations). Experiments show that our automatic evaluation metrics using multiple reference derivations are reliable, and that R4C assesses different skills from an existing benchmark.",,,,ACL
603,2020,Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension,"Hongyu Gong,Yelong Shen,Dian Yu,Jianshu Chen","In this paper, we study machine reading comprehension (MRC) on long texts: where a model takes as inputs a lengthy document and a query, extracts a text span from the document as an answer. State-of-the-art models (e.g., BERT) tend to use a stack of transformer layers that are pre-trained from a large number of unlabeled language corpora to encode the joint contextual information of query and document. However, these transformer models can only take as input a fixed-length (e.g., 512) text. To deal with even longer text inputs, previous approaches usually chunk them into equally-spaced segments and predict answers based on each segment independently without considering the information from other segments. As a result, they may form segments that fail to cover complete answers or retain insufficient contexts around the correct answer required for question answering. Moreover, they are less capable of answering questions that need cross-segment information. We propose to let a model learn to chunk in a more flexible way via reinforcement learning: a model can decide the next segment that it wants to process in either direction. We also apply recurrent mechanisms to enable information to flow across segments. Experiments on three MRC tasks – CoQA, QuAC, and TriviaQA – demonstrate the effectiveness of our proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions.",,,,ACL
604,2020,RikiNet: Reading Wikipedia Pages for Natural Question Answering,"Dayiheng Liu,Yeyun Gong,Jie Fu,Yu Yan","Reading long documents to answer open-domain questions remains challenging in natural language understanding. In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question answering. RikiNet contains a dynamic paragraph dual-attention reader and a multi-level cascaded answer predictor. The reader dynamically represents the document and question by utilizing a set of complementary attention mechanisms. The representations are then fed into the predictor to obtain the span of the short answer, the paragraph of the long answer, and the answer type in a cascaded manner. On the Natural Questions (NQ) dataset, a single RikiNet achieves 74.3 F1 and 57.9 F1 on long-answer and short-answer tasks. To our best knowledge, it is the first single model that outperforms the single human performance. Furthermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer tasks, achieving the best performance on the official NQ leaderboard.",,,,ACL
605,2020,Parsing into Variable-in-situ Logico-Semantic Graphs,"Yufei Chen,Weiwei Sun","We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing. The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification, negation and modality, in a way that is consistent with the state-of-the-art underspecification approach. Moreover, the well-formedness of such a graph is clear, since model-theoretic interpretation is available. We demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for English Resource Semantics. At the core of this parser is a novel neural graph rewriting system which combines the strengths of Hyperedge Replacement Grammar, a knowledge-intensive model, and Graph Neural Networks, a data-intensive model. Our parser achieves an accuracy of 92.39% in terms of elementary dependency match, which is a 2.88 point improvement over the best data-driven model in the literature. The output of our parser is highly coherent: at least 91% graphs are valid, in that they allow at least one sound scope-resolved logical form.",,,,ACL
606,2020,Semantic Parsing for English as a Second Language,"Yuanyuan Zhao,Weiwei Sun,Junjie Cao,Xiaojun Wan","This paper is concerned with semantic parsing for English as a second language (ESL). Motivated by the theoretical emphasis on the learning challenges that occur at the syntax-semantics interface during second language acquisition, we formulate the task based on the divergence between literal and intended meanings. We combine the complementary strengths of English Resource Grammar, a linguistically-precise hand-crafted deep grammar, and TLE, an existing manually annotated ESL UD-TreeBank with a novel reranking model. Experiments demonstrate that in comparison to human annotations, our method can obtain a very promising SemBanking quality. By means of the newly created corpus, we evaluate state-of-the-art semantic parsing as well as grammatical error correction models. The evaluation profiles the performance of neural NLP techniques for handling ESL data and suggests some research directions.",,,,ACL
607,2020,Semi-Supervised Semantic Dependency Parsing Using CRF Autoencoders,"Zixia Jia,Youmi Ma,Jiong Cai,Kewei Tu","Semantic dependency parsing, which aims to find rich bi-lexical relationships, allows words to have multiple dependency heads, resulting in graph-structured representations. We propose an approach to semi-supervised learning of semantic dependency parsers based on the CRF autoencoder framework. Our encoder is a discriminative neural semantic dependency parser that predicts the latent parse graph of the input sentence. Our decoder is a generative neural model that reconstructs the input sentence conditioned on the latent parse graph. Our model is arc-factored and therefore parsing and learning are both tractable. Experiments show our model achieves significant and consistent improvement over the supervised baseline.",,,,ACL
608,2020,Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing,"Ruisheng Cao,Su Zhu,Chenyu Yang,Chen Liu","One daunting problem for semantic parsing is the scarcity of annotation. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. The downstream naive semantic parser accepts the intermediate output and returns the target logical form. Furthermore, the entire training process is split into two phases: pre-training and cycle learning. Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.",,,,ACL
609,2020,DRTS Parsing with Structure-Aware Encoding and Decoding,"Qiankun Fu,Yue Zhang,Jiangming Liu,Meishan Zhang","Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently. State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem. Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the DRTS parsing. In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (GAT) is exploited for effectively modeling. Experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature.",,,,ACL
610,2020,A Two-Stage Masked LM Method for Term Set Expansion,"Guy Kushilevitz,Shaul Markovitch,Yoav Goldberg","We tackle the task of Term Set Expansion (TSE): given a small seed set of example terms from a semantic class, finding more members of that class. The task is of great practical utility, and also of theoretical utility as it requires generalization from few examples. Previous approaches to the TSE task can be characterized as either distributional or pattern-based. We harness the power of neural masked language models (MLM) and propose a novel TSE algorithm, which combines the pattern-based and distributional approaches. Due to the small size of the seed set, fine-tuning methods are not effective, calling for more creative use of the MLM. The gist of the idea is to use the MLM to first mine for informative patterns with respect to the seed set, and then to obtain more members of the seed class by generalizing these patterns. Our method outperforms state-of-the-art TSE algorithms. Implementation is available at: https://github.com/ guykush/TermSetExpansion-MPB/",,,,ACL
611,2020,FLAT: Chinese NER Using Flat-Lattice Transformer,"Xiaonan Li,Hang Yan,Xipeng Qiu,Xuanjing Huang","Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information. However, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference speed. In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans. Each span corresponds to a character or latent word and its position in the original lattice. With the power of Transformer and well-designed position encoding, FLAT can fully leverage the lattice information and has an excellent parallel ability. Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency.",,,,ACL
612,2020,Improving Entity Linking through Semantic Reinforced Entity Embeddings,"Feng Hou,Ruili Wang,Jun He,Yi Zhou","Entity embeddings, which represent different aspects of each entity with a single vector like word embeddings, are a key component of neural entity linking models. Existing entity embeddings are learned from canonical Wikipedia articles and local contexts surrounding target entities. Such entity embeddings are effective, but too distinctive for linking models to learn contextual commonality. We propose a simple yet effective method, FGS2EE, to inject fine-grained semantic information into entity embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality. FGS2EE first uses the embeddings of semantic type words to generate semantic embeddings, and then combines them with existing entity embeddings through linear aggregation. Extensive experiments show the effectiveness of such embeddings. Based on our entity embeddings, we achieved new sate-of-the-art performance on entity linking.",,,,ACL
613,2020,Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain,"Shadi Saleh,Pavel Pecina","We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013–2015 containing English documents and queries in several European languages. We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach). The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages. NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages, but still, QT provides generally better retrieval results than DT.",,,,ACL
614,2020,Learning Robust Models for e-Commerce Product Search,"Thanh Nguyen,Nikhil Rao,Karthik Subbian","Showing items that do not match search query intent degrades customer experience in e-commerce. These mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs. Mitigating the problem requires a large labeled dataset, which is expensive and time-consuming to obtain. In this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier. We train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples. This not only makes the classifier more robust but also boosts the overall ranking performance. Our model achieves a relative gain compared to baselines by over 26% in F-score, and over 17% in Area Under PR curve. On live search traffic, our model gains significant improvement in multiple countries.",,,,ACL
615,2020,Generalized Entropy Regularization or: There’s Nothing Special about Label Smoothing,"Clara Meister,Elizabeth Salesky,Ryan Cotterell","Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting. This class of techniques, of which label smoothing is one, has a connection to entropy regularization. Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored. We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks. We also find that variance in model performance can be explained largely by the resulting entropy of the model. Lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place.",,,,ACL
616,2020,Highway Transformer: Self-Gating Enhanced Self-Attentive Networks,"Yekun Chai,Shuo Jin,Xinwen Hou","Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.",,,,ACL
617,2020,Low-Dimensional Hyperbolic Knowledge Graph Embeddings,"Ines Chami,Adva Wolf,Da-Cheng Juan,Frederic Sala","Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns. Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that different geometric transformations capture different types of relations while attention- based transformations generalize to multiple relations. In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10.",,,,ACL
618,2020,Classification-Based Self-Learning for Weakly Supervised Bilingual Lexicon Induction,"Mladen Karan,Ivan Vulić,Anna Korhonen,Goran Glavaš","Effective projection-based cross-lingual word embedding (CLWE) induction critically relies on the iterative self-learning procedure. It gradually expands the initial small seed dictionary to learn improved cross-lingual mappings. In this work, we present ClassyMap, a classification-based approach to self-learning, yielding a more robust and a more effective induction of projection-based CLWEs. Unlike prior self-learning methods, our approach allows for integration of diverse features into the iterative process. We show the benefits of ClassyMap for bilingual lexicon induction: we report consistent improvements in a weakly supervised setup (500 seed translation pairs) on a benchmark with 28 language pairs.",,,,ACL
619,2020,Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus,"Luisa Bentivogli,Beatrice Savoldi,Matteo Negri,Mattia A. Di Gangi","Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines. This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included. Exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. But what happens with speech translation, where the input is an audio signal? Can audio provide additional information to reduce gender bias? We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French).",,,,ACL
620,2020,Uncertainty-Aware Curriculum Learning for Neural Machine Translation,"Yikai Zhou,Baosong Yang,Derek F. Wong,Yu Wan","Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.",,,,ACL
621,2020,Closing the Gap: Joint De-Identification and Concept Extraction in the Clinical Domain,"Lukas Lange,Heike Adel,Jannik Strötgen","Exploiting natural language processing in the clinical domain requires de-identification, i.e., anonymization of personal information in texts. However, current research considers de-identification and downstream tasks, such as concept extraction, only in isolation and does not study the effects of de-identification on other tasks. In this paper, we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identification and concept extraction. In particular, we propose a stacked model with restricted access to privacy sensitive information and a multitask model. We set the new state of the art on benchmark datasets in English (96.1% F1 for de-identification and 88.9% F1 for concept extraction) and Spanish (91.4% F1 for concept extraction).",,,,ACL
622,2020,CorefQA: Coreference Resolution as Query-based Span Prediction,"Wei Wu,Fei Wang,Arianna Yuan,Fei Wu","In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task, like in question answering: A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the question answering framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing question answering datasets can be used for data augmentation to improve the model’s generalization capability. Experiments demonstrate significant performance boost over previous models, with 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark and 87.5 (+2.5) F1 score on the GAP benchmark.",,,,ACL
623,2020,Estimating predictive uncertainty for rumour verification models,"Elena Kochkina,Maria Liakata","The inability to correctly resolve rumours circulating online can have harmful real-world consequences. We present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verification. We show that these estimates can be used to filter out model predictions likely to be erroneous so that these difficult instances can be prioritised by a human fact-checker. We propose two methods for uncertainty-based instance rejection, supervised and unsupervised. We also show how uncertainty estimates can be used to interpret model performance as a rumour unfolds.",,,,ACL
624,2020,From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains,"Jan-Christoph Klie,Richard Eckart de Castilho,Iryna Gurevych","Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data. However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35% compared to annotating without interactive support; users report that they strongly prefer our system. An open-source and ready-to-use implementation based on the text annotation platform INCEpTION (https://inception-project.github.io) is made available.",,,,ACL
625,2020,Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions,"Tian Jin,Zhun Liu,Shengjia Yan,Alexandre Eichenberger","Transfer learning using ImageNet pre-trained models has been the de facto approach in a wide range of computer vision tasks. However, fine-tuning still requires task-specific training data. In this paper, we propose N3 (Neural Networks from Natural Language) - a new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model. N3 leverages language descriptions to generate parameter adaptations as well as a new task-specific classification layer for a pre-trained neural network, effectively “fine-tuning” the network for a new task using only language descriptions as input. To the best of our knowledge, N3 is the first method to synthesize entire neural networks from natural language. Experimental results show that N3 can out-perform previous natural-language based zero-shot learning methods across 4 different zero-shot image classification benchmarks. We also demonstrate a simple method to help identify keywords in language descriptions leveraged by N3 when synthesizing model parameters.",,,,ACL
626,2020,Controlled Crowdsourcing for High-Quality QA-SRL Annotation,"Paul Roit,Ayal Klein,Daniela Stepanov,Jonathan Mamou","Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.",,,,ACL
627,2020,Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus,"Hao Fei,Meishan Zhang,Donghong Ji","Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English. While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances. Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection. In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations. Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly.",,,,ACL
628,2020,Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity,"Nina Poerner,Ulli Waltinger,Hinrich Schütze","We address the task of unsupervised Semantic Textual Similarity (STS) by ensembling diverse pre-trained sentence encoders into sentence meta-embeddings. We apply, extend and evaluate different meta-embedding methods from the word embedding literature at the sentence level, including dimensionality reduction (Yin and Schütze, 2016), generalized Canonical Correlation Analysis (Rastogi et al., 2015) and cross-view auto-encoders (Bollegala and Bao, 2018). Our sentence meta-embeddings set a new unsupervised State of The Art (SoTA) on the STS Benchmark and on the STS12-STS16 datasets, with gains of between 3.7% and 6.4% Pearson’s r over single-source systems.",,,,ACL
629,2020,Transition-based Semantic Dependency Parsing with Pointer Networks,"Daniel Fernández-González,Carlos Gómez-Rodríguez","Transition-based parsers implemented with Pointer Networks have become the new state of the art in dependency parsing, excelling in producing labelled syntactic trees and outperforming graph-based models in this task. In order to further test the capabilities of these powerful neural networks on a harder NLP problem, we propose a transition system that, thanks to Pointer Networks, can straightforwardly produce labelled directed acyclic graphs and perform semantic dependency parsing. In addition, we enhance our approach with deep contextualized word embeddings extracted from BERT. The resulting system not only outperforms all existing transition-based models, but also matches the best fully-supervised accuracy to date on the SemEval 2015 Task 18 datasets among previous state-of-the-art graph-based parsers.",,,,ACL
630,2020,tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection,"Nicole Peinelt,Dong Nguyen,Maria Liakata",Semantic similarity detection is a fundamental task in natural language understanding. Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. There is currently no standard way of combining topics with pretrained contextual representations such as BERT. We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets. We find that the addition of topics to BERT helps particularly with resolving domain-specific cases.,,,,ACL
631,2020,Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation,"Kun Li,Chengbo Chen,Xiaojun Quan,Qing Ling","Aspect term extraction aims to extract aspect terms from review texts as opinion targets for sentiment analysis. One of the big challenges with this task is the lack of sufficient annotated data. While data augmentation is potentially an effective technique to address the above issue, it is uncontrollable as it may change aspect words and aspect labels unexpectedly. In this paper, we formulate the data augmentation as a conditional generation task: generating a new sentence while preserving the original opinion targets and labels. We propose a masked sequence-to-sequence method for conditional augmentation of aspect term extraction. Unlike existing augmentation approaches, ours is controllable and allows to generate more diversified sentences. Experimental results confirm that our method alleviates the data scarcity problem significantly. It also effectively boosts the performances of several current models for aspect term extraction.",,,,ACL
632,2020,Exploiting Personal Characteristics of Debaters for Predicting Persuasiveness,"Khalid Al Khatib,Michael Völske,Shahbaz Syed,Nikolay Kolyada","Predicting the persuasiveness of arguments has applications as diverse as writing assistance, essay scoring, and advertising. While clearly relevant to the task, the personal characteristics of an argument’s source and audience have not yet been fully exploited toward automated persuasiveness prediction. In this paper, we model debaters’ prior beliefs, interests, and personality traits based on their previous activity, without dependence on explicit user profiles or questionnaires. Using a dataset of over 60,000 argumentative discussions, comprising more than three million individual posts collected from the subreddit r/ChangeMyView, we demonstrate that our modeling of debater’s characteristics enhances the prediction of argument persuasiveness as well as of debaters’ resistance to persuasion.",,,,ACL
633,2020,Out of the Echo Chamber: Detecting Countering Debate Speeches,"Matan Orbach,Yonatan Bilu,Assaf Toledo,Dan Lahav","An educated and informed consumption of media content has become a challenge in modern times. With the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in “echo chambers” and may fall prey to fake news and disinformation, lacking easy access to dissenting views. We suggest a novel task aiming to alleviate some of these concerns – that of detecting articles that most effectively counter the arguments – and not just the stance – made in a given text. We study this problem in the context of debate speeches. Given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it. We provide a large dataset of 3,685 such speeches (in English), annotated for this relation, which hopefully would be of general interest to the NLP community. We explore several algorithms addressing this task, and while some are successful, all fall short of expert human performance, suggesting room for further research. All data collected during this work is freely available for research.",,,,ACL
634,2020,Diversifying Dialogue Generation with Non-Conversational Text,"Hui Su,Xiaoyu Shen,Sanqiang Zhao,Zhou Xiao","Neural network-based sequence-to-sequence (seq2seq) models strongly suffer from the low-diversity problem when it comes to open-domain dialogue generation. As bland and generic utterances usually dominate the frequency distribution in our daily chitchat, avoiding them to generate more interesting responses requires complex data filtering, sampling techniques or modifying the training objective. In this paper, we propose a new perspective to diversify dialogue generation by leveraging non-conversational text. Compared with bilateral conversations, non-conversational text are easier to obtain, more diverse and cover a much broader range of topics. We collect a large-scale non-conversational corpus from multi sources including forum comments, idioms and book snippets. We further present a training paradigm to effectively incorporate these text via iterative back translation. The resulting model is tested on two conversational datasets from different domains and is shown to produce significantly more diverse responses without sacrificing the relevance with context.",,,,ACL
635,2020,KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation,"Hao Zhou,Chujie Zheng,Kaili Huang,Minlie Huang","The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs. Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics. To facilitate the following research on this corpus, we provide several benchmark models. Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available.",,,,ACL
636,2020,Meta-Reinforced Multi-Domain State Generator for Dialogue Systems,"Yi Huang,Junlan Feng,Min Hu,Xiaoting Wu","A Dialogue State Tracker (DST) is a core component of a modular task-oriented dialogue system. Tremendous progress has been made in recent years. However, the major challenges remain. The state-of-the-art accuracy for DST is below 50% for a multi-domain dialogue task. A learnable DST for any new domain requires a large amount of labeled in-domain data and training from scratch. In this paper, we propose a Meta-Reinforced Multi-Domain State Generator (MERET). Our first contribution is to improve the DST accuracy. We enhance a neural model based DST generator with a reward manager, which is built on policy gradient reinforcement learning (RL) to fine-tune the generator. With this change, we are able to improve the joint accuracy of DST from 48.79% to 50.91% on the MultiWOZ corpus. Second, we explore to train a DST meta-learning model with a few domains as source domains and a new domain as target domain. We apply the model-agnostic meta-learning algorithm (MAML) to DST and the obtained meta-learning model is used for new domain adaptation. Our experimental results show this solution is able to outperform the traditional training approach with extremely less training data in target domain.",,,,ACL
637,2020,Modeling Long Context for Task-Oriented Dialogue State Generation,"Jun Quan,Deyi Xiong","Based on the recently proposed transferable dialogue state generator (TRADE) that predicts dialogue states from utterance-concatenated dialogue context, we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation. By enabling the model to learn a better representation of the long dialogue context, our approaches attempt to solve the problem that the performance of the baseline significantly drops when the input dialogue context sequence is long. In our experiments, our proposed model achieves a 7.03% relative improvement over the baseline, establishing a new state-of-the-art joint goal accuracy of 52.04% on the MultiWOZ 2.0 dataset.",,,,ACL
638,2020,Multi-Domain Dialogue Acts and Response Co-Generation,"Kai Wang,Junfeng Tian,Rui Wang,Xiaojun Quan","Generating fluent and informative responses is of critical importance for task-oriented dialogue systems. Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation. There are at least two shortcomings with such approaches. First, the inherent structures of multi-domain dialogue acts are neglected. Second, the semantic associations between acts and responses are not taken into account for response generation. To address these issues, we propose a neural co-generation model that generates dialogue acts and responses concurrently. Unlike those pipeline approaches, our act generation module preserves the semantic structures of multi-domain dialogue acts and our response generation module dynamically attends to different acts as needed. We train the two modules jointly using an uncertainty loss to adjust their task weights adaptively. Extensive experiments are conducted on the large-scale MultiWOZ dataset and the results show that our model achieves very favorable improvement over several state-of-the-art models in both automatic and human evaluations.",,,,ACL
639,2020,Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer,"Chulun Zhou,Liangyu Chen,Jiachen Liu,Xinyan Xiao","Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data. In current dominant approaches, owing to the lack of fine-grained control on the influence from the target style, they are unable to yield desirable output sentences. In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer. Specifically, we first pretrain a style classifier, where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation. In a denoising auto-encoding manner, we train an attentional Seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously. In this way, this model is endowed with the ability to automatically predict the style relevance of each output word. Then, we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer. Particularly, we fine-tune this model using a carefully-designed objective function involving style transfer, style relevance consistency, content preservation and fluency modeling loss terms. Experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation.",,,,ACL
640,2020,Heterogeneous Graph Transformer for Graph-to-Sequence Learning,"Shaowei Yao,Tianming Wang,Xiaojun Wan","The graph-to-sequence (Graph2Seq) learning aims to transduce graph-structured representations to word sequences for text generation. Recent studies propose various models to encode graph structure. However, most previous works ignore the indirect relations between distance nodes, or treat indirect relations and direct relations in the same way. In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between nodes. Experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of AMR-to-text generation and syntax-based neural machine translation.",,,,ACL
641,2020,Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence,"Xiaoyu Shen,Ernie Chang,Hui Su,Cheng Niu","The neural attention model has achieved great success in data-to-text generation tasks. Though usually excelling at producing fluent text, it suffers from the problem of information missing, repetition and “hallucination”. Due to the black-box nature of the neural attention architecture, avoiding these problems in a systematic way is non-trivial. To address this concern, we propose to explicitly segment target text into fragment units and align them with their data correspondences. The segmentation and correspondence are jointly learned as latent variables without any human annotations. We further impose a soft statistical constraint to regularize the segmental granularity. The resulting architecture maintains the same expressive power as neural attention models, while being able to generate fully interpretable outputs with several times less computational cost. On both E2E and WebNLG benchmarks, we show the proposed model consistently outperforms its neural attention counterparts.",,,,ACL
642,2020,Aligned Dual Channel Graph Convolutional Network for Visual Question Answering,"Qingbao Huang,Jielong Wei,Yi Cai,Changmeng Zheng","Visual question answering aims to answer the natural language question about a given image. Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question. To simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question, we propose a novel dual channel graph convolutional network (DC-GCN) for better combining visual and textual advantages. The DC-GCN model consists of three parts: an I-GCN module to capture the relations between objects in an image, a Q-GCN module to capture the syntactic dependency relations between words in a question, and an attention alignment module to align image representations and question representations. Experimental results show that our model achieves comparable performance with the state-of-the-art approaches.",,,,ACL
643,2020,Multimodal Neural Graph Memory Networks for Visual Question Answering,Mahmoud Khademi,"We introduce a new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for visual question answering. The MN-GMN uses graph structure with different region features as node attributes and applies a recently proposed powerful graph neural network model, Graph Network (GN), to reason about objects and their interactions in an image. The input module of the MN-GMN generates a set of visual features plus a set of encoded region-grounded captions (RGCs) for the image. The RGCs capture object attributes and their relationships. Two GNs are constructed from the input module using the visual features and encoded RGCs. Each node of the GNs iteratively computes a question-guided contextualized representation of the visual/textual information assigned to it. Then, to combine the information from both GNs, the nodes write the updated representations to an external spatial memory. The final states of the memory cells are fed into an answer module to predict an answer. Experiments show MN-GMN rivals the state-of-the-art models on Visual7W, VQA-v2.0, and CLEVR datasets.",,,,ACL
644,2020,Refer360∘: A Referring Expression Recognition Dataset in 360∘ Images,"Volkan Cirik,Taylor Berg-Kirkpatrick,Louis-Philippe Morency","We propose a novel large-scale referring expression recognition dataset, Refer360°, consisting of 17,137 instruction sequences and ground-truth actions for completing these instructions in 360° scenes. Refer360° differs from existing related datasets in three ways. First, we propose a more realistic scenario where instructors and the followers have partial, yet dynamic, views of the scene – followers continuously modify their field-of-view (FoV) while interpreting instructions that specify a final target location. Second, instructions to find the target location consist of multiple steps for followers who will start at random FoVs. As a result, intermediate instructions are strongly grounded in object references, and followers must identify intermediate FoVs to find the final target location correctly. Third, the target locations are neither restricted to predefined objects nor chosen by annotators; instead, they are distributed randomly across scenes. This “point anywhere” approach leads to more linguistically complex instructions, as shown in our analyses. Our examination of the dataset shows that Refer360° manifests linguistically rich phenomena in a language grounding task that poses novel challenges for computational modeling of language, vision, and navigation.",,,,ACL
645,2020,CamemBERT: a Tasty French Language Model,"Louis Martin,Benjamin Muller,Pedro Javier Ortiz Suárez,Yoann Dupont","Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models –in all languages except English– very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.",,,,ACL
646,2020,Effective Estimation of Deep Generative Language Models,"Tom Pelsmaeker,Wilker Aziz","Advances in variational inference enable parameterisation of probabilistic models by deep neural networks. This combines the statistical transparency of the probabilistic modelling framework with the representational power of deep learning. Yet, due to a problem known as posterior collapse, it is difficult to estimate such models in the context of language modelling effectively. We concentrate on one such model, the variational auto-encoder, which we argue is an important building block in hierarchical probabilistic models of language. This paper contributes a sober view of the problem, a survey of techniques to address it, novel techniques, and extensions to the model. To establish a ranking of techniques, we perform a systematic comparison using Bayesian optimisation and find that many techniques perform reasonably similar, given enough resources. Still, a favourite can be named based on convenience. We also make several empirical observations and recommendations of best practices that should help researchers interested in this exciting field.",,,,ACL
647,2020,Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection,"Shauli Ravfogel,Yanai Elazar,Hila Gonen,Michael Twiton","The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.",,,,ACL
648,2020,2kenize: Tying Subword Sequences for Chinese Script Conversion,"Pranav A,Isabelle Augenstein","Simplified Chinese to Traditional Chinese character conversion is a common preprocessing step in Chinese NLP. Despite this, current approaches have insufficient performance because they do not take into account that a simplified Chinese character can correspond to multiple traditional characters. Here, we propose a model that can disambiguate between mappings and convert between the two scripts. The model is based on subword segmentation, two language models, as well as a method for mapping between subword sequences. We further construct benchmark datasets for topic classification and script conversion. Our proposed method outperforms previous Chinese Character conversion approaches by 6 points in accuracy. These results are further confirmed in a downstream application, where 2kenize is used to convert pretraining dataset for topic classification. An error analysis reveals that our method’s particular strengths are in dealing with code mixing and named entities.",,,,ACL
649,2020,Predicting the Growth of Morphological Families from Social and Linguistic Factors,"Valentin Hofmann,Janet Pierrehumbert,Hinrich Schütze","We present the first study that examines the evolution of morphological families, i.e., sets of morphologically related words such as “trump”, “antitrumpism”, and “detrumpify”, in social media. We introduce the novel task of Morphological Family Expansion Prediction (MFEP) as predicting the increase in the size of a morphological family. We create a ten-year Reddit corpus as a benchmark for MFEP and evaluate a number of baselines on this benchmark. Our experiments demonstrate very good performance on MFEP.",,,,ACL
650,2020,Semi-supervised Contextual Historical Text Normalization,"Peter Makarov,Simon Clematide","Historical text normalization, the task of mapping historical word forms to their modern counterparts, has recently attracted a lot of interest (Bollmann, 2019; Tang et al., 2018; Lusetti et al., 2018; Bollmann et al., 2018;Robertson and Goldwater, 2018; Bollmannet al., 2017; Korchagina, 2017). Yet, virtually all approaches suffer from the two limitations: 1) They consider a fully supervised setup, often with impractically large manually normalized datasets; 2) Normalization happens on words in isolation. By utilizing a simple generative normalization model and obtaining powerful contextualization from the target-side language model, we train accurate models with unlabeled historical data. In realistic training scenarios, our approach often leads to reduction in manually normalized data at the same accuracy levels.",,,,ACL
651,2020,ClarQ: A large-scale and diverse dataset for Clarification Question Generation,"Vaibhav Kumar,Alan W Black","Question answering and conversational systems are often baffled and need help clarifying certain ambiguities. However, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions. In order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision) that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange. The framework utilises a neural network based architecture for classifying clarification questions. It is a two-step method where the first aims to increase the precision of the classifier and second aims to increase its recall. We quantitatively demonstrate the utility of the newly created dataset by applying it to the downstream task of question-answering. The final dataset, ClarQ, consists of ~2M examples distributed across 173 domains of stackexchange. We release this dataset in order to foster research into the field of clarification question generation with the larger goal of enhancing dialog and question answering systems.",,,,ACL
652,2020,DoQA - Accessing Domain-Specific FAQs via Conversational QA,"Jon Ander Campos,Arantxa Otegi,Aitor Soroa,Jan Deriu","The goal of this work is to build conversational Question Answering (QA) interfaces for the large body of domain-specific information available in FAQ sites. We present DoQA, a dataset with 2,437 dialogues and 10,917 QA pairs. The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing. Compared to previous work, DoQA comprises well-defined information needs, leading to more coherent and natural conversations with less factoid questions and is multi-domain. In addition, we introduce a more realistic information retrieval (IR) scenario where the system needs to find the answer in any of the FAQ documents. The results of an existing, strong, system show that, thanks to transfer learning from a Wikipedia QA dataset and fine tuning on a single FAQ domain, it is possible to build high quality conversational QA systems for FAQs without in-domain training data. The good results carry over into the more challenging IR scenario. In both cases, there is still ample room for improvement, as indicated by the higher human upperbound.",,,,ACL
653,2020,MLQA: Evaluating Cross-lingual Extractive Question Answering,"Patrick Lewis,Barlas Oguz,Ruty Rinott,Sebastian Riedel","Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.",,,,ACL
654,2020,Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering,"Ming Yan,Hao Zhang,Di Jin,Joey Tianyi Zhou","Multiple-choice question answering (MCQA) is one of the most challenging tasks in machine reading comprehension since it requires more advanced reading comprehension skills such as logical reasoning, summarization, and arithmetic operations. Unfortunately, most existing MCQA datasets are small in size, which increases the difficulty of model learning and generalization. To address this challenge, we propose a multi-source meta transfer (MMT) for low-resource MCQA. In this framework, we first extend meta learning by incorporating multiple training sources to learn a generalized feature representation across domains. To bridge the distribution gap between training sources and the target, we further introduce the meta transfer that can be integrated into the multi-source meta training. More importantly, the proposed MMT is independent of backbone language models. Extensive experiments demonstrate the superiority of MMT over state-of-the-arts, and continuous improvements can be achieved on different backbone networks on both supervised and unsupervised domain adaptation settings.",,,,ACL
655,2020,Fine-grained Fact Verification with Kernel Graph Attention Network,"Zhenghao Liu,Chenyan Xiong,Maosong Sun,Zhiyuan Liu","Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims. This paper presents Kernel Graph Attention Network (KGAT), which conducts more fine-grained fact verification with kernel-based attentions. Given a claim and a set of potential evidence sentences that form an evidence graph, KGAT introduces node kernels, which better measure the importance of the evidence node, and edge kernels, which conduct fine-grained evidence propagation in the graph, into Graph Attention Networks for more accurate fact verification. KGAT achieves a 70.38% FEVER score and significantly outperforms existing fact verification models on FEVER, a large-scale benchmark for fact verification. Our analyses illustrate that, compared to dot-product attentions, the kernel-based attention concentrates more on relevant evidence sentences and meaningful clues in the evidence graph, which is the main source of KGAT’s effectiveness. All source codes of this work are available at https://github.com/thunlp/KernelGAT.",,,,ACL
656,2020,Generating Fact Checking Explanations,"Pepa Atanasova,Jakob Grue Simonsen,Christina Lioma,Isabelle Augenstein","Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process – generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.",,,,ACL
657,2020,Premise Selection in Natural Language Mathematical Texts,"Deborah Ferreira,André Freitas","The discovery of supporting evidence for addressing complex mathematical problems is a semantically challenging task, which is still unexplored in the field of natural language processing for mathematical text. The natural language premise selection task consists in using conjectures written in both natural language and mathematical formulae to recommend premises that most likely will be useful to prove a particular statement. We propose an approach to solve this task as a link prediction problem, using Deep Convolutional Graph Neural Networks. This paper also analyses how different baselines perform in this task and shows that a graph structure can provide higher F1-score, especially when considering multi-hop premise selection.",,,,ACL
658,2020,A Call for More Rigor in Unsupervised Cross-lingual Learning,"Mikel Artetxe,Sebastian Ruder,Dani Yogatama,Gorka Labaka","We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world’s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.",,,,ACL
659,2020,A Tale of a Probe and a Parser,"Rowan Hall Maudslay,Josef Valvoda,Tiago Pimentel,Adina Williams","Measuring what linguistic information is encoded in neural models of language has become popular in NLP. Researchers approach this enterprise by training “probes”—supervised models designed to extract linguistic structure from another model’s output. One such probe is the structural probe (Hewitt and Manning, 2019), designed to quantify the extent to which syntactic information is encoded in contextualised word representations. The structural probe has a novel design, unattested in the parsing literature, the precise benefit of which is not immediately obvious. To explore whether syntactic probes would do better to make use of existing techniques, we compare the structural probe to a more traditional parser with an identical lightweight parameterisation. The parser outperforms structural probe on UUAS in seven of nine analysed languages, often by a substantial amount (e.g. by 11.1 points in English). Under a second less common metric, however, there is the opposite trend—the structural probe outperforms the parser. This begs the question: which metric should we prefer?",,,,ACL
660,2020,From SPMRL to NMRL: What Did We Learn (and Unlearn) in a Decade of Parsing Morphologically-Rich Languages (MRLs)?,"Reut Tsarfaty,Dan Bareket,Stav Klein,Amit Seker","It has been exactly a decade since the first establishment of SPMRL, a research initiative unifying multiple research efforts to address the peculiar challenges of Statistical Parsing for Morphologically-Rich Languages (MRLs). Here we reflect on parsing MRLs in that decade, highlight the solutions and lessons learned for the architectural, modeling and lexical challenges in the pre-neural era, and argue that similar challenges re-emerge in neural architectures for MRLs. We then aim to offer a climax, suggesting that incorporating symbolic ideas proposed in SPMRL terms into nowadays neural architectures has the potential to push NLP for MRLs to a new level. We sketch a strategies for designing Neural Models for MRLs (NMRL), and showcase preliminary support for these strategies via investigating the task of multi-tagging in Hebrew, a morphologically-rich, high-fusion, language.",,,,ACL
661,2020,Speech Translation and the End-to-End Promise: Taking Stock of Where We Are,"Matthias Sperber,Matthias Paulik","Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives. Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations. However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity. This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.",,,,ACL
662,2020,What Question Answering can Learn from Trivia Nerds,"Jordan Boyd-Graber,Benjamin Börschinger","In addition to the traditional task of machines answering questions, question answering (QA) research creates interesting, challenging questions that help systems how to answer questions and reveal the best systems. We argue that creating a QA dataset—and the ubiquitous leaderboard that goes with it—closely resembles running a trivia tournament: you write questions, have agents (either humans or machines) answer the questions, and declare a winner. However, the research community has ignored the hard-learned lessons from decades of the trivia community creating vibrant, fair, and effective question answering competitions. After detailing problems with existing QA datasets, we outline the key lessons—removing ambiguity, discriminating skill, and adjudicating disputes—that can transfer to QA research and how they might be implemented.",,,,ACL
663,2020,What are the Goals of Distributional Semantics?,Guy Emerson,"Distributional semantic models have become a mainstay in NLP, providing useful features for downstream tasks. However, assessing long-term progress requires explicit long-term goals. In this paper, I take a broad linguistic perspective, looking at how well current models can deal with various semantic challenges. Given stark differences between models proposed in different subfields, a broad perspective is needed to see how we could integrate them. I conclude that, while linguistic insights can guide the design of model architectures, future progress will require balancing the often conflicting demands of linguistic expressiveness and computational tractability.",,,,ACL
664,2020,Improving Image Captioning with Better Use of Caption,"Zhan Shi,Xu Zhou,Xipeng Qiu,Xiaodan Zhu","Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics. The code of our paper has been made publicly available.",,,,ACL
665,2020,Shape of Synth to Come: Why We Should Use Synthetic Data for English Surface Realization,"Henry Elder,Robert Burke,Alexander O’Connor,Jennifer Foster","The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language Generation shared tasks with the goal of exploring approaches to surface realization from Universal-Dependency-like trees to surface strings for several languages. In the 2018 shared task there was very little difference in the absolute performance of systems trained with and without additional, synthetically created data, and a new rule prohibiting the use of synthetic data was introduced for the 2019 shared task. Contrary to the findings of the 2018 shared task, we show, in experiments on the English 2018 dataset, that the use of synthetic data can have a substantial positive effect – an improvement of almost 8 BLEU points for a previously state-of-the-art system. We analyse the effects of synthetic data, and we argue that its use should be encouraged rather than prohibited so that future research efforts continue to explore systems that can take advantage of such data.",,,,ACL
666,2020,Toward Better Storylines with Sentence-Level Language Models,"Daphne Ippolito,David Grangier,Douglas Eck,Chris Callison-Burch","We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multi-sentence coherence. Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. Notably this allows us to consider a large number of candidates for the next sentence during training. We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks.",,,,ACL
667,2020,A Two-Step Approach for Implicit Event Argument Detection,"Zhisong Zhang,Xiang Kong,Zhengzhong Liu,Xuezhe Ma","In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.",,,,ACL
668,2020,Machine Reading of Historical Events,"Or Honovich,Lucas Torroba Hennigen,Omri Abend,Shay B. Cohen","Machine reading is an ambitious goal in NLP that subsumes a wide range of text understanding capabilities. Within this broad framework, we address the task of machine reading the time of historical events, compile datasets for the task, and develop a model for tackling it. Given a brief textual description of an event, we show that good performance can be achieved by extracting relevant sentences from Wikipedia, and applying a combination of task-specific and general-purpose feature embeddings for the classification. Furthermore, we establish a link between the historical event ordering task and the event focus time task from the information retrieval literature, showing they also provide a challenging test case for machine reading algorithms.",,,,ACL
669,2020,Revisiting Unsupervised Relation Extraction,"Thy Thy Tran,Phong Le,Sophia Ananiadou","Unsupervised relation extraction (URE) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases (KBs). URE methods can be categorised into generative and discriminative approaches, which rely either on hand-crafted features or surface form. However, we demonstrate that by using only named entities to induce relation types, we can outperform existing methods on two popular datasets. We conduct a comparison and evaluation of our findings with other URE techniques, to ascertain the important features in URE. We conclude that entity types provide a strong inductive bias for URE.",,,,ACL
670,2020,SciREX: A Challenge Dataset for Document-Level Information Extraction,"Sarthak Jain,Madeleine van Zuylen,Hannaneh Hajishirzi,Iz Beltagy","Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .",,,,ACL
671,2020,Contrastive Self-Supervised Learning for Commonsense Reasoning,"Tassilo Klein,Moin Nabi","We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge problems. Our approach exploits the characteristic structure of training corpora related to so-called “trigger” words, which are responsible for flipping the answer in pronoun disambiguation. We achieve such commonsense reasoning by constructing pair-wise contrastive auxiliary predictions. To this end, we leverage a mutual exclusive loss regularized by a contrastive margin. Our architecture is based on the recently introduced transformer networks, BERT, that exhibits strong performance on many NLP benchmarks. Empirical results show that our method alleviates the limitation of current supervised approaches for commonsense reasoning. This study opens up avenues for exploiting inexpensive self-supervision to achieve performance gain in commonsense reasoning tasks.",,,,ACL
672,2020,Do Transformers Need Deep Long-Range Memory?,"Jack Rae,Ali Razavi","Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL — a Transformer augmented with a long-range memory of past activations — has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.",,,,ACL
673,2020,Improving Disentangled Text Representation Learning with Information-Theoretic Guidance,"Pengyu Cheng,Martin Renqiang Min,Dinghan Shen,Christopher Malon","Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.",,,,ACL
674,2020,Understanding Advertisements with BERT,"Kanika Kalra,Bhargav Kurma,Silpa Vadakkeeveetil Sreelatha,Manasi Patwardhan","We consider a task based on CVPR 2018 challenge dataset on advertisement (Ad) understanding. The task involves detecting the viewer’s interpretation of an Ad image captured as text. Recent results have shown that the embedded scene-text in the image holds a vital cue for this task. Motivated by this, we fine-tune the base BERT model for a sentence-pair classification task. Despite utilizing the scene-text as the only source of visual information, we could achieve a hit-or-miss accuracy of 84.95% on the challenge test data. To enable BERT to process other visual information, we append image captions to the scene-text. This achieves an accuracy of 89.69%, which is an improvement of 4.7%. This is the best reported result for this task.",,,,ACL
675,2020,Non-Linear Instance-Based Cross-Lingual Mapping for Non-Isomorphic Embedding Spaces,"Goran Glavaš,Ivan Vulić","We present InstaMap, an instance-based method for learning projection-based cross-lingual word embeddings. Unlike prior work, it deviates from learning a single global linear projection. InstaMap is a non-parametric model that learns a non-linear projection by iteratively: (1) finding a globally optimal rotation of the source embedding space relying on the Kabsch algorithm, and then (2) moving each point along an instance-specific translation vector estimated from the translation vectors of the point’s nearest neighbours in the training dictionary. We report performance gains with InstaMap over four representative state-of-the-art projection-based models on bilingual lexicon induction across a set of 28 diverse language pairs. We note prominent improvements, especially for more distant language pairs (i.e., languages with non-isomorphic monolingual spaces).",,,,ACL
676,2020,Good-Enough Compositional Data Augmentation,Jacob Andreas,"We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic tasks from the SCAN dataset and 16% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1% on small corpora in several languages.",,,,ACL
677,2020,RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers,"Bailin Wang,Richard Shin,Xiaodong Liu,Oleksandr Polozov","When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2%, surpassing its best counterparts by 8.7% absolute improvement. Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6% on the Spider leaderboard. In addition, we observe qualitative improvements in the model’s understanding of schema linking and alignment. Our implementation will be open-sourced at https://github.com/Microsoft/rat-sql.",,,,ACL
678,2020,Temporal Common Sense Acquisition with Minimal Supervision,"Ben Zhou,Qiang Ning,Daniel Khashabi,Dan Roth","Temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language. However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly. This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build TacoLM, a temporal common sense language model. Our method is shown to give quality predictions of various dimensions of temporal common sense (on UDST and a newly collected dataset from RealNews). It also produces representations of events for relevant tasks such as duration comparison, parent-child relations, event coreference and temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the standard BERT. Thus, it will be an important component of temporal NLP.",,,,ACL
679,2020,The Sensitivity of Language Models and Humans to Winograd Schema Perturbations,"Mostafa Abdou,Vinit Ravishankar,Maria Barrett,Yonatan Belinkov","Large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the Winograd Schema Challenge, a widely employed test of commonsense reasoning ability. We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding. Our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones.",,,,ACL
680,2020,Temporally-Informed Analysis of Named Entity Recognition,"Shruti Rijhwani,Daniel Preotiuc-Pietro","Natural language processing models often have to make predictions on text data that evolves over time as a result of changes in language use or the information described in the text. However, evaluation results on existing data sets are seldom reported by taking the timestamp of the document into account. We analyze and propose methods that make better use of temporally-diverse training data, with a focus on the task of named entity recognition. To support these experiments, we introduce a novel data set of English tweets annotated with named entities. We empirically demonstrate the effect of temporal drift on performance, and how the temporal information of documents can be used to obtain better models compared to those that disregard temporal information. Our analysis gives insights into why this information is useful, in the hope of informing potential avenues of improvement for named entity recognition as well as other NLP tasks under similar experimental setups.",,,,ACL
681,2020,Towards Open Domain Event Trigger Identification using Adversarial Domain Adaptation,"Aakanksha Naik,Carolyn Rose","We tackle the task of building supervised event trigger identification models which can generalize better across domains. Our work leverages the adversarial domain adaptation (ADA) framework to introduce domain-invariance. ADA uses adversarial training to construct representations that are predictive for trigger identification, but not predictive of the example’s domain. It requires no labeled data from the target domain, making it completely unsupervised. Experiments with two domains (English literature and news) show that ADA leads to an average F1 score improvement of 3.9 on out-of-domain data. Our best performing model (BERT-A) reaches 44-49 F1 across both domains, using no labeled target data. Preliminary experiments reveal that finetuning on 1% labeled data, followed by self-training leads to substantial improvement, reaching 51.5 and 67.2 F1 on literature and news respectively.",,,,ACL
682,2020,CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning,"Alessandro Suglia,Ioannis Konstas,Andrea Vanzo,Emanuele Bastianelli","Approaches to Grounded Language Learning are commonly focused on a single task-based final performance measure which may not depend on desirable properties of the learned hidden representations, such as their ability to predict object attributes or generalize to unseen situations. To remedy this, we present GroLLA, an evaluation framework for Grounded Language Learning with Attributes based on three sub-tasks: 1) Goal-oriented evaluation; 2) Object attribute prediction evaluation; and 3) Zero-shot evaluation. We also propose a new dataset CompGuessWhat?! as an instance of this framework for evaluating the quality of learned neural representations, in particular with respect to attribute grounding. To this end, we extend the original GuessWhat?! dataset by including a semantic layer on top of the perceptual one. Specifically, we enrich the VisualGenome scene graphs associated with the GuessWhat?! images with several attributes from resources such as VISA and ImSitu. We then compare several hidden state representations from current state-of-the-art approaches to Grounded Language Learning. By using diagnostic classifiers, we show that current models’ learned representations are not expressive enough to encode object attributes (average F1 of 44.27). In addition, they do not learn strategies nor representations that are robust enough to perform well when novel scenes or objects are involved in gameplay (zero-shot best accuracy 50.06%).",,,,ACL
683,2020,Cross-Modality Relevance for Reasoning on Language and Vision,"Chen Zheng,Quan Guo,Parisa Kordjamshidi","This work deals with the challenge of learning and reasoning over language and vision data for the related downstream tasks such as visual question answering (VQA) and natural language for visual reasoning (NLVR). We design a novel cross-modality relevance module that is used in an end-to-end framework to learn the relevance representation between components of various input modalities under the supervision of a target task, which is more generalizable to unobserved data compared to merely reshaping the original representation space. In addition to modeling the relevance between the textual entities and visual entities, we model the higher-order relevance between entity relations in the text and object relations in the image. Our proposed approach shows competitive performance on two different language and vision tasks using public benchmarks and improves the state-of-the-art published results. The learned alignments of input spaces and their relevance representations by NLVR task boost the training efficiency of VQA task.",,,,ACL
684,2020,Learning Web-based Procedures by Reasoning over Explanations and Demonstrations in Context,"Shashank Srivastava,Oleksandr Polozov,Nebojsa Jojic,Christopher Meek","We explore learning web-based tasks from a human teacher through natural language explanations and a single demonstration. Our approach investigates a new direction for semantic parsing that models explaining a demonstration in a context, rather than mapping explanations to demonstrations. By leveraging the idea of inverse semantics from program synthesis to reason backwards from observed demonstrations, we ensure that all considered interpretations are consistent with executable actions in any context, thus simplifying the problem of search over logical forms. We present a dataset of explanations paired with demonstrations for web-based tasks. Our methods show better task completion rates than a supervised semantic parsing baseline (40% relative improvement on average), and are competitive with simple exploration-and-demonstration based methods, while requiring no exploration of the environment. In learning to align explanations with demonstrations, basic properties of natural language syntax emerge as learned behavior. This is an interesting example of pragmatic language acquisition without any linguistic annotation.",,,,ACL
685,2020,Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning,"Angeliki Lazaridou,Anna Potapenko,Olivier Tieleman","We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language. Our starting point is a language model that has been trained on generic, not task-specific language data. We then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model, turning it into a task-conditional language model. We introduce a new way for combining the two types of learning based on the idea of reranking language model samples, and show that this method outperforms others in communicating with humans in a visual referential communication task. Finally, we present a taxonomy of different types of language drift that can occur alongside a set of measures to detect them.",,,,ACL
686,2020,HAT: Hardware-Aware Transformers for Efficient Natural Language Processing,"Hanrui Wang,Zhanghao Wu,Zhijian Liu,Han Cai","Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT’14 translation task on Raspberry Pi-4, HAT can achieve 3× speedup, 3.7× smaller size over baseline Transformer; 2.7× speedup, 3.6× smaller size over Evolved Transformer with 12,041× less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.",,,,ACL
687,2020,Hard-Coded Gaussian Attention for Neural Machine Translation,"Weiqiu You,Simeng Sun,Mohit Iyyer","Recent work has questioned the importance of the Transformer’s multi-headed attention for achieving high translation quality. We push further in this direction by developing a “hard-coded” attention variant without any learned parameters. Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs. However, additionally, hard-coding cross attention (which connects the decoder to the encoder) significantly lowers BLEU, suggesting that it is more important than self-attention. Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer. Taken as a whole, our results offer insight into which components of the Transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models.",,,,ACL
688,2020,"In Neural Machine Translation, What Does Transfer Learning Transfer?","Alham Fikri Aji,Nikolay Bogoychev,Kenneth Heafield,Rico Sennrich","Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers. We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning. Word embeddings play an important role in transfer learning, particularly if they are properly aligned. Although transfer learning can be performed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs.",,,,ACL
689,2020,Learning a Multi-Domain Curriculum for Neural Machine Translation,"Wei Wang,Ye Tian,Jiquan Ngiam,Yinfei Yang","Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.",,,,ACL
690,2020,Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem,"Danielle Saunders,Bill Byrne","Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a ‘balanced’ dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is ‘catastrophic forgetting’, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.",,,,ACL
691,2020,Translationese as a Language in “Multilingual” NMT,"Parker Riley,Isaac Caswell,Markus Freitag,David Grangier","Machine translation has an undesirable propensity to produce “translationese” artifacts, which can lead to higher BLEU scores while being liked less by human raters. Motivated by this, we model translationese and original (i.e. natural) text as separate languages in a multilingual model, and pose the question: can we perform zero-shot translation between original source text and original target text? There is no data with original source and original target, so we train a sentence-level classifier to distinguish translationese from original target text, and use this classifier to tag the training data for an NMT model. Using this technique we bias the model to produce more natural outputs at test time, yielding gains in human evaluation scores on both accuracy and fluency. Additionally, we demonstrate that it is possible to bias the model to produce translationese and game the BLEU score, increasing it while decreasing human-rated quality. We analyze these outputs using metrics measuring the degree of translationese, and present an analysis of the volatility of heuristic-based train-data tagging.",,,,ACL
692,2020,Unsupervised Domain Clusters in Pretrained Language Models,"Roee Aharoni,Yoav Goldberg","The notion of “in-domain data” in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision – suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.",,,,ACL
693,2020,Using Context in Neural Machine Translation Training Objectives,"Danielle Saunders,Felix Stahlberg,Bill Byrne","We present Neural Machine Translation (NMT) training using document-level metrics with batch-level documents. Previous sequence-objective approaches to NMT training focus exclusively on sentence-level metrics like sentence BLEU which do not correspond to the desired evaluation metric, typically document BLEU. Meanwhile research into document-level NMT training focuses on data or model architecture rather than training procedure. We find that each of these lines of research has a clear space in it for the other, and propose merging them with a scheme that allows a document-level evaluation metric to be used in the NMT training objective. We first sample pseudo-documents from sentence samples. We then approximate the expected document BLEU gradient with Monte Carlo sampling for use as a cost function in Minimum Risk Training (MRT). This two-level sampling procedure gives NMT performance gains over sequence MRT and maximum-likelihood training. We demonstrate that training is more robust for document-level metrics than with sequence metrics. We further demonstrate improvements on NMT with TER and Grammatical Error Correction (GEC) using GLEU, both metrics used at the document level for evaluations.",,,,ACL
694,2020,Variational Neural Machine Translation with Normalizing Flows,"Hendra Setiawan,Matthias Sperber,Udhyakumar Nallasamy,Matthias Paulik","Variational Neural Machine Translation (VNMT) is an attractive framework for modeling the generation of target translations, conditioned not only on the source sentence but also on some latent random variables. The latent variable modeling may introduce useful statistical dependencies that can improve translation accuracy. Unfortunately, learning informative latent variables is non-trivial, as the latent space can be prohibitively large, and the latent codes are prone to be ignored by many translation models at training time. Previous works impose strong assumptions on the distribution of the latent code and limit the choice of the NMT architecture. In this paper, we propose to apply the VNMT framework to the state-of-the-art Transformer and introduce a more flexible approximate posterior based on normalizing flows. We demonstrate the efficacy of our proposal under both in-domain and out-of-domain conditions, significantly outperforming strong baselines.",,,,ACL
695,2020,The Paradigm Discovery Problem,"Alexander Erdmann,Micha Elsner,Shijie Wu,Ryan Cotterell","This work treats the paradigm discovery problem (PDP), the task of learning an inflectional morphological system from unannotated sentences. We formalize the PDP and develop evaluation metrics for judging systems. Using currently available resources, we construct datasets for the task. We also devise a heuristic benchmark for the PDP and report empirical results on five diverse languages. Our benchmark system first makes use of word embeddings and string similarity to cluster forms by cell and by paradigm. Then, we bootstrap a neural transducer on top of the clustered data to predict words to realize the empty paradigm slots. An error analysis of our system suggests clustering by cell across different inflection classes is the most pressing challenge for future work.",,,,ACL
696,2020,Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in Hindi and Punjabi,"Aryaman Arora,Luke Gessler,Nathan Schneider","Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries. Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification.",,,,ACL
697,2020,Automated Evaluation of Writing – 50 Years and Counting,"Beata Beigman Klebanov,Nitin Madnani","In this theme paper, we focus on Automated Writing Evaluation (AWE), using Ellis Page’s seminal 1966 paper to frame the presentation. We discuss some of the current frontiers in the field and offer some thoughts on the emergent uses of this technology.",,,,ACL
698,2020,"Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly","Nora Kassner,Hinrich Schütze","Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (‘‘Birds cannot [MASK]”) and non-negated (‘‘Birds can [MASK]”) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add “misprimes” to cloze questions (‘‘Talk? Birds can [MASK]”). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",,,,ACL
699,2020,On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology,"Marcel Bollmann,Desmond Elliott","The field of natural language processing is experiencing a period of unprecedented growth, and with it a surge of published papers. This represents an opportunity for us to take stock of how we cite the work of other researchers, and whether this growth comes at the expense of “forgetting” about older literature. In this paper, we address this question through bibliographic analysis. By looking at the age of outgoing citations in papers published at selected ACL venues between 2010 and 2019, we find that there is indeed a tendency for recent papers to cite more recent work, but the rate at which papers older than 15 years are cited has remained relatively stable.",,,,ACL
700,2020,Returning the N to NLP: Towards Contextually Personalized Classification Models,Lucie Flek,"Most NLP models today treat language as universal, even though socio- and psycholingustic research shows that the communicated message is influenced by the characteristics of the speaker as well as the target audience. This paper surveys the landscape of personalization in natural language processing and related fields, and offers a path forward to mitigate the decades of deviation of the NLP tools from sociolingustic findings, allowing to flexibly process the “natural” language of each user rather than enforcing a uniform NLP treatment. It outlines a possible direction to incorporate these aspects into neural NLP models by means of socially contextual personalization, and proposes to shift the focus of our evaluation strategies accordingly.",,,,ACL
701,2020,"To Test Machine Comprehension, Start by Defining Comprehension","Jesse Dunietz,Greg Burnham,Akash Bharadwaj,Owen Rambow","Many tasks aim to measure machine reading comprehension (MRC), often focusing on question types presumed to be difficult. Rarely, however, do task designers start by considering what systems should in fact comprehend. In this paper we make two key contributions. First, we argue that existing approaches do not adequately define comprehension; they are too unsystematic about what content is tested. Second, we present a detailed definition of comprehension—a “Template of Understanding”—for a widely useful class of texts, namely short narratives. We then conduct an experiment that strongly suggests existing systems are not up to the task of narrative understanding as we define it.",,,,ACL
702,2020,Gender Gap in Natural Language Processing Research: Disparities in Authorship and Citations,Saif M. Mohammad,"Disparities in authorship and citations across gender can have substantial adverse consequences not just on the disadvantaged genders, but also on the field of study as a whole. Measuring gender gaps is a crucial step towards addressing them. In this work, we examine female first author percentages and the citations to their papers in Natural Language Processing (1965 to 2019). We determine aggregate-level statistics using an existing manually curated author--gender list as well as first names strongly associated with a gender. We find that only about 29% of first authors are female and only about 25% of last authors are female. Notably, this percentage has not improved since the mid 2000s. We also show that, on average, female first authors are cited less than male first authors, even when controlling for experience and area of research. Finally, we discuss the ethical considerations involved in automatic demographic analysis.",,,,ACL
703,2020,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","Mike Lewis,Yinhan Liu,Naman Goyal,Marjan Ghazvininejad","We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",,,,ACL
704,2020,BLEURT: Learning Robust Metrics for Text Generation,"Thibault Sellam,Dipanjan Das,Ankur Parikh","Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.",,,,ACL
705,2020,Distilling Knowledge Learned in BERT for Text Generation,"Yen-Chun Chen,Zhe Gan,Yu Cheng,Jingzhou Liu","Large-scale pre-trained language model such as BERT has achieved great success in language understanding tasks. However, it remains an open question how to utilize BERT for language generation. In this paper, we present a novel approach, Conditional Masked Language Modeling (C-MLM), to enable the finetuning of BERT on target generation tasks. The finetuned BERT (teacher) is exploited as extra supervision to improve conventional Seq2Seq models (student) for better text generation performance. By leveraging BERT’s idiosyncratic bidirectional nature, distilling knowledge learned in BERT can encourage auto-regressive Seq2Seq models to plan ahead, imposing global sequence-level supervision for coherent text generation. Experiments show that the proposed approach significantly outperforms strong Transformer baselines on multiple language generation tasks such as machine translation and text summarization. Our proposed model also achieves new state of the art on IWSLT German-English and English-Vietnamese MT datasets.",,,,ACL
706,2020,ESPRIT: Explaining Solutions to Physical Reasoning Tasks,"Nazneen Fatema Rajani,Rui Zhang,Yi Chern Tan,Stephan Zheng","Neural networks lack the ability to reason about qualitative physics and so cannot generalize to scenarios and tasks unseen during training. We propose ESPRIT, a framework for commonsense reasoning about qualitative physics in natural language that generates interpretable descriptions of physical events. We use a two-step approach of first identifying the pivotal physical events in an environment and then generating natural language descriptions of those events using a data-to-text approach. Our framework learns to generate explanations of how the physical simulation will causally evolve so that an agent or a human can easily reason about a solution using those interpretable descriptions. Human evaluations indicate that ESPRIT produces crucial fine-grained details and has high coverage of physical concepts compared to even human annotations. Dataset, code and documentation are available at https://github.com/salesforce/esprit.",,,,ACL
707,2020,Iterative Edit-Based Unsupervised Sentence Simplification,"Dhruv Kumar,Lili Mou,Lukasz Golab,Olga Vechtomova","We present a novel iterative, edit-based approach to unsupervised sentence simplification. Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. Then, we iteratively perform word and phrase-level edits on the complex sentence. Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable. Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches.",,,,ACL
708,2020,Logical Natural Language Generation from Open-Domain Tables,"Wenhu Chen,Jianshu Chen,Yu Su,Zhiyu Chen","Neural natural language generation (NLG) models have recently shown remarkable progress in fluency and coherence. However, existing studies on neural NLG are primarily focused on surface-level realizations with limited emphasis on logical inference, an important aspect of human thinking and language. In this paper, we suggest a new NLG task where a model is tasked with generating natural language statements that can be logically entailed by the facts in an open-domain semi-structured table. To facilitate the study of the proposed logical NLG problem, we use the existing TabFact dataset~(CITATION) featured with a wide range of logical/symbolic inferences as our testbed, and propose new automatic metrics to evaluate the fidelity of generation models w.r.t. logical inference. The new task poses challenges to the existing monotonic generation frameworks due to the mismatch between sequence order and logical order. In our experiments, we comprehensively survey different generation architectures (LSTM, Transformer, Pre-Trained LM) trained with different algorithms (RL, Adversarial Training, Coarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained LM can significantly boost both the fluency and logical fidelity metrics, 2) RL and Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine generation can help partially alleviate the fidelity issue while maintaining high language fluency. The code and data are available at https://github.com/wenhuchen/LogicNLG.",,,,ACL
709,2020,Neural CRF Model for Sentence Alignment in Text Simplification,"Chao Jiang,Mounica Maddela,Wuwei Lan,Yang Zhong","The success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus, which are extracted by aligning sentences between parallel articles. To evaluate and improve sentence alignment quality, we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora, Newsela and Wikipedia. We propose a novel neural CRF alignment model which not only leverages the sequential nature of sentences in parallel documents but also utilizes a neural sentence pair model to capture semantic similarity. Experiments demonstrate that our proposed approach outperforms all the previous work on monolingual sentence alignment task by more than 5 points in F1. We apply our CRF aligner to construct two new text simplification datasets, Newsela-Auto and Wiki-Auto, which are much larger and of better quality compared to the existing datasets. A Transformer-based seq2seq model trained on our datasets establishes a new state-of-the-art for text simplification in both automatic and human evaluation.",,,,ACL
710,2020,One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases,"Xingdi Yuan,Tong Wang,Rui Meng,Khushboo Thaker","Different texts shall by nature correspond to different number of keyphrases. This desideratum is largely missing from existing neural keyphrase generation models. In this study, we address this problem from both modeling and evaluation perspectives. We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences. Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states. In contrast to previous approaches, our model is capable of generating diverse keyphrases and controlling number of outputs. We further propose two evaluation metrics tailored towards the variable-number generation. We also introduce a new dataset StackEx that expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks. With both previous and new evaluation metrics, our model outperforms strong baselines on all datasets.",,,,ACL
711,2020,"Rˆ3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge","Tuhin Chakrabarty,Debanjan Ghosh,Smaranda Muresan,Nanyun Peng","We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence. Our method employs a retrieve-and-edit framework to instantiate two major characteristics of sarcasm: reversal of valence and semantic incongruity with the context, which could include shared commonsense or world knowledge between the speaker and the listener. While prior works on sarcasm generation predominantly focus on context incongruity, we show that combining valence reversal and semantic incongruity based on the commonsense knowledge generates sarcasm of higher quality. Human evaluation shows that our system generates sarcasm better than humans 34% of the time, and better than a reinforced hybrid baseline 90% of the time.",,,,ACL
712,2020,Structural Information Preserving for Graph-to-Text Generation,"Linfeng Song,Ante Wang,Jinsong Su,Yue Zhang","The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs. As a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs. We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information. In particular, we introduce two types of autoencoding losses, each individually focusing on different aspects (a.k.a. views) of input graphs. The losses are then back-propagated to better calibrate our model via multi-task training. Experiments on two benchmarks for graph-to-text generation show the effectiveness of our approach over a state-of-the-art baseline.",,,,ACL
713,2020,A Joint Neural Model for Information Extraction with Global Features,"Ying Lin,Heng Ji,Fei Huang,Lingfei Wu","Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.",,,,ACL
714,2020,Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding,"Xinya Du,Claire Cardie","Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.",,,,ACL
715,2020,Exploiting the Syntax-Model Consistency for Neural Relation Extraction,"Amir Pouran Ben Veyseh,Franck Dernoncourt,Dejing Dou,Thien Huu Nguyen","This paper studies the task of Relation Extraction (RE) that aims to identify the semantic relations between two entity mentions in text. In the deep learning models for RE, it has been beneficial to incorporate the syntactic structures from the dependency trees of the input sentences. In such models, the dependency trees are often used to directly structure the network architectures or to obtain the dependency relations between the word pairs to inject the syntactic information into the models via multi-task learning. The major problem with these approaches is the lack of generalization beyond the syntactic structures in the training data or the failure to capture the syntactic importance of the words for RE. In order to overcome these issues, we propose a novel deep learning model for RE that uses the dependency trees to extract the syntax-based importance scores for the words, serving as a tree representation to introduce syntactic information into the models with greater generalization. In particular, we leverage Ordered-Neuron Long-Short Term Memory Networks (ON-LSTM) to infer the model-based importance scores for RE for every word in the sentences that are then regulated to be consistent with the syntax-based scores to enable syntactic information injection. We perform extensive experiments to demonstrate the effectiveness of the proposed method, leading to the state-of-the-art performance on three RE benchmark datasets.",,,,ACL
716,2020,From English to Code-Switching: Transfer Learning with Strong Morphological Clues,"Gustavo Aguilar,Thamar Solorio","Linguistic Code-switching (CS) is still an understudied phenomenon in natural language processing. The NLP community has mostly focused on monolingual and multi-lingual scenarios, but little attention has been given to CS in particular. This is partly because of the lack of resources and annotated data, despite its increasing occurrence in social media platforms. In this paper, we aim at adapting monolingual models to code-switched text in various tasks. Specifically, we transfer English knowledge from a pre-trained ELMo model to different code-switched language pairs (i.e., Nepali-English, Spanish-English, and Hindi-English) using the task of language identification. Our method, CS-ELMo, is an extension of ELMo with a simple yet effective position-aware attention mechanism inside its character convolutions. We show the effectiveness of this transfer learning step by outperforming multilingual BERT and homologous CS-unaware ELMo models and establishing a new state of the art in CS tasks, such as NER and POS tagging. Our technique can be expanded to more English-paired code-switched languages, providing more resources to the CS community.",,,,ACL
717,2020,"Learning Interpretable Relationships between Entities, Relations and Concepts via Bayesian Structure Learning on Open Domain Facts","Jingyuan Zhang,Mingming Sun,Yue Feng,Ping Li","Concept graphs are created as universal taxonomies for text understanding in the open-domain knowledge. The nodes in concept graphs include both entities and concepts. The edges are from entities to concepts, showing that an entity is an instance of a concept. In this paper, we propose the task of learning interpretable relationships from open-domain facts to enrich and refine concept graphs. The Bayesian network structures are learned from open-domain facts as the interpretable relationships between relations of facts and concepts of entities. We conduct extensive experiments on public English and Chinese datasets. Compared to the state-of-the-art methods, the learned network structures help improving the identification of concepts for entities based on the relations of entities on both datasets.",,,,ACL
718,2020,Multi-Sentence Argument Linking,"Seth Ebner,Patrick Xia,Ryan Culkin,Kyle Rawlins","We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.",,,,ACL
719,2020,Rationalizing Medical Relation Prediction from Corpus-level Statistics,"Zhen Wang,Jennifer Lee,Simon Lin,Huan Sun","Nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain. Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition. Given the corpus-level statistics, i.e., a global co-occurrence graph of a clinical text corpus, to predict the relations between two entities, we first recall rich contexts associated with the target entities, and then recognize relational interactions between these contexts to form model rationales, which will contribute to the final prediction. We conduct experiments on a real-world public clinical dataset and show that our framework can not only achieve competitive predictive performance against a comprehensive list of neural baseline models, but also present rationales to justify its prediction. We further collaborate with medical experts deeply to verify the usefulness of our model rationales for clinical decision making.",,,,ACL
720,2020,Sources of Transfer in Multilingual Named Entity Recognition,"David Mueller,Nicholas Andrews,Mark Dredze","Named-entities are inherently multilingual, and annotations in any given language may be limited. This motivates us to consider polyglot named-entity recognition (NER), where one model is trained using annotated data drawn from more than one language. However, a straightforward implementation of this simple idea does not always work in practice: naive training of NER models using annotated data drawn from multiple languages consistently underperforms models trained on monolingual data alone, despite having access to more training data. The starting point of this paper is a simple solution to this problem, in which polyglot models are fine-tuned on monolingual data to consistently and significantly outperform their monolingual counterparts. To explain this phenomena, we explore the sources of multilingual transfer in polyglot NER models and examine the weight structure of polyglot models compared to their monolingual counterparts. We find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters.",,,,ACL
721,2020,ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages,"Colin Lockard,Prashant Shiralkar,Xin Luna Dong,Hannaneh Hajishirzi","In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color. Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template. In this work, we propose a solution for “zero-shot” open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals. Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates. Experiments show this approach provides a 31% F1 gain over a baseline for zero-shot extraction in a new subject vertical.",,,,ACL
722,2020,Soft Gazetteers for Low-Resource Named Entity Recognition,"Shruti Rijhwani,Shuyan Zhou,Graham Neubig,Jaime Carbonell","Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data. However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. To address this problem, we propose a method of “soft gazetteers” that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.",,,,ACL
723,2020,A Prioritization Model for Suicidality Risk Assessment,"Han-Chin Shing,Philip Resnik,Douglas Oard","We reframe suicide risk assessment from social media as a ranking problem whose goal is maximizing detection of severely at-risk individuals given the time available. Building on measures developed for resource-bounded document retrieval, we introduce a well founded evaluation paradigm, and demonstrate using an expert-annotated test collection that meaningful improvements over plausible cascade model baselines can be achieved using an approach that jointly ranks individuals and their social media posts.",,,,ACL
724,2020,CluHTM - Semantic Hierarchical Topic Modeling based on CluWords,"Felipe Viegas,Washington Cunha,Christian Gomes,Antônio Pereira","Hierarchical Topic modeling (HTM) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration. Despite advantages over traditional topic modeling, HTM poses its own challenges, such as (1) topic incoherence, (2) unreasonable (hierarchical) structure, and (3) issues related to the definition of the “ideal” number of topics and depth of the hierarchy. In this paper, we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM, a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM. CluHTM’s novel contributions include: (i) the exploration of richer text representation that encapsulates both, global (dataset level) and local semantic information – when combined, these pieces of information help to solve the topic incoherence problem as well as issues related to the unreasonable structure; (ii) the exploitation of a stability analysis metric for defining the number of topics and the “shape” the hierarchical structure. In our evaluation, considering twelve datasets and seven state-of-the-art baselines, CluHTM outperformed the baselines in the vast majority of the cases, with gains of around 500% over the strongest state-of-the-art baselines. We also provide qualitative and quantitative statistical analyses of why our solution works so well.",,,,ACL
725,2020,Empower Entity Set Expansion via Language Model Probing,"Yunyi Zhang,Jiaming Shen,Jingbo Shang,Jiawei Han","Entity set expansion, aiming at expanding a small seed entity set with new entities belonging to the same semantic class, is a critical task that benefits many downstream NLP and IR applications, such as question answering, query understanding, and taxonomy construction. Existing set expansion methods bootstrap the seed entity set by adaptively selecting context features and extracting new entities. A key challenge for entity set expansion is to avoid selecting ambiguous context features which will shift the class semantics and lead to accumulative errors in later iterations. In this study, we propose a novel iterative set expansion framework that leverages automatically generated class names to address the semantic drift issue. In each iteration, we select one positive and several negative class names by probing a pre-trained language model, and further score each candidate entity based on selected class names. Experiments on two datasets show that our framework generates high-quality class names and outperforms previous state-of-the-art methods significantly.",,,,ACL
726,2020,Feature Projection for Improved Text Classification,"Qi Qin,Wenpeng Hu,Bing Liu","In classification, there are usually some good features that are indicative of class labels. For example, in sentiment classification, words like good and nice are indicative of the positive sentiment and words like bad and terrible are indicative of the negative sentiment. However, there are also many common features (e.g., words) that are not indicative of any specific class (e.g., voice and screen, which are common to both sentiment classes and are not discriminative for classification). Although deep learning has made significant progresses in generating discriminative features through its powerful representation learning, we believe there is still room for improvement. In this paper, we propose a novel angle to further improve this representation learning, i.e., feature projection. This method projects existing features into the orthogonal space of the common features. The resulting projection is thus perpendicular to the common features and more discriminative for classification. We apply this new method to improve CNN, RNN, Transformer, and Bert based text classification and obtain markedly better results.",,,,ACL
727,2020,A negative case analysis of visual grounding methods for VQA,"Robik Shrestha,Kushal Kafle,Christopher Kanan","Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. To address this issue, recent bias mitigation methods for VQA propose to incorporate visual cues (e.g., human attention maps) to better ground the VQA models, showcasing impressive gains. However, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to linguistic priors. For instance, we find that it is not actually necessary to provide proper, human-based cues; random, insensible cues also result in similar improvements. Based on this observation, we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on VQA-CPv2.",,,,ACL
728,2020,History for Visual Dialog: Do we really need it?,"Shubham Agarwal,Trung Bui,Joon-Young Lee,Ioannis Konstas","Visual Dialogue involves “understanding” the dialogue history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to accurately generate the correct response. In this paper, we show that co-attention models which explicitly encode dialoh history outperform models that don’t, achieving state-of-the-art performance (72 % NDCG on val set). However, we also expose shortcomings of the crowdsourcing dataset collection procedure, by showing that dialogue history is indeed only required for a small amount of the data, and that the current evaluation metric encourages generic replies. To that end, we propose a challenging subset (VisdialConv) of the VisdialVal set and the benchmark NDCG of 63%.",,,,ACL
729,2020,Mapping Natural Language Instructions to Mobile UI Action Sequences,"Yang Li,Jiacong He,Xin Zhou,Yuan Zhang","We present a new problem: grounding natural language instructions to mobile user interface actions, and create three new datasets for it. For full task evaluation, we create PixelHelp, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator. To scale training, we decouple the language and action data by (a) annotating action phrase spans in How-To instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces. We use a Transformer to extract action phrase tuples from long-range natural language instructions. A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions. Given a starting screen and instruction, our model achieves 70.59% accuracy on predicting complete ground-truth action sequences in PixelHelp.",,,,ACL
730,2020,TVQA+: Spatio-Temporal Grounding for Video Question Answering,"Jie Lei,Licheng Yu,Tamara Berg,Mohit Bansal","We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations.",,,,ACL
731,2020,Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting,"Po-Yao Huang,Junjie Hu,Xiaojun Chang,Alexander Hauptmann","Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time.",,,,ACL
732,2020,A Multitask Learning Approach for Diacritic Restoration,"Sawsan Alqahtani,Ajay Mishra,Mona Diab","In many languages like Arabic, diacritics are used to specify pronunciations as well as meanings. Such diacritics are often omitted in written text, increasing the number of possible pronunciations and meanings for a word. This results in a more ambiguous text making computational processing on such text more difficult. Diacritic restoration is the task of restoring missing diacritics in the written text. Most state-of-the-art diacritic restoration models are built on character level information which helps generalize the model to unseen data, but presumably lose useful information at the word level. Thus, to compensate for this loss, we investigate the use of multi-task learning to jointly optimize diacritic restoration with related NLP problems namely word segmentation, part-of-speech tagging, and syntactic diacritization. We use Arabic as a case study since it has sufficient data resources for tasks that we consider in our joint modeling. Our joint models significantly outperform the baselines and are comparable to the state-of-the-art models that are more complex relying on morphological analyzers and/or a lot more data (e.g. dialectal data).",,,,ACL
733,2020,Frugal Paradigm Completion,"Alexander Erdmann,Tom Kenter,Markus Becker,Christian Schallhart","Lexica distinguishing all morphologically related forms of each lexeme are crucial to many language technologies, yet building them is expensive. We propose a frugal paradigm completion approach that predicts all related forms in a morphological paradigm from as few manually provided forms as possible. It induces typological information during training which it uses to determine the best sources at test time. We evaluate our language-agnostic approach on 7 diverse languages. Compared to popular alternative approaches, ours reduces manual labor by 16-63% and is the most robust to typological variation.",,,,ACL
734,2020,Improving Chinese Word Segmentation with Wordhood Memory Networks,"Yuanhe Tian,Yan Song,Fei Xia,Tong Zhang","Contextual features always play an important role in Chinese word segmentation (CWS). Wordhood information, being one of the contextual features, is proved to be useful in many conventional character-based segmenters. However, this feature receives less attention in recent neural models and it is also challenging to design a framework that can properly integrate wordhood information from different wordhood measures to existing neural frameworks. In this paper, we therefore propose a neural framework, WMSeg, which uses memory networks to incorporate wordhood information with several popular encoder-decoder combinations for CWS. Experimental results on five benchmark datasets indicate the memory mechanism successfully models wordhood information for neural segmenters and helps WMSeg achieve state-of-the-art performance on all those datasets. Further experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments.",,,,ACL
735,2020,Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-way Attentions of Auto-analyzed Knowledge,"Yuanhe Tian,Yan Song,Xiang Ao,Fei Xia","Chinese word segmentation (CWS) and part-of-speech (POS) tagging are important fundamental tasks for Chinese language processing, where joint learning of them is an effective one-step solution for both tasks. Previous studies for joint CWS and POS tagging mainly follow the character-based tagging paradigm with introducing contextual information such as n-gram features or sentential representations from recurrent neural models. However, for many cases, the joint tagging needs not only modeling from context features but also knowledge attached to them (e.g., syntactic relations among words); limited efforts have been made by existing research to meet such needs. In this paper, we propose a neural model named TwASP for joint CWS and POS tagging following the character-based sequence labeling paradigm, where a two-way attention mechanism is used to incorporate both context feature and their corresponding syntactic knowledge for each input character. Particularly, we use existing language processing toolkits to obtain the auto-analyzed syntactic knowledge for the context, and the proposed attention module can learn and benefit from them although their quality may not be perfect. Our experiments illustrate the effectiveness of the two-way attentions for joint CWS and POS tagging, where state-of-the-art performance is achieved on five benchmark datasets.",,,,ACL
736,2020,"Joint Diacritization, Lemmatization, Normalization, and Fine-Grained Morphological Tagging","Nasser Zalmout,Nizar Habash","The written forms of Semitic languages are both highly ambiguous and morphologically rich: a word can have multiple interpretations and is one of many inflected forms of the same concept or lemma. This is further exacerbated for dialectal content, which is more prone to noise and lacks a standard orthography. The morphological features can be lexicalized, like lemmas and diacritized forms, or non-lexicalized, like gender, number, and part-of-speech tags, among others. Joint modeling of the lexicalized and non-lexicalized features can identify more intricate morphological patterns, which provide better context modeling, and further disambiguate ambiguous lexical choices. However, the different modeling granularity can make joint modeling more difficult. Our approach models the different features jointly, whether lexicalized (on the character-level), or non-lexicalized (on the word-level). We use Arabic as a test case, and achieve state-of-the-art results for Modern Standard Arabic with 20% relative error reduction, and Egyptian Arabic with 11% relative error reduction.",,,,ACL
737,2020,Phonetic and Visual Priors for Decipherment of Informal Romanization,"Maria Ryskina,Matthew R. Gormley,Taylor Berg-Kirkpatrick","Informal romanization is an idiosyncratic process used by humans in informal digital communication to encode non-Latin script languages into Latin character sets found on common keyboards. Character substitution choices differ between users but have been shown to be governed by the same main principles observed across a variety of languages—namely, character pairs are often associated through phonetic or visual similarity. We propose a noisy-channel WFST cascade model for deciphering the original non-Latin script from observed romanized text in an unsupervised fashion. We train our model directly on romanized data from two languages: Egyptian Arabic and Russian. We demonstrate that adding inductive bias through phonetic and visual priors on character mappings substantially improves the model’s performance on both languages, yielding results much closer to the supervised skyline. Finally, we introduce a new dataset of romanized Russian, collected from a Russian social network website and partially annotated for our experiments.",,,,ACL
738,2020,Active Learning for Coreference Resolution using Discrete Annotation,"Belinda Z. Li,Gabriel Stanovsky,Luke Zettlemoyer","We improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent. This simple modification, when combined with a novel mention clustering algorithm for selecting which examples to label, is much more efficient in terms of the performance obtained per annotation budget. In experiments with existing benchmark coreference datasets, we show that the signal from this additional question leads to significant performance gains per human-annotation hour. Future work can use our annotation protocol to effectively develop coreference models for new domains. Our code is publicly available.",,,,ACL
739,2020,Beyond Possession Existence: Duration and Co-Possession,"Dhivya Chinnappa,Srikala Murugan,Eduardo Blanco","This paper introduces two tasks: determining (a) the duration of possession relations and (b) co-possessions, i.e., whether multiple possessors possess a possessee at the same time. We present new annotations on top of corpora annotating possession existence and experimental results. Regarding possession duration, we derive the time spans we work with empirically from annotations indicating lower and upper bounds. Regarding co-possessions, we use a binary label. Cohen’s kappa coefficients indicate substantial agreement, and experimental results show that text is more useful than the image for solving these tasks.",,,,ACL
740,2020,Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks,"Suchin Gururangan,Ana Marasović,Swabha Swayamdipta,Kyle Lo","Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",,,,ACL
741,2020,Estimating Mutual Information Between Dense Word Embeddings,"Vitalii Zhelezniak,Aleksandar Savkov,Nils Hammerla","Word embedding-based similarity measures are currently among the top-performing methods on unsupervised semantic textual similarity (STS) tasks. Recent work has increasingly adopted a statistical view on these embeddings, with some of the top approaches being essentially various correlations (which include the famous cosine similarity). Another excellent candidate for a similarity measure is mutual information (MI), which can capture arbitrary dependencies between the variables and has a simple and intuitive expression. Unfortunately, its use in the context of dense word embeddings has so far been avoided due to difficulties with estimating MI for continuous data. In this work we go through a vast literature on estimating MI in such cases and single out the most promising methods, yielding a simple and elegant similarity measure for word embeddings. We show that mutual information is a viable alternative to correlations, gives an excellent signal that correlates well with human judgements of similarity and rivals existing state-of-the-art unsupervised methods.",,,,ACL
742,2020,Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing,"Alane Suhr,Ming-Wei Chang,Peter Shaw,Kenton Lee","We study the task of cross-database semantic parsing (XSP), where a system that maps natural language utterances to executable SQL queries is evaluated on databases unseen during training. Recently, several datasets, including Spider, were proposed to support development of XSP systems. We propose a challenging evaluation setup for cross-database semantic parsing, focusing on variation across database schemas and in-domain language use. We re-purpose eight semantic parsing datasets that have been well-studied in the setting where in-domain training data is available, and instead use them as additional evaluation data for XSP systems instead. We build a system that performs well on Spider, and find that it struggles to generalize to our re-purposed set. Our setup uncovers several generalization challenges for cross-database semantic parsing, demonstrating the need to use and develop diverse training and evaluation datasets.",,,,ACL
743,2020,Predicting the Focus of Negation: Model and Error Analysis,"Md Mosharaf Hossain,Kathleen Hamilton,Alexis Palmer,Eduardo Blanco","The focus of a negation is the set of tokens intended to be negated, and a key component for revealing affirmative alternatives to negated utterances. In this paper, we experiment with neural networks to predict the focus of negation. Our main novelty is leveraging a scope detector to introduce the scope of negation as an additional input to the network. Experimental results show that doing so obtains the best results to date. Additionally, we perform a detailed error analysis providing insights into the main error categories, and analyze errors depending on whether the model takes into account scope and context information.",,,,ACL
744,2020,Structured Tuning for Semantic Role Labeling,"Tao Li,Parth Anand Jawale,Martha Palmer,Vivek Srikumar","Recent neural network-driven semantic role labeling (SRL) systems have shown impressive improvements in F1 scores. These improvements are due to expressive input representations, which, at least at the surface, are orthogonal to knowledge-rich constrained decoding mechanisms that helped linear SRL models. Introducing the benefits of structure to inform neural models presents a methodological challenge. In this paper, we present a structured tuning framework to improve models using softened constraints only at training time. Our framework leverages the expressiveness of neural networks and provides supervision with structured loss components. We start with a strong baseline (RoBERTa) to validate the impact of our approach, and show that our framework outperforms the baseline by learning to comply with declarative constraints. Additionally, our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios.",,,,ACL
745,2020,TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data,"Pengcheng Yin,Graham Neubig,Wen-tau Yih,Sebastian Riedel","Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider.",,,,ACL
746,2020,Universal Decompositional Semantic Parsing,"Elias Stengel-Eskin,Aaron Steven White,Sheng Zhang,Benjamin Van Durme","We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS) representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attribute scores. We also introduce a strong pipeline model for parsing into the UDS graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction. By analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups.",,,,ACL
747,2020,Unsupervised Cross-lingual Representation Learning at Scale,"Alexis Conneau,Kartikay Khandelwal,Naman Goyal,Vishrav Chaudhary","This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",,,,ACL
748,2020,A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization,"Dongfang Xu,Zeyu Zhang,Steven Bethard","Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large. In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology. In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT. The ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any concept, not just those seen during training. We further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training. Our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets.",,,,ACL
749,2020,Hierarchical Entity Typing via Multi-level Learning to Rank,"Tongfei Chen,Yunmo Chen,Benjamin Van Durme","We propose a novel method for hierarchical entity classification that embraces ontological structure at both training and during prediction. At training, our novel multi-level learning-to-rank loss compares positive types against negative siblings according to the type tree. During prediction, we define a coarse-to-fine decoder that restricts viable candidates at each level of the ontology based on already predicted parent type(s). Our approach significantly outperform prior work on strict accuracy, demonstrating the effectiveness of our method.",,,,ACL
750,2020,Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference,"Jing Wang,Mayank Kulkarni,Daniel Preotiuc-Pietro","Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input. However, domain transfer of NER models with data from multiple genres has not been widely studied. To this end, we conduct NER experiments in three predictive setups on data from: a) multiple domains; b) multiple domains where the genre label is unknown at inference time; c) domains not encountered in training. We introduce a new architecture tailored to this task by using shared and private domain parameters and multi-task learning. This consistently outperforms all other baseline and competitive methods on all three experimental setups, with differences ranging between +1.95 to +3.11 average F1 across multiple genres when compared to standard approaches. These results illustrate the challenges that need to be taken into account when building real-world NLP applications that are robust to various types of text and the methods that can help, at least partially, alleviate these issues.",,,,ACL
751,2020,TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories,"Giannis Karamanolakis,Jun Ma,Xin Luna Dong","Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce. State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Commerce scenarios, which often contain thousands of diverse categories. This paper proposes TXtract, a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy. Through category conditional self-attention and multi-task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts category-specific attribute values. Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state-of-the-art approaches by up to 10% in F1 and 15% in coverage across all categories.",,,,ACL
752,2020,TriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition,"Bill Yuchen Lin,Dong-Ho Lee,Ming Shen,Ryan Moreno","Training neural models for named entity recognition (NER) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect. Thus, a crucial research question is how to obtain supervision in a cost-effective way. In this paper, we introduce “entity triggers,” an effective proxy of human explanations for facilitating label-efficient learning of NER models. An entity trigger is defined as a group of words in a sentence that helps to explain why humans would recognize an entity in the sentence. We crowd-sourced 14k entity triggers for two well-studied NER datasets. Our proposed model, Trigger Matching Network, jointly learns trigger representations and soft matching module with self-attention such that can generalize to unseen sentences easily for tagging. Our framework is significantly more cost-effective than the traditional neural NER frameworks. Experiments show that using only 20% of the trigger-annotated sentences results in a comparable performance as using 70% of conventional annotated sentences.",,,,ACL
753,2020,Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation,"Arya D. McCarthy,Xian Li,Jiatao Gu,Ning Dong","This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs). It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings. Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task. As a result, the proposed model yields improved translation quality compared to existing variational NMT models on WMT Ro↔En and De↔En. With latent variables being effectively utilized, our model demonstrates improved robustness over non-latent Transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 BLEU), and training with weakly aligned web-mined parallel data (up to +4.7 BLEU).",,,,ACL
754,2020,Balancing Training for Multilingual Neural Machine Translation,"Xinyi Wang,Yulia Tsvetkov,Graham Neubig","When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.",,,,ACL
755,2020,Evaluating Robustness to Input Perturbations for Neural Machine Translation,"Xing Niu,Prashant Mathur,Georgiana Dinu,Yaser Al-Onaizan",Neural Machine Translation (NMT) models are sensitive to small perturbations in the input. Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input. This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input. We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed. Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used.,,,,ACL
756,2020,Parallel Corpus Filtering via Pre-trained Language Models,"Boliang Zhang,Ajay Nagesh,Kevin Knight","Web-crawled data provides a good source of parallel corpora for training machine translation models. It is automatically obtained, but extremely noisy, and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods. In this paper, we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models. We measure sentence parallelism by leveraging the multilingual capability of BERT and use the Generative Pre-training (GPT) language model as a domain filter to balance data domains. We evaluate the proposed method on the WMT 2018 Parallel Corpus Filtering shared task, and on our own web-crawled Japanese-Chinese parallel corpus. Our method significantly outperforms baselines and achieves a new state-of-the-art. In an unsupervised setting, our method achieves comparable performance to the top-1 supervised method. We also evaluate on a web-crawled Japanese-Chinese parallel corpus that we make publicly available.",,,,ACL
757,2020,Regularized Context Gates on Transformer for Machine Translation,"Xintong Li,Lemao Liu,Rui Wang,Guoping Huang","Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN. This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer. In addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline.",,,,ACL
758,2020,A Multi-Perspective Architecture for Semantic Code Search,"Rajarshi Haldar,Lingfei Wu,JinJun Xiong,Julia Hockenmaier","The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multi-perspective cross-lingual neural framework for code–text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space.",,,,ACL
759,2020,Automated Topical Component Extraction Using Neural Network Attention Scores from Source-based Essay Scoring,"Haoran Zhang,Diane Litman","While automated essay scoring (AES) can reliably grade essays at scale, automated writing evaluation (AWE) additionally provides formative feedback to guide essay revision. However, a neural AES typically does not provide useful feature representations for supporting AWE. This paper presents a method for linking AWE and neural AES, by extracting Topical Components (TCs) representing evidence from a source text using the intermediate output of attention layers. We evaluate performance using a feature-based AES requiring TCs. Results show that performance is comparable whether using automatically or manually constructed TCs for 1) representing essays as rubric-based features, 2) grading essays.",,,,ACL
760,2020,Clinical Concept Linking with Contextualized Neural Representations,"Elliot Schumacher,Andriy Mulyar,Mark Dredze","In traditional approaches to entity linking, linking decisions are based on three sources of information – the similarity of the mention string to an entity’s name, the similarity of the context of the document to the entity, and broader information about the knowledge base (KB). In some domains, there is little contextual information present in the KB and thus we rely more heavily on mention string similarity. We consider one example of this, concept linking, which seeks to link mentions of medical concepts to a medical concept ontology. We propose an approach to concept linking that leverages recent work in contextualized neural models, such as ELMo (Peters et al. 2018), which create a token representation that integrates the surrounding context of the mention and concept name. We find a neural ranking approach paired with contextualized embeddings provides gains over a competitive baseline (Leaman et al. 2013). Additionally, we find that a pre-training step using synonyms from the ontology offers a useful initialization for the ranker.",,,,ACL
761,2020,DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking,"Christopher Hidey,Tuhin Chakrabarty,Tariq Alhindi,Siddharth Varia","The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence. The Fact Extraction and VERification (FEVER) dataset provides such a resource for evaluating endto- end fact-checking, requiring retrieval of evidence from Wikipedia to validate a veracity prediction. We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-checking – multiple propositions, temporal reasoning, and ambiguity and lexical variation – and introduce a resource with these types of claims. Then we present a system designed to be resilient to these “attacks” using multiple pointer networks for document selection and jointly modeling a sequence of evidence sentences and veracity relation predictions. We find that in handling these attacks we obtain state-of-the-art results on FEVER, largely due to improved evidence retrieval.",,,,ACL
762,2020,Let Me Choose: From Verbal Context to Font Selection,"Amirreza Shirani,Franck Dernoncourt,Jose Echevarria,Paul Asente","In this paper, we aim to learn associations between visual attributes of fonts and the verbal context of the texts they are typically applied to. Compared to related work leveraging the surrounding visual context, we choose to focus only on the input text, which can enable new applications for which the text is the only visual element in the document. We introduce a new dataset, containing examples of different topics in social media posts and ads, labeled through crowd-sourcing. Due to the subjective nature of the task, multiple fonts might be perceived as acceptable for an input text, which makes this problem challenging. To this end, we investigate different end-to-end models to learn label distributions on crowd-sourced data, to capture inter-subjectivity across all annotations.",,,,ACL
763,2020,Multi-Label and Multilingual News Framing Analysis,"Afra Feyza Akyürek,Lei Guo,Randa Elanwar,Prakash Ishwar","News framing refers to the practice in which aspects of specific issues are highlighted in the news to promote a particular interpretation. In NLP, although recent works have studied framing in English news, few have studied how the analysis can be extended to other languages and in a multi-label setting. In this work, we explore multilingual transfer learning to detect multiple frames from just the news headline in a genuinely low-resource context where there are few/no frame annotations in the target language. We propose a novel method that can leverage elementary resources consisting of a dictionary and few annotations to detect frames in the target language. Our method performs comparably or better than translating the entire target language headline to the source language for which we have annotated data. This work opens up an exciting new capability of scaling up frame analysis to many languages, even those without existing translation technologies. Lastly, we apply our method to detect frames on the issue of U.S. gun violence in multiple languages and obtain exciting insights on the relationship between different frames of the same problem across different countries with different languages.",,,,ACL
764,2020,Predicting Performance for Natural Language Processing Tasks,"Mengzhou Xia,Antonios Anastasopoulos,Ruochen Xu,Yiming Yang","Given the complexity of combinations of tasks, languages, and domains in natural language processing (NLP) research, it is computationally prohibitive to exhaustively test newly proposed models on each possible experimental setting. In this work, we attempt to explore the possibility of gaining plausible judgments of how well an NLP model can perform under an experimental setting, without actually training or testing the model. To do so, we build regression models to predict the evaluation score of an NLP experiment given the experimental settings as input. Experimenting on~9 different NLP tasks, we find that our predictors can produce meaningful predictions over unseen languages and different modeling architectures, outperforming reasonable baselines as well as human experts. %we represent experimental settings using an array of features. Going further, we outline how our predictor can be used to find a small subset of representative experiments that should be run in order to obtain plausible predictions for all other experimental settings.",,,,ACL
765,2020,ScriptWriter: Narrative-Guided Script Generation,"Yutao Zhu,Ruihua Song,Zhicheng Dou,Jian-Yun Nie","It is appealing to have a system that generates a story or scripts automatically from a storyline, even though this is still out of our reach. In dialogue systems, it would also be useful to drive dialogues by a dialogue plan. In this paper, we address a key problem involved in these applications - guiding a dialogue by a narrative. The proposed model ScriptWriter selects the best response among the candidates that fit the context as well as the given narrative. It keeps track of what in the narrative has been said and what is to be said. A narrative plays a different role than the context (i.e., previous utterances), which is generally used in current dialogue systems. Due to the unavailability of data for this new application, we construct a new large-scale data collection GraphMovie from a movie website where end- users can upload their narratives freely when watching a movie. Experimental results on the dataset show that our proposed approach based on narratives significantly outperforms the baselines that simply use the narrative as a kind of context.",,,,ACL
766,2020,Should All Cross-Lingual Embeddings Speak English?,"Antonios Anastasopoulos,Graham Neubig","Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.",,,,ACL
767,2020,Smart To-Do: Automatic Generation of To-Do Items from Emails,"Sudipto Mukherjee,Subhabrata Mukherjee,Marcello Hasegawa,Ahmed Hassan Awadallah","Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks. In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails. We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action. We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining BLEU and ROUGE scores of 0.23 and 0.63 for this task. To the best of our knowledge, this is the first work to address the problem of composing To-Do items from emails.",,,,ACL
768,2020,Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition,"Paloma Jeretic,Alex Warstadt,Suvrat Bhooshan,Adina Williams","Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudied. We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of 32K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types. We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although MultiNLI appears to contain very few pairs illustrating these inference types, we find that BERT learns to draw pragmatic inferences. It reliably treats scalar implicatures triggered by “some” as entailments. For some presupposition triggers like “only”, BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. BOW and InferSent show weaker evidence of pragmatic reasoning. We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.",,,,ACL
769,2020,End-to-End Bias Mitigation by Modelling Biases in Corpora,"Rabeeh Karimi Mahabadi,Yonatan Belinkov,James Henderson","Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. During training, the bias-only models’ predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets. Our code and data are publicly available in https://github.com/rabeehk/robust-nli.",,,,ACL
770,2020,Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance,"Prasetya Ajie Utama,Nafise Sadat Moosavi,Iryna Gurevych","Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution. Recently, several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance. However, their improvements come at the expense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity. This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the resulting models on broader types of examples beyond the small subset represented in the out-of-distribution data. In this paper, we address this trade-off by introducing a novel debiasing method, called confidence regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy.",,,,ACL
771,2020,NILE : Natural Language Inference with Faithful Natural Language Explanations,"Sawan Kumar,Partha Talukdar","The recent growth in the popularity and success of deep learning models on NLP classification tasks has accompanied the need for generating some form of natural language explanation of the predicted labels. Such generated natural language (NL) explanations are expected to be faithful, i.e., they should correlate well with the model’s internal decision making. In this work, we focus on the task of natural language inference (NLI) and address the following question: can we build NLI systems which produce labels with high accuracy, while also generating faithful explanations of its decisions? We propose Natural-language Inference over Label-specific Explanations (NILE), a novel NLI method which utilizes auto-generated label-specific NL explanations to produce labels along with its faithful explanation. We demonstrate NILE’s effectiveness over previously reported methods through automated and human evaluation of the produced labels and explanations. Our evaluation of NILE also supports the claim that accurate systems capable of providing testable explanations of their decisions can be designed. We discuss the faithfulness of NILE’s explanations in terms of sensitivity of the decisions to the corresponding explanations. We argue that explicit evaluation of faithfulness, in addition to label and explanation accuracy, is an important step in evaluating model’s explanations. Further, we demonstrate that task-specific probes are necessary to establish such sensitivity.",,,,ACL
772,2020,QuASE: Question-Answer Driven Sentence Encoding,"Hangfeng He,Qiang Ning,Dan Roth","Question-answering (QA) data often encodes essential information in many facets. This paper studies a natural question: Can we get supervision from QA data for other tasks (typically, non-QA ones)? For example, can we use QAMR (Michael et al., 2017) to improve named entity recognition? We suggest that simply further pre-training BERT is often not the best option, and propose the question-answer driven sentence encoding (QuASE) framework. QuASE learns representations from QA data, using BERT or other state-of-the-art contextual language models. In particular, we observe the need to distinguish between two types of sentence encodings, depending on whether the target task is a single- or multi-sentence input; in both cases, the resulting encoding is shown to be an easy-to-use plugin for many downstream tasks. This work may point out an alternative way to supervise NLP tasks.",,,,ACL
773,2020,Towards Robustifying NLI Models Against Lexical Dataset Biases,"Xiang Zhou,Mohit Bansal","While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics. Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases. First, we debias the dataset through data augmentation and enhancement, but show that the model bias cannot be fully removed via this method. Next, we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance. The first approach aims to remove the label bias at the embedding level. The second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models. We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy.",,,,ACL
774,2020,Uncertain Natural Language Inference,"Tongfei Chen,Zhengping Jiang,Adam Poliak,Keisuke Sakaguchi","We introduce Uncertain Natural Language Inference (UNLI), a refinement of Natural Language Inference (NLI) that shifts away from categorical labels, targeting instead the direct prediction of subjective probability assessments. We demonstrate the feasibility of collecting annotations for UNLI by relabeling a portion of the SNLI dataset under a probabilistic scale, where items even with the same categorical label differ in how likely people judge them to be true given a premise. We describe a direct scalar regression modeling approach, and find that existing categorically-labeled NLI data can be used in pre-training. Our best models correlate well with humans, demonstrating models are capable of more subtle inferences than the categorical bin assignment employed in current NLI tasks.",,,,ACL
775,2020,"Extracting Headless MWEs from Dependency Parse Trees: Parsing, Tagging, and Joint Modeling Approaches","Tianze Shi,Lillian Lee","An interesting and frequent type of multi-word expression (MWE) is the headless MWE, for which there are no true internal syntactic dominance relations; examples include many named entities (“Wells Fargo”) and dates (“July 5, 2020”) as well as certain productive constructions (“blow for blow”, “day after day”). Despite their special status and prevalence, current dependency-annotation schemes require treating such flat structures as if they had internal syntactic heads, and most current parsers handle them in the same fashion as headed constructions. Meanwhile, outside the context of parsing, taggers are typically used for identifying MWEs, but taggers might benefit from structural information. We empirically compare these two common strategies—parsing and tagging—for predicting flat MWEs. Additionally, we propose an efficient joint decoding algorithm that combines scores from both strategies. Experimental results on the MWE-Aware English Dependency Corpus and on six non-English dependency treebanks with frequent flat structures show that: (1) tagging is more accurate than parsing for identifying flat-structure MWEs, (2) our joint decoder reconciles the two different views and, for non-BERT features, leads to higher accuracies, and (3) most of the gains result from feature sharing between the parsers and taggers.",,,,ACL
776,2020,Revisiting Higher-Order Dependency Parsers,"Erick Fonseca,André F. T. Martins","Neural encoders have allowed dependency parsers to shift from higher-order structured models to simpler first-order ones, making decoding faster and still achieving better accuracy than non-neural parsers. This has led to a belief that neural encoders can implicitly encode structural constraints, such as siblings and grandparents in a tree. We tested this hypothesis and found that neural parsers may benefit from higher-order features, even when employing a powerful pre-trained encoder, such as BERT. While the gains of higher-order features are small in the presence of a powerful encoder, they are consistent for long-range dependencies and long sentences. In particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures.",,,,ACL
777,2020,SeqVAT: Virtual Adversarial Training for Semi-Supervised Sequence Labeling,"Luoxin Chen,Weitong Ruan,Xinyue Liu,Jianhua Lu","Virtual adversarial training (VAT) is a powerful technique to improve model robustness in both supervised and semi-supervised settings. It is effective and can be easily adopted on lots of image classification and text classification tasks. However, its benefits to sequence labeling tasks such as named entity recognition (NER) have not been shown as significant, mostly, because the previous approach can not combine VAT with the conditional random field (CRF). CRF can significantly boost accuracy for sequence models by putting constraints on label transitions, which makes it an essential component in most state-of-the-art sequence labeling model architectures. In this paper, we propose SeqVAT, a method which naturally applies VAT to sequence labeling models with CRF. Empirical studies show that SeqVAT not only significantly improves the sequence labeling performance over baselines under supervised settings, but also outperforms state-of-the-art approaches under semi-supervised settings.",,,,ACL
778,2020,Treebank Embedding Vectors for Out-of-Domain Dependency Parsing,"Joachim Wagner,James Barry,Jennifer Foster","A recent advance in monolingual dependency parsing is the idea of a treebank embedding vector, which allows all treebanks for a particular language to be used as training data while at the same time allowing the model to prefer training data from one treebank over others and to select the preferred treebank at test time. We build on this idea by 1) introducing a method to predict a treebank vector for sentences that do not come from a treebank used in training, and 2) exploring what happens when we move away from predefined treebank embedding vectors during test time and instead devise tailored interpolations. We show that 1) there are interpolated vectors that are superior to the predefined ones, and 2) treebank vectors can be predicted with sufficient accuracy, for nine out of ten test languages, to match the performance of an oracle approach that knows the most suitable predefined treebank embedding for the test set.",,,,ACL
