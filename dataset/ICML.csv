,year,title,author,abstract,link,pdf,name
0,2019,PAC Learnability of Node Functions in Networked Dynamical Systems,"Abhijin Adiga,         Chris J Kuhlman,         Madhav Marathe,         S Ravi,         Anil Vullikanti","We consider the PAC learnability of the local functions at the vertices of a discrete networked dynamical system, assuming that the underlying network is known. Our focus is on the learnability of threshold functions. We show that several variants of threshold functions are PAC learnable and provide tight bounds on the sample complexity. In general, when the input consists of positive and negative examples, we show that the concept class of threshold functions is not efficiently PAC learnable, unless NP = RP. Using a dynamic programming approach, we show efficient PAC learnability when the number of negative examples is small. We also present an efficient learner which is consistent with all the positive examples and at least (1-1/e) fraction of the negative examples. This algorithm is based on maximizing a submodular function under matroid constraints. By performing experiments on both synthetic and real-world networks, we study how the network structure and sample complexity influence the quality of the inferred system.",http://proceedings.mlr.press/v97/adiga19a.html,http://proceedings.mlr.press/v97/adiga19a/adiga19a.pdf,ICML
1,2019,Finding Mixed Nash Equilibria of Generative Adversarial Networks,"Ya-Ping Hsieh,         Chen Liu,         Volkan Cevher","Generative adversarial networks (GANs) are known to achieve the state-of-the-art performance on various generative tasks, but these results come at the expense of a notoriously difficult training phase. Current training strategies typically draw a connection to optimization theory, whose scope is restricted to local convergence due to the presence of non-convexity. In this work, we tackle the training of GANs by rethinking the problem formulation from the mixed Nash Equilibria (NE) perspective. Via a classical lifting trick, we show that essentially all existing GAN objectives can be relaxed into their mixed strategy forms, whose global optima can be solved via sampling, in contrast to the exclusive use of optimization framework in previous work. We further propose a mean-approximation sampling scheme, which allows to systematically exploit methods for bi-affine games to delineate novel, practical training algorithms of GANs. Finally, we provide experimental evidence that our approach yields comparable or superior results to contemporary training algorithms, and outperforms classical methods such as SGD, Adam, and RMSProp.",http://proceedings.mlr.press/v97/hsieh19b.html,http://proceedings.mlr.press/v97/hsieh19b/hsieh19b.pdf,ICML
2,2019,Nonparametric Bayesian Deep Networks with Local Competition,"Konstantinos Panousis,         Sotirios Chatzis,         Sergios Theodoridis","The aim of this work is to enable inference of deep networks that retain high accuracy for the least possible model complexity, with the latter deduced from the data during inference. To this end, we revisit deep networks that comprise competing linear units, as opposed to nonlinear units that do not entail any form of (local) competition. In this context, our main technical innovation consists in an inferential setup that leverages solid arguments from Bayesian nonparametrics. We infer both the needed set of connections or locally competing sets of units, as well as the required floating-point precision for storing the network parameters. Specifically, we introduce auxiliary discrete latent variables representing which initial network components are actually needed for modeling the data at hand, and perform Bayesian inference over them by imposing appropriate stick-breaking priors. As we experimentally show using benchmark datasets, our approach yields networks with less computational footprint than the state-of-the-art, and with no compromises in predictive accuracy.",http://proceedings.mlr.press/v97/panousis19a.html,http://proceedings.mlr.press/v97/panousis19a/panousis19a.pdf,ICML
3,2019,Improved Dynamic Graph Learning through Fault-Tolerant Sparsification,"Chunjiang Zhu,         Sabine Storandt,         Kam-Yiu Lam,         Song Han,         Jinbo Bi","Graph sparsification has been used to improve the computational cost of learning over graphs, e.g., Laplacian-regularized estimation and graph semi-supervised learning (SSL). However, when graphs vary over time, repeated sparsification requires polynomial order computational cost per update. We propose a new type of graph sparsification namely fault-tolerant (FT) sparsification to significantly reduce the cost to only a constant. Then the computational cost of subsequent graph learning tasks can be significantly improved with limited loss in their accuracy. In particular, we give theoretical analyze to upper bound the loss in the accuracy of the subsequent Laplacian-regularized estimation and graph SSL, due to the FT sparsification. In addition, FT spectral sparsification can be generalized to FT cut sparsification, for cut-based graph learning. Extensive experiments have confirmed the computational efficiencies and accuracies of the proposed methods for learning on dynamic graphs.",http://proceedings.mlr.press/v97/zhu19b.html,http://proceedings.mlr.press/v97/zhu19b/zhu19b.pdf,ICML
4,2019,Tensor Variable Elimination for Plated Factor Graphs,"Fritz Obermeyer,         Eli Bingham,         Martin Jankowiak,         Neeraj Pradhan,         Justin Chiu,         Alexander Rush,         Noah Goodman","A wide class of machine learning algorithms can be reduced to variable elimination on factor graphs. While factor graphs provide a unifying notation for these algorithms, they do not provide a compact way to express repeated structure when compared to plate diagrams for directed graphical models. To exploit efficient tensor algebra in graphs with plates of variables, we generalize undirected factor graphs to plated factor graphs and variable elimination to a tensor variable elimination algorithm that operates directly on plated factor graphs. Moreover, we generalize complexity bounds based on treewidth and characterize the class of plated factor graphs for which inference is tractable. As an application, we integrate tensor variable elimination into the Pyro probabilistic programming language to enable exact inference in discrete latent variable models with repeated structure. We validate our methods with experiments on both directed and undirected graphical models, including applications to polyphonic music modeling, animal movement modeling, and latent sentiment analysis.",http://proceedings.mlr.press/v97/obermeyer19a.html,http://proceedings.mlr.press/v97/obermeyer19a/obermeyer19a.pdf,ICML
5,2019,The Evolved Transformer,"David So,         Quoc Le,         Chen Liang","Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments – the Evolved Transformer – demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT’14 English-German; at smaller sizes, it achieves the same quality as the original ""big"" Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.",http://proceedings.mlr.press/v97/so19a.html,http://proceedings.mlr.press/v97/so19a/so19a.pdf,ICML
6,2019,On Sparse Linear Regression in the Local Differential Privacy Model,"Di Wang,         Jinhui Xu","In this paper, we study the sparse linear regression problem under the Local Differential Privacy (LDP) model. We first show that polynomial dependency on the dimensionality pp of the space is unavoidable for the estimation error in both non-interactive and sequential interactive local models, if the privacy of the whole dataset needs to be preserved. Similar limitations also exist for other types of error measurements and in the relaxed local models. This indicates that differential privacy in high dimensional space is unlikely achievable for the problem. With the understanding of this limitation, we then present two algorithmic results. The first one is a sequential interactive LDP algorithm for the low dimensional sparse case, called Locally Differentially Private Iterative Hard Thresholding (LDP-IHT), which achieves a near optimal upper bound. This algorithm is actually rather general and can be used to solve quite a few other problems, such as (Local) DP-ERM with sparsity constraints and sparse regression with non-linear measurements. The second one is for the restricted (high dimensional) case where only the privacy of the responses (labels) needs to be preserved. For this case, we show that the optimal rate of the error estimation can be made logarithmically depending on pp (i.e., logp\log p) in the local model, where an upper bound is obtained by a label-privacy version of LDP-IHT. Experiments on real world and synthetic datasets confirm our theoretical analysis.",http://proceedings.mlr.press/v97/wang19m.html,http://proceedings.mlr.press/v97/wang19m/wang19m.pdf,ICML
7,2019,Warm-starting Contextual Bandits: Robustly Combining Supervised and Bandit Feedback,"Chicheng Zhang,         Alekh Agarwal,         Hal Daumé Iii,         John Langford,         Sahand Negahban","We investigate the feasibility of learning from both fully-labeled supervised data and contextual bandit data. We specifically consider settings in which the underlying learning signal may be different between these two data sources. Theoretically, we state and prove no-regret algorithms for learning that is robust to divergences between the two sources. Empirically, we evaluate some of these algorithms on a large selection of datasets, showing that our approaches are feasible, and helpful in practice.",http://proceedings.mlr.press/v97/zhang19b.html,http://proceedings.mlr.press/v97/zhang19b/zhang19b.pdf,ICML
8,2019,AutoVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss,"Kaizhi Qian,         Yang Zhang,         Shiyu Chang,         Xuesong Yang,         Mark Hasegawa-Johnson","Despite the progress in voice conversion, many-to-many voice conversion trained on non-parallel data, as well as zero-shot voice conversion, remains under-explored. Deep style transfer algorithms, generative adversarial networks (GAN) in particular, are being applied as new solutions in this field. However, GAN training is very sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on self-reconstruction loss. Based on this scheme, we proposed AutoVC, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.",http://proceedings.mlr.press/v97/qian19c.html,http://proceedings.mlr.press/v97/qian19c/qian19c.pdf,ICML
9,2019,Similarity of Neural Network Representations Revisited,"Simon Kornblith,         Mohammad Norouzi,         Honglak Lee,         Geoffrey Hinton","Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.",http://proceedings.mlr.press/v97/kornblith19a.html,http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf,ICML
10,2019,Agnostic Federated Learning,"Mehryar Mohri,         Gary Sivek,         Ananda Theertha Suresh","A key learning scenario in large-scale applications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we propose a new framework of agnostic federated learning, where the centralized model is optimized for any target distribution formed by a mixture of the client distributions. We further show that this framework naturally yields a notion of fairness. We present data-dependent Rademacher complexity guarantees for learning with this objective, which guide the definition of an algorithm for agnostic federated learning. We also give a fast stochastic optimization algorithm for solving the corresponding optimization problem, for which we prove convergence bounds, assuming a convex loss function and a convex hypothesis set. We further empirically demonstrate the benefits of our approach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other learning scenarios such as cloud computing, domain adaptation, drifting, and other contexts where the training and test distributions do not coincide.",http://proceedings.mlr.press/v97/mohri19a.html,http://proceedings.mlr.press/v97/mohri19a/mohri19a.pdf,ICML
11,2019,Imitating Latent Policies from Observation,"Ashley Edwards,         Himanshu Sahni,         Yannick Schroecker,         Charles Isbell","In this paper, we describe a novel approach to imitation learning that infers latent policies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment interactions to determine a mapping between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that it performs better than standard approaches. Code for this work is available at https://github.com/ashedwards/ILPO.",http://proceedings.mlr.press/v97/edwards19a.html,http://proceedings.mlr.press/v97/edwards19a/edwards19a.pdf,ICML
12,2019,Maximum Entropy-Regularized Multi-Goal Reinforcement Learning,"Rui Zhao,         Xudong Sun,         Volker Tresp","In Multi-Goal Reinforcement Learning, an agent learns to achieve multiple goals with a goal-conditioned policy. During learning, the agent first collects the trajectories into a replay buffer, and later these trajectories are selected randomly for replay. However, the achieved goals in the replay buffer are often biased towards the behavior policies. From a Bayesian perspective, when there is no prior knowledge about the target goal distribution, the agent should learn uniformly from diverse achieved goals. Therefore, we first propose a novel multi-goal RL objective based on weighted entropy. This objective encourages the agent to maximize the expected return, as well as to achieve more diverse goals. Secondly, we developed a maximum entropy-based prioritization framework to optimize the proposed objective. For evaluation of this framework, we combine it with Deep Deterministic Policy Gradient, both with or without Hindsight Experience Replay. On a set of multi-goal robotic tasks of OpenAI Gym, we compare our method with other baselines and show promising improvements in both performance and sample-efficiency.",http://proceedings.mlr.press/v97/zhao19d.html,http://proceedings.mlr.press/v97/zhao19d/zhao19d.pdf,ICML
13,2019,Sublinear quantum algorithms for training linear and kernel-based classifiers,"Tongyang Li,         Shouvanik Chakrabarti,         Xiaodi Wu","We investigate quantum algorithms for classification, a fundamental problem in machine learning, with provable guarantees. Given nnn ddd-dimensional data points, the state-of-the-art (and optimal) classical algorithm for training classifiers with constant margin by Clarkson et al. runs in O~(n+d)O~(n+d)\tilde{O}(n +d), which is also optimal in its input/output model. We design sublinear quantum algorithms for the same task running in O~(n−−√+d−−√)O~(n+d)\tilde{O}(\sqrt{n} +\sqrt{d}), a quadratic improvement in both nnn and ddd. Moreover, our algorithms use the standard quantization of the classical input and generate the same classical output, suggesting minimal overheads when used as subroutines for end-to-end applications. We also demonstrate a tight lower bound (up to poly-log factors) and discuss the possibility of implementation on near-term quantum machines.",http://proceedings.mlr.press/v97/li19b.html,http://proceedings.mlr.press/v97/li19b/li19b.pdf,ICML
14,2019,Approximating Orthogonal Matrices with Effective Givens Factorization,"Thomas Frerix,         Joan Bruna","We analyze effective approximation of unitary matrices. In our formulation, a unitary matrix is represented as a product of rotations in two-dimensional subspaces, so-called Givens rotations. Instead of the quadratic dimension dependence when applying a dense matrix, applying such an approximation scales with the number factors, each of which can be implemented efficiently. Consequently, in settings where an approximation is once computed and then applied many times, such a representation becomes advantageous. Although effective Givens factorization is not possible for generic unitary operators, we show that minimizing a sparsity-inducing objective with a coordinate descent algorithm on the unitary group yields good factorizations for structured matrices. Canonical applications of such a setup are orthogonal basis transforms. We demonstrate numerical results of approximating the graph Fourier transform, which is the matrix obtained when diagonalizing a graph Laplacian.",http://proceedings.mlr.press/v97/frerix19a.html,http://proceedings.mlr.press/v97/frerix19a/frerix19a.pdf,ICML
15,2019,Variational Implicit Processes,"Chao Ma,         Yingzhen Li,         Jose Miguel Hernandez-Lobato","We introduce the implicit processes (IPs), a stochastic process that places implicitly defined multivariate distributions over any finite collections of random variables. IPs are therefore highly flexible implicit priors over functions, with examples including data simulators, Bayesian neural networks and non-linear transformations of stochastic processes. A novel and efficient approximate inference algorithm for IPs, namely the variational implicit processes (VIPs), is derived using generalised wake-sleep updates. This method returns simple update equations and allows scalable hyper-parameter learning with stochastic optimization. Experiments show that VIPs return better uncertainty estimates and lower errors over existing inference methods for challenging models such as Bayesian neural networks, and Gaussian processes.",http://proceedings.mlr.press/v97/ma19b.html,http://proceedings.mlr.press/v97/ma19b/ma19b.pdf,ICML
16,2019,Good Initializations of Variational Bayes for Deep Models,"Simone Rossi,         Pietro Michiardi,         Maurizio Filippone","Stochastic variational inference is an established way to carry out approximate Bayesian inference for deep models flexibly and at scale. While there have been effective proposals for good initializations for loss minimization in deep learning, far less attention has been devoted to the issue of initialization of stochastic variational inference. We address this by proposing a novel layer-wise initialization strategy based on Bayesian linear models. The proposed method is extensively validated on regression and classification tasks, including Bayesian Deep Nets and Conv Nets, showing faster and better convergence compared to alternatives inspired by the literature on initializations for loss minimization.",http://proceedings.mlr.press/v97/rossi19a.html,http://proceedings.mlr.press/v97/rossi19a/rossi19a.pdf,ICML
17,2019,Gradient Descent Finds Global Minima of Deep Neural Networks,"Simon Du,         Jason Lee,         Haochuan Li,         Liwei Wang,         Xiyu Zhai",Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.,http://proceedings.mlr.press/v97/du19c.html,http://proceedings.mlr.press/v97/du19c/du19c.pdf,ICML
18,2019,Neural Separation of Observed and Unobserved Distributions,"Tavi Halperin,         Ariel Ephrat,         Yedid Hoshen","Separating mixed distributions is a long standing challenge for machine learning and signal processing. Most current methods either rely on making strong assumptions on the source distributions or rely on having training samples of each source in the mixture. In this work, we introduce a new method—Neural Egg Separation—to tackle the scenario of extracting a signal from an unobserved distribution additively mixed with a signal from an observed distribution. Our method iteratively learns to separate the known distribution from progressively finer estimates of the unknown distribution. In some settings, Neural Egg Separation is initialization sensitive, we therefore introduce Latent Mixture Masking which ensures a good initialization. Extensive experiments on audio and image separation tasks show that our method outperforms current methods that use the same level of supervision, and often achieves similar performance to full supervision.",http://proceedings.mlr.press/v97/halperin19a.html,http://proceedings.mlr.press/v97/halperin19a/halperin19a.pdf,ICML
19,2019,Random Matrix Improved Covariance Estimation for a Large Class of Metrics,"Malik Tiomoko,         Romain Couillet,         Florent Bouchard,         Guillaume Ginolhac","Relying on recent advances in statistical estimation of covariance distances based on random matrix theory, this article proposes an improved covariance and precision matrix estimation for a wide family of metrics. The method is shown to largely outperform the sample covariance matrix estimate and to compete with state-of-the-art methods, while at the same time being computationally simpler and faster. Applications to linear and quadratic discriminant analyses also show significant gains, therefore suggesting practical interest to statistical machine learning.",http://proceedings.mlr.press/v97/tiomoko19a.html,http://proceedings.mlr.press/v97/tiomoko19a/tiomoko19a.pdf,ICML
20,2019,Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty,"Youngjin Kim,         Wontae Nam,         Hyunwoo Kim,         Ji-Hoon Kim,         Gunhee Kim","Exploration based on state novelty has brought great success in challenging reinforcement learning problems with sparse rewards. However, existing novelty-based strategies become inefficient in real-world problems where observation contains not only task-dependent state novelty of our interest but also task-irrelevant information that should be ignored. We introduce an information- theoretic exploration strategy named Curiosity-Bottleneck that distills task-relevant information from observation. Based on the information bottleneck principle, our exploration bonus is quantified as the compressiveness of observation with respect to the learned representation of a compressive value network. With extensive experiments on static image classification, grid-world and three hard-exploration Atari games, we show that Curiosity-Bottleneck learns an effective exploration strategy by robustly measuring the state novelty in distractive environments where state-of-the-art exploration methods often degenerate.",http://proceedings.mlr.press/v97/kim19c.html,http://proceedings.mlr.press/v97/kim19c/kim19c.pdf,ICML
21,2019,Unsupervised Deep Learning by Neighbourhood Discovery,"Jiabo Huang,         Qi Dong,         Shaogang Gong,         Xiatian Zhu","Deep convolutional neural networks (CNNs) have demonstrated remarkable success in computer vision by supervisedly learning strong visual feature representations. However, training CNNs relies heavily on the availability of exhaustive training data annotations, limiting significantly their deployment and scalability in many application scenarios. In this work, we introduce a generic unsupervised deep learning approach to training deep models without the need for any manual label supervision. Specifically, we progressively discover sample anchored/centred neighbourhoods to reason and learn the underlying class decision boundaries iteratively and accumulatively. Every single neighbourhood is specially formulated so that all the member samples can share the same unseen class labels at high probability for facilitating the extraction of class discriminative feature representations during training. Experiments on image classification show the performance advantages of the proposed method over the state-of-the-art unsupervised learning models on six benchmarks including both coarse-grained and fine-grained object image categorisation.",http://proceedings.mlr.press/v97/huang19b.html,http://proceedings.mlr.press/v97/huang19b/huang19b.pdf,ICML
22,2019,Generalized Majorization-Minimization,"Sobhan Naderi Parizi,         Kun He,         Reza Aghajani,         Stan Sclaroff,         Pedro Felzenszwalb","Non-convex optimization is ubiquitous in machine learning. Majorization-Minimization (MM) is a powerful iterative procedure for optimizing non-convex functions that works by optimizing a sequence of bounds on the function. In MM, the bound at each iteration is required to touch the objective function at the optimizer of the previous bound. We show that this touching constraint is unnecessary and overly restrictive. We generalize MM by relaxing this constraint, and propose a new optimization framework, named Generalized Majorization-Minimization (G-MM), that is more flexible. For instance, G-MM can incorporate application-specific biases into the optimization procedure without changing the objective function. We derive G-MM algorithms for several latent variable models and show empirically that they consistently outperform their MM counterparts in optimizing non-convex objectives. In particular, G-MM algorithms appear to be less sensitive to initialization.",http://proceedings.mlr.press/v97/parizi19a.html,http://proceedings.mlr.press/v97/parizi19a/parizi19a.pdf,ICML
23,2019,Gaining Free or Low-Cost Interpretability with Interpretable Partial Substitute,Tong Wang,"This work addresses the situation where a black-box model with good predictive performance is chosen over its interpretable competitors, and we show interpretability is still achievable in this case. Our solution is to find an interpretable substitute on a subset of data where the black-box model is overkill or nearly overkill while leaving the rest to the black-box. This transparency is obtained at minimal cost or no cost of the predictive performance. Under this framework, we develop a Hybrid Rule Sets (HyRS) model that uses decision rules to capture the subspace of data where the rules are as accurate or almost as accurate as the black-box provided. To train a HyRS, we devise an efficient search algorithm that iteratively finds the optimal model and exploits theoretically grounded strategies to reduce computation. Our framework is agnostic to the black-box during training. Experiments on structured and text data show that HyRS obtains an effective trade-off between transparency and interpretability.",http://proceedings.mlr.press/v97/wang19a.html,http://proceedings.mlr.press/v97/wang19a/wang19a.pdf,ICML
24,2019,Fairwashing: the risk of rationalization,"Ulrich Aivodji,         Hiromi Arai,         Olivier Fortineau,         Sébastien Gambs,         Satoshi Hara,         Alain Tapp","Black-box explanation is the problem of explaining how a machine learning model – whose internal logic is hidden to the auditor and generally complex – produces its outcomes. Current approaches for solving this problem include model explanation, outcome explanation as well as model inspection. While these techniques can be beneficial by providing interpretability, they can be used in a negative manner to perform fairwashing, which we define as promoting the false perception that a machine learning model respects some ethical values. In particular, we demonstrate that it is possible to systematically rationalize decisions taken by an unfair black-box model using the model explanation as well as the outcome explanation approaches with a given fairness metric. Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists approximating an unfair black-box model. We empirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.",http://proceedings.mlr.press/v97/aivodji19a.html,http://proceedings.mlr.press/v97/aivodji19a/aivodji19a.pdf,ICML
25,2019,Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation,"Ruohan Wang,         Carlo Ciliberto,         Pierluigi Vito Amadori,         Yiannis Demiris","We consider the problem of imitation learning from a finite set of expert trajectories, without access to reinforcement signals. The classical approach of extracting the expert’s reward function via inverse reinforcement learning, followed by reinforcement learning is indirect and may be computationally expensive. Recent generative adversarial methods based on matching the policy distribution between the expert and the agent could be unstable during training. We propose a new framework for imitation learning by estimating the support of the expert policy to compute a fixed reward function, which allows us to re-frame imitation learning within the standard reinforcement learning setting. We demonstrate the efficacy of our reward function on both discrete and continuous domains, achieving comparable or better performance than the state of the art under different reinforcement learning algorithms.",http://proceedings.mlr.press/v97/wang19d.html,http://proceedings.mlr.press/v97/wang19d/wang19d.pdf,ICML
26,2019,Trainable Decoding of Sets of Sequences for Neural Sequence Models,"Ashwin Kalyan,         Peter Anderson,         Stefan Lee,         Dhruv Batra","Many sequence prediction tasks admit multiple correct outputs and so, it is often useful to decode a set of outputs that maximize some task-specific set-level metric. However, retooling standard sequence prediction procedures tailored towards predicting the single best output leads to the decoding of sets containing very similar sequences; failing to capture the variation in the output space. To address this, we propose ∇∇\nablaBS, a trainable decoding procedure that outputs a set of sequences, highly valued according to the metric. Our method tightly integrates the training and decoding phases and further allows for the optimization of the task-specific metric addressing the shortcomings of standard sequence prediction. Further, we discuss the trade-offs of commonly used set-level metrics and motivate a new set-level metric that naturally evaluates the notion of “capturing the variation in the output space”. Finally, we show results on the image captioning task and find that our model outperforms standard techniques and natural ablations.",http://proceedings.mlr.press/v97/kalyan19a.html,http://proceedings.mlr.press/v97/kalyan19a/kalyan19a.pdf,ICML
27,2019,Fast Context Adaptation via Meta-Learning,"Luisa Zintgraf,         Kyriacos Shiarli,         Vitaly Kurin,         Katja Hofmann,         Shimon Whiteson","We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, only the context parameters are updated, leading to a low-dimensional task representation. We show empirically that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Our experiments also highlight weaknesses in current benchmarks, in that the amount of adaptation needed in some cases is small.",http://proceedings.mlr.press/v97/zintgraf19a.html,http://proceedings.mlr.press/v97/zintgraf19a/zintgraf19a.pdf,ICML
28,2019,Concrete Autoencoders: Differentiable Feature Selection and Reconstruction,"Muhammed Fatih Balın,         Abubakar Abid,         James Zou","We introduce the concrete autoencoder, an end-to-end differentiable method for global feature selection, which efficiently identifies a subset of the most informative features and simultaneously learns a neural network to reconstruct the input data from the selected features. Our method is unsupervised, and is based on using a concrete selector layer as the encoder and using a standard neural network as the decoder. During the training phase, the temperature of the concrete selector layer is gradually decreased, which encourages a user-specified number of discrete features to be learned; during test time, the selected features can be used with the decoder network to reconstruct the remaining input features. We evaluate concrete autoencoders on a variety of datasets, where they significantly outperform state-of-the-art methods for feature selection and data reconstruction. In particular, on a large-scale gene expression dataset, the concrete autoencoder selects a small subset of genes whose expression levels can be used to impute the expression levels of the remaining genes; in doing so, it improves on the current widely-used expert-curated L1000 landmark genes, potentially reducing measurement costs by 20%. The concrete autoencoder can be implemented by adding just a few lines of code to a standard autoencoder, and the code for the algorithm and experiments is publicly available.",http://proceedings.mlr.press/v97/balin19a.html,http://proceedings.mlr.press/v97/balin19a/balin19a.pdf,ICML
29,2019,Dead-ends and Secure Exploration in Reinforcement Learning,"Mehdi Fatemi,         Shikhar Sharma,         Harm Van Seijen,         Samira Ebrahimi Kahou","Many interesting applications of reinforcement learning (RL) involve MDPs that include numerous “dead-end"" states. Upon reaching a dead-end state, the agent continues to interact with the environment in a dead-end trajectory before reaching an undesired terminal state, regardless of whatever actions are chosen. The situation is even worse when existence of many dead-end states is coupled with distant positive rewards from any initial state (we term this as Bridge Effect). Hence, conventional exploration techniques often incur prohibitively many training steps before convergence. To deal with the bridge effect, we propose a condition for exploration, called security. We next establish formal results that translate the security condition into the learning problem of an auxiliary value function. This new value function is used to cap “any"" given exploration policy and is guaranteed to make it secure. As a special case, we use this theory and introduce secure random-walk. We next extend our results to the deep RL settings by identifying and addressing two main challenges that arise. Finally, we empirically compare secure random-walk with standard benchmarks in two sets of experiments including the Atari game of Montezuma’s Revenge.",http://proceedings.mlr.press/v97/fatemi19a.html,http://proceedings.mlr.press/v97/fatemi19a/fatemi19a.pdf,ICML
30,2019,Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN,"Dror Freirich,         Tzahi Shimkin,         Ron Meir,         Aviv Tamar","The recently proposed distributional approach to reinforcement learning (DiRL) is centered on learning the distribution of the reward-to-go, often referred to as the value distribution. In this work, we show that the distributional Bellman equation, which drives DiRL methods, is equivalent to a generative adversarial network (GAN) model. In this formulation, DiRL can be seen as learning a deep generative model of the value distribution, driven by the discrepancy between the distribution of the current value, and the distribution of the sum of current reward and next value. We use this insight to propose a GAN-based approach to DiRL, which leverages the strengths of GANs in learning distributions of high dimensional data. In particular, we show that our GAN approach can be used for DiRL with multivariate rewards, an important setting which cannot be tackled with prior methods. The multivariate setting also allows us to unify learning the distribution of values and state transitions, and we exploit this idea to devise a novel exploration method that is driven by the discrepancy in estimating both values and states.",http://proceedings.mlr.press/v97/freirich19a.html,http://proceedings.mlr.press/v97/freirich19a/freirich19a.pdf,ICML
31,2019,A Kernel Theory of Modern Data Augmentation,"Tri Dao,         Albert Gu,         Alexander Ratner,         Virginia Smith,         Chris De Sa,         Christopher Re","Data augmentation, a technique in which a training set is expanded with class-preserving transformations, is ubiquitous in modern machine learning pipelines. In this paper, we seek to establish a theoretical framework for understanding data augmentation. We approach this from two directions: First, we provide a general model of augmentation as a Markov process, and show that kernels appear naturally with respect to this model, even when we do not employ kernel classification. Next, we analyze more directly the effect of augmentation on kernel classifiers, showing that data augmentation can be approximated by first-order feature averaging and second-order variance regularization components. These frameworks both serve to illustrate the ways in which data augmentation affects the downstream learning model, and the resulting analyses provide novel connections between prior work in invariant kernels, tangent propagation, and robust optimization. Finally, we provide several proof-of-concept applications showing that our theory can be useful for accelerating machine learning workflows, such as reducing the amount of computation needed to train using augmented data, and predicting the utility of a transformation prior to training.",http://proceedings.mlr.press/v97/dao19b.html,http://proceedings.mlr.press/v97/dao19b/dao19b.pdf,ICML
32,2019,Conditional Gradient Methods via Stochastic Path-Integrated Differential Estimator,"Alp Yurtsever,         Suvrit Sra,         Volkan Cevher","We propose a class of variance-reduced stochastic conditional gradient methods. By adopting the recent stochastic path-integrated differential estimator technique (SPIDER) of Fang et. al. (2018) for the classical Frank-Wolfe (FW) method, we introduce SPIDER-FW for finite-sum minimization as well as the more general expectation minimization problems. SPIDER-FW enjoys superior complexity guarantees in the non-convex setting, while matching the best known FW variants in the convex case. We also extend our framework a la conditional gradient sliding (CGS) of Lan & Zhou. (2016), and propose SPIDER-CGS.",http://proceedings.mlr.press/v97/yurtsever19b.html,http://proceedings.mlr.press/v97/yurtsever19b/yurtsever19b.pdf,ICML
33,2019,MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets,"Pierre-Alexandre Mattei,         Jes Frellsen","We consider the problem of handling missing data with deep latent variable models (DLVMs). First, we present a simple technique to train DLVMs when the training set contains missing-at-random data. Our approach, called MIWAE, is based on the importance-weighted autoencoder (IWAE), and maximises a potentially tight lower bound of the log-likelihood of the observed data. Compared to the original IWAE, our algorithm does not induce any additional computational overhead due to the missing data. We also develop Monte Carlo techniques for single and multiple imputation using a DLVM trained on an incomplete data set. We illustrate our approach by training a convolutional DLVM on incomplete static binarisations of MNIST. Moreover, on various continuous data sets, we show that MIWAE provides extremely accurate single imputations, and is highly competitive with state-of-the-art methods.",http://proceedings.mlr.press/v97/mattei19a.html,http://proceedings.mlr.press/v97/mattei19a/mattei19a.pdf,ICML
34,2019,Adaptive Monte Carlo Multiple Testing via Multi-Armed Bandits,"Martin Zhang,         James Zou,         David Tse","Monte Carlo (MC) permutation test is considered the gold standard for statistical hypothesis testing, especially when standard parametric assumptions are not clear or likely to fail. However, in modern data science settings where a large number of hypothesis tests need to be performed simultaneously, it is rarely used due to its prohibitive computational cost. In genome-wide association studies, for example, the number of hypothesis tests mm is around 10610^6 while the number of MC samples nn for each test could be greater than 10810^8, totaling more than nmnm=101410^{14} samples. In this paper, we propose \texttt{A}daptive \texttt{M}C multiple \texttt{T}esting (\texttt{AMT}) to estimate MC p-values and control false discovery rate in multiple testing. The algorithm outputs the same result as the standard full MC approach with high probability while requiring only O~(n−−√m)\tilde{O}(\sqrt{n}m) samples. This sample complexity is shown to be optimal. On a Parkinson GWAS dataset, the algorithm reduces the running time from 2 months for full MC to an hour. The \texttt{AMT} algorithm is derived based on the theory of multi-armed bandits.",http://proceedings.mlr.press/v97/zhang19t.html,http://proceedings.mlr.press/v97/zhang19t/zhang19t.pdf,ICML
35,2019,Insertion Transformer: Flexible Sequence Generation via Insertion Operations,"Mitchell Stern,         William Chan,         Jamie Kiros,         Jakob Uszkoreit","We present the Insertion Transformer, an iterative, partially autoregressive model for sequence generation based on insertion operations. Unlike typical autoregressive models which rely on a fixed, often left-to-right ordering of the output, our approach accommodates arbitrary orderings by allowing for tokens to be inserted anywhere in the sequence during decoding. This flexibility confers a number of advantages: for instance, not only can our model be trained to follow specific orderings such as left-to-right generation or a binary tree traversal, but it can also be trained to maximize entropy over all valid insertions for robustness. In addition, our model seamlessly accommodates both fully autoregressive generation (one insertion at a time) and partially autoregressive generation (simultaneous insertions at multiple locations). We validate our approach by analyzing its performance on the WMT 2014 English-German machine translation task under various settings for training and decoding. We find that the Insertion Transformer outperforms many prior non-autoregressive approaches to translation at comparable or better levels of parallelism, and successfully recovers the performance of the original Transformer while requiring only logarithmically many iterations during decoding.",http://proceedings.mlr.press/v97/stern19a.html,http://proceedings.mlr.press/v97/stern19a/stern19a.pdf,ICML
36,2019,Learning Novel Policies For Tasks,"Yunbo Zhang,         Wenhao Yu,         Greg Turk","In this work, we present a reinforcement learning algorithm that can find a variety of policies (novel policies) for a task that is given by a task reward function. Our method does this by creating a second reward function that recognizes previously seen state sequences and rewards those by novelty, which is measured using autoencoders that have been trained on state sequences from previously discovered policies. We present a two-objective update technique for policy gradient algorithms in which each update of the policy is a compromise between improving the task reward and improving the novelty reward. Using this method, we end up with a collection of policies that solves a given task as well as carrying out action sequences that are distinct from one another. We demonstrate this method on maze navigation tasks, a reaching task for a simulated robot arm, and a locomotion task for a hopper. We also demonstrate the effectiveness of our approach on deceptive tasks in which policy gradient methods often get stuck.",http://proceedings.mlr.press/v97/zhang19q.html,http://proceedings.mlr.press/v97/zhang19q/zhang19q.pdf,ICML
37,2019,Almost surely constrained convex optimization,"Olivier Fercoq,         Ahmet Alacaoglu,         Ion Necoara,         Volkan Cevher","We propose a stochastic gradient framework for solving stochastic composite convex optimization problems with (possibly) infinite number of linear inclusion constraints that need to be satisfied almost surely. We use smoothing and homotopy techniques to handle constraints without the need for matrix-valued projections. We show for our stochastic gradient algorithm O(log(k)/k−−√)O(log⁡(k)/k)\mathcal{O}(\log(k)/\sqrt{k}) convergence rate for general convex objectives and O(log(k)/k)O(log⁡(k)/k)\mathcal{O}(\log(k)/k) convergence rate for restricted strongly convex objectives. These rates are known to be optimal up to logarithmic factor, even without constraints. We conduct numerical experiments on basis pursuit, hard margin support vector machines and portfolio optimization problems and show that our algorithm achieves state-of-the-art practical performance.",http://proceedings.mlr.press/v97/fercoq19a.html,http://proceedings.mlr.press/v97/fercoq19a/fercoq19a.pdf,ICML
38,2019,Active Learning for Probabilistic Structured Prediction of Cuts and Matchings,"Sima Behpour,         Anqi Liu,         Brian Ziebart","Active learning methods, like uncertainty sampling, combined with probabilistic prediction techniques have achieved success in various problems like image classification and text classification. For more complex multivariate prediction tasks, the relationships between labels play an important role in designing structured classifiers with better performance. However, computational time complexity limits prevalent probabilistic methods from effectively supporting active learning. Specifically, while non-probabilistic methods based on structured support vector ma-chines can be tractably applied to predicting cuts and bipartite matchings, conditional random fields are intractable for these structures. We propose an adversarial approach for active learning with structured prediction domains that is tractable for cuts and matching. We evaluate this approach algorithmically in two important structured prediction problems: multi-label classification and object tracking in videos. We demonstrate better accuracy and computational efficiency for our proposed method.",http://proceedings.mlr.press/v97/behpour19a.html,http://proceedings.mlr.press/v97/behpour19a/behpour19a.pdf,ICML
39,2019,Interpreting Adversarially Trained Convolutional Neural Networks,"Tianyuan Zhang,         Zhanxing Zhu","We attempt to interpret how adversarially trained convolutional neural networks (AT-CNNs) recognize objects. We design systematic approaches to interpret AT-CNNs in both qualitative and quantitative ways and compare them with normally trained models. Surprisingly, we find that adversarial training alleviates the texture bias of standard CNNs when trained on object recognition tasks, and helps CNNs learn a more shape-biased representation. We validate our hypothesis from two aspects. First, we compare the salience maps of AT-CNNs and standard CNNs on clean images and images under different transformations. The comparison could visually show that the prediction of the two types of CNNs is sensitive to dramatically different types of features. Second, to achieve quantitative verification, we construct additional test datasets that destroy either textures or shapes, such as style-transferred version of clean data, saturated images and patch-shuffled ones, and then evaluate the classification accuracy of AT-CNNs and normal CNNs on these datasets. Our findings shed some light on why AT-CNNs are more robust than those normally trained ones and contribute to a better understanding of adversarial training over CNNs from an interpretation perspective.",http://proceedings.mlr.press/v97/zhang19s.html,http://proceedings.mlr.press/v97/zhang19s/zhang19s.pdf,ICML
40,2019,Nearest Neighbor and Kernel Survival Analysis: Nonasymptotic Error Bounds and Strong Consistency Rates,George Chen,"We establish the first nonasymptotic error bounds for Kaplan-Meier-based nearest neighbor and kernel survival probability estimators where feature vectors reside in metric spaces. Our bounds imply rates of strong consistency for these nonparametric estimators and, up to a log factor, match an existing lower bound for conditional CDF estimation. Our proof strategy also yields nonasymptotic guarantees for nearest neighbor and kernel variants of the Nelson-Aalen cumulative hazards estimator. We experimentally compare these methods on four datasets. We find that for the kernel survival estimator, a good choice of kernel is one learned using random survival forests.",http://proceedings.mlr.press/v97/chen19a.html,http://proceedings.mlr.press/v97/chen19a/chen19a.pdf,ICML
41,2019,Almost Unsupervised Text to Speech and Automatic Speech Recognition,"Yi Ren,         Xu Tan,         Tao Qin,         Sheng Zhao,         Zhou Zhao,         Tie-Yan Liu","Text to speech (TTS) and automatic speech recognition (ASR) are two dual tasks in speech processing and both achieve impressive performance thanks to the recent advance in deep learning and large amount of aligned speech and text data. However, the lack of aligned data poses a major practical problem for TTS and ASR on low-resource languages. In this paper, by leveraging the dual nature of the two tasks, we propose an almost unsupervised learning method that only leverages few hundreds of paired data and extra unpaired data for TTS and ASR. Our method consists of the following components: (1) denoising auto-encoder, which reconstructs speech and text sequences respectively to develop the capability of language modeling both in speech and text domain; (2) dual transformation, where the TTS model transforms the text yyy into speech x^x^\hat{x}, and the ASR model leverages the transformed pair (x^,y)(x^,y)(\hat{x},y) for training, and vice versa, to boost the accuracy of the two tasks; (3) bidirectional sequence modeling, which address the error propagation problem especially in the long speech and text sequence when training with few paired data; (4) a unified model structure, which combines all the above components for TTS and ASR based on Transformer model. Our method achieves 99.84% in terms of word level intelligible rate and 2.68 MOS for TTS, and 11.7% PER for ASR on LJSpeech dataset, by leveraging only 200 paired speech and text data (about 20 minutes audio), together with extra unpaired speech and text data.",http://proceedings.mlr.press/v97/ren19a.html,http://proceedings.mlr.press/v97/ren19a/ren19a.pdf,ICML
42,2019,Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,"Kate Rakelly,         Aurick Zhou,         Chelsea Finn,         Sergey Levine,         Deirdre Quillen","Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (meta-RL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness on sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.",http://proceedings.mlr.press/v97/rakelly19a.html,http://proceedings.mlr.press/v97/rakelly19a/rakelly19a.pdf,ICML
43,2019,Neural Inverse Knitting: From Images to Manufacturing Instructions,"Alexandre Kaspar,         Tae-Hyun Oh,         Liane Makatura,         Petr Kellnhofer,         Wojciech Matusik","Motivated by the recent potential of mass customization brought by whole-garment knitting machines, we introduce the new problem of automatic machine instruction generation using a single image of the desired physical product, which we apply to machine knitting. We propose to tackle this problem by directly learning to synthesize regular machine instructions from real images. We create a cured dataset of real samples with their instruction counterpart and propose to use synthetic images to augment it in a novel way. We theoretically motivate our data mixing framework and show empirical results suggesting that making real images look more synthetic is beneficial in our problem setup.",http://proceedings.mlr.press/v97/kaspar19a.html,http://proceedings.mlr.press/v97/kaspar19a/kaspar19a.pdf,ICML
44,2019,Communication-Constrained Inference and the Role of Shared Randomness,"Jayadev Acharya,         Clement Canonne,         Himanshu Tyagi","A central server needs to perform statistical inference based on samples that are distributed over multiple users who can each send a message of limited length to the center. We study problems of distribution learning and identity testing in this distributed inference setting and examine the role of shared randomness as a resource. We propose a general purpose simulate-and-infer strategy that uses only private-coin communication protocols and is sample-optimal for distribution learning. This general strategy turns out to be sample-optimal even for distribution testing among private-coin protocols. Interestingly, we propose a public-coin protocol that outperforms simulate-and-infer for distribution testing and is, in fact, sample-optimal. Underlying our public-coin protocol is a random hash that when applied to the samples minimally contracts the chi-squared distance of their distribution from the uniform distribution.",http://proceedings.mlr.press/v97/acharya19a.html,http://proceedings.mlr.press/v97/acharya19a/acharya19a.pdf,ICML
45,2019,Differentiable Dynamic Normalization for Learning Deep Representation,"Ping Luo,         Peng Zhanglin,         Shao Wenqi,         Zhang Ruimao,         Ren Jiamin,         Wu Lingyun","This work presents Dynamic Normalization (DN), which is able to learn arbitrary normalization operations for different convolutional layers in a deep ConvNet. Unlike existing normalization approaches that predefined computations of the statistics (mean and variance), DN learns to estimate them. DN has several appealing benefits. First, it adapts to various networks, tasks, and batch sizes. Second, it can be easily implemented and trained in a differentiable end-to-end manner with merely small number of parameters. Third, its matrix formulation represents a wide range of normalization methods, shedding light on analyzing them theoretically. Extensive studies show that DN outperforms its counterparts in CIFAR10 and ImageNet.",http://proceedings.mlr.press/v97/luo19a.html,http://proceedings.mlr.press/v97/luo19a/luo19a.pdf,ICML
46,2019,On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning,"Hoda Heidari,         Vedant Nanda,         Krishna Gummadi","Most existing notions of algorithmic fairness are one-shot: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse impact of the algorithmic decisions today on the long-term welfare and prosperity of certain segments of the population. We take a broader perspective on algorithmic fairness. We propose an effort-based measure of fairness and present a data-driven framework for characterizing the long-term impact of algorithmic policies on reshaping the underlying population. Motivated by the psychological literature on social learning and the economic literature on equality of opportunity, we propose a micro-scale model of how individuals may respond to decision-making algorithms. We employ existing measures of segregation from sociology and economics to quantify the resulting macro- scale population-level change. Importantly, we observe that different models may shift the group- conditional distribution of qualifications in different directions. Our findings raise a number of important questions regarding the formalization of fairness for decision-making models.",http://proceedings.mlr.press/v97/heidari19a.html,http://proceedings.mlr.press/v97/heidari19a/heidari19a.pdf,ICML
47,2019,On the Connection Between Adversarial Robustness and Saliency Map Interpretability,"Christian Etmann,         Sebastian Lunz,         Peter Maass,         Carola Schoenlieb","Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts. We aim to quantify this behaviour by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows, so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the nonlinear nature of neural networks weakens the relation.",http://proceedings.mlr.press/v97/etmann19a.html,http://proceedings.mlr.press/v97/etmann19a/etmann19a.pdf,ICML
48,2019,Sublinear Space Private Algorithms Under the Sliding Window Model,Jalaj Upadhyay,"The Differential privacy overview of Apple states, “Apple retains the collected data for a maximum of three months."" Analysis of recent data is formalized by the sliding window model. This begs the question: what is the price of privacy in the sliding window model? In this paper, we study heavy hitters in the sliding window model with window size www. Previous works of Chan et al. (2012) estimates heavy hitters with an error of order θwθw\theta w for a constant θ>0θ>0\theta >0. In this paper, we give an efficient differentially private algorithm to estimate heavy hitters in the sliding window model with O˜(w3/4)O~(w3/4)\widetilde O(w^{3/4}) additive error and using O˜(w−−√)O~(w)\widetilde O(\sqrt{w}) space.",http://proceedings.mlr.press/v97/upadhyay19a.html,http://proceedings.mlr.press/v97/upadhyay19a/upadhyay19a.pdf,ICML
49,2019,Generalized Linear Rule Models,"Dennis Wei,         Sanjeeb Dash,         Tian Gao,         Oktay Gunluk","This paper considers generalized linear models using rule-based features, also referred to as rule ensembles, for regression and probabilistic classification. Rules facilitate model interpretation while also capturing nonlinear dependences and interactions. Our problem formulation accordingly trades off rule set complexity and prediction accuracy. Column generation is used to optimize over an exponentially large space of rules without pre-generating a large subset of candidates or greedily boosting rules one by one. The column generation subproblem is solved using either integer programming or a heuristic optimizing the same objective. In experiments involving logistic and linear regression, the proposed methods obtain better accuracy-complexity trade-offs than existing rule ensemble algorithms. At one end of the trade-off, the methods are competitive with less interpretable benchmark models.",http://proceedings.mlr.press/v97/wei19a.html,http://proceedings.mlr.press/v97/wei19a/wei19a.pdf,ICML
50,2019,Overcoming Multi-model Forgetting,"Yassine Benyahia,         Kaicheng Yu,         Kamil Bennani Smires,         Martin Jaggi,         Anthony C. Davison,         Mathieu Salzmann,         Claudiu Musat","We identify a phenomenon, which we refer to as multi-model forgetting, that occurs when sequentially training multiple deep networks with partially-shared parameters; the performance of previously-trained models degrades as one optimizes a subsequent one, due to the overwriting of shared parameters. To overcome this, we introduce a statistically-justified weight plasticity loss that regularizes the learning of a model’s shared parameters according to their importance for the previous models, and demonstrate its effectiveness when training two models sequentially and for neural architecture search. Adding weight plasticity in neural architecture search preserves the best models to the end of the search and yields improved results in both natural language processing and computer vision tasks.",http://proceedings.mlr.press/v97/benyahia19a.html,http://proceedings.mlr.press/v97/benyahia19a/benyahia19a.pdf,ICML
51,2019,Dynamic Learning with Frequent New Product Launches: A Sequential Multinomial Logit Bandit Problem,"Junyu Cao,         Wei Sun","Motivated by the phenomenon that companies introduce new products to keep abreast with customers’ rapidly changing tastes, we consider a novel online learning setting where a profit-maximizing seller needs to learn customers’ preferences through offering recommendations, which may contain existing products and new products that are launched in the middle of a selling period. We propose a sequential multinomial logit (SMNL) model to characterize customers’ behavior when product recommendations are presented in tiers. For the offline version with known customers’ preferences, we propose a polynomial-time algorithm and characterize the properties of the optimal tiered product recommendation. For the online problem, we propose a learning algorithm and quantify its regret bound. Moreover, we extend the setting to incorporate a constraint which ensures every new product is learned to a given accuracy. Our results demonstrate the tier structure can be used to mitigate the risks associated with learning new products.",http://proceedings.mlr.press/v97/cao19a.html,http://proceedings.mlr.press/v97/cao19a/cao19a.pdf,ICML
52,2019,Safe Grid Search with Optimal Complexity,"Eugene Ndiaye,         Tam Le,         Olivier Fercoq,         Joseph Salmon,         Ichiro Takeuchi","Popular machine learning estimators involve regularization parameters that can be challenging to tune, and standard strategies rely on grid search for this task. In this paper, we revisit the techniques of approximating the regularization path up to predefined tolerance ϵϵ\epsilon in a unified framework and show that its complexity is O(1/ϵ√d)O(1/ϵd)O(1/\sqrt[d]{\epsilon}) for uniformly convex loss of order d≥2d≥2d \geq 2 and O(1/ϵ√)O(1/ϵ)O(1/\sqrt{\epsilon}) for Generalized Self-Concordant functions. This framework encompasses least-squares but also logistic regression, a case that as far as we know was not handled as precisely in previous works. We leverage our technique to provide refined bounds on the validation error as well as a practical algorithm for hyperparameter tuning. The latter has global convergence guarantee when targeting a prescribed accuracy on the validation set. Last but not least, our approach helps relieving the practitioner from the (often neglected) task of selecting a stopping criterion when optimizing over the training set: our method automatically calibrates this criterion based on the targeted accuracy on the validation set.",http://proceedings.mlr.press/v97/ndiaye19a.html,http://proceedings.mlr.press/v97/ndiaye19a/ndiaye19a.pdf,ICML
53,2019,Optimal Mini-Batch and Step Sizes for SAGA,"Nidham Gazagnadou,         Robert Gower,         Joseph Salmon","Recently it has been shown that the step sizes of a family of variance reduced gradient methods called the JacSketch methods depend on the expected smoothness constant. In particular, if this expected smoothness constant could be calculated a priori, then one could safely set much larger step sizes which would result in a much faster convergence rate. We fill in this gap, and provide simple closed form expressions for the expected smoothness constant and careful numerical experiments verifying these bounds. Using these bounds, and since the SAGA algorithm is part of this JacSketch family, we suggest a new standard practice for setting the step and mini-batch sizes for SAGA that are competitive with a numerical grid search. Furthermore, we can now show that the total complexity of the SAGA algorithm decreases linearly in the mini-batch size up to a pre-defined value: the optimal mini-batch size. This is a rare result in the stochastic variance reduced literature, only previously shown for the Katyusha algorithm. Finally we conjecture that this is the case for many other stochastic variance reduced methods and that our bounds and analysis of the expected smoothness constant is key to extending these results.",http://proceedings.mlr.press/v97/gazagnadou19a.html,http://proceedings.mlr.press/v97/gazagnadou19a/gazagnadou19a.pdf,ICML
54,2019,On the Complexity of Approximating Wasserstein Barycenters,"Alexey Kroshnin,         Nazarii Tupitsa,         Darina Dvinskikh,         Pavel Dvurechensky,         Alexander Gasnikov,         Cesar Uribe","We study the complexity of approximating the Wasserstein barycenter of mmm discrete measures, or histograms of size nnn, by contrasting two alternative approaches that use entropic regularization. The first approach is based on the Iterative Bregman Projections (IBP) algorithm for which our novel analysis gives a complexity bound proportional to mn2/ε2mn2/ε2{mn^2}/{\varepsilon^2} to approximate the original non-regularized barycenter. On the other hand, using an approach based on accelerated gradient descent, we obtain a complexity proportional to mn2/εmn2/ε{mn^{2}}/{\varepsilon}. As a byproduct, we show that the regularization parameter in both approaches has to be proportional to εε\varepsilon, which causes instability of both algorithms when the desired accuracy is high. To overcome this issue, we propose a novel proximal-IBP algorithm, which can be seen as a proximal gradient method, which uses IBP on each iteration to make a proximal step. We also consider the question of scalability of these algorithms using approaches from distributed optimization and show that the first algorithm can be implemented in a centralized distributed setting (master/slave), while the second one is amenable to a more general decentralized distributed setting with an arbitrary network topology.",http://proceedings.mlr.press/v97/kroshnin19a.html,http://proceedings.mlr.press/v97/kroshnin19a/kroshnin19a.pdf,ICML
55,2019,Scaling Up Ordinal Embedding: A Landmark Approach,"Jesse Anderton,         Javed Aslam","Ordinal Embedding is the problem of placing n objects into R^d to satisfy constraints like ""object a is closer to b than to c."" It can accommodate data that embeddings from features or distances cannot, but is a more difficult problem. We propose a novel landmark-based method as a partial solution. At small to medium scales, we present a novel combination of existing methods with some new theoretical justification. For very large values of n optimizing over an entire embedding breaks down, so we propose a novel method which first embeds a subset of m << n objects and then embeds the remaining objects independently and in parallel. We prove a distance error bound for our method in terms of m and that it has O(dn log m) time complexity, and show empirically that it is able to produce high quality embeddings in a fraction of the time needed for any published method.",http://proceedings.mlr.press/v97/anderton19a.html,http://proceedings.mlr.press/v97/anderton19a/anderton19a.pdf,ICML
56,2019,Fast and Simple Natural-Gradient Variational Inference with Mixture of Exponential-family Approximations,"Wu Lin,         Mohammad Emtiyaz Khan,         Mark Schmidt","Natural-gradient methods enable fast and simple algorithms for variational inference, but due to computational difficulties, their use is mostly limited to minimal exponential-family (EF) approximations. In this paper, we extend their application to estimate structured approximations such as mixtures of EF distributions. Such approximations can fit complex, multimodal posterior distributions and are generally more accurate than unimodal EF approximations. By using a minimal conditional-EF representation of such approximations, we derive simple natural-gradient updates. Our empirical results demonstrate a faster convergence of our natural-gradient method compared to black-box gradient-based methods. Our work expands the scope of natural gradients for Bayesian inference and makes them more widely applicable than before.",http://proceedings.mlr.press/v97/lin19b.html,http://proceedings.mlr.press/v97/lin19b/lin19b.pdf,ICML
57,2019,EMI: Exploration with Mutual Information,"Hyoungseok Kim,         Jaekyeom Kim,         Yeonwoo Jeong,         Sergey Levine,         Hyun Oh Song","Reinforcement learning algorithms struggle when the reward signal is very sparse. In these cases, naive random exploration methods essentially rely on a random walk to stumble onto a rewarding state. Recent works utilize intrinsic motivation to guide the exploration via generative models, predictive forward models, or discriminative modeling of novelty. We propose EMI, which is an exploration method that constructs embedding representation of states and actions that does not rely on generative decoding of the full observation but extracts predictive signals that can be used to guide exploration based on forward prediction in the representation space. Our experiments show competitive results on challenging locomotion tasks with continuous control and on image-based exploration tasks with discrete actions on Atari. The source code is available at https://github.com/snu-mllab/EMI.",http://proceedings.mlr.press/v97/kim19a.html,http://proceedings.mlr.press/v97/kim19a/kim19a.pdf,ICML
58,2019,Connectivity-Optimized Representation Learning via Persistent Homology,"Christoph Hofer,         Roland Kwitt,         Marc Niethammer,         Mandar Dixit","We study the problem of learning representations with controllable connectivity properties. This is beneficial in situations when the imposed structure can be leveraged upstream. In particular, we control the connectivity of an autoencoder’s latent space via a novel type of loss, operating on information from persistent homology. Under mild conditions, this loss is differentiable and we present a theoretical analysis of the properties induced by the loss. We choose one-class learning as our upstream task and demonstrate that the imposed structure enables informed parameter selection for modeling the in-class distribution via kernel density estimators. Evaluated on computer vision data, these one-class models exhibit competitive performance and, in a low sample size regime, outperform other methods by a large margin. Notably, our results indicate that a single autoencoder, trained on auxiliary (unlabeled) data, yields a mapping into latent space that can be reused across datasets for one-class learning.",http://proceedings.mlr.press/v97/hofer19a.html,http://proceedings.mlr.press/v97/hofer19a/hofer19a.pdf,ICML
59,2019,Self-similar Epochs: Value in arrangement,"Eliav Buchnik,         Edith Cohen,         Avinatan Hasidim,         Yossi Matias","Optimization of machine learning models is commonly performed through stochastic gradient updates on randomly ordered training examples. This practice means that each fraction of an epoch comprises an independent random sample of the training data that may not preserve informative structure present in the full data. We hypothesize that the training can be more effective with self-similar arrangements that potentially allow each epoch to provide benefits of multiple ones. We study this for “matrix factorization” – the common task of learning metric embeddings of entities such as queries, videos, or words from example pairwise associations. We construct arrangements that preserve the weighted Jaccard similarities of rows and columns and experimentally observe training acceleration of 3%-37% on synthetic and recommendation datasets. Principled arrangements of training examples emerge as a novel and potentially powerful enhancement to SGD that merits further exploration.",http://proceedings.mlr.press/v97/buchnik19a.html,http://proceedings.mlr.press/v97/buchnik19a/buchnik19a.pdf,ICML
60,2019,Bayesian Nonparametric Federated Learning of Neural Networks,"Mikhail Yurochkin,         Mayank Agarwal,         Soumya Ghosh,         Kristjan Greenewald,         Nghia Hoang,         Yasaman Khazaeni","In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited. We develop a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to provide local neural network weights, which are modeled through our framework. We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision, data pooling and with as few as a single communication round. We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.",http://proceedings.mlr.press/v97/yurochkin19a.html,http://proceedings.mlr.press/v97/yurochkin19a/yurochkin19a.pdf,ICML
61,2019,Weak Detection of Signal in the Spiked Wigner Model,"Hye Won Chung,         Ji Oon Lee","We consider the problem of detecting the presence of the signal in a rank-one signal-plus-noise data matrix. In case the signal-to-noise ratio is under the threshold below which a reliable detection is impossible, we propose a hypothesis test based on the linear spectral statistics of the data matrix. When the noise is Gaussian, the error of the proposed test is optimal as it matches the error of the likelihood ratio test that minimizes the sum of the Type-I and Type-II errors. The test is data-driven and does not depend on the distribution of the signal or the noise. If the density of the noise is known, it can be further improved by an entrywise transformation to lower the error of the test.",http://proceedings.mlr.press/v97/chung19a.html,http://proceedings.mlr.press/v97/chung19a/chung19a.pdf,ICML
62,2019,A Theory of Regularized Markov Decision Processes,"Matthieu Geist,         Bruno Scherrer,         Olivier Pietquin","Many recent successful (deep) reinforcement learning algorithms make use of regularization, generally based on entropy or Kullback-Leibler divergence. We propose a general theory of regularized Markov Decision Processes that generalizes these approaches in two directions: we consider a larger class of regularizers, and we consider the general modified policy iteration approach, encompassing both policy iteration and value iteration. The core building blocks of this theory are a notion of regularized Bellman operator and the Legendre-Fenchel transform, a classical tool of convex optimization. This approach allows for error propagation analyses of general algorithmic schemes of which (possibly variants of) classical algorithms such as Trust Region Policy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy Programming are special cases. This also draws connections to proximal convex optimization, especially to Mirror Descent.",http://proceedings.mlr.press/v97/geist19a.html,http://proceedings.mlr.press/v97/geist19a/geist19a.pdf,ICML
63,2019,Importance Sampling Policy Evaluation with an Estimated Behavior Policy,"Josiah Hanna,         Scott Niekum,         Peter Stone","We consider the problem of off-policy evaluation in Markov decision processes. Off-policy evaluation is the task of evaluating the expected return of one policy with data generated by a different, behavior policy. Importance sampling is a technique for off-policy evaluation that re-weights off-policy returns to account for differences in the likelihood of the returns between the two policies. In this paper, we study importance sampling with an estimated behavior policy where the behavior policy estimate comes from the same set of data used to compute the importance sampling estimate. We find that this estimator often lowers the mean squared error of off-policy evaluation compared to importance sampling with the true behavior policy or using a behavior policy that is estimated from a separate data set. Intuitively, estimating the behavior policy in this way corrects for error due to sampling in the action-space. Our empirical results also extend to other popular variants of importance sampling and show that estimating a non-Markovian behavior policy can further lower large-sample mean squared error even when the true behavior policy is Markovian.",http://proceedings.mlr.press/v97/hanna19a.html,http://proceedings.mlr.press/v97/hanna19a/hanna19a.pdf,ICML
64,2019,Trading Redundancy for Communication: Speeding up Distributed SGD for Non-convex Optimization,"Farzin Haddadpour,         Mohammad Mahdi Kamani,         Mehrdad Mahdavi,         Viveck Cadambe","Communication overhead is one of the key challenges that hinders the scalability of distributed optimization algorithms to train large neural networks. In recent years, there has been a great deal of research to alleviate communication cost by compressing the gradient vector or using local updates and periodic model averaging. In this paper, we advocate the use of redundancy towards communication-efficient distributed stochastic algorithms for non-convex optimization. In particular, we, both theoretically and practically, show that by properly infusing redundancy to the training data with model averaging, it is possible to significantly reduce the number of communication rounds. To be more precise, we show that redundancy reduces residual error in local averaging, thereby reaching the same level of accuracy with fewer rounds of communication as compared with previous algorithms. Empirical studies on CIFAR10, CIFAR100 and ImageNet datasets in a distributed environment complement our theoretical results; they show that our algorithms have additional beneficial aspects including tolerance to failures, as well as greater gradient diversity.",http://proceedings.mlr.press/v97/haddadpour19a.html,http://proceedings.mlr.press/v97/haddadpour19a/haddadpour19a.pdf,ICML
65,2019,Generalized Approximate Survey Propagation for High-Dimensional Estimation,"Carlo Lucibello,         Luca Saglietti,         Yue Lu","In Generalized Linear Estimation (GLE) problems, we seek to estimate a signal that is observed through a linear transform followed by a component-wise, possibly nonlinear and noisy, channel. In the Bayesian optimal setting, Generalized Approximate Message Passing (GAMP) is known to achieve optimal performance for GLE. However, its performance can significantly deteriorate whenever there is a mismatch between the assumed and the true generative model, a situation frequently encountered in practice. In this paper, we propose a new algorithm, named Generalized Approximate Survey Propagation (GASP), for solving GLE in the presence of prior or model misspecifications. As a prototypical example, we consider the phase retrieval problem, where we show that GASP outperforms the corresponding GAMP, reducing the reconstruction threshold and, for certain choices of its parameters, approaching Bayesian optimal performance. Furthermore, we present a set of state evolution equations that can precisely characterize the performance of GASP in the high-dimensional limit.",http://proceedings.mlr.press/v97/lucibello19a.html,http://proceedings.mlr.press/v97/lucibello19a/lucibello19a.pdf,ICML
66,2019,Fast Rates for a kNN Classifier Robust to Unknown Asymmetric Label Noise,"Henry Reeve,         Ata Kaban","We consider classification in the presence of class-dependent asymmetric label noise with unknown noise probabilities. In this setting, identifiability conditions are known, but additional assumptions were shown to be required for finite sample rates, and so far only the parametric rate has been obtained. Assuming these identifiability conditions, together with a measure-smoothness condition on the regression function and Tsybakov’s margin condition, we show that the Robust kNN classifier of Gao et al. attains, the mini-max optimal rates of the noise-free setting, up to a log factor, even when trained on data with unknown asymmetric label noise. Hence, our results provide a solid theoretical backing for this empirically successful algorithm. By contrast the standard kNN is not even consistent in the setting of asymmetric label noise. A key idea in our analysis is a simple kNN based method for estimating the maximum of a function that requires far less assumptions than existing mode estimators do, and which may be of independent interest for noise proportion estimation and randomised optimisation problems.",http://proceedings.mlr.press/v97/reeve19a.html,http://proceedings.mlr.press/v97/reeve19a/reeve19a.pdf,ICML
67,2019,Maximum Likelihood Estimation for Learning Populations of Parameters,"Ramya Korlakai Vinayak,         Weihao Kong,         Gregory Valiant,         Sham Kakade","Consider a setting with NNN independent individuals, each with an unknown parameter, pi∈[0,1]pi∈[0,1]p_i \in [0, 1] drawn from some unknown distribution P⋆P⋆P^\star. After observing the outcomes of ttt independent Bernoulli trials, i.e., Xi∼Binomial(t,pi)Xi∼Binomial(t,pi)X_i \sim \text{Binomial}(t, p_i) per individual, our objective is to accurately estimate P⋆P⋆P^\star in the sparse regime, namely when t≪Nt≪Nt \ll N. This problem arises in numerous domains, including the social sciences, psychology, health-care, and biology, where the size of the population under study is usually large yet the number of observations per individual is often limited. Our main result shows that, in this sparse regime where t≪Nt≪Nt \ll N, the maximum likelihood estimator (MLE) is both statistically minimax optimal and efficiently computable. Precisely, for sufficiently large NNN, the MLE achieves the information theoretic optimal error bound of O(1t)O(1t)\mathcal{O}(\frac{1}{t}) for t<clogNt<clog⁡Nt < c\log{N}, with regards to the earth mover’s distance (between the estimated and true distributions). More generally, in an exponentially large interval of ttt beyond clogNclog⁡Nc \log{N}, the MLE achieves the minimax error bound of O(1tlogN√)O(1tlog⁡N)\mathcal{O}(\frac{1}{\sqrt{t\log N}}). In contrast, regardless of how large NNN is, the naive ""plug-in"" estimator for this problem only achieves the sub-optimal error of Θ(1t√)Θ(1t)\Theta(\frac{1}{\sqrt{t}}). Empirically, we also demonstrate the MLE performs well on both synthetic as well as real datasets.",http://proceedings.mlr.press/v97/vinayak19a.html,http://proceedings.mlr.press/v97/vinayak19a/vinayak19a.pdf,ICML
68,2019,Deep Gaussian Processes with Importance-Weighted Variational Inference,"Hugh Salimbeni,         Vincent Dutordoir,         James Hensman,         Marc Deisenroth","Deep Gaussian processes (DGPs) can model complex marginal densities as well as complex mappings. Non-Gaussian marginals are essential for modelling real-world data, and can be generated from the DGP by incorporating uncorrelated variables to the model. Previous work in the DGP model has introduced noise additively, and used variational inference with a combination of sparse Gaussian processes and mean-field Gaussians for the approximate posterior. Additive noise attenuates the signal, and the Gaussian form of variational distribution may lead to an inaccurate posterior. We instead incorporate noisy variables as latent covariates, and propose a novel importance-weighted objective, which leverages analytic results and provides a mechanism to trade off computation for improved accuracy. Our results demonstrate that the importance-weighted objective works well in practice and consistently outperforms classical variational inference, especially for deeper models.",http://proceedings.mlr.press/v97/salimbeni19a.html,http://proceedings.mlr.press/v97/salimbeni19a/salimbeni19a.pdf,ICML
69,2019,Faster Stochastic Alternating Direction Method of Multipliers for Nonconvex Optimization,"Feihu Huang,         Songcan Chen,         Heng Huang","In this paper, we propose a faster stochastic alternating direction method of multipliers (ADMM) for nonconvex optimization by using a new stochastic path-integrated differential estimator (SPIDER), called as SPIDER-ADMM. Moreover, we prove that the SPIDER-ADMM achieves a record-breaking incremental first-order oracle (IFO) complexity for finding an ϵϵ\epsilon-approximate solution. As one of major contribution of this paper, we provide a new theoretical analysis framework for nonconvex stochastic ADMM methods with providing the optimal IFO complexity. Based on this new analysis framework, we study the unsolved optimal IFO complexity of the existing non-convex SVRG-ADMM and SAGA-ADMM methods, and prove their the optimal IFO complexity. Thus, the SPIDER-ADMM improves the existing stochastic ADMM methods. Moreover, we extend SPIDER-ADMM to the online setting, and propose a faster online SPIDER-ADMM. Our theoretical analysis also derives the IFO complexity of the online SPIDER-ADMM. Finally, the experimental results on benchmark datasets validate that the proposed algorithms have faster convergence rate than the existing ADMM algorithms for nonconvex optimization.",http://proceedings.mlr.press/v97/huang19a.html,http://proceedings.mlr.press/v97/huang19a/huang19a.pdf,ICML
70,2019,Moment-Based Variational Inference for Markov Jump Processes,"Christian Wildner,         Heinz Koeppl","We propose moment-based variational inference as a flexible framework for approximate smoothing of latent Markov jump processes. The main ingredient of our approach is to partition the set of all transitions of the latent process into classes. This allows to express the Kullback-Leibler divergence from the approximate to the posterior process in terms of a set of moment functions that arise naturally from the chosen partition. To illustrate possible choices of the partition, we consider special classes of jump processes that frequently occur in applications. We then extend the results to latent parameter inference and demonstrate the method on several examples.",http://proceedings.mlr.press/v97/wildner19a.html,http://proceedings.mlr.press/v97/wildner19a/wildner19a.pdf,ICML
71,2019,New results on information theoretic clustering,"Ferdinando Cicalese,         Eduardo Laber,         Lucas Murtinho","We study the problem of optimizing the clustering of a set of vectors when the quality of the clustering is measured by the Entropy or the Gini impurity measure. Our results contribute to the state of the art both in terms of best known approximation guarantees and inapproximability bounds: (i) we give the first polynomial time algorithm for Entropy impurity based clustering with approximation guarantee independent of the number of vectors and (ii) we show that the problem of clustering based on entropy impurity does not admit a PTAS. This also implies an inapproximability result in information theoretic clustering for probability distributions closing a problem left open in [Chaudhury and McGregor, COLT08] and [Ackermann et al., ECCC11]. We also report experiments with a new clustering method that was designed on top of the theoretical tools leading to the above results. These experiments suggest a practical applicability for our method, in particular, when the number of clusters is large.",http://proceedings.mlr.press/v97/cicalese19a.html,http://proceedings.mlr.press/v97/cicalese19a/cicalese19a.pdf,ICML
72,2019,Data Poisoning Attacks on Stochastic Bandits,"Fang Liu,         Ness Shroff","Stochastic multi-armed bandits form a class of online learning problems that have important applications in online recommendation systems, adaptive medical treatment, and many others. Even though potential attacks against these learning algorithms may hijack their behavior, causing catastrophic loss in real-world applications, little is known about adversarial attacks on bandit algorithms. In this paper, we propose a framework of offline attacks on bandit algorithms and study convex optimization based attacks on several popular bandit algorithms. We show that the attacker can force the bandit algorithm to pull a target arm with high probability by a slight manipulation of the rewards in the data. Then we study a form of online attacks on bandit algorithms and propose an adaptive attack strategy against any bandit algorithm without the knowledge of the bandit algorithm. Our adaptive attack strategy can hijack the behavior of the bandit algorithm to suffer a linear regret with only a logarithmic cost to the attacker. Our results demonstrate a significant security threat to stochastic bandits.",http://proceedings.mlr.press/v97/liu19e.html,http://proceedings.mlr.press/v97/liu19e/liu19e.pdf,ICML
73,2019,Rao-Blackwellized Stochastic Gradients for Discrete Distributions,"Runjing Liu,         Jeffrey Regier,         Nilesh Tripuraneni,         Michael Jordan,         Jon Mcauliffe","We wish to compute the gradient of an expectation over a finite or countably infinite sample space having K ≤≤\leq ∞∞\infty categories. When K is indeed infinite, or finite but very large, the relevant summation is intractable. Accordingly, various stochastic gradient estimators have been proposed. In this paper, we describe a technique that can be applied to reduce the variance of any such estimator, without changing its bias{—}in particular, unbiasedness is retained. We show that our technique is an instance of Rao-Blackwellization, and we demonstrate the improvement it yields on a semi-supervised classification problem and a pixel attention task.",http://proceedings.mlr.press/v97/liu19c.html,http://proceedings.mlr.press/v97/liu19c/liu19c.pdf,ICML
74,2019,Improving Adversarial Robustness via Promoting Ensemble Diversity,"Tianyu Pang,         Kun Xu,         Chao Du,         Ning Chen,         Jun Zhu","Though deep neural networks have achieved significant progress on various tasks, often enhanced by model ensemble, existing high-performance models can be vulnerable to adversarial attacks. Many efforts have been devoted to enhancing the robustness of individual networks and then constructing a straightforward ensemble, e.g., by directly averaging the outputs, which ignores the interaction among networks. This paper presents a new method that explores the interaction among individual networks to improve robustness for ensemble models. Technically, we define a new notion of ensemble diversity in the adversarial setting as the diversity among non-maximal predictions of individual members, and present an adaptive diversity promoting (ADP) regularizer to encourage the diversity, which leads to globally better robustness for the ensemble by making adversarial examples difficult to transfer among individual members. Our method is computationally efficient and compatible with the defense methods acting on individual networks. Empirical results on various datasets verify that our method can improve adversarial robustness while maintaining state-of-the-art accuracy on normal examples.",http://proceedings.mlr.press/v97/pang19a.html,http://proceedings.mlr.press/v97/pang19a/pang19a.pdf,ICML
75,2019,Replica Conditional Sequential Monte Carlo,"Alex Shestopaloff,         Arnaud Doucet","We propose a Markov chain Monte Carlo (MCMC) scheme to perform state inference in non-linear non-Gaussian state-space models. Current state-of-the-art methods to address this problem rely on particle MCMC techniques and its variants, such as the iterated conditional Sequential Monte Carlo (cSMC) scheme, which uses a Sequential Monte Carlo (SMC) type proposal within MCMC. A deficiency of standard SMC proposals is that they only use observations up to time ttt to propose states at time ttt when an entire observation sequence is available. More sophisticated SMC based on lookahead techniques could be used but they can be difficult to put in practice. We propose here replica cSMC where we build SMC proposals for one replica using information from the entire observation sequence by conditioning on the states of the other replicas. This approach is easily parallelizable and we demonstrate its excellent empirical performance when compared to the standard iterated cSMC scheme at fixed computational complexity.",http://proceedings.mlr.press/v97/shestopaloff19a.html,http://proceedings.mlr.press/v97/shestopaloff19a/shestopaloff19a.pdf,ICML
76,2019,Distributed Learning with Sublinear Communication,"Jayadev Acharya,         Chris De Sa,         Dylan Foster,         Karthik Sridharan","In distributed statistical learning, NNN samples are split across mmm machines and a learner wishes to use minimal communication to learn as well as if the examples were on a single machine. This model has received substantial interest in machine learning due to its scalability and potential for parallel speedup. However, in high-dimensional settings, where the number examples is smaller than the number of features (‘""dimension""), the speedup afforded by distributed learning may be overshadowed by the cost of communicating a single example. This paper investigates the following question: When is it possible to learn a ddd-dimensional model in the distributed setting with total communication sublinear in ddd? Starting with a negative result, we observe that for learning ℓ1ℓ1\ell_1-bounded or sparse linear models, no algorithm can obtain optimal error until communication is linear in dimension. Our main result is that by slightly relaxing the standard boundedness assumptions for linear models, we can obtain distributed algorithms that enjoy optimal error with communication logarithmic in dimension. This result is based on a family of algorithms that combine mirror descent with randomized sparsification/quantization of iterates, and extends to the general stochastic convex optimization model.",http://proceedings.mlr.press/v97/acharya19b.html,http://proceedings.mlr.press/v97/acharya19b/acharya19b.pdf,ICML
77,2019,Analyzing and Improving Representations with the Soft Nearest Neighbor Loss,"Nicholas Frosst,         Nicolas Papernot,         Geoffrey Hinton","We explore and expand the Soft Nearest Neighbor Loss to measure the entanglement of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that maximizing the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to better-calibrated estimates of uncertainty on outlier data but also marginally improved generalization. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.",http://proceedings.mlr.press/v97/frosst19a.html,http://proceedings.mlr.press/v97/frosst19a/frosst19a.pdf,ICML
78,2019,On Dropout and Nuclear Norm Regularization,"Poorya Mianjy,         Raman Arora","We give a formal and complete characterization of the explicit regularizer induced by dropout in deep linear networks with squared loss. We show that (a) the explicit regularizer is composed of an ℓ2ℓ2\ell_2-path regularizer and other terms that are also re-scaling invariant, (b) the convex envelope of the induced regularizer is the squared nuclear norm of the network map, and (c) for a sufficiently large dropout rate, we characterize the global optima of the dropout objective. We validate our theoretical findings with empirical results.",http://proceedings.mlr.press/v97/mianjy19a.html,http://proceedings.mlr.press/v97/mianjy19a/mianjy19a.pdf,ICML
79,2019,Simplifying Graph Convolutional Networks,"Felix Wu,         Amauri Souza,         Tianyi Zhang,         Christopher Fifty,         Tao Yu,         Kilian Weinberger","Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.",http://proceedings.mlr.press/v97/wu19e.html,http://proceedings.mlr.press/v97/wu19e/wu19e.pdf,ICML
80,2019,Sublinear Time Nearest Neighbor Search over Generalized Weighted Space,"Yifan Lei,         Qiang Huang,         Mohan Kankanhalli,         Anthony Tung","Nearest Neighbor Search (NNS) over generalized weighted space is a fundamental problem which has many applications in various fields. However, to the best of our knowledge, there is no sublinear time solution to this problem. Based on the idea of Asymmetric Locality-Sensitive Hashing (ALSH), we introduce a novel spherical asymmetric transformation and propose the first two novel weight-oblivious hashing schemes SL-ALSH and S2-ALSH accordingly. We further show that both schemes enjoy a quality guarantee and can answer the NNS queries in sublinear time. Evaluations over three real datasets demonstrate the superior performance of the two proposed schemes.",http://proceedings.mlr.press/v97/lei19a.html,http://proceedings.mlr.press/v97/lei19a/lei19a.pdf,ICML
81,2019,Collaborative Channel Pruning for Deep Networks,"Hanyu Peng,         Jiaxiang Wu,         Shifeng Chen,         Junzhou Huang","Deep networks have achieved impressive performance in various domains, but their applications are largely limited by the prohibitive computational overhead. In this paper, we propose a novel algorithm, namely collaborative channel pruning (CCP), to reduce the computational overhead with negligible performance degradation. The joint impact of pruned/preserved channels on the loss function is quantitatively analyzed, and such interchannel dependency is exploited to determine which channels to be pruned. The channel selection problem is then reformulated as a constrained 0-1 quadratic optimization problem, and the Hessian matrix, which is essential in constructing the above optimization, can be efficiently approximated. Empirical evaluation on two benchmark data sets indicates that our proposed CCP algorithm achieves higher classification accuracy with similar computational complexity than other stateof-the-art channel pruning algorithms",http://proceedings.mlr.press/v97/peng19c.html,http://proceedings.mlr.press/v97/peng19c/peng19c.pdf,ICML
82,2019,Parameter-Efficient Transfer Learning for NLP,"Neil Houlsby,         Andrei Giurgiu,         Stanislaw Jastrzebski,         Bruna Morrone,         Quentin De Laroussilhe,         Andrea Gesmundo,         Mona Attariyan,         Sylvain Gelly","Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to 262626 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.80.80.8% of the performance of full fine-tuning, adding only 3.63.63.6% parameters per task. By contrast, fine-tuning trains 100100100% of the parameters per task.",http://proceedings.mlr.press/v97/houlsby19a.html,http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf,ICML
83,2019,SELFIE: Refurbishing Unclean Samples for Robust Deep Learning,"Hwanjun Song,         Minseok Kim,         Jae-Gil Lee","Owing to the extremely high expressive power of deep neural networks, their side effect is to totally memorize training data even when the labels are extremely noisy. To overcome overfitting on the noisy labels, we propose a novel robust training method called SELFIE. Our key idea is to selectively refurbish and exploit unclean samples that can be corrected with high precision, thereby gradually increasing the number of available training samples. Taking advantage of this design, SELFIE effectively prevents the risk of noise accumulation from the false correction and fully exploits the training data. To validate the superiority of SELFIE, we conducted extensive experimentation using four real-world or synthetic data sets. The result showed that SELFIE remarkably improved absolute test error compared with two state-of-the-art methods.",http://proceedings.mlr.press/v97/song19b.html,http://proceedings.mlr.press/v97/song19b/song19b.pdf,ICML
84,2019,Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models,"Kaspar Märtens,         Kieran Campbell,         Christopher Yau","The interpretation of complex high-dimensional data typically requires the use of dimensionality reduction techniques to extract explanatory low-dimensional representations. However, in many real-world problems these representations may not be sufficient to aid interpretation on their own, and it would be desirable to interpret the model in terms of the original features themselves. Our goal is to characterise how feature-level variation depends on latent low-dimensional representations, external covariates, and non-linear interactions between the two. In this paper, we propose to achieve this through a structured kernel decomposition in a hybrid Gaussian Process model which we call the Covariate Gaussian Process Latent Variable Model (c-GPLVM). We demonstrate the utility of our model on simulated examples and applications in disease progression modelling from high-dimensional gene expression data in the presence of additional phenotypes. In each setting we show how the c-GPLVM can extract low-dimensional structures from high-dimensional data sets whilst allowing a breakdown of feature-level variability that is not present in other commonly used dimensionality reduction approaches.",http://proceedings.mlr.press/v97/martens19a.html,http://proceedings.mlr.press/v97/martens19a/martens19a.pdf,ICML
85,2019,Noise2Self: Blind Denoising by Self-Supervision,"Joshua Batson,         Loic Royer","We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement, while the true signal exhibits some correlation. For a broad class of functions (“JJ\mathcal{J}-invariant”), it is then possible to estimate the performance of a denoiser from noisy data alone. This allows us to calibrate JJ\mathcal{J}-invariant versions of any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.",http://proceedings.mlr.press/v97/batson19a.html,http://proceedings.mlr.press/v97/batson19a/batson19a.pdf,ICML
86,2019,Multi-Object Representation Learning with Iterative Variational Inference,"Klaus Greff,         Raphaël Lopez Kaufman,         Rishabh Kabra,         Nick Watters,         Christopher Burgess,         Daniel Zoran,         Loic Matthey,         Matthew Botvinick,         Alexander Lerchner","Human perception is structured around objects which form the basis for our higher-level cognition and impressive systematic generalization abilities. Yet most work on representation learning focuses on feature learning without even considering multiple objects, or treats segmentation as an (often supervised) preprocessing step. Instead, we argue for the importance of learning to segment and represent objects jointly. We demonstrate that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations. Our method learns – without supervision – to inpaint occluded parts, and extrapolates to scenes with more objects and to unseen objects with novel feature combinations. We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal posteriors for ambiguous inputs and extends naturally to sequences.",http://proceedings.mlr.press/v97/greff19a.html,http://proceedings.mlr.press/v97/greff19a/greff19a.pdf,ICML
87,2019,Game Theoretic Optimization via Gradient-based Nikaido-Isoda Function,"Arvind Raghunathan,         Anoop Cherian,         Devesh Jha","Computing Nash equilibrium (NE) of multi-player games has witnessed renewed interest due to recent advances in generative adversarial networks. However, computing equilibrium efficiently is challenging. To this end, we introduce the Gradient-based Nikaido-Isoda (GNI) function which serves: (i) as a merit function, vanishing only at the first-order stationary points of each player’s optimization problem, and (ii) provides error bounds to a stationary Nash point. Gradient descent is shown to converge sublinearly to a first-order stationary point of the GNI function. For the particular case of bilinear min-max games and multi-player quadratic games, the GNI function is convex. Hence, the application of gradient descent in this case yields linear convergence to an NE (when one exists). In our numerical experiments, we observe that the GNI formulation always converges to the first-order stationary point of each player’s optimization problem.",http://proceedings.mlr.press/v97/raghunathan19a.html,http://proceedings.mlr.press/v97/raghunathan19a/raghunathan19a.pdf,ICML
88,2019,Neural Joint Source-Channel Coding,"Kristy Choi,         Kedar Tatwawadi,         Aditya Grover,         Tsachy Weissman,         Stefano Ermon","For reliable transmission across a noisy communication channel, classical results from information theory show that it is asymptotically optimal to separate out the source and channel coding processes. However, this decomposition can fall short in the finite bit-length regime, as it requires non-trivial tuning of hand-crafted codes and assumes infinite computational power for decoding. In this work, we propose to jointly learn the encoding and decoding processes using a new discrete variational autoencoder model. By adding noise into the latent codes to simulate the channel during training, we learn to both compress and error-correct given a fixed bit-length and computational budget. We obtain codes that are not only competitive against several separation schemes, but also learn useful robust representations of the data for downstream tasks such as classification. Finally, inference amortization yields an extremely fast neural decoder, almost an order of magnitude faster compared to standard decoding methods based on iterative belief propagation.",http://proceedings.mlr.press/v97/choi19a.html,http://proceedings.mlr.press/v97/choi19a/choi19a.pdf,ICML
89,2019,Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search,"Youhei Akimoto,         Shinichi Shirakawa,         Nozomu Yoshinari,         Kento Uchida,         Shota Saito,         Kouhei Nishida","High sensitivity of neural architecture search (NAS) methods against their input such as step-size (i.e., learning rate) and search space prevents practitioners from applying them out-of-the-box to their own problems, albeit its purpose is to automate a part of tuning process. Aiming at a fast, robust, and widely-applicable NAS, we develop a generic optimization framework for NAS. We turn a coupled optimization of connection weights and neural architecture into a differentiable optimization by means of stochastic relaxation. It accepts arbitrary search space (widely-applicable) and enables to employ a gradient-based simultaneous optimization of weights and architecture (fast). We propose a stochastic natural gradient method with an adaptive step-size mechanism built upon our theoretical investigation (robust). Despite its simplicity and no problem-dependent parameter tuning, our method exhibited near state-of-the-art performances with low computational budgets both on image classification and inpainting tasks.",http://proceedings.mlr.press/v97/akimoto19a.html,http://proceedings.mlr.press/v97/akimoto19a/akimoto19a.pdf,ICML
90,2019,Sequential Facility Location: Approximate Submodularity and Greedy Algorithm,Ehsan Elhamifar,"We develop and analyze a novel utility function and a fast optimization algorithm for subset selection in sequential data that incorporates the dynamic model of data. We propose a cardinality-constrained sequential facility location function that finds a fixed number of representatives, where the sequence of representatives is compatible with the dynamic model and well encodes the data. As maximizing this new objective function is NP-hard, we develop a fast greedy algorithm based on submodular maximization. Unlike the conventional facility location, the computation of the marginal gain in our case cannot be done by operations on each item independently. We exploit the sequential structure of the problem and develop an efficient dynamic programming-based algorithm that computes the marginal gain exactly. We investigate conditions on the dynamic model, under which our utility function is (ϵϵ\epsilon-approximately) submodualr, hence, the greedy algorithm comes with performance guarantees. By experiments on synthetic data and the problem of procedure learning from instructional videos, we show that our framework significantly improves the computational time, achieves better objective function values and obtains more coherent summaries.",http://proceedings.mlr.press/v97/elhamifar19a.html,http://proceedings.mlr.press/v97/elhamifar19a/elhamifar19a.pdf,ICML
91,2019,The advantages of multiple classes for reducing overfitting from test set reuse,"Vitaly Feldman,         Roy Frostig,         Moritz Hardt","Excessive reuse of holdout data can lead to overfitting. However, there is little concrete evidence of significant overfitting due to holdout reuse in popular multiclass benchmarks today. Known results show that, in the worst-case, revealing the accuracy of kkk adaptively chosen classifiers on a data set of size nnn allows to create a classifier with bias of Θ(√k/n)Θ(k/n−−−√)\Theta(\sqrt{k/n}) for any binary prediction problem. We show a new upper bound of ˜O(max{√klog(n)/(mn),k/n})O~(max{klog(n)/(mn)−−−−−−−−−−−√,k/n})\tilde O(\max\{\sqrt{k\log(n)/(mn)}, k/n\}) on the worst-case bias that any attack can achieve in a prediction problem with mmm classes. Moreover, we present an efficient attack that achieve a bias of Ω(√k/(m2n))Ω(k/(m2n)−−−−−−−√)\Omega(\sqrt{k/(m^2 n)}) and improves on previous work for the binary setting (m=2m=2m=2). We also present an inefficient attack that achieves a bias of ˜Ω(k/n)Ω~(k/n)\tilde\Omega(k/n). Complementing our theoretical work, we give new practical attacks to stress-test multiclass benchmarks by aiming to create as large a bias as possible with a given number of queries. Our experiments show that the additional uncertainty of prediction with a large number of classes indeed mitigates the effect of our best attacks.",http://proceedings.mlr.press/v97/feldman19a.html,http://proceedings.mlr.press/v97/feldman19a/feldman19a.pdf,ICML
92,2019,Guided evolutionary strategies: augmenting random search with surrogate gradients,"Niru Maheswaranathan,         Luke Metz,         George Tucker,         Dami Choi,         Jascha Sohl-Dickstein","Many applications in machine learning require optimizing a function whose true gradient is unknown or computationally expensive, but where surrogate gradient information, directions that may be correlated with the true gradient, is cheaply available. For example, this occurs when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in reinforcement learning or training networks with discrete variables). We propose Guided Evolutionary Strategies (GES), a method for optimally using surrogate gradient directions to accelerate random search. GES defines a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients and estimates a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace and use this to derive a setting of the hyperparameters that works well across problems. We evaluate GES on several example problems, demonstrating an improvement over both standard evolutionary strategies and first-order methods that directly follow the surrogate gradient.",http://proceedings.mlr.press/v97/maheswaranathan19a.html,http://proceedings.mlr.press/v97/maheswaranathan19a/maheswaranathan19a.pdf,ICML
93,2019,"Trimming the ℓ1ℓ1\ell_1 Regularizer: Statistical Analysis, Optimization, and Applications to Deep Learning","Jihun Yun,         Peng Zheng,         Eunho Yang,         Aurelie Lozano,         Aleksandr Aravkin","We study high-dimensional estimators with the trimmed ℓ1ℓ1\ell_1 penalty, which leaves the h largest parameter entries penalty-free. While optimization techniques for this nonconvex penalty have been studied, the statistical properties have not yet been analyzed. We present the first statistical analyses for M-estimation, and characterize support recovery, ℓ∞ℓ∞\ell_\infty and ℓ2ℓ2\ell_2 error of the trimmed ℓ1ℓ1\ell_1 estimates as a function of the trimming parameter h. Our results show different regimes based on how h compares to the true support size. Our second contribution is a new algorithm for the trimmed regularization problem, which has the same theoretical convergence rate as difference of convex (DC) algorithms, but in practice is faster and finds lower objective values. Empirical evaluation of ℓ1ℓ1\ell_1 trimming for sparse linear regression and graphical model estimation indicate that trimmed ℓ1ℓ1\ell_1 can outperform vanilla ℓ1ℓ1\ell_1 and non-convex alternatives. Our last contribution is to show that the trimmed penalty is beneficial beyond M-estimation, and yields promising results for two deep learning tasks: input structures recovery and network sparsification.",http://proceedings.mlr.press/v97/yun19a.html,http://proceedings.mlr.press/v97/yun19a/yun19a.pdf,ICML
94,2019,COMIC: Multi-view Clustering Without Parameter Selection,"Xi Peng,         Zhenyu Huang,         Jiancheng Lv,         Hongyuan Zhu,         Joey Tianyi Zhou","In this paper, we study two challenges in clustering analysis, namely, how to cluster multi-view data and how to perform clustering without parameter selection on cluster size. To this end, we propose a novel objective function to project raw data into one space in which the projection embraces the geometric consistency (GC) and the cluster assignment consistency (CAC). To be specific, the GC aims to learn a connection graph from a projection space wherein the data points are connected if and only if they belong to the same cluster. The CAC aims to minimize the discrepancy of pairwise connection graphs induced from different views based on the view-consensus assumption, i.e., different views could produce the same cluster assignment structure as they are different portraits of the same object. Thanks to the view-consensus derived from the connection graph, our method could achieve promising performance in learning view-specific representation and eliminating the heterogeneous gaps across different views. Furthermore, with the proposed objective, it could learn almost all parameters including the cluster number from data without labor-intensive parameter selection. Extensive experimental results show the promising performance achieved by our method on five datasets comparing with nine state-of-the-art multi-view clustering approaches.",http://proceedings.mlr.press/v97/peng19a.html,http://proceedings.mlr.press/v97/peng19a/peng19a.pdf,ICML
95,2019,Trajectory-Based Off-Policy Deep Reinforcement Learning,"Andreas Doerr,         Michael Volpp,         Marc Toussaint,         Trimpe Sebastian,         Christian Daniel","Policy gradient methods are powerful reinforcement learning algorithms and have been demonstrated to solve many complex tasks. However, these methods are also data-inefficient, afflicted with high variance gradient estimates, and frequently get stuck in local optima. This work addresses these weaknesses by combining recent improvements in the reuse of off-policy data and exploration in parameter space with deterministic behavioral policies. The resulting objective is amenable to standard neural network optimization strategies like stochastic gradient descent or stochastic gradient Hamiltonian Monte Carlo. Incorporation of previous rollouts via importance sampling greatly improves data-efficiency, whilst stochastic optimization schemes facilitate the escape from local optima. We evaluate the proposed approach on a series of continuous control benchmark tasks. The results show that the proposed algorithm is able to successfully and reliably learn solutions using fewer system interactions than standard policy gradient methods.",http://proceedings.mlr.press/v97/doerr19a.html,http://proceedings.mlr.press/v97/doerr19a/doerr19a.pdf,ICML
96,2019,Faster Algorithms for Binary Matrix Factorization,"Ravi Kumar,         Rina Panigrahy,         Ali Rahimi,         David Woodruff","We give faster approximation algorithms for well-studied variants of Binary Matrix Factorization (BMF), where we are given a binary m×nm×nm \times n matrix AAA and would like to find binary rank-kkk matrices U,VU,VU, V to minimize the Frobenius norm of U⋅V−AU⋅V−AU \cdot V - A. In the first setting, U⋅VU⋅VU \cdot V denotes multiplication over ZZ\mathbb{Z}, and we give a constant-factor approximation algorithm that runs in 2O(k2logk)poly(mn)2O(k2log⁡k)poly(mn)2^{O(k^2 \log k)} \textrm{poly}(mn) time, improving upon the previous min(22k,2n)poly(mn)min(22k,2n)poly(mn)\min(2^{2^k}, 2^n) \textrm{poly}(mn) time. Our techniques generalize to minimizing ∥U⋅V−A∥p‖U⋅V−A‖p\|U \cdot V - A\|_p for p≥1p≥1p \geq 1, in 2O(k⌈p/2⌉+1logk)poly(mn)2O(k⌈p/2⌉+1log⁡k)poly(mn)2^{O(k^{\lceil p/2 \rceil + 1}\log k)} \textrm{poly}(mn) time. For p=1p=1p = 1, this has a graph-theoretic consequence, namely, a 2O(k2)\poly(mn)2O(k2)\poly(mn)2^{O(k^2)} \poly(mn)-time algorithm to approximate a graph as a union of disjoint bicliques. In the second setting, U⋅VU⋅VU \cdot V is over \GF(2)\GF(2)\GF(2), and we give a bicriteria constant-factor approximation algorithm that runs in 2O(k3)\poly(mn)2O(k3)\poly(mn)2^{O(k^3)} \poly(mn) time to find binary rank-O(klogm)O(klog⁡m)O(k \log m) matrices UUU, VVV whose cost is as good as the best rank-kkk approximation, improving upon min(22kmn,min(m,n)kO(1)poly(mn))min(22kmn,min(m,n)kO(1)poly(mn))\min(2^{2^k}mn, \min(m,n)^{k^{O(1)}} \textrm{poly}(mn)) time.",http://proceedings.mlr.press/v97/kumar19a.html,http://proceedings.mlr.press/v97/kumar19a/kumar19a.pdf,ICML
97,2019,Domain Agnostic Learning with Disentangled Representations,"Xingchao Peng,         Zijun Huang,         Ximeng Sun,         Kate Saenko","Unsupervised model transfer has the potential to greatly improve the generalizability of deep models to novel domains. Yet the current literature assumes that the separation of target data into distinct domains is known a priori. In this paper, we propose the task of Domain-Agnostic Learning (DAL): How to transfer knowledge from a labeled source domain to unlabeled data from arbitrary target domains? To tackle this problem, we devise a novel Deep Adversarial Disentangled Autoencoder (DADA) capable of disentangling domain-specific features from class identity. We demonstrate experimentally that when the target domain labels are unknown, DADA leads to state-of-the-art performance on several image classification datasets.",http://proceedings.mlr.press/v97/peng19b.html,http://proceedings.mlr.press/v97/peng19b/peng19b.pdf,ICML
98,2019,Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,"Anastasia Koloskova,         Sebastian Stich,         Martin Jaggi","We consider decentralized stochastic optimization with the objective function (e.g. data samples for machine learning tasks) being distributed over n machines that can only communicate to their neighbors on a fixed communication graph. To address the communication bottleneck, the nodes compress (e.g. quantize or sparsify) their model updates. We cover both unbiased and biased compression operators with quality denoted by \delta <= 1 (\delta=1 meaning no compression). We (i) propose a novel gossip-based stochastic gradient descent algorithm, CHOCO-SGD, that converges at rate O(1/(nT) + 1/(T \rho^2 \delta)^2) for strongly convex objectives, where T denotes the number of iterations and \rho the eigengap of the connectivity matrix. We (ii) present a novel gossip algorithm, CHOCO-GOSSIP, for the average consensus problem that converges in time O(1/(\rho^2\delta) \log (1/\epsilon)) for accuracy \epsilon > 0. This is (up to our knowledge) the first gossip algorithm that supports arbitrary compressed messages for \delta > 0 and still exhibits linear convergence. We (iii) show in experiments that both of our algorithms do outperform the respective state-of-the-art baselines and CHOCO-SGD can reduce communication by at least two orders of magnitudes.",http://proceedings.mlr.press/v97/koloskova19a.html,http://proceedings.mlr.press/v97/koloskova19a/koloskova19a.pdf,ICML
99,2019,On Learning Invariant Representations for Domain Adaptation,"Han Zhao,         Remi Tachet Des Combes,         Kun Zhang,         Geoffrey Gordon","Due to the ability of deep neural nets to learn rich representations, recent advances in unsupervised domain adaptation have focused on learning domain-invariant features that achieve a small error on the source domain. The hope is that the learnt representation, together with the hypothesis learnt from the source domain, can generalize to the target domain. In this paper, we first construct a simple counterexample showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adaptation. In particular, the counterexample exhibits conditional shift: the class-conditional distributions of input features change between source and target domains. To give a sufficient condition for domain adaptation, we propose a natural and interpretable generalization upper bound that explicitly takes into account the aforementioned shift. Moreover, we shed new light on the problem by proving an information-theoretic lower bound on the joint error of any domain adaptation method that attempts to learn invariant representations. Our result characterizes a fundamental tradeoff between learning invariant representations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of domain adaptation and representation learning algorithms.",http://proceedings.mlr.press/v97/zhao19a.html,http://proceedings.mlr.press/v97/zhao19a/zhao19a.pdf,ICML
100,2019,Correlated bandits or: How to minimize mean-squared error online,"Vinay Praneeth Boda,         Prashanth L.A.","While the objective in traditional multi-armed bandit problems is to find the arm with the highest mean, in many settings, finding an arm that best captures information about other arms is of interest. This objective, however, requires learning the underlying correlation structure and not just the means. Sensors placement for industrial surveillance and cellular network monitoring are a few applications, where the underlying correlation structure plays an important role. Motivated by such applications, we formulate the correlated bandit problem, where the objective is to find the arm with the lowest mean-squared error (MSE) in estimating all the arms. To this end, we derive first an MSE estimator based on sample variances/covariances and show that our estimator exponentially concentrates around the true MSE. Under a best-arm identification framework, we propose a successive rejects type algorithm and provide bounds on the probability of error in identifying the best arm. Using minimax theory, we also derive fundamental performance limits for the correlated bandit problem.",http://proceedings.mlr.press/v97/boda19a.html,http://proceedings.mlr.press/v97/boda19a/boda19a.pdf,ICML
101,2019,On the Spectral Bias of Neural Networks,"Nasim Rahaman,         Aristide Baratin,         Devansh Arpit,         Felix Draxler,         Min Lin,         Fred Hamprecht,         Yoshua Bengio,         Aaron Courville","Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions – i.e. functions that vary globally without local fluctuations – which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.",http://proceedings.mlr.press/v97/rahaman19a.html,http://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf,ICML
102,2019,Geometry Aware Convolutional Filters for Omnidirectional Images Representation,"Renata Khasanova,         Pascal Frossard","Due to their wide field of view, omnidirectional cameras are frequently used by autonomous vehicles, drones and robots for navigation and other computer vision tasks. The images captured by such cameras, are often analyzed and classified with techniques designed for planar images that unfortunately fail to properly handle the native geometry of such images and therefore results in suboptimal performance. In this paper we aim at improving popular deep convolutional neural networks so that they can properly take into account the specific properties of omnidirectional data. In particular we propose an algorithm that adapts convolutional layers, which often serve as a core building block of a CNN, to the properties of omnidirectional images. Thus, our filters have a shape and size that adapt to the location on the omnidirectional image. We show that our method is not limited to spherical surfaces and is able to incorporate the knowledge about any kind of projective geometry inside the deep learning network. As depicted by our experiments, our method outperforms the existing deep neural network techniques for omnidirectional image classification and compression tasks.",http://proceedings.mlr.press/v97/khasanova19a.html,http://proceedings.mlr.press/v97/khasanova19a/khasanova19a.pdf,ICML
103,2019,Active Manifolds: A non-linear analogue to Active Subspaces,"Robert Bridges,         Anthony Gruber,         Christopher Felder,         Miki Verma,         Chelsey Hoff","We present an approach to analyze C1(Rm)C^1(\mathbb{R}^m) functions that addresses limitations present in the Active Subspaces (AS) method of Constantine et al. (2014; 2015). Under appropriate hypotheses, our Active Manifolds (AM) method identifies a 1-D curve in the domain (the active manifold) on which nearly all values of the unknown function are attained, which can be exploited for approximation or analysis, especially when mm is large (high-dimensional input space). We provide theorems justifying our AM technique and an algorithm permitting functional approximation and sensitivity analysis. Using accessible, low-dimensional functions as initial examples, we show AM reduces approximation error by an order of magnitude compared to AS, at the expense of more computation. Following this, we revisit the sensitivity analysis by Glaws et al. (2017), who apply AS to analyze a magnetohydrodynamic power generator model, and compare the performance of AM on the same data. Our analysis provides detailed information not captured by AS, exhibiting the influence of each parameter individually along an active manifold. Overall, AM represents a novel technique for analyzing functional models with benefits including: reducing mm-dimensional analysis to a 1-D analogue, permitting more accurate regression than AS (at more computational expense), enabling more informative sensitivity analysis, and granting accessible visualizations (2-D plots) of parameter sensitivity along the AM.",http://proceedings.mlr.press/v97/bridges19a.html,http://proceedings.mlr.press/v97/bridges19a/bridges19a.pdf,ICML
104,2019,Dual Entangled Polynomial Code: Three-Dimensional Coding for Distributed Matrix Multiplication,"Pedro Soto,         Jun Li,         Xiaodi Fan","Matrix multiplication is a fundamental building block in various machine learning algorithms. When the matrix comes from a large dataset, the multiplication can be split into multiple tasks which calculate the multiplication of submatrices on different nodes. As some nodes may be stragglers, coding schemes have been proposed to tolerate stragglers in such distributed matrix multiplication. However, existing coding schemes typically split the matrices in only one or two dimensions, limiting their capabilities to handle large-scale matrix multiplication. Three-dimensional coding, however, does not have any code construction that achieves the optimal number of tasks required for decoding, with the best result achieved by entangled polynomial (EP) codes. In this paper, we propose dual entangled polynomial (DEP) codes that require around 25% fewer tasks than EP codes by executing two matrix multiplications on each task. With experiments in a real cloud environment, we show that DEP codes can also save the decoding overhead and memory consumption of tasks.",http://proceedings.mlr.press/v97/soto19a.html,http://proceedings.mlr.press/v97/soto19a/soto19a.pdf,ICML
105,2019,"Anytime Online-to-Batch, Optimism and Acceleration",Ashok Cutkosky,"A standard way to obtain convergence guarantees in stochastic convex optimization is to run an online learning algorithm and then output the average of its iterates: the actual iterates of the online learning algorithm do not come with individual guarantees. We close this gap by introducing a black-box modification to any online learning algorithm whose iterates converge to the optimum in stochastic scenarios. We then consider the case of smooth losses, and show that combining our approach with optimistic online learning algorithms immediately yields a fast convergence rate of O(L/T3/2+σ/T−−√)O(L/T3/2+σ/T)O(L/T^{3/2}+\sigma/\sqrt{T}) on LLL-smooth problems with σ2σ2\sigma^2 variance in the gradients. Finally, we provide a reduction that converts any adaptive online algorithm into one that obtains the optimal accelerated rate of O~(L/T2+σ/T−−√)O~(L/T2+σ/T)\tilde O(L/T^2 + \sigma/\sqrt{T}), while still maintaining O~(1/T−−√)O~(1/T)\tilde O(1/\sqrt{T}) convergence in the non-smooth setting. Importantly, our algorithms adapt to LLL and σσ\sigma automatically: they do not need to know either to obtain these rates.",http://proceedings.mlr.press/v97/cutkosky19a.html,http://proceedings.mlr.press/v97/cutkosky19a/cutkosky19a.pdf,ICML
106,2019,Active Learning for Decision-Making from Imbalanced Observational Data,"Iiris Sundin,         Peter Schulam,         Eero Siivola,         Aki Vehtari,         Suchi Saria,         Samuel Kaski","Machine learning can help personalized decision support by learning models to predict individual treatment effects (ITE). This work studies the reliability of prediction-based decision-making in a task of deciding which action aaa to take for a target unit after observing its covariates x~x~\tilde{x} and predicted outcomes p^(y~∣x~,a)p^(y~∣x~,a)\hat{p}(\tilde{y} \mid \tilde{x}, a). An example case is personalized medicine and the decision of which treatment to give to a patient. A common problem when learning these models from observational data is imbalance, that is, difference in treated/control covariate distributions, which is known to increase the upper bound of the expected ITE estimation error. We propose to assess the decision-making reliability by estimating the ITE model’s Type S error rate, which is the probability of the model inferring the sign of the treatment effect wrong. Furthermore, we use the estimated reliability as a criterion for active learning, in order to collect new (possibly expensive) observations, instead of making a forced choice based on unreliable predictions. We demonstrate the effectiveness of this decision-making aware active learning in two decision-making tasks: in simulated data with binary outcomes and in a medical dataset with synthetic and continuous treatment outcomes.",http://proceedings.mlr.press/v97/sundin19a.html,http://proceedings.mlr.press/v97/sundin19a/sundin19a.pdf,ICML
107,2019,Learning What and Where to Transfer,"Yunhun Jang,         Hankook Lee,         Sung Ju Hwang,         Jinwoo Shin","As the application of deep learning has expanded to real-world problems with insufficient volume of training data, transfer learning recently has gained much attention as means of improving the performance in such small-data regime. However, when existing methods are applied between heterogeneous architectures and tasks, it becomes more important to manage their detailed configurations and often requires exhaustive tuning on them for the desired performance. To address the issue, we propose a novel transfer learning approach based on meta-learning that can automatically learn what knowledge to transfer from the source network to where in the target network. Given source and target networks, we propose an efficient training scheme to learn meta-networks that decide (a) which pairs of layers between the source and target networks should be matched for knowledge transfer and (b) which features and how much knowledge from each feature should be transferred. We validate our meta-transfer approach against recent transfer learning methods on various datasets and network architectures, on which our automated scheme significantly outperforms the prior baselines that find “what and where to transfer” in a hand-crafted manner.",http://proceedings.mlr.press/v97/jang19b.html,http://proceedings.mlr.press/v97/jang19b/jang19b.pdf,ICML
108,2019,Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation,"Xinyang Chen,         Sinan Wang,         Mingsheng Long,         Jianmin Wang","Adversarial domain adaptation has made remarkable advances in learning transferable representations for knowledge transfer across domains. While adversarial learning strengthens the feature transferability which the community focuses on, its impact on the feature discriminability has not been fully explored. In this paper, a series of experiments based on spectral analysis of the feature representations have been conducted, revealing an unexpected deterioration of the discriminability while learning transferable features adversarially. Our key finding is that the eigenvectors with the largest singular values will dominate the feature transferability. As a consequence, the transferability is enhanced at the expense of over penalization of other eigenvectors that embody rich structures crucial for discriminability. Towards this problem, we present Batch Spectral Penalization (BSP), a general approach to penalizing the largest singular values so that other eigenvectors can be relatively strengthened to boost the feature discriminability. Experiments show that the approach significantly improves upon representative adversarial domain adaptation methods to yield state of the art results.",http://proceedings.mlr.press/v97/chen19i.html,http://proceedings.mlr.press/v97/chen19i/chen19i.pdf,ICML
109,2019,Hierarchical Importance Weighted Autoencoders,"Chin-Wei Huang,         Kris Sankaran,         Eeshan Dhekane,         Alexandre Lacoste,         Aaron Courville","Importance weighted variational inference (Burda et al., 2015) uses multiple i.i.d. samples to have a tighter variational lower bound. We believe a joint proposal has the potential of reducing the number of redundant samples, and introduce a hierarchical structure to induce correlation. The hope is that the proposals would coordinate to make up for the error made by one another to reduce the variance of the importance estimator. Theoretically, we analyze the condition under which convergence of the estimator variance can be connected to convergence of the lower bound. Empirically, we confirm that maximization of the lower bound does implicitly minimize variance. Further analysis shows that this is a result of negative correlation induced by the proposed hierarchical meta sampling scheme, and performance of inference also improves when the number of samples increases.",http://proceedings.mlr.press/v97/huang19d.html,http://proceedings.mlr.press/v97/huang19d/huang19d.pdf,ICML
110,2019,SelectiveNet: A Deep Neural Network with an Integrated Reject Option,"Yonatan Geifman,         Ran El-Yaniv","We consider the problem of selective prediction (also known as reject option) in deep neural networks, and introduce SelectiveNet, a deep neural architecture with an integrated reject option. Existing rejection mechanisms are based mostly on a threshold over the prediction confidence of a pre-trained network. In contrast, SelectiveNet is trained to optimize both classification (or regression) and rejection simultaneously, end-to-end. The result is a deep neural network that is optimized over the covered domain. In our experiments, we show a consistently improved risk-coverage trade-off over several well-known classification and regression datasets, thus reaching new state-of-the-art results for deep selective classification.",http://proceedings.mlr.press/v97/geifman19a.html,http://proceedings.mlr.press/v97/geifman19a/geifman19a.pdf,ICML
111,2019,Co-Representation Network for Generalized Zero-Shot Learning,"Fei Zhang,         Guangming Shi","Generalized zero-shot learning is a significant topic but faced with bias problem, which leads to unseen classes being easily misclassified into seen classes. Hence we propose a embedding model called co-representation network to learn a more uniform visual embedding space that effectively alleviates the bias problem and helps with classification. We mathematically analyze our model and find it learns a projection with high local linearity, which is proved to cause less bias problem. The network consists of a cooperation module for representation and a relation module for classification, it is simple in structure and can be easily trained in an end-to-end manner. Experiments show that our method outperforms existing generalized zero-shot learning methods on several benchmark datasets.",http://proceedings.mlr.press/v97/zhang19l.html,http://proceedings.mlr.press/v97/zhang19l/zhang19l.pdf,ICML
112,2019,Causal Discovery and Forecasting in Nonstationary Environments with State-Space Models,"Biwei Huang,         Kun Zhang,         Mingming Gong,         Clark Glymour","In many scientific fields, such as economics and neuroscience, we are often faced with nonstationary time series, and concerned with both finding causal relations and forecasting the values of variables of interest, both of which are particularly challenging in such nonstationary environments. In this paper, we study causal discovery and forecasting for nonstationary time series. By exploiting a particular type of state-space model to represent the processes, we show that nonstationarity helps to identify the causal structure, and that forecasting naturally benefits from learned causal knowledge. Specifically, we allow changes in both causal strengths and noise variances in the nonlinear state-space models, which, interestingly, renders both the causal structure and model parameters identifiable. Given the causal model, we treat forecasting as a problem in Bayesian inference in the causal model, which exploits the time-varying property of the data and adapts to new observations in a principled manner. Experimental results on synthetic and real-world data sets demonstrate the efficacy of the proposed methods.",http://proceedings.mlr.press/v97/huang19g.html,http://proceedings.mlr.press/v97/huang19g/huang19g.pdf,ICML
113,2019,Learning to Convolve: A Generalized Weight-Tying Approach,"Nichita Diaconu,         Daniel Worrall","Recent work (Cohen & Welling, 2016) has shown that generalizations of convolutions, based on group theory, provide powerful inductive biases for learning. In these generalizations, filters are not only translated but can also be rotated, flipped, etc. However, coming up with exact models of how to rotate a 3x3 filter on a square pixel-grid is difficult. In this paper, we learn how to transform filters for use in the group convolution, focussing on roto-translation. For this, we learn a filter basis and all rotated versions of that filter basis. Filters are then encoded by a set of rotation invariant coefficients. To rotate a filter, we switch the basis. We demonstrate we can produce feature maps with low sensitivity to input rotations, while achieving high performance on MNIST and CIFAR-10.",http://proceedings.mlr.press/v97/diaconu19a.html,http://proceedings.mlr.press/v97/diaconu19a/diaconu19a.pdf,ICML
114,2019,Improved Convergence for ℓ1ℓ1\ell_1 and ℓ∞ℓ∞\ell_∞ Regression via Iteratively Reweighted Least Squares,"Alina Ene,         Adrian Vladu","The iteratively reweighted least squares method (IRLS) is a popular technique used in practice for solving regression problems. Various versions of this method have been proposed, but their theoretical analyses failed to capture the good practical performance. In this paper we propose a simple and natural version of IRLS for solving ℓ∞ℓ∞\ell_\infty and ℓ1ℓ1\ell_1 regression, which provably converges to a (1+ϵ)(1+ϵ)(1+\epsilon)-approximate solution in O(m1/3log(1/ϵ)/ϵ2/3+logm/ϵ2)O(m1/3log⁡(1/ϵ)/ϵ2/3+log⁡m/ϵ2)O(m^{1/3}\log(1/\epsilon)/\epsilon^{2/3} + \log m/\epsilon^2) iterations, where mmm is the number of rows of the input matrix. Interestingly, this running time is independent of the conditioning of the input, and the dominant term of the running time depends sublinearly in ϵ−1ϵ−1\epsilon^{-1}, which is atypical for the optimization of non-smooth functions. This improves upon the more complex algorithms of Chin et al. (ITCS ’12), and Christiano et al. (STOC ’11) by a factor of at least 1/ϵ21/ϵ21/\epsilon^2, and yields a truly efficient natural algorithm for the slime mold dynamics (Straszak-Vishnoi, SODA ’16, ITCS ’16, ITCS ’17).",http://proceedings.mlr.press/v97/ene19a.html,http://proceedings.mlr.press/v97/ene19a/ene19a.pdf,ICML
115,2019,Cautious Regret Minimization: Online Optimization with Long-Term Budget Constraints,"Nikolaos Liakopoulos,         Apostolos Destounis,         Georgios Paschos,         Thrasyvoulos Spyropoulos,         Panayotis Mertikopoulos","We study a class of online convex optimization problems with long-term budget constraints that arise naturally as reliability guarantees or total consumption constraints. In this general setting, prior work by Mannor et al. (2009) has shown that achieving no regret is impossible if the functions defining the agent’s budget are chosen by an adversary. To overcome this obstacle, we refine the agent’s regret metric by introducing the notion of a ""K-benchmark"", i.e., a comparator which meets the problem’s allotted budget over any window of length K. The impossibility analysis of Mannor et al. (2009) is recovered when K=T; however, for K=o(T), we show that it is possible to minimize regret while still meeting the problem’s long-term budget constraints. We achieve this via an online learning policy based on Cautious Online Lagrangiant Descent (COLD) for which we derive explicit bounds, in terms of both the incurred regret and the residual budget violations.",http://proceedings.mlr.press/v97/liakopoulos19a.html,http://proceedings.mlr.press/v97/liakopoulos19a/liakopoulos19a.pdf,ICML
116,2019,"Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition","Yao Qin,         Nicholas Carlini,         Garrison Cottrell,         Ian Goodfellow,         Colin Raffel","Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial examples on speech recognition systems have neither of these properties: humans can easily identify the adversarial perturbations, and they are not effective when played over-the-air. This paper makes progress on both of these fronts. First, we develop effectively imperceptible audio adversarial examples (verified through a human study) by leveraging the psychoacoustic principle of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets. Then, we make progress towards physical-world audio adversarial examples by constructing perturbations which remain effective even after applying highly-realistic simulated environmental distortions.",http://proceedings.mlr.press/v97/qin19a.html,http://proceedings.mlr.press/v97/qin19a/qin19a.pdf,ICML
117,2019,Meta-Learning Neural Bloom Filters,"Jack Rae,         Sergey Bartunov,         Timothy Lillicrap","There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression. In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In applications where inputs arrive at high throughput, or are ephemeral, training a network from scratch is not practical. This motivates the need for few-shot neural data structures. In this paper we explore the learning of approximate set membership over a set of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which is able to achieve significant compression gains over classical Bloom Filters and existing memory-augmented neural networks.",http://proceedings.mlr.press/v97/rae19a.html,http://proceedings.mlr.press/v97/rae19a/rae19a.pdf,ICML
118,2019,Random Shuffling Beats SGD after Finite Epochs,"Jeff Haochen,         Suvrit Sra","A long-standing problem in stochastic optimization is proving that \rsgd, the without-replacement version of \sgd, converges faster than the usual with-replacement \sgd. Building upon \citep{gurbuzbalaban2015random}, we present the first (to our knowledge) non-asymptotic results for this problem by proving that after a reasonable number of epochs \rsgd converges faster than \sgd. Specifically, we prove that for strongly convex, second-order smooth functions, the iterates of \rsgd converge to the optimal solution as O(\nicefrac1T2+\nicefracn3T3)O(\nicefrac1T2+\nicefracn3T3)\mathcal{O}(\nicefrac{1}{T^2} + \nicefrac{n^3}{T^3}), where nnn is the number of components in the objective, and TTT is number of iterations. This result implies that after O(√n)O(n−−√)\mathcal{O}(\sqrt{n}) epochs, \rsgd is strictly better than \sgd (which converges as O(\nicefrac1T)O(\nicefrac1T)\mathcal{O}(\nicefrac{1}{T})). The key step toward showing this better dependence on TTT is the introduction of nnn into the bound; and as our analysis shows, in general a dependence on nnn is unavoidable without further changes. To understand how \rsgd works in practice, we further explore two empirically useful settings: data sparsity and over-parameterization. For sparse data, \rsgd has the rate O(1T2)\mathcal{O}\left(\frac{1}{T^2}\right), again strictly better than \sgd. Under a setting closely related to over-parameterization, \rsgd is shown to converge faster than \sgd after any arbitrary number of iterations. Finally, we extend the analysis of \rsgd to smooth non-convex and convex functions.",http://proceedings.mlr.press/v97/haochen19a.html,http://proceedings.mlr.press/v97/haochen19a/haochen19a.pdf,ICML
119,2019,Large-Scale Sparse Kernel Canonical Correlation Analysis,"Viivi Uurtio,         Sahely Bhadra,         Juho Rousu","This paper presents gradKCCA, a large-scale sparse non-linear canonical correlation method. Like Kernel Canonical Correlation Analysis (KCCA), our method finds non-linear relations through kernel functions, but it does not rely on a kernel matrix, a known bottleneck for scaling up kernel methods. gradKCCA corresponds to solving KCCA with the additional constraint that the canonical projection directions in the kernel-induced feature space have preimages in the original data space. Firstly, this modification allows us to very efficiently maximize kernel canonical correlation through an alternating projected gradient algorithm working in the original data space. Secondly, we can control the sparsity of the projection directions by constraining the ℓ1ℓ1\ell_1 norm of the preimages of the projection directions, facilitating the interpretation of the discovered patterns, which is not available through KCCA. Our empirical experiments demonstrate that gradKCCA outperforms state-of-the-art CCA methods in terms of speed and robustness to noise both in simulated and real-world datasets.",http://proceedings.mlr.press/v97/uurtio19a.html,http://proceedings.mlr.press/v97/uurtio19a/uurtio19a.pdf,ICML
120,2019,On the Computation and Communication Complexity of Parallel SGD with Dynamic Batch Sizes for Stochastic Non-Convex Optimization,"Hao Yu,         Rong Jin","For SGD based distributed stochastic optimization, computation complexity, measured by the convergence rate in terms of the number of stochastic gradient calls, and communication complexity, measured by the number of inter-node communication rounds, are two most important performance metrics. The classical data-parallel implementation of SGD over N workers can achieve linear speedup of its convergence rate but incurs an inter-node communication round at each batch. We study the benefit of using dynamically increasing batch sizes in parallel SGD for stochastic non-convex optimization by charactering the attained convergence rate and the required number of communication rounds. We show that for stochastic non-convex optimization under the P-L condition, the classical data-parallel SGD with exponentially increasing batch sizes can achieve the fastest known O(1/(NT))O(1/(NT))O(1/(NT)) convergence with linear speedup using only log(T)log⁡(T)\log(T) communication rounds. For general stochastic non-convex optimization, we propose a Catalyst-like algorithm to achieve the fastest known O(1/NT−−−√)O(1/NT)O(1/\sqrt{NT}) convergence with only O(NT−−−√log(TN))O(NTlog⁡(TN))O(\sqrt{NT}\log(\frac{T}{N})) communication rounds.",http://proceedings.mlr.press/v97/yu19c.html,http://proceedings.mlr.press/v97/yu19c/yu19c.pdf,ICML
121,2019,Counterfactual Visual Explanations,"Yash Goyal,         Ziyan Wu,         Jan Ernst,         Dhruv Batra,         Devi Parikh,         Stefan Lee","In this work, we develop a technique to produce counterfactual visual explanations. Given a ‘query’ image III for which a vision system predicts class ccc, a counterfactual visual explanation identifies how III could change such that the system would output a different specified class c′c′c’. To do this, we select a ‘distractor’ image I′I′I’ that the system predicts as class c′c′c’ and identify spatial regions in III and I′I′I’ such that replacing the identified region in III with the identified region in I′I′I’ would push the system towards classifying III as c′c′c’. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.",http://proceedings.mlr.press/v97/goyal19a.html,http://proceedings.mlr.press/v97/goyal19a/goyal19a.pdf,ICML
122,2019,Lorentzian Distance Learning for Hyperbolic Representations,"Marc Law,         Renjie Liao,         Jake Snell,         Richard Zemel","We introduce an approach to learn representations based on the Lorentzian distance in hyperbolic geometry. Hyperbolic geometry is especially suited to hierarchically-structured datasets, which are prevalent in the real world. Current hyperbolic representation learning methods compare examples with the Poincaré distance. They try to minimize the distance of each node in a hierarchy with its descendants while maximizing its distance with other nodes. This formulation produces node representations close to the centroid of their descendants. To obtain efficient and interpretable algorithms, we exploit the fact that the centroid w.r.t the squared Lorentzian distance can be written in closed-form. We show that the Euclidean norm of such a centroid decreases as the curvature of the hyperbolic space decreases. This property makes it appropriate to represent hierarchies where parent nodes minimize the distances to their descendants and have smaller Euclidean norm than their children. Our approach obtains state-of-the-art results in retrieval and classification tasks on different datasets.",http://proceedings.mlr.press/v97/law19a.html,http://proceedings.mlr.press/v97/law19a/law19a.pdf,ICML
123,2019,Invertible Residual Networks,"Jens Behrmann,         Will Grathwohl,         Ricky T. Q. Chen,         David Duvenaud,         Joern-Henrik Jacobsen","We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.",http://proceedings.mlr.press/v97/behrmann19a.html,http://proceedings.mlr.press/v97/behrmann19a/behrmann19a.pdf,ICML
124,2019,Dimensionality Reduction for Tukey Regression,"Kenneth Clarkson,         Ruosong Wang,         David Woodruff","We give the first dimensionality reduction methods for the overconstrained Tukey regression problem. The Tukey loss function ‖y‖M=∑iM(yi)∥y∥M=∑iM(yi)\|y\|_M = \sum_i M(y_i) has M(yi)≈|yi|pM(yi)≈|yi|pM(y_i) \approx |y_i|^p for residual errors yiyiy_i smaller than a prescribed threshold ττ\tau, but M(yi)M(yi)M(y_i) becomes constant for errors |yi|>τ|yi|>τ|y_i| > \tau. Our results depend on a new structural result, proven constructively, showing that for any ddd-dimensional subspace L⊂RnL⊂RnL \subset \mathbb{R}^n, there is a fixed bounded-size subset of coordinates containing, for every y∈Ly∈Ly \in L, all the large coordinates, with respect to the Tukey loss function, of yyy. Our methods reduce a given Tukey regression problem to a smaller weighted version, whose solution is a provably good approximate solution to the original problem. Our reductions are fast, simple and easy to implement, and we give empirical results demonstrating their practicality, using existing heuristic solvers for the small versions. We also give exponential-time algorithms giving provably good solutions, and hardness results suggesting that a significant speedup in the worst case is unlikely.",http://proceedings.mlr.press/v97/clarkson19a.html,http://proceedings.mlr.press/v97/clarkson19a/clarkson19a.pdf,ICML
125,2019,Rates of Convergence for Sparse Variational Gaussian Process Regression,"David Burt,         Carl Edward Rasmussen,         Mark Van Der Wilk","Excellent variational approximations to Gaussian process posteriors have been developed which avoid the O(N3)O(N3)\mathcal{O}\left(N^3\right) scaling with dataset size NNN. They reduce the computational cost to O(NM2)O(NM2)\mathcal{O}\left(NM^2\right), with M≪NM≪NM\ll N the number of inducing variables, which summarise the process. While the computational cost seems to be linear in NNN, the true complexity of the algorithm depends on how MMM must increase to ensure a certain quality of approximation. We show that with high probability the KL divergence can be made arbitrarily small by growing MMM more slowly than NNN. A particular case is that for regression with normally distributed inputs in D-dimensions with the Squared Exponential kernel, M=O(logDN)M=O(logD⁡N)M=\mathcal{O}(\log^D N) suffices. Our results show that as datasets grow, Gaussian process posteriors can be approximated cheaply, and provide a concrete rule for how to increase MMM in continual learning scenarios.",http://proceedings.mlr.press/v97/burt19a.html,http://proceedings.mlr.press/v97/burt19a/burt19a.pdf,ICML
126,2019,Bayesian Optimization Meets Bayesian Optimal Stopping,"Zhongxiang Dai,         Haibin Yu,         Bryan Kian Hsiang Low,         Patrick Jaillet","Bayesian optimization (BO) is a popular paradigm for optimizing the hyperparameters of machine learning (ML) models due to its sample efficiency. Many ML models require running an iterative training procedure (e.g., stochastic gradient descent). This motivates the question whether information available during the training process (e.g., validation accuracy after each epoch) can be exploited for improving the epoch efficiency of BO algorithms by early-stopping model training under hyperparameter settings that will end up under-performing and hence eliminating unnecessary training epochs. This paper proposes to unify BO (specifically, Gaussian process-upper confidence bound (GP-UCB)) with Bayesian optimal stopping (BO-BOS) to boost the epoch efficiency of BO. To achieve this, while GP-UCB is sample-efficient in the number of function evaluations, BOS complements it with epoch efficiency for each function evaluation by providing a principled optimal stopping mechanism for early stopping. BO-BOS preserves the (asymptotic) no-regret performance of GP-UCB using our specified choice of BOS parameters that is amenable to an elegant interpretation in terms of the exploration-exploitation trade-off. We empirically evaluate the performance of BO-BOS and demonstrate its generality in hyperparameter optimization of ML models and two other interesting applications.",http://proceedings.mlr.press/v97/dai19a.html,http://proceedings.mlr.press/v97/dai19a/dai19a.pdf,ICML
127,2019,Revisiting precision recall definition for generative modeling,"Loic Simon,         Ryan Webster,         Julien Rabin","In this article we revisit the definition of Precision-Recall (PR) curves for generative models proposed by (Sajjadi et al., 2018). Rather than providing a scalar for generative quality, PR curves distinguish mode-collapse (poor recall) and bad quality (poor precision). We first generalize their formulation to arbitrary measures hence removing any restriction to finite support. We also expose a bridge between PR curves and type I and type II error (a.k.a. false detection and rejection) rates of likelihood ratio classifiers on the task of discriminating between samples of the two distributions. Building upon this new perspective, we propose a novel algorithm to approximate precision-recall curves, that shares some interesting methodological properties with the hypothesis testing technique from (Lopez-Paz & Oquab, 2017). We demonstrate the interest of the proposed formulation over the original approach on controlled multi-modal datasets.",http://proceedings.mlr.press/v97/simon19a.html,http://proceedings.mlr.press/v97/simon19a/simon19a.pdf,ICML
128,2019,Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization,"Seungyong Moon,         Gaon An,         Hyun Oh Song","Solving for adversarial examples with projected gradient descent has been demonstrated to be highly effective in fooling the neural network based classifiers. However, in the black-box setting, the attacker is limited only to the query access to the network and solving for a successful adversarial example becomes much more difficult. To this end, recent methods aim at estimating the true gradient signal based on the input queries but at the cost of excessive queries. We propose an efficient discrete surrogate to the optimization problem which does not require estimating the gradient and consequently becomes free of the first order update hyperparameters to tune. Our experiments on Cifar-10 and ImageNet show the state of the art black-box attack performance with significant reduction in the required queries compared to a number of recently proposed methods. The source code is available at https://github.com/snu-mllab/parsimonious-blackbox-attack.",http://proceedings.mlr.press/v97/moon19a.html,http://proceedings.mlr.press/v97/moon19a/moon19a.pdf,ICML
129,2019,Understanding Priors in Bayesian Neural Networks at the Unit Level,"Mariia Vladimirova,         Jakob Verbeek,         Pablo Mesejo,         Julyan Arbel","We investigate deep Bayesian neural networks with Gaussian priors on the weights and a class of ReLU-like nonlinearities. Bayesian neural networks with Gaussian priors are well known to induce an L2, “weight decay”, regularization. Our results indicate a more intricate regularization effect at the level of the unit activations. Our main result establishes that the induced prior distribution on the units before and after activation becomes increasingly heavy-tailed with the depth of the layer. We show that first layer units are Gaussian, second layer units are sub-exponential, and units in deeper layers are characterized by sub-Weibull distributions. Our results provide new theoretical insight on deep Bayesian neural networks, which we corroborate with simulation experiments.",http://proceedings.mlr.press/v97/vladimirova19a.html,http://proceedings.mlr.press/v97/vladimirova19a/vladimirova19a.pdf,ICML
130,2019,Learning Hawkes Processes Under Synchronization Noise,"William Trouleau,         Jalal Etesami,         Matthias Grossglauser,         Negar Kiyavash,         Patrick Thiran","Multivariate Hawkes processes (MHP) are widely used in a variety of fields to model the occurrence of discrete events. Prior work on learning MHPs has only focused on inference in the presence of perfect traces without noise. We address the problem of learning the causal structure of MHPs when observations are subject to an unknown delay. In particular, we introduce the so-called synchronization noise, where the stream of events generated by each dimension is subject to a random and unknown time shift. We characterize the robustness of the classic maximum likelihood estimator to synchronization noise, and we introduce a new approach for learning the causal structure in the presence of noise. Our experimental results show that our approach accurately recovers the causal structure of MHPs for a wide range of noise levels, and significantly outperforms classic estimation methods.",http://proceedings.mlr.press/v97/trouleau19a.html,http://proceedings.mlr.press/v97/trouleau19a/trouleau19a.pdf,ICML
131,2019,Bayesian Optimization of Composite Functions,"Raul Astudillo,         Peter Frazier","We consider optimization of composite objective functions, i.e., of the form f(x)=g(h(x))f(x)=g(h(x))f(x)=g(h(x)), where hhh is a black-box derivative-free expensive-to-evaluate function with vector-valued outputs, and ggg is a cheap-to-evaluate real-valued function. While these problems can be solved with standard Bayesian optimization, we propose a novel approach that exploits the composite structure of the objective function to substantially improve sampling efficiency. Our approach models hhh using a multi-output Gaussian process and chooses where to sample using the expected improvement evaluated on the implied non-Gaussian posterior on fff, which we call expected improvement for composite functions (EI-CF). Although EI-CF cannot be computed in closed form, we provide a novel stochastic gradient estimator that allows its efficient maximization. We also show that our approach is asymptotically consistent, i.e., that it recovers a globally optimal solution as sampling effort grows to infinity, generalizing previous convergence results for classical expected improvement. Numerical experiments show that our approach dramatically outperforms standard Bayesian optimization benchmarks, reducing simple regret by several orders of magnitude.",http://proceedings.mlr.press/v97/astudillo19a.html,http://proceedings.mlr.press/v97/astudillo19a/astudillo19a.pdf,ICML
132,2019,Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity,"Matthew Fahrbach,         Vahab Mirrokni,         Morteza Zadimoghaddam","Submodular maximization is a general optimization problem with a wide range of applications in machine learning (e.g., active learning, clustering, and feature selection). In large-scale optimization, the parallel running time of an algorithm is governed by its adaptivity, which measures the number of sequential rounds needed if the algorithm can execute polynomially-many independent oracle queries in parallel. While low adaptivity is ideal, it is not sufficient for an algorithm to be efficient in practice—there are many applications of distributed submodular optimization where the number of function evaluations becomes prohibitively expensive. Motivated by these applications, we study the adaptivity and query complexity of submodular maximization. In this paper, we give the first constant-factor approximation algorithm for maximizing a non-monotone submodular function subject to a cardinality constraint kkk that runs in O(log(n))O(log⁡(n))O(\log(n)) adaptive rounds and makes O(nlog(k))O(nlog⁡(k))O(n \log(k)) oracle queries in expectation. In our empirical study, we use three real-world applications to compare our algorithm with several benchmarks for non-monotone submodular maximization. The results demonstrate that our algorithm finds competitive solutions using significantly fewer rounds and queries.",http://proceedings.mlr.press/v97/fahrbach19a.html,http://proceedings.mlr.press/v97/fahrbach19a/fahrbach19a.pdf,ICML
133,2019,Bounding User Contributions: A Bias-Variance Trade-off in Differential Privacy,"Kareem Amin,         Alex Kulesza,         Andres Munoz,         Sergei Vassilvtiskii","Differentially private learning algorithms protect individual participants in the training dataset by guaranteeing that their presence does not significantly change the resulting model. In order to make this promise, such algorithms need to know the maximum contribution that can be made by a single user: the more data an individual can contribute, the more noise will need to be added to protect them. While most existing analyses assume that the maximum contribution is known and fixed in advance{—}indeed, it is often assumed that each user contributes only a single example{—}we argue that in practice there is a meaningful choice to be made. On the one hand, if we allow users to contribute large amounts of data, we may end up adding excessive noise to protect a few outliers, even when the majority contribute only modestly. On the other hand, limiting users to small contributions keeps noise levels low at the cost of potentially discarding significant amounts of excess data, thus introducing bias. Here, we characterize this trade-off for an empirical risk minimization setting, showing that in general there is a “sweet spot” that depends on measurable properties of the dataset, but that there is also a concrete cost to privacy that cannot be avoided simply by collecting more data.",http://proceedings.mlr.press/v97/amin19a.html,http://proceedings.mlr.press/v97/amin19a/amin19a.pdf,ICML
134,2019,Hybrid Models with Deep and Invertible Features,"Eric Nalisnick,         Akihiro Matsukawa,         Yee Whye Teh,         Dilan Gorur,         Balaji Lakshminarayanan","We propose a neural hybrid model consisting of a linear model defined on a set of features computed by a deep, invertible transformation (i.e. a normalizing flow). An attractive property of our model is that both p(features), the density of the features, and p(targets|features), the predictive distribution, can be computed exactly in a single feed-forward pass. We show that our hybrid model, despite the invertibility constraints, achieves similar accuracy to purely predictive models. Yet the generative component remains a good model of the input features despite the hybrid optimization objective. This offers additional capabilities such as detection of out-of-distribution inputs and enabling semi-supervised learning. The availability of the exact joint density p(targets, features) also allows us to compute many quantities readily, making our hybrid model a useful building block for downstream applications of probabilistic deep learning.",http://proceedings.mlr.press/v97/nalisnick19b.html,http://proceedings.mlr.press/v97/nalisnick19b/nalisnick19b.pdf,ICML
135,2019,Learning Models from Data with Measurement Error: Tackling Underreporting,"Roy Adams,         Yuelong Ji,         Xiaobin Wang,         Suchi Saria","Measurement error in observational datasets can lead to systematic bias in inferences based on these datasets. As studies based on observational data are increasingly used to inform decisions with real-world impact, it is critical that we develop a robust set of techniques for analyzing and adjusting for these biases. In this paper we present a method for estimating the distribution of an outcome given a binary exposure that is subject to underreporting. Our method is based on a missing data view of the measurement error problem, where the true exposure is treated as a latent variable that is marginalized out of a joint model. We prove three different conditions under which the outcome distribution can still be identified from data containing only error-prone observations of the exposure. We demonstrate this method on synthetic data and analyze its sensitivity to near violations of the identifiability conditions. Finally, we use this method to estimate the effects of maternal smoking and heroin use during pregnancy on childhood obesity, two import problems from public health. Using the proposed method, we estimate these effects using only subject-reported drug use data and refine the range of estimates generated by a sensitivity analysis-based approach. Further, the estimates produced by our method are consistent with existing literature on both the effects of maternal smoking and the rate at which subjects underreport smoking.",http://proceedings.mlr.press/v97/adams19a.html,http://proceedings.mlr.press/v97/adams19a/adams19a.pdf,ICML
136,2019,Amortized Monte Carlo Integration,"Adam Golinski,         Frank Wood,         Tom Rainforth","Current approaches to amortizing Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is, in turn, used to calculate expectations for one or more target functions{—}a computational pipeline which is inefficient when the target function(s) are known upfront. In this paper, we address this inefficiency by introducing AMCI, a method for amortizing Monte Carlo integration directly. AMCI operates similarly to amortized inference but produces three distinct amortized proposals, each tailored to a different component of the overall expectation calculation. At runtime, samples are produced separately from each amortized proposal, before being combined to an overall estimate of the expectation. We show that while existing approaches are fundamentally limited in the level of accuracy they can achieve, AMCI can theoretically produce arbitrarily small errors for any integrable target function using only a single sample from each proposal at runtime. We further show that it is able to empirically outperform the theoretically optimal selfnormalized importance sampler on a number of example problems. Furthermore, AMCI allows not only for amortizing over datasets but also amortizing over target functions.",http://proceedings.mlr.press/v97/golinski19a.html,http://proceedings.mlr.press/v97/golinski19a/golinski19a.pdf,ICML
137,2019,Making Convolutional Networks Shift-Invariant Again,Richard Zhang,"Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and average-pooling, ignore the sampling theorem. The well-known signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks leads to performance degradation; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling. The technique is general and can be incorporated across layer types and applications, such as image classification and conditional image generation. In addition to increased shift-invariance, we also observe, surprisingly, that anti-aliasing boosts accuracy in ImageNet classification, across several commonly-used architectures. This indicates that anti-aliasing serves as effective regularization. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks.",http://proceedings.mlr.press/v97/zhang19a.html,http://proceedings.mlr.press/v97/zhang19a/zhang19a.pdf,ICML
138,2019,Lower Bounds for Smooth Nonconvex Finite-Sum Optimization,"Dongruo Zhou,         Quanquan Gu","Smooth finite-sum optimization has been widely studied in both convex and nonconvex settings. However, existing lower bounds for finite-sum optimization are mostly limited to the setting where each component function is (strongly) convex, while the lower bounds for nonconvex finite-sum optimization remain largely unsolved. In this paper, we study the lower bounds for smooth nonconvex finite-sum optimization, where the objective function is the average of nnn nonconvex component functions. We prove tight lower bounds for the complexity of finding ϵϵ\epsilon-suboptimal point and ϵϵ\epsilon-approximate stationary point in different settings, for a wide regime of the smallest eigenvalue of the Hessian of the objective function (or each component function). Given our lower bounds, we can show that existing algorithms including {KatyushaX} \citep{allen2018katyushax}, {Natasha} \citep{allen2017natasha} and {StagewiseKatyusha} \citep{yang2018does} have achieved optimal {Incremental First-order Oracle} (IFO) complexity (i.e., number of IFO calls) up to logarithm factors for nonconvex finite-sum optimization. We also point out potential ways to further improve these complexity results, in terms of making stronger assumptions or by a different convergence analysis.",http://proceedings.mlr.press/v97/zhou19b.html,http://proceedings.mlr.press/v97/zhou19b/zhou19b.pdf,ICML
139,2019,Static Automatic Batching In TensorFlow,Ashish Agarwal,"Dynamic neural networks are becoming increasingly common, and yet it is hard to implement them efficiently. On-the-fly operation batching for such models is sub-optimal and suffers from run time overheads, while writing manually batched versions can be hard and error-prone. To address this we extend TensorFlow with pfor, a parallel-for loop optimized using static loop vectorization. With pfor, users can express computation using nested loops and conditional constructs, but get performance resembling that of a manually batched version. Benchmarks demonstrate speedups of one to two orders of magnitude on range of tasks, from jacobian computation, to Graph Neural Networks.",http://proceedings.mlr.press/v97/agarwal19a.html,http://proceedings.mlr.press/v97/agarwal19a/agarwal19a.pdf,ICML
140,2019,CapsAndRuns: An Improved Method for Approximately Optimal Algorithm Configuration,"Gellert Weisz,         Andras Gyorgy,         Csaba Szepesvari","We consider the problem of configuring general-purpose solvers to run efficiently on problem instances drawn from an unknown distribution, a problem of major interest in solver autoconfiguration. Following previous work, we focus on designing algorithms that find a configuration with near-optimal expected capped runtime while doing the least amount of work, with the cap chosen in a configuration-specific way so that most instances are solved. In this paper we present a new algorithm, CapsAndRuns, which finds a near-optimal configuration while using time that scales (in a problem dependent way) with the optimal expected capped runtime, significantly strengthening previous results which could only guarantee a bound that scaled with the potentially much larger optimal expected uncapped runtime. The new algorithm is simpler and more intuitive than the previous methods: first it estimates the optimal runtime cap for each configuration, then it uses a Bernstein race to find a near optimal configuration given the caps. Experiments verify that our method can significantly outperform its competitors.",http://proceedings.mlr.press/v97/weisz19a.html,http://proceedings.mlr.press/v97/weisz19a/weisz19a.pdf,ICML
141,2019,Optimal Transport for structured data with application on graphs,"Vayer Titouan,         Nicolas Courty,         Romain Tavenard,         Chapel Laetitia,         Rémi Flamary","This work considers the problem of computing distances between structured objects such as undirected graphs, seen as probability distributions in a specific metric space. We consider a new transportation distance ( i.e. that minimizes a total cost of transporting probability masses) that unveils the geometric nature of the structured objects space. Unlike Wasserstein or Gromov-Wasserstein metrics that focus solely and respectively on features (by considering a metric in the feature space) or structure (by seeing structure as a metric space), our new distance exploits jointly both information, and is consequently called Fused Gromov-Wasserstein (FGW). After discussing its properties and computational aspects, we show results on a graph classification task, where our method outperforms both graph kernels and deep graph convolutional networks. Exploiting further on the metric properties of FGW, interesting geometric objects such as Fr{é}chet means or barycenters of graphs are illustrated and discussed in a clustering context.",http://proceedings.mlr.press/v97/titouan19a.html,http://proceedings.mlr.press/v97/titouan19a/titouan19a.pdf,ICML
142,2019,Kernel Normalized Cut: a Theoretical Revisit,"Yoshikazu Terada,         Michio Yamamoto","In this paper, we study the theoretical properties of clustering based on the kernel normalized cut. Our first contribution is to derive a nonasymptotic upper bound on the expected distortion rate of the kernel normalized cut. From this result, we show that the solution of the kernel normalized cut converges to that of the population-level weighted k-means clustering on a certain reproducing kernel Hilbert space (RKHS). Our second contribution is the discover of the interesting fact that the population-level weighted k-means clustering in the RKHS is equivalent to the population-level normalized cut. Combining these results, we can see that the kernel normalized cut converges to the population-level normalized cut. The criterion of the population-level normalized cut can be considered as an indivisibility of the population distribution, and this criterion plays an important role in the theoretical analysis of spectral clustering in Schiebinger et al. (2015). We believe that our results will provide deep insights into the behavior of both normalized cut and spectral clustering.",http://proceedings.mlr.press/v97/terada19a.html,http://proceedings.mlr.press/v97/terada19a/terada19a.pdf,ICML
143,2019,Disentangled Graph Convolutional Networks,"Jianxin Ma,         Peng Cui,         Kun Kuang,         Xin Wang,         Wenwu Zhu","The formation of a real-world graph typically arises from the highly complex interaction of many latent factors. The existing deep learning methods for graph-structured data neglect the entanglement of the latent factors, rendering the learned representations non-robust and hardly explainable. However, learning representations that disentangle the latent factors poses great challenges and remains largely unexplored in the literature of graph neural networks. In this paper, we introduce the disentangled graph convolutional network (DisenGCN) to learn disentangled node representations. In particular, we propose a novel neighborhood routing mechanism, which is capable of dynamically identifying the latent factor that may have caused the edge between a node and one of its neighbors, and accordingly assigning the neighbor to a channel that extracts and convolutes features specific to that factor. We theoretically prove the convergence properties of the routing mechanism. Empirical results show that our proposed model can achieve significant performance gains, especially when the data demonstrate the existence of many entangled factors.",http://proceedings.mlr.press/v97/ma19a.html,http://proceedings.mlr.press/v97/ma19a/ma19a.pdf,ICML
144,2019,Hyperbolic Disk Embeddings for Directed Acyclic Graphs,"Ryota Suzuki,         Ryusuke Takahama,         Shun Onoda","Obtaining continuous representations of structural data such as directed acyclic graphs (DAGs) has gained attention in machine learning and artificial intelligence. However, embedding complex DAGs in which both ancestors and descendants of nodes are exponentially increasing is difficult. Tackling in this problem, we develop Disk Embeddings, which is a framework for embedding DAGs into quasi-metric spaces. Existing state-of-the-art methods, Order Embeddings and Hyperbolic Entailment Cones, are instances of Disk Embedding in Euclidean space and spheres respectively. Furthermore, we propose a novel method Hyperbolic Disk Embeddings to handle exponential growth of relations. The results of our experiments show that our Disk Embedding models outperform existing methods especially in complex DAGs other than trees.",http://proceedings.mlr.press/v97/suzuki19a.html,http://proceedings.mlr.press/v97/suzuki19a/suzuki19a.pdf,ICML
145,2019,"Distributed, Egocentric Representations of Graphs for Detecting Critical Structures","Ruo-Chun Tzeng,         Shan-Hung Wu","We study the problem of detecting critical structures using a graph embedding model. Existing graph embedding models lack the ability to precisely detect critical structures that are specific to a task at the global scale. In this paper, we propose a novel graph embedding model, called the Ego-CNNs, that employs the ego-convolutions convolutions at each layer and stacks up layers using an ego-centric way to detects precise critical structures efficiently. An Ego-CNN can be jointly trained with a task model and help explain/discover knowledge for the task. We conduct extensive experiments and the results show that Ego-CNNs (1) can lead to comparable task performance as the state-of-the-art graph embedding models, (2) works nicely with CNN visualization techniques to illustrate the detected structures, and (3) is efficient and can incorporate with scale-free priors, which commonly occurs in social network datasets, to further improve the training efficiency.",http://proceedings.mlr.press/v97/tzeng19a.html,http://proceedings.mlr.press/v97/tzeng19a/tzeng19a.pdf,ICML
146,2019,"Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks","Charith Mendis,         Alex Renda,         Dr.Saman Amarasinghe,         Michael Carbin","Predicting the number of clock cycles a processor takes to execute a block of assembly instructions in steady state (the throughput) is important for both compiler designers and performance engineers. Building an analytical model to do so is especially complicated in modern x86-64 Complex Instruction Set Computer (CISC) machines with sophisticated processor microarchitectures in that it is tedious, error prone, and must be performed from scratch for each processor generation. In this paper we present Ithemal, the first tool which learns to predict the throughput of a set of instructions. Ithemal uses a hierarchical LSTM–based approach to predict throughput based on the opcodes and operands of instructions in a basic block. We show that Ithemal is more accurate than state-of-the-art hand-written tools currently used in compiler backends and static machine code analyzers. In particular, our model has less than half the error of state-of-the-art analytical models (LLVM’s llvm-mca and Intel’s IACA). Ithemal is also able to predict these throughput values just as fast as the aforementioned tools, and is easily ported across a variety of processor microarchitectures with minimal developer effort.",http://proceedings.mlr.press/v97/mendis19a.html,http://proceedings.mlr.press/v97/mendis19a/mendis19a.pdf,ICML
147,2019,Graphical-model based estimation and inference for differential privacy,"Ryan Mckenna,         Daniel Sheldon,         Gerome Miklau","Many privacy mechanisms reveal high-level information about a data distribution through noisy measurements. It is common to use this information to estimate the answers to new queries. In this work, we provide an approach to solve this estimation problem efficiently using graphical models, which is particularly effective when the distribution is high-dimensional but the measurements are over low-dimensional marginals. We show that our approach is far more efficient than existing estimation techniques from the privacy literature and that it can improve the accuracy and scalability of many state-of-the-art mechanisms.",http://proceedings.mlr.press/v97/mckenna19a.html,http://proceedings.mlr.press/v97/mckenna19a/mckenna19a.pdf,ICML
148,2019,On Symmetric Losses for Learning from Corrupted Labels,"Nontawat Charoenphakdee,         Jongyeong Lee,         Masashi Sugiyama","This paper aims to provide a better understanding of a symmetric loss. First, we emphasize that using a symmetric loss is advantageous in the balanced error rate (BER) minimization and area under the receiver operating characteristic curve (AUC) maximization from corrupted labels. Second, we prove general theoretical properties of symmetric losses, including a classification-calibration condition, excess risk bound, conditional risk minimizer, and AUC-consistency condition. Third, since all nonnegative symmetric losses are non-convex, we propose a convex barrier hinge loss that benefits significantly from the symmetric condition, although it is not symmetric everywhere. Finally, we conduct experiments to validate the relevance of the symmetric condition.",http://proceedings.mlr.press/v97/charoenphakdee19a.html,http://proceedings.mlr.press/v97/charoenphakdee19a/charoenphakdee19a.pdf,ICML
149,2019,kernelPSI: a Post-Selection Inference Framework for Nonlinear Variable Selection,"Lotfi Slim,         Clément Chatelain,         Chloe-Agathe Azencott,         Jean-Philippe Vert","Model selection is an essential task for many applications in scientific discovery. The most common approaches rely on univariate linear measures of association between each feature and the outcome. Such classical selection procedures fail to take into account nonlinear effects and interactions between features. Kernel-based selection procedures have been proposed as a solution. However, current strategies for kernel selection fail to measure the significance of a joint model constructed through the combination of the basis kernels. In the present work, we exploit recent advances in post-selection inference to propose a valid statistical test for the association of a joint model of the selected kernels with the outcome. The kernels are selected via a step-wise procedure which we model as a succession of quadratic constraints in the outcome variable.",http://proceedings.mlr.press/v97/slim19a.html,http://proceedings.mlr.press/v97/slim19a/slim19a.pdf,ICML
150,2019,Nonlinear Distributional Gradient Temporal-Difference Learning,"Chao Qu,         Shie Mannor,         Huan Xu","We devise a distributional variant of gradient temporal-difference (TD) learning. Distributional reinforcement learning has been demonstrated to outperform the regular one in the recent study \citep{bellemare2017distributional}. In the policy evaluation setting, we design two new algorithms called distributional GTD2 and distributional TDC using the Cram{é}r distance on the distributional version of the Bellman error objective function, which inherits advantages of both the nonlinear gradient TD algorithms and the distributional RL approach. In the control setting, we propose the distributional Greedy-GQ using similar derivation. We prove the asymptotic almost-sure convergence of distributional GTD2 and TDC to a local optimal solution for general smooth function approximators, which includes neural networks that have been widely used in recent study to solve the real-life RL problems. In each step, the computational complexity of above three algorithms is linear w.r.t. the number of the parameters of the function approximator, thus can be implemented efficiently for neural networks.",http://proceedings.mlr.press/v97/qu19b.html,http://proceedings.mlr.press/v97/qu19b/qu19b.pdf,ICML
151,2019,Neuron birth-death dynamics accelerates gradient descent and converges asymptotically,"Grant Rotskoff,         Samy Jelassi,         Joan Bruna,         Eric Vanden-Eijnden","Neural networks with a large number of parameters admit a mean-field description, which has recently served as a theoretical explanation for the favorable training properties of models with a large number of parameters. In this regime, gradient descent obeys a deterministic partial differential equation (PDE) that converges to a globally optimal solution for networks with a single hidden layer under appropriate assumptions. In this work, we propose a non-local mass transport dynamics that leads to a modified PDE with the same minimizer. We implement this non-local dynamics as a stochastic neuronal birth/death process and we prove that it accelerates the rate of convergence in the mean-field limit. We subsequently realize this PDE with two classes of numerical schemes that converge to the mean-field equation, each of which can easily be implemented for neural networks with finite numbers of parameters. We illustrate our algorithms with two models to provide intuition for the mechanism through which convergence is accelerated.",http://proceedings.mlr.press/v97/rotskoff19a.html,http://proceedings.mlr.press/v97/rotskoff19a/rotskoff19a.pdf,ICML
152,2019,Graph Convolutional Gaussian Processes,"Ian Walker,         Ben Glocker","We propose a novel Bayesian nonparametric method to learn translation-invariant relationships on non-Euclidean domains. The resulting graph convolutional Gaussian processes can be applied to problems in machine learning for which the input observations are functions with domains on general graphs. The structure of these models allows for high dimensional inputs while retaining expressibility, as is the case with convolutional neural networks. We present applications of graph convolutional Gaussian processes to images and triangular meshes, demonstrating their versatility and effectiveness, comparing favorably to existing methods, despite being relatively simple models.",http://proceedings.mlr.press/v97/walker19a.html,http://proceedings.mlr.press/v97/walker19a/walker19a.pdf,ICML
153,2019,Provably efficient RL with Rich Observations via Latent State Decoding,"Simon Du,         Akshay Krishnamurthy,         Nan Jiang,         Alekh Agarwal,         Miroslav Dudik,         John Langford","We study the exploration problem in episodic MDPs with rich observations generated from a small number of latent states. Under certain identifiability assumptions, we demonstrate how to estimate a mapping from the observations to latent states inductively through a sequence of regression and clustering steps—where previously decoded latent states provide labels for later regression problems—and use it to construct good exploration policies. We provide finite-sample guarantees on the quality of the learned state decoding function and exploration policies, and complement our theory with an empirical evaluation on a class of hard exploration problems. Our method exponentially improves over QQQ-learning with naïve exploration, even when QQQ-learning has cheating access to latent states.",http://proceedings.mlr.press/v97/du19b.html,http://proceedings.mlr.press/v97/du19b/du19b.pdf,ICML
154,2019,Uniform Convergence Rate of the Kernel Density Estimator Adaptive to Intrinsic Volume Dimension,"Jisu Kim,         Jaehyeok Shin,         Alessandro Rinaldo,         Larry Wasserman","We derive concentration inequalities for the supremum norm of the difference between a kernel density estimator (KDE) and its point-wise expectation that hold uniformly over the selection of the bandwidth and under weaker conditions on the kernel and the data generating distribution than previously used in the literature. We first propose a novel concept, called the volume dimension, to measure the intrinsic dimension of the support of a probability distribution based on the rates of decay of the probability of vanishing Euclidean balls. Our bounds depend on the volume dimension and generalize the existing bounds derived in the literature. In particular, when the data-generating distribution has a bounded Lebesgue density or is supported on a sufficiently well-behaved lower-dimensional manifold, our bound recovers the same convergence rate depending on the intrinsic dimension of the support as ones known in the literature. At the same time, our results apply to more general cases, such as the ones of distribution with unbounded densities or supported on a mixture of manifolds with different dimensions. Analogous bounds are derived for the derivative of the KDE, of any order. Our results are generally applicable but are especially useful for problems in geometric inference and topological data analysis, including level set estimation, density-based clustering, modal clustering and mode hunting, ridge estimation and persistent homology.",http://proceedings.mlr.press/v97/kim19e.html,http://proceedings.mlr.press/v97/kim19e/kim19e.pdf,ICML
155,2019,Online Adaptive Principal Component Analysis and Its extensions,"Jianjun Yuan,         Andrew Lamperski","We propose algorithms for online principal component analysis (PCA) and variance minimization for adaptive settings. Previous literature has focused on upper bounding the static adversarial regret, whose comparator is the optimal fixed action in hindsight. However, static regret is not an appropriate metric when the underlying environment is changing. Instead, we adopt the adaptive regret metric from the previous literature and propose online adaptive algorithms for PCA and variance minimization, that have sub-linear adaptive regret guarantees. We demonstrate both theoretically and experimentally that the proposed algorithms can adapt to the changing environments.",http://proceedings.mlr.press/v97/yuan19a.html,http://proceedings.mlr.press/v97/yuan19a/yuan19a.pdf,ICML
156,2019,Target Tracking for Contextual Bandits: Application to Demand Side Management,"Margaux Brégère,         Pierre Gaillard,         Yannig Goude,         Gilles Stoltz","We propose a contextual-bandit approach for demand side management by offering price incentives. More precisely, a target mean consumption is set at each round and the mean consumption is modeled as a complex function of the distribution of prices sent and of some contextual variables such as the temperature, weather, and so on. The performance of our strategies is measured in quadratic losses through a regret criterion. We offer T2/3T2/3T^{2/3} upper bounds on this regret (up to poly-logarithmic terms)—and even faster rates under stronger assumptions—for strategies inspired by standard strategies for contextual bandits (like LinUCB, see Li et al., 2010). Simulations on a real data set gathered by UK Power Networks, in which price incentives were offered, show that our strategies are effective and may indeed manage demand response by suitably picking the price levels.",http://proceedings.mlr.press/v97/bregere19a.html,http://proceedings.mlr.press/v97/bregere19a/bregere19a.pdf,ICML
157,2019,Policy Certificates: Towards Accountable Reinforcement Learning,"Christoph Dann,         Lihong Li,         Wei Wei,         Emma Brunskill","The performance of a reinforcement learning algorithm can vary drastically during learning because of exploration. Existing algorithms provide little information about the quality of their current policy before executing it, and thus have limited use in high-stakes applications like healthcare. We address this lack of accountability by proposing that algorithms output policy certificates. These certificates bound the sub-optimality and return of the policy in the next episode, allowing humans to intervene when the certified quality is not satisfactory. We further introduce two new algorithms with certificates and present a new framework for theoretical analysis that guarantees the quality of their policies and certificates. For tabular MDPs, we show that computing certificates can even improve the sample-efficiency of optimism-based exploration. As a result, one of our algorithms is the first to achieve minimax-optimal PAC bounds up to lower-order terms, and this algorithm also matches (and in some settings slightly improves upon) existing minimax regret bounds.",http://proceedings.mlr.press/v97/dann19a.html,http://proceedings.mlr.press/v97/dann19a/dann19a.pdf,ICML
158,2019,PAC Identification of Many Good Arms in Stochastic Multi-Armed Bandits,"Arghya Roy Chaudhuri,         Shivaram Kalyanakrishnan","We consider the problem of identifying any k out of the best m arms in an n-armed stochastic multi-armed bandit; framed in the PAC setting, this particular problem generalises both the problem of “best subset selection” (Kalyanakrishnan & Stone, 2010) and that of selecting “one out of the best m” arms (Roy Chaudhuri & Kalyanakrishnan, 2017). We present a lower bound on the worst-case sample complexity for general k, and a fully sequential PAC algorithm, LUCB-k-m, which is more sample-efficient on easy instances. Also, extending our analysis to infinite-armed bandits, we present a PAC algorithm that is independent of n, which identifies an arm from the best ρρ\rho fraction of arms using at most an additive poly-log number of samples than compared to the lower bound, thereby improving over Roy Chaudhuri & Kalyanakrishnan (2017) and Aziz et al. (2018). The problem of identifying k > 1 distinct arms from the best ρρ\rho fraction is not always well-defined; for a special class of this problem, we present lower and upper bounds. Finally, through a reduction, we establish a relation between upper bounds for the “one out of the best ρρ\rho” problem for infinite instances and the “one out of the best m” problem for finite instances. We conjecture that it is more efficient to solve “small” finite instances using the latter formulation, rather than going through the former.",http://proceedings.mlr.press/v97/chaudhuri19a.html,http://proceedings.mlr.press/v97/chaudhuri19a/chaudhuri19a.pdf,ICML
159,2019,Complexity of Linear Regions in Deep Networks,"Boris Hanin,         David Rolnick","It is well-known that the expressivity of a neural network depends on its architecture, with deeper networks expressing more complex functions. In the case of networks that compute piecewise linear functions, such as those with ReLU activation, the number of distinct linear regions is a natural measure of expressivity. It is possible to construct networks with merely a single region, or for which the number of linear regions grows exponentially with depth; it is not clear where within this range most networks fall in practice, either before or after training. In this paper, we provide a mathematical framework to count the number of linear regions of a piecewise linear network and measure the volume of the boundaries between these regions. In particular, we prove that for networks at initialization, the average number of regions along any one-dimensional subspace grows linearly in the total number of neurons, far below the exponential upper bound. We also find that the average distance to the nearest region boundary at initialization scales like the inverse of the number of neurons. Our theory suggests that, even after training, the number of linear regions is far below exponential, an intuition that matches our empirical observations. We conclude that the practical expressivity of neural networks is likely far below that of the theoretical maximum, and that this gap can be quantified.",http://proceedings.mlr.press/v97/hanin19a.html,http://proceedings.mlr.press/v97/hanin19a/hanin19a.pdf,ICML
160,2019,The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise Interactions in High Dimensions,"Raj Agrawal,         Brian Trippe,         Jonathan Huggins,         Tamara Broderick","Discovering interaction effects on a response of interest is a fundamental problem faced in biology, medicine, economics, and many other scientific disciplines. In theory, Bayesian methods for discovering pairwise interactions enjoy many benefits such as coherent uncertainty quantification, the ability to incorporate background knowledge, and desirable shrinkage properties. In practice, however, Bayesian methods are often computationally intractable for even moderate- dimensional problems. Our key insight is that many hierarchical models of practical interest admit a Gaussian process representation such that rather than maintaining a posterior over all O(p^2) interactions, we need only maintain a vector of O(p) kernel hyper-parameters. This implicit representation allows us to run Markov chain Monte Carlo (MCMC) over model hyper-parameters in time and memory linear in p per iteration. We focus on sparsity-inducing models and show on datasets with a variety of covariate behaviors that our method: (1) reduces runtime by orders of magnitude over naive applications of MCMC, (2) provides lower Type I and Type II error relative to state-of-the-art LASSO-based approaches, and (3) offers improved computational scaling in high dimensions relative to existing Bayesian and LASSO-based approaches.",http://proceedings.mlr.press/v97/agrawal19a.html,http://proceedings.mlr.press/v97/agrawal19a/agrawal19a.pdf,ICML
161,2019,Information-Theoretic Considerations in Batch Reinforcement Learning,"Jinglin Chen,         Nan Jiang","Value-function approximation methods that operate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guarantees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the necessity (“why do we need them?”) and the naturalness (“when do they hold?”) of such assumptions have largely eluded the literature. In this paper, we revisit these assumptions and provide theoretical results towards answering the above questions, and make steps towards a deeper understanding of value-function approximation.",http://proceedings.mlr.press/v97/chen19e.html,http://proceedings.mlr.press/v97/chen19e/chen19e.pdf,ICML
162,2019,Unreproducible Research is Reproducible,"Xavier Bouthillier,         César Laurent,         Pascal Vincent","The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.",http://proceedings.mlr.press/v97/bouthillier19a.html,http://proceedings.mlr.press/v97/bouthillier19a/bouthillier19a.pdf,ICML
163,2019,Graphite: Iterative Generative Modeling of Graphs,"Aditya Grover,         Aaron Zweig,         Stefano Ermon","Graphs are a fundamental abstraction for modeling relational data. However, graphs are discrete and combinatorial in nature, and learning representations suitable for machine learning tasks poses statistical and computational challenges. In this work, we propose Graphite, an algorithmic framework for unsupervised learning of representations over nodes in large graphs using deep latent variable generative models. Our model parameterizes variational autoencoders (VAE) with graph neural networks, and uses a novel iterative graph refinement strategy inspired by low-rank approximations for decoding. On a wide variety of synthetic and benchmark datasets, Graphite outperforms competing approaches for the tasks of density estimation, link prediction, and node classification. Finally, we derive a theoretical connection between message passing in graph neural networks and mean-field variational inference.",http://proceedings.mlr.press/v97/grover19a.html,http://proceedings.mlr.press/v97/grover19a/grover19a.pdf,ICML
164,2019,Neurally-Guided Structure Inference,"Sidi Lu,         Jiayuan Mao,         Joshua Tenenbaum,         Jiajun Wu","Most structure inference methods either rely on exhaustive search or are purely data-driven. Exhaustive search robustly infers the structure of arbitrarily complex data, but it is slow. Data-driven methods allow efficient inference, but do not generalize when test data have more complex structures than training data. In this paper, we propose a hybrid inference algorithm, the Neurally-Guided Structure Inference (NG-SI), keeping the advantages of both search-based and data-driven methods. The key idea of NG-SI is to use a neural network to guide the hierarchical, layer-wise search over the compositional space of structures. We evaluate our algorithm on two representative structure inference tasks: probabilistic matrix decomposition and symbolic program parsing. It outperforms data-driven and search-based alternatives on both tasks.",http://proceedings.mlr.press/v97/lu19b.html,http://proceedings.mlr.press/v97/lu19b/lu19b.pdf,ICML
165,2019,Transfer of Samples in Policy Search via Multiple Importance Sampling,"Andrea Tirinzoni,         Mattia Salvini,         Marcello Restelli","We consider the transfer of experience samples in reinforcement learning. Most of the previous works in this context focused on value-based settings, where transferring instances conveniently reduces to the transfer of (s,a,s’,r) tuples. In this paper, we consider the more complex case of reusing samples in policy search methods, in which the agent is required to transfer entire trajectories between environments with different transition models. By leveraging ideas from multiple importance sampling, we propose robust gradient estimators that effectively achieve this goal, along with several techniques to reduce their variance. In the case where the transition models are known, we theoretically establish the robustness to the negative transfer for our estimators. In the case of unknown models, we propose a method to efficiently estimate them when the target task belongs to a finite set of possible tasks and when it belongs to some reproducing kernel Hilbert space. We provide empirical results to show the effectiveness of our estimators.",http://proceedings.mlr.press/v97/tirinzoni19a.html,http://proceedings.mlr.press/v97/tirinzoni19a/tirinzoni19a.pdf,ICML
166,2019,Understanding and Accelerating Particle-Based Variational Inference,"Chang Liu,         Jingwei Zhuo,         Pengyu Cheng,         Ruiyi Zhang,         Jun Zhu","Particle-based variational inference methods (ParVIs) have gained attention in the Bayesian inference literature, for their capacity to yield flexible and accurate approximations. We explore ParVIs from the perspective of Wasserstein gradient flows, and make both theoretical and practical contributions. We unify various finite-particle approximations that existing ParVIs use, and recognize that the approximation is essentially a compulsory smoothing treatment, in either of two equivalent forms. This novel understanding reveals the assumptions and relations of existing ParVIs, and also inspires new ParVIs. We propose an acceleration framework and a principled bandwidth-selection method for general ParVIs; these are based on the developed theory and leverage the geometry of the Wasserstein space. Experimental results show the improved convergence by the acceleration framework and enhanced sample accuracy by the bandwidth-selection method.",http://proceedings.mlr.press/v97/liu19i.html,http://proceedings.mlr.press/v97/liu19i/liu19i.pdf,ICML
167,2019,Online Variance Reduction with Mixtures,"Zalán Borsos,         Sebastian Curi,         Kfir Yehuda Levy,         Andreas Krause","Adaptive importance sampling for stochastic optimization is a promising approach that offers improved convergence through variance reduction. In this work, we propose a new framework for variance reduction that enables the use of mixtures over predefined sampling distributions, which can naturally encode prior knowledge about the data. While these sampling distributions are fixed, the mixture weights are adapted during the optimization process. We propose VRM, a novel and efficient adaptive scheme that asymptotically recovers the best mixture weights in hindsight and can also accommodate sampling distributions over sets of points. We empirically demonstrate the versatility of VRM in a range of applications.",http://proceedings.mlr.press/v97/borsos19a.html,http://proceedings.mlr.press/v97/borsos19a/borsos19a.pdf,ICML
168,2019,Imitation Learning from Imperfect Demonstration,"Yueh-Hua Wu,         Nontawat Charoenphakdee,         Han Bao,         Voot Tangkaratt,         Masashi Sugiyama","Imitation learning (IL) aims to learn an optimal policy from demonstrations. However, such demonstrations are often imperfect since collecting optimal ones is costly. To effectively learn from imperfect demonstrations, we propose a novel approach that utilizes confidence scores, which describe the quality of demonstrations. More specifically, we propose two confidence-based IL methods, namely two-step importance weighting IL (2IWIL) and generative adversarial IL with imperfect demonstration and confidence (IC-GAIL). We show that confidence scores given only to a small portion of sub-optimal demonstrations significantly improve the performance of IL both theoretically and empirically.",http://proceedings.mlr.press/v97/wu19a.html,http://proceedings.mlr.press/v97/wu19a/wu19a.pdf,ICML
169,2019,Supervised Hierarchical Clustering with Exponential Linkage,"Nishant Yadav,         Ari Kobren,         Nicholas Monath,         Andrew Mccallum","In supervised clustering, standard techniques for learning a pairwise dissimilarity function often suffer from a discrepancy between the training and clustering objectives, leading to poor cluster quality. Rectifying this discrepancy necessitates matching the procedure for training the dissimilarity function to the clustering algorithm. In this paper, we introduce a method for training the dissimilarity function in a way that is tightly coupled with hierarchical clustering, in particular single linkage. However, the appropriate clustering algorithm for a given dataset is often unknown. Thus we introduce an approach to supervised hierarchical clustering that smoothly interpolates between single, average, and complete linkage, and we give a training procedure that simultaneously learns a linkage function and a dissimilarity function. We accomplish this with a novel Exponential Linkage function that has a learnable parameter that controls the interpolation. In experiments on four datasets, our joint training procedure consistently matches or outperforms the next best training procedure/linkage function pair and gives up to 8 points improvement in dendrogram purity over discrepant pairs.",http://proceedings.mlr.press/v97/yadav19a.html,http://proceedings.mlr.press/v97/yadav19a/yadav19a.pdf,ICML
170,2019,Estimating Information Flow in Deep Neural Networks,"Ziv Goldfeld,         Ewout Van Den Berg,         Kristjan Greenewald,         Igor Melnyk,         Nam Nguyen,         Brian Kingsbury,         Yury Polyanskiy","We study the estimation of the mutual information I(X;T_ℓℓ\ell) between the input X to a deep neural network (DNN) and the output vector T_ℓℓ\ell of its ℓℓ\ell-th hidden layer (an “internal representation”). Focusing on feedforward networks with fixed weights and noisy internal representations, we develop a rigorous framework for accurate estimation of I(X;T_ℓℓ\ell). By relating I(X;T_ℓℓ\ell) to information transmission over additive white Gaussian noise channels, we reveal that compression, i.e. reduction in I(X;T_ℓℓ\ell) over the course of training, is driven by progressive geometric clustering of the representations of samples from the same class. Experimental results verify this connection. Finally, we shift focus to purely deterministic DNNs, where I(X;T_ℓℓ\ell) is provably vacuous, and show that nevertheless, these models also cluster inputs belonging to the same class. The binning-based approximation of I(X;T_ℓℓ\ell) employed in past works to measure compression is identified as a measure of clustering, thus clarifying that these experiments were in fact tracking the same clustering phenomenon. Leveraging the clustering perspective, we provide new evidence that compression and generalization may not be causally related and discuss potential future research ideas.",http://proceedings.mlr.press/v97/goldfeld19a.html,http://proceedings.mlr.press/v97/goldfeld19a/goldfeld19a.pdf,ICML
171,2019,MONK Outlier-Robust Mean Embedding Estimation by Median-of-Means,"Matthieu Lerasle,         Zoltan Szabo,         Timothée Mathieu,         Guillaume Lecue","Mean embeddings provide an extremely flexible and powerful tool in machine learning and statistics to represent probability distributions and define a semi-metric (MMD, maximum mean discrepancy; also called N-distance or energy distance), with numerous successful applications. The representation is constructed as the expectation of the feature map defined by a kernel. As a mean, its classical empirical estimator, however, can be arbitrary severely affected even by a single outlier in case of unbounded features. To the best of our knowledge, unfortunately even the consistency of the existing few techniques trying to alleviate this serious sensitivity bottleneck is unknown. In this paper, we show how the recently emerged principle of median-of-means can be used to design estimators for kernel mean embedding and MMD with excessive resistance properties to outliers, and optimal sub-Gaussian deviation bounds under mild assumptions.",http://proceedings.mlr.press/v97/lerasle19a.html,http://proceedings.mlr.press/v97/lerasle19a/lerasle19a.pdf,ICML
172,2019,Data Poisoning Attacks in Multi-Party Learning,"Saeed Mahloujifar,         Mohammad Mahmoody,         Ameer Mohammed","In this work, we demonstrate universal multi-party poisoning attacks that adapt and apply to any multi-party learning process with arbitrary interaction pattern between the parties. More generally, we introduce and study (k,p)(k,p)(k,p)-poisoning attacks in which an adversary controls k∈[m]k∈[m]k\in[m] of the parties, and for each corrupted party PiPiP_i, the adversary submits some poisoned data T′iTi′T’_i on behalf of PiPiP_i that is still ""(1−p)(1−p)(1-p)-close"" to the correct data TiTiT_i (e.g., 1−p1−p1-p fraction of T′iTi′T’_i is still honestly generated).We prove that for any ""bad"" property BBB of the final trained hypothesis hhh (e.g., hhh failing on a particular test example or having ""large"" risk) that has an arbitrarily small constant probability of happening without the attack, there always is a (k,p)(k,p)(k,p)-poisoning attack that increases the probability of BBB from μμ\mu to by μ1−p⋅k/m=μ+Ω(p⋅k/m)μ1−p⋅k/m=μ+Ω(p⋅k/m)\mu^{1-p \cdot k/m} = \mu + \Omega(p \cdot k/m). Our attack only uses clean labels, and it is online, as it only knows the the data shared so far.",http://proceedings.mlr.press/v97/mahloujifar19a.html,http://proceedings.mlr.press/v97/mahloujifar19a/mahloujifar19a.pdf,ICML
173,2019,Band-limited Training and Inference for Convolutional Neural Networks,"Adam Dziedzic,         John Paparrizos,         Sanjay Krishnan,         Aaron Elmore,         Michael Franklin","The convolutional layers are core building blocks of neural network architectures. In general, a convolutional filter applies to the entire frequency spectrum of the input data. We explore artificially constraining the frequency spectra of these filters and data, called band-limiting, during training. The frequency domain constraints apply to both the feed-forward and back-propagation steps. Experimentally, we observe that Convolutional Neural Networks (CNNs) are resilient to this compression scheme and results suggest that CNNs learn to leverage lower-frequency components. In particular, we found: (1) band-limited training can effectively control the resource usage (GPU and memory); (2) models trained with band-limited layers retain high prediction accuracy; and (3) requires no modification to existing training algorithms or neural network architectures to use unlike other compression schemes.",http://proceedings.mlr.press/v97/dziedzic19a.html,http://proceedings.mlr.press/v97/dziedzic19a/dziedzic19a.pdf,ICML
174,2019,Fault Tolerance in Iterative-Convergent Machine Learning,"Aurick Qiao,         Bryon Aragam,         Bingjing Zhang,         Eric Xing","Machine learning (ML) training algorithms often possess an inherent self-correcting behavior due to their iterative- convergent nature. Recent systems exploit this property to achieve adaptability and efficiency in unreliable computing environments by relaxing the consistency of execution and allowing calculation errors to be self-corrected during training. However, the behavior of such systems are only well understood for specific types of calculation errors, such as those caused by staleness, reduced precision, or asynchronicity, and for specific algorithms, such as stochastic gradient descent. In this paper, we develop a general framework to quantify the effects of calculation errors on iterative-convergent algorithms. We then use this framework to derive a worst-case upper bound on the cost of arbitrary perturbations to model parameters during training and to design new strategies for checkpoint-based fault tolerance. Our system, SCAR, can reduce the cost of partial failures by 78%{–}95% when compared with traditional checkpoint-based fault tolerance across a variety of ML models and training algorithms, providing near-optimal performance in recovering from failures.",http://proceedings.mlr.press/v97/qiao19a.html,http://proceedings.mlr.press/v97/qiao19a/qiao19a.pdf,ICML
175,2019,Quantifying Generalization in Reinforcement Learning,"Karl Cobbe,         Oleg Klimov,         Chris Hesse,         Taehoon Kim,         John Schulman","In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent’s ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.",http://proceedings.mlr.press/v97/cobbe19a.html,http://proceedings.mlr.press/v97/cobbe19a/cobbe19a.pdf,ICML
176,2019,Adversarially Learned Representations for Information Obfuscation and Inference,"Martin Bertran,         Natalia Martinez,         Afroditi Papadaki,         Qiang Qiu,         Miguel Rodrigues,         Galen Reeves,         Guillermo Sapiro","Data collection and sharing are pervasive aspects of modern society. This process can either be voluntary, as in the case of a person taking a facial image to unlock his/her phone, or incidental, such as traffic cameras collecting videos on pedestrians. An undesirable side effect of these processes is that shared data can carry information about attributes that users might consider as sensitive, even when such information is of limited use for the task. It is therefore desirable for both data collectors and users to design procedures that minimize sensitive information leakage. Balancing the competing objectives of providing meaningful individualized service levels and inference while obfuscating sensitive information is still an open problem. In this work, we take an information theoretic approach that is implemented as an unconstrained adversarial game between Deep Neural Networks in a principled, data-driven manner. This approach enables us to learn domain-preserving stochastic transformations that maintain performance on existing algorithms while minimizing sensitive information leakage.",http://proceedings.mlr.press/v97/bertran19a.html,http://proceedings.mlr.press/v97/bertran19a/bertran19a.pdf,ICML
177,2019,On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization,"Hao Yu,         Rong Jin,         Sen Yang","Recent developments on large-scale distributed machine learning applications, e.g., deep neural networks, benefit enormously from the advances in distributed non-convex optimization techniques, e.g., distributed Stochastic Gradient Descent (SGD). A series of recent works study the linear speedup property of distributed SGD variants with reduced communication. The linear speedup property enables us to scale out the computing capability by adding more computing nodes into our system. The reduced communication complexity is desirable since communication overhead is often the performance bottleneck in distributed systems. Recently, momentum methods are more and more widely adopted by practitioners to train machine learning models since they can often converge faster and generalize better. However, it remains unclear whether any distributed momentum SGD possesses the same linear speedup property as distributed SGD and has reduced communication complexity. This paper fills the gap by considering a distributed communication efficient momentum SGD method and proving its linear speedup property.",http://proceedings.mlr.press/v97/yu19d.html,http://proceedings.mlr.press/v97/yu19d/yu19d.pdf,ICML
178,2019,Learning to bid in revenue-maximizing auctions,"Thomas Nedelec,         Noureddine El Karoui,         Vianney Perchet","We consider the problem of the optimization of bidding strategies in prior-dependent revenue-maximizing auctions, when the seller fixes the reserve prices based on the bid distributions. Our study is done in the setting where one bidder is strategic. Using a variational approach, we study the complexity of the original objective and we introduce a relaxation of the objective functional in order to use gradient descent methods. Our approach is simple, general and can be applied to various value distributions and revenue-maximizing mechanisms. The new strategies we derive yield massive uplifts compared to the traditional truthfully bidding strategy.",http://proceedings.mlr.press/v97/nedelec19a.html,http://proceedings.mlr.press/v97/nedelec19a/nedelec19a.pdf,ICML
179,2019,Heterogeneous Model Reuse via Optimizing Multiparty Multiclass Margin,"Xi-Zhu Wu,         Song Liu,         Zhi-Hua Zhou","Nowadays, many problems require learning a model from data owned by different participants who are restricted to share their examples due to privacy concerns, which is referred to as multiparty learning in the literature. In conventional multiparty learning, a global model is usually trained from scratch via a communication protocol, ignoring the fact that each party may already have a local model trained on her own dataset. In this paper, we define a multiparty multiclass margin to measure the global behavior of a set of heterogeneous local models, and propose a general learning method called HMR (Heterogeneous Model Reuse) to optimize the margin. Our method reuses local models to approximate a global model, even when data are non-i.i.d distributed among parties, by exchanging few examples under predefined budget. Experiments on synthetic and real-world data covering different multiparty scenarios show the effectiveness of our proposal.",http://proceedings.mlr.press/v97/wu19c.html,http://proceedings.mlr.press/v97/wu19c/wu19c.pdf,ICML
180,2019,Stein Point Markov Chain Monte Carlo,"Wilson Ye Chen,         Alessandro Barp,         Francois-Xavier Briol,         Jackson Gorham,         Mark Girolami,         Lester Mackey,         Chris Oates","An important task in machine learning and statistics is the approximation of a probability measure by an empirical measure supported on a discrete point set. Stein Points are a class of algorithms for this task, which proceed by sequentially minimising a Stein discrepancy between the empirical measure and the target and, hence, require the solution of a non-convex optimisation problem to obtain each new point. This paper removes the need to solve this optimisation problem by, instead, selecting each new point based on a Markov chain sample path. This significantly reduces the computational cost of Stein Points and leads to a suite of algorithms that are straightforward to implement. The new algorithms are illustrated on a set of challenging Bayesian inference problems, and rigorous theoretical guarantees of consistency are established.",http://proceedings.mlr.press/v97/chen19b.html,http://proceedings.mlr.press/v97/chen19b/chen19b.pdf,ICML
181,2019,Efficient Dictionary Learning with Gradient Descent,"Dar Gilboa,         Sam Buchanan,         John Wright","Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem – complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well.",http://proceedings.mlr.press/v97/gilboa19a.html,http://proceedings.mlr.press/v97/gilboa19a/gilboa19a.pdf,ICML
182,2019,Towards a Deep and Unified Understanding of Deep Neural Models in NLP,"Chaoyu Guan,         Xiting Wang,         Quanshi Zhang,         Runjin Chen,         Di He,         Xing Xie","We define a unified information-based measure to provide quantitative explanations on how intermediate layers of deep Natural Language Processing (NLP) models leverage information of input words. Our method advances existing explanation methods by addressing issues in coherency and generality. Explanations generated by using our method are consistent and faithful across different timestamps, layers, and models. We show how our method can be applied to four widely used models in NLP and explain their performances on three real-world benchmark datasets.",http://proceedings.mlr.press/v97/guan19a.html,http://proceedings.mlr.press/v97/guan19a/guan19a.pdf,ICML
183,2019,Target-Based Temporal-Difference Learning,"Donghwan Lee,         Niao He","The use of target networks has been a popular and key component of recent deep Q-learning algorithms for reinforcement learning, yet little is known from the theory side. In this work, we introduce a new family of target-based temporal difference (TD) learning algorithms that maintain two separate learning parameters {–} the target variable and online variable. We propose three members in the family, the averaging TD, double TD, and periodic TD, where the target variable is updated through an averaging, symmetric, or periodic fashion, respectively, mirroring those techniques used in deep Q-learning practice. We establish asymptotic convergence analyses for both averaging TD and double TD and a finite sample analysis for periodic TD. In addition, we provide some simulation results showing potentially superior convergence of these target-based TD algorithms compared to the standard TD-learning. While this work focuses on linear function approximation and policy evaluation setting, we consider this as a meaningful step towards the theoretical understanding of deep Q-learning variants with target networks.",http://proceedings.mlr.press/v97/lee19a.html,http://proceedings.mlr.press/v97/lee19a/lee19a.pdf,ICML
184,2019,Understanding the Impact of Entropy on Policy Optimization,"Zafarali Ahmed,         Nicolas Le Roux,         Mohammad Norouzi,         Dale Schuurmans","Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with exploration by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. We then qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.",http://proceedings.mlr.press/v97/ahmed19a.html,http://proceedings.mlr.press/v97/ahmed19a/ahmed19a.pdf,ICML
185,2019,Learning Distance for Sequences by Learning a Ground Metric,"Bing Su,         Ying Wu","Learning distances that operate directly on multi-dimensional sequences is challenging because such distances are structural by nature and the vectors in sequences are not independent. Generally, distances for sequences heavily depend on the ground metric between the vectors in sequences. We propose to learn the distance for sequences through learning a ground Mahalanobis metric for the vectors in sequences. The learning samples are sequences of vectors for which how the ground metric between vectors induces the overall distance is given, and the objective is that the distance induced by the learned ground metric produces large values for sequences from different classes and small values for those from the same class. We formulate the metric as a parameter of the distance, bring closer each sequence to an associated virtual sequence w.r.t. the distance to reduce the number of constraints, and develop a general iterative solution for any ground-metric-based sequence distance. Experiments on several sequence datasets demonstrate the effectiveness and efficiency of our method.",http://proceedings.mlr.press/v97/su19b.html,http://proceedings.mlr.press/v97/su19b/su19b.pdf,ICML
186,2019,Deep Factors for Forecasting,"Yuyang Wang,         Alex Smola,         Danielle Maddix,         Jan Gasthaus,         Dean Foster,         Tim Januschowski","Producing probabilistic forecasts for large collections of similar and/or dependent time series is a practically highly relevant, yet challenging task. Classical time series models fail to capture complex patterns in the data and multivariate techniques struggle to scale to large problem sizes, but their reliance on strong structural assumptions makes them data-efficient and allows them to provide estimates of uncertainty. The converse is true for models based on deep neural networks, which can learn complex patterns and dependencies given enough data. In this paper, we propose a hybrid model that incorporates the benefits of both approaches. Our new method is data-driven and scalable via a latent, global, deep component. It also handles uncertainty through a local classical model. We provide both theoretical and empirical evidence for the soundness of our approach through a necessary and sufficient decomposition of exchangeable time series into a global and a local part and extensive experiments. Our experiments demonstrate the advantages of our model both in term of data efficiency and computational complexity.",http://proceedings.mlr.press/v97/wang19k.html,http://proceedings.mlr.press/v97/wang19k/wang19k.pdf,ICML
187,2019,Position-aware Graph Neural Networks,"Jiaxuan You,         Rex Ying,         Jure Leskovec","Learning node embeddings that capture a node’s position within the broader graph structure is crucial for many prediction tasks on graphs. However, existing Graph Neural Network (GNN) architectures have limited power in capturing the position/location of a given node with respect to all other nodes of the graph. Here we propose Position-aware Graph Neural Networks (P-GNNs), a new class of GNNs for computing position-aware node embeddings. P-GNN first samples sets of anchor nodes, computes the distance of a given target node to each anchor-set, and then learns a non-linear distance-weighted aggregation scheme over the anchor-sets. This way P-GNNs can capture positions/locations of nodes with respect to the anchor nodes. P-GNNs have several advantages: they are inductive, scalable, and can incorporate node feature information. We apply P-GNNs to multiple prediction tasks including link prediction and community detection. We show that P-GNNs consistently outperform state of the art GNNs, with up to 66% improvement in terms of the ROC AUC score.",http://proceedings.mlr.press/v97/you19b.html,http://proceedings.mlr.press/v97/you19b/you19b.pdf,ICML
188,2019,Teaching a black-box learner,"Sanjoy Dasgupta,         Daniel Hsu,         Stefanos Poulis,         Xiaojin Zhu","One widely-studied model of teaching calls for a teacher to provide the minimal set of labeled examples that uniquely specifies a target concept. The assumption is that the teacher knows the learner’s hypothesis class, which is often not true of real-life teaching scenarios. We consider the problem of teaching a learner whose representation and hypothesis class are unknown—that is, the learner is a black box. We show that a teacher who does not interact with the learner can do no better than providing random examples. We then prove, however, that with interaction, a teacher can efficiently find a set of teaching examples that is a provably good approximation to the optimal set. As an illustration, we show how this scheme can be used to shrink training sets for any family of classifiers: that is, to find an approximately-minimal subset of training instances that yields the same classifier as the entire set.",http://proceedings.mlr.press/v97/dasgupta19a.html,http://proceedings.mlr.press/v97/dasgupta19a/dasgupta19a.pdf,ICML
189,2019,Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations,"Tri Dao,         Albert Gu,         Matthew Eichhorn,         Atri Rudra,         Christopher Re","Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural prior they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(NlogN)O(Nlog⁡N)O(N \log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions NNN up to 102410241024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points—the first time a structured approach has done so—with 4X faster inference speed and 40X fewer parameters.",http://proceedings.mlr.press/v97/dao19a.html,http://proceedings.mlr.press/v97/dao19a/dao19a.pdf,ICML
190,2019,Counterfactual Off-Policy Evaluation with Gumbel-Max Structural Causal Models,"Michael Oberst,         David Sontag","We introduce an off-policy evaluation procedure for highlighting episodes where applying a reinforcement learned (RL) policy is likely to have produced a substantially different outcome than the observed policy. In particular, we introduce a class of structural causal models (SCMs) for generating counterfactual trajectories in finite partially observable Markov Decision Processes (POMDPs). We see this as a useful procedure for off-policy “debugging” in high-risk settings (e.g., healthcare); by decomposing the expected difference in reward between the RL and observed policy into specific episodes, we can identify episodes where the counterfactual difference in reward is most dramatic. This in turn can be used to facilitate review of specific episodes by domain experts. We demonstrate the utility of this procedure with a synthetic environment of sepsis management.",http://proceedings.mlr.press/v97/oberst19a.html,http://proceedings.mlr.press/v97/oberst19a/oberst19a.pdf,ICML
191,2019,Direct Uncertainty Prediction for Medical Second Opinions,"Maithra Raghu,         Katy Blumer,         Rory Sayres,         Ziad Obermeyer,         Bobby Kleinberg,         Sendhil Mullainathan,         Jon Kleinberg","The issue of disagreements amongst human experts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresponds to doctor disagreements on a patient diagnosis. In this work, we show that machine learning models can be successfully trained to give uncertainty scores to data instances that result in high expert disagreements. In particular, they can identify patient cases that would benefit most from a medical second opinion. Our central methodological finding is that Direct Uncertainty Prediction (DUP), training a model to predict an uncertainty score directly from the raw patient features, works better than Uncertainty Via Classification, the two step process of training a classifier and postprocessing the output distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application.",http://proceedings.mlr.press/v97/raghu19a.html,http://proceedings.mlr.press/v97/raghu19a/raghu19a.pdf,ICML
192,2019,Optimality Implies Kernel Sum Classifiers are Statistically Efficient,"Raphael Meyer,         Jean Honorio","We propose a novel combination of optimization tools with learning theory bounds in order to analyze the sample complexity of optimal kernel sum classifiers. This contrasts the typical learning theoretic results which hold for all (potentially suboptimal) classifiers. Our work also justifies assumptions made in prior work on multiple kernel learning. As a byproduct of our analysis, we also provide a new form of Rademacher complexity for hypothesis classes containing only optimal classifiers.",http://proceedings.mlr.press/v97/meyer19a.html,http://proceedings.mlr.press/v97/meyer19a/meyer19a.pdf,ICML
193,2019,Analyzing Federated Learning through an Adversarial Lens,"Arjun Nitin Bhagoji,         Supriyo Chakraborty,         Prateek Mittal,         Seraphin Calo","Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server to train an overall global model. In this work, we explore how the federated learning setting gives rise to a new threat, namely model poisoning, which differs from traditional data poisoning. Model poisoning is carried out by an adversary controlling a small number of malicious agents (usually 1) with the aim of causing the global model to misclassify a set of chosen inputs with high conﬁdence. We explore a number of strategies to carry out this attack on deep neural networks, starting with targeted model poisoning using a simple boosting of the malicious agent’s update to overcome the effects of other agents. We also propose two critical notions of stealth to detect malicious updates. We bypass these by including them in the adversarial objective to carry out stealthy model poisoning. We improve its stealth with the use of an alternating minimization strategy which alternately optimizes for stealth and the adversarial objective. We also empirically demonstrate that Byzantine-resilient aggregation strategies are not robust to our attacks. Our results indicate that highly constrained adversaries can carry out model poisoning attacks while maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develop effective defense strategies.",http://proceedings.mlr.press/v97/bhagoji19a.html,http://proceedings.mlr.press/v97/bhagoji19a/bhagoji19a.pdf,ICML
194,2019,CompILE: Compositional Imitation Learning and Execution,"Thomas Kipf,         Yujia Li,         Hanjun Dai,         Vinicius Zambaldi,         Alvaro Sanchez-Gonzalez,         Edward Grefenstette,         Pushmeet Kohli,         Peter Battaglia","We introduce Compositional Imitation Learning and Execution (CompILE): a framework for learning reusable, variable-length segments of hierarchically-structured behavior from demonstration data. CompILE uses a novel unsupervised, fully-differentiable sequence segmentation module to learn latent encodings of sequential data that can be re-composed and executed to perform new tasks. Once trained, our model generalizes to sequences of longer length and from environment instances not seen during training. We evaluate CompILE in a challenging 2D multi-task environment and a continuous control task, and show that it can find correct task boundaries and event encodings in an unsupervised manner. Latent codes and associated behavior policies discovered by CompILE can be used by a hierarchical agent, where the high-level policy selects actions in the latent code space, and the low-level, task-specific policies are simply the learned decoders. We found that our CompILE-based agent could learn given only sparse rewards, where agents without task-specific policies struggle.",http://proceedings.mlr.press/v97/kipf19a.html,http://proceedings.mlr.press/v97/kipf19a/kipf19a.pdf,ICML
195,2019,Bias Also Matters: Bias Attribution for Deep Neural Network Explanation,"Shengjie Wang,         Tianyi Zhou,         Jeff Bilmes","The gradient of a deep neural network (DNN) w.r.t. the input provides information that can be used to explain the output prediction in terms of the input features and has been widely studied to assist in interpreting DNNs. In a linear model (i.e., g(x) = wx + b), the gradient corresponds to the weights w. Such a model can reasonably locally-linearly approximate a smooth nonlinear DNN, and hence the weights of this local model are the gradient. The bias b, however, is usually overlooked in attribution methods. In this paper, we observe that since the bias in a DNN also has a non-negligible contribution to the correctness of predictions, it can also play a significant role in understanding DNN behavior. We propose a backpropagation-type algorithm “bias back-propagation (BBp)” that starts at the output layer and iteratively attributes the bias of each layer to its input nodes as well as combining the resulting bias term of the previous layer. Together with the backpropagation of the gradient generating w, we can fully recover the locally linear model g(x) = wx + b. In experiments, we show that BBp can generate complementary and highly interpretable explanations.",http://proceedings.mlr.press/v97/wang19p.html,http://proceedings.mlr.press/v97/wang19p/wang19p.pdf,ICML
196,2019,Optimal Minimal Margin Maximization with Boosting,"Alexander Mathiasen,         Kasper Green Larsen,         Allan Grønlund","Boosting algorithms iteratively produce linear combinations of more and more base hypotheses and it has been observed experimentally that the generalization error keeps improving even after achieving zero training error. One popular explanation attributes this to improvements in margins. A common goal in a long line of research, is to obtain large margins using as few base hypotheses as possible, culminating with the AdaBoostV algorithm by R{ä}tsch and Warmuth [JMLR’05]. The AdaBoostV algorithm was later conjectured to yield an optimal trade-off between number of hypotheses trained and the minimal margin over all training points (Nie, Warmuth, Vishwanathan and Zhang [JMLR’13]). Our main contribution is a new algorithm refuting this conjecture. Furthermore, we prove a lower bound which implies that our new algorithm is optimal.",http://proceedings.mlr.press/v97/mathiasen19a.html,http://proceedings.mlr.press/v97/mathiasen19a/mathiasen19a.pdf,ICML
197,2019,Estimate Sequences for Variance-Reduced Stochastic Composite Optimization,"Andrei Kulunchakov,         Julien Mairal","In this paper, we propose a unified view of gradient-based algorithms for stochastic convex composite optimization by extending the concept of estimate sequence introduced by Nesterov. This point of view covers the stochastic gradient descent method, variants of the approaches SAGA, SVRG, and has several advantages: (i) we provide a generic proof of convergence for the aforementioned methods; (ii) we show that this SVRG variant is adaptive to strong convexity; (iii) we naturally obtain new algorithms with the same guarantees; (iv) we derive generic strategies to make these algorithms robust to stochastic noise, which is useful when data is corrupted by small random perturbations. Finally, we show that this viewpoint is useful to obtain new accelerated algorithms in the sense of Nesterov.",http://proceedings.mlr.press/v97/kulunchakov19a.html,http://proceedings.mlr.press/v97/kulunchakov19a/kulunchakov19a.pdf,ICML
198,2019,Improving Neural Network Quantization without Retraining using Outlier Channel Splitting,"Ritchie Zhao,         Yuwei Hu,         Jordan Dotzel,         Chris De Sa,         Zhiru Zhang","Quantization can improve the execution latency and energy efficiency of neural networks on both commodity GPUs and specialized accelerators. The majority of existing literature focuses on training quantized DNNs, while this work examines the less-studied topic of quantizing a floating-point model without (re)training. DNN weights and activations follow a bell-shaped distribution post-training, while practical hardware uses a linear quantization grid. This leads to challenges in dealing with outliers in the distribution. Prior work has addressed this by clipping the outliers or using specialized hardware. In this work, we propose outlier channel splitting (OCS), which duplicates channels containing outliers, then halves the channel values. The network remains functionally identical, but affected outliers are moved toward the center of the distribution. OCS requires no additional training and works on commodity hardware. Experimental evaluation on ImageNet classification and language modeling shows that OCS can outperform state-of-the-art clipping techniques with only minor overhead.",http://proceedings.mlr.press/v97/zhao19c.html,http://proceedings.mlr.press/v97/zhao19c/zhao19c.pdf,ICML
199,2019,Active Embedding Search via Noisy Paired Comparisons,"Gregory Canal,         Andy Massimino,         Mark Davenport,         Christopher Rozell","Suppose that we wish to estimate a user’s preference vector www from paired comparisons of the form “does user www prefer item ppp or item qqq?,” where both the user and items are embedded in a low-dimensional Euclidean space with distances that reflect user and item similarities. Such observations arise in numerous settings, including psychometrics and psychology experiments, search tasks, advertising, and recommender systems. In such tasks, queries can be extremely costly and subject to varying levels of response noise; thus, we aim to actively choose pairs that are most informative given the results of previous comparisons. We provide new theoretical insights into the benefits and challenges of greedy information maximization in this setting, and develop two novel strategies that maximize lower bounds on information gain and are simpler to analyze and compute respectively. We use simulated responses from a real-world dataset to validate our strategies through their similar performance to greedy information maximization, and their superior preference estimation over state-of-the-art selection methods as well as random queries.",http://proceedings.mlr.press/v97/canal19a.html,http://proceedings.mlr.press/v97/canal19a/canal19a.pdf,ICML
200,2019,Semi-Cyclic Stochastic Gradient Descent,"Hubert Eichner,         Tomer Koren,         Brendan Mcmahan,         Nathan Srebro,         Kunal Talwar","We consider convex SGD updates with a block-cyclic structure, i.e., where each cycle consists of a small number of blocks, each with many samples from a possibly different, block-specific, distribution. This situation arises, e.g., in Federated Learning where the mobile devices available for updates at different times during the day have different characteristics. We show that such block-cyclic structure can significantly deteriorate the performance of SGD, but propose a simple approach that allows prediction with the same guarantees as for i.i.d., non-cyclic, sampling.",http://proceedings.mlr.press/v97/eichner19a.html,http://proceedings.mlr.press/v97/eichner19a/eichner19a.pdf,ICML
201,2019,Incorporating Grouping Information into Bayesian Decision Tree Ensembles,"Junliang Du,         Antonio Linero","We consider the problem of nonparametric regression in the high-dimensional setting in which P≫NP≫NP \gg N. We study the use of overlapping group structures to improve prediction and variable selection. These structures arise commonly when analyzing DNA microarray data, where genes can naturally be grouped according to genetic pathways. We incorporate overlapping group structure into a Bayesian additive regression trees model using a prior constructed so that, if a variable from some group is used to construct a split, this increases the probability that subsequent splits will use predictors from the same group. We refer to our model as an overlapping group Bayesian additive regression trees (OG-BART) model, and our prior on the splits an overlapping group Dirichlet (OG-Dirichlet) prior. Like the sparse group lasso, our prior encourages sparsity both within and between groups. We study the correlation structure of the prior, illustrate the proposed methodology on simulated data, and apply the methodology to gene expression data to learn which genetic pathways are predictive of breast cancer tumor metastasis.",http://proceedings.mlr.press/v97/du19d.html,http://proceedings.mlr.press/v97/du19d/du19d.pdf,ICML
202,2019,Improved Parallel Algorithms for Density-Based Network Clustering,"Mohsen Ghaffari,         Silvio Lattanzi,         Slobodan Mitrović","Clustering large-scale networks is a central topic in unsupervised learning with many applications in machine learning and data mining. A classic approach to cluster a network is to identify regions of high edge density, which in the literature is captured by two fundamental problems: the densest subgraph and the kkk-core decomposition problems. We design massively parallel computation (MPC) algorithms for these problems that are considerably faster than prior work. In the case of kkk-core decomposition, our work improves exponentially on the algorithm provided by Esfandiari et al. (ICML’18). Compared to the prior work on densest subgraph presented by Bahmani et al. (VLDB’12, ’14), our result requires quadratically fewer MPC rounds. We complement our analysis with an experimental scalability analysis of our techniques.",http://proceedings.mlr.press/v97/ghaffari19a.html,http://proceedings.mlr.press/v97/ghaffari19a/ghaffari19a.pdf,ICML
203,2019,Learning Discrete and Continuous Factors of Data via Alternating Disentanglement,"Yeonwoo Jeong,         Hyun Oh Song","We address the problem of unsupervised disentanglement of discrete and continuous explanatory factors of data. We first show a simple procedure for minimizing the total correlation of the continuous latent variables without having to use a discriminator network or perform importance sampling, via cascading the information flow in the beta-VAE framework. Furthermore, we propose a method which avoids offloading the entire burden of jointly modeling the continuous and discrete factors to the variational encoder by employing a separate discrete inference procedure. This leads to an interesting alternating minimization problem which switches between finding the most likely discrete configuration given the continuous factors and updating the variational encoder based on the computed discrete factors. Experiments show that the proposed method clearly disentangles discrete factors and significantly outperforms current disentanglement methods based on the disentanglement score and inference network classification score. The source code is available at https://github.com/snumllab/DisentanglementICML19.",http://proceedings.mlr.press/v97/jeong19d.html,http://proceedings.mlr.press/v97/jeong19d/jeong19d.pdf,ICML
204,2019,DeepNose: Using artificial neural networks to represent the space of odorants,"Ngoc Tran,         Daniel Kepple,         Sergey Shuvaev,         Alexei Koulakov","The olfactory system employs an ensemble of odorant receptors (ORs) to sense odorants and to derive olfactory percepts. We trained artificial neural networks to represent the chemical space of odorants and used this representation to predict human olfactory percepts. We hypothesized that ORs may be considered 3D convolutional filters that extract molecular features and, as such, can be trained using machine learning methods. First, we trained a convolutional autoencoder, called DeepNose, to deduce a low-dimensional representation of odorant molecules which were represented by their 3D spatial structure. Next, we tested the ability of DeepNose features in predicting physical properties and odorant percepts based on 3D molecular structure alone. We found that, despite the lack of human expertise, DeepNose features often outperformed molecular descriptors used in computational chemistry in predicting both physical properties and human perceptions. We propose that DeepNose network can extract de novo chemical features predictive of various bioactivities and can help understand the factors influencing the composition of ORs ensemble.",http://proceedings.mlr.press/v97/tran19b.html,http://proceedings.mlr.press/v97/tran19b/tran19b.pdf,ICML
205,2019,Beyond Adaptive Submodularity: Approximation Guarantees of Greedy Policy with Adaptive Submodularity Ratio,"Kaito Fujii,         Shinsaku Sakaue","We propose a new concept named adaptive submodularity ratio to study the greedy policy for sequential decision making. While the greedy policy is known to perform well for a wide variety of adaptive stochastic optimization problems in practice, its theoretical properties have been analyzed only for a limited class of problems. We narrow the gap between theory and practice by using adaptive submodularity ratio, which enables us to prove approximation guarantees of the greedy policy for a substantially wider class of problems. Examples of newly analyzed problems include important applications such as adaptive influence maximization and adaptive feature selection. Our adaptive submodularity ratio also provides bounds of adaptivity gaps. Experiments confirm that the greedy policy performs well with the applications being considered compared to standard heuristics.",http://proceedings.mlr.press/v97/fujii19a.html,http://proceedings.mlr.press/v97/fujii19a/fujii19a.pdf,ICML
206,2019,Lossless or Quantized Boosting with Integer Arithmetic,"Richard Nock,         Robert Williamson","In supervised learning, efficiency often starts with the choice of a good loss: support vector machines popularised Hinge loss, Adaboost popularised the exponential loss, etc. Recent trends in machine learning have highlighted the necessity for training routines to meet tight requirements on communication, bandwidth, energy, operations, encoding, among others. Fitting the often decades-old state of the art training routines into these new constraints does not go without pain and uncertainty or reduction in the original guarantees. Our paper starts with the design of a new strictly proper canonical, twice differentiable loss called the Q-loss. Importantly, its mirror update over (arbitrary) rational inputs uses only integer arithmetics – more precisely, the sole use of +,−,/,×,|.|+,−,/,×,|.|+, -, /, \times, |.|. We build a learning algorithm which is able, under mild assumptions, to achieve a lossless boosting-compliant training. We give conditions for a quantization of its main memory footprint, weights, to be done while keeping the whole algorithm boosting-compliant. Experiments display that the algorithm can achieve a fast convergence during the early boosting rounds compared to AdaBoost, even with a weight storage that can be 30+ times smaller. Lastly, we show that the Bayes risk of the Q-loss can be used as node splitting criterion for decision trees and guarantees optimal boosting convergence.",http://proceedings.mlr.press/v97/nock19a.html,http://proceedings.mlr.press/v97/nock19a/nock19a.pdf,ICML
207,2019,Scalable Metropolis-Hastings for Exact Bayesian Inference with Large Datasets,"Rob Cornish,         Paul Vanetti,         Alexandre Bouchard-Cote,         George Deligiannidis,         Arnaud Doucet","Bayesian inference via standard Markov Chain Monte Carlo (MCMC) methods such as Metropolis-Hastings is too computationally intensive to handle large datasets, since the cost per step usually scales like O(n)O(n)O(n) in the number of data points nnn. We propose the Scalable Metropolis-Hastings (SMH) kernel that only requires processing on average O(1)O(1)O(1) or even O(1/n−−√)O(1/n)O(1/\sqrt{n}) data points per step. This scheme is based on a combination of factorized acceptance probabilities, procedures for fast simulation of Bernoulli processes, and control variate ideas. Contrary to many MCMC subsampling schemes such as fixed step-size Stochastic Gradient Langevin Dynamics, our approach is exact insofar as the invariant distribution is the true posterior and not an approximation to it. We characterise the performance of our algorithm theoretically, and give realistic and verifiable conditions under which it is geometrically ergodic. This theory is borne out by empirical results that demonstrate overall performance benefits over standard Metropolis-Hastings and various subsampling algorithms.",http://proceedings.mlr.press/v97/cornish19a.html,http://proceedings.mlr.press/v97/cornish19a/cornish19a.pdf,ICML
208,2019,Provably Efficient Imitation Learning from Observation Alone,"Wen Sun,         Anirudh Vemula,         Byron Boots,         Drew Bagnell","We study Imitation Learning (IL) from Observations alone (ILFO) in large-scale MDPs. While most IL algorithms rely on an expert to directly provide actions to the learner, in this setting the expert only supplies sequences of observations. We design a new model-free algorithm for ILFO, Forward Adversarial Imitation Learning (FAIL), which learns a sequence of time-dependent policies by minimizing an Integral Probability Metric between the observation distributions of the expert policy and the learner. FAIL provably learns a near-optimal policy with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The resulting theory extends the domain of provably sample efficient learning algorithms beyond existing results that typically only consider tabular RL settings or settings that require access to a near-optimal reset distribution. We also demonstrate the efficacy ofFAIL on multiple OpenAI Gym control tasks.",http://proceedings.mlr.press/v97/sun19b.html,http://proceedings.mlr.press/v97/sun19b/sun19b.pdf,ICML
209,2019,Homomorphic Sensing,"Manolis Tsakiris,         Liangzu Peng","A recent line of research termed ""unlabeled sensing"" and ""shuffled linear regression"" has been exploring under great generality the recovery of signals from subsampled and permuted measurements; a challenging problem in diverse fields of data science and machine learning. In this paper we introduce an abstraction of this problem which we call ""homomorphic sensing"". Given a linear subspace and a finite set of linear transformations we develop an algebraic theory which establishes conditions guaranteeing that points in the subspace are uniquely determined from their homomorphic image under some transformation in the set. As a special case, we recover known conditions for unlabeled sensing, as well as new results and extensions. On the algorithmic level we exhibit two dynamic programming based algorithms, which to the best of our knowledge are the first working solutions for the unlabeled sensing problem for small dimensions. One of them, additionally based on branch-and-bound, when applied to image registration under affine transformations, performs on par with or outperforms state-of-the-art methods on benchmark datasets.",http://proceedings.mlr.press/v97/tsakiris19a.html,http://proceedings.mlr.press/v97/tsakiris19a/tsakiris19a.pdf,ICML
210,2019,Stochastic Blockmodels meet Graph Neural Networks,"Nikhil Mehta,         Lawrence Carin Duke,         Piyush Rai","Stochastic blockmodels (SBM) and their variants, e.g.e.g., mixed-membership and overlapping stochastic blockmodels, are latent variable based generative models for graphs. They have proven to be successful for various tasks, such as discovering the community structure and link prediction on graph-structured data. Recently, graph neural networks, e.g.e.g., graph convolutional networks, have also emerged as a promising approach to learn powerful representations (embeddings) for the nodes in the graph, by exploiting graph properties such as locality and invariance. In this work, we unify these two directions by developing a sparse variational autoencoder for graphs, that retains the interpretability of SBMs, while also enjoying the excellent predictive performance of graph neural nets. Moreover, our framework is accompanied by a fast recognition model that enables fast inference of the node embeddings (which are of independent interest for inference in SBM and its variants). Although we develop this framework for a particular type of SBM, namely the overlapping stochastic blockmodel, the proposed framework can be adapted readily for other types of SBMs. Experimental results on several benchmarks demonstrate encouraging results on link prediction while learning an interpretable latent structure that can be used for community discovery.",http://proceedings.mlr.press/v97/mehta19a.html,http://proceedings.mlr.press/v97/mehta19a/mehta19a.pdf,ICML
211,2019,AReS and MaRS Adversarial and MMD-Minimizing Regression for SDEs,"Gabriele Abbati,         Philippe Wenk,         Michael A. Osborne,         Andreas Krause,         Bernhard Schölkopf,         Stefan Bauer","Stochastic differential equations are an important modeling class in many disciplines. Consequently, there exist many methods relying on various discretization and numerical integration schemes. In this paper, we propose a novel, probabilistic model for estimating the drift and diffusion given noisy observations of the underlying stochastic system. Using state-of-the-art adversarial and moment matching inference techniques, we avoid the discretization schemes of classical approaches. This leads to significant improvements in parameter accuracy and robustness given random initial guesses. On four established benchmark systems, we compare the performance of our algorithms to state-of-the-art solutions based on extended Kalman filtering and Gaussian processes.",http://proceedings.mlr.press/v97/abbati19a.html,http://proceedings.mlr.press/v97/abbati19a/abbati19a.pdf,ICML
212,2019,Refined Complexity of PCA with Outliers,"Kirill Simonov,         Fedor Fomin,         Petr Golovach,         Fahad Panolan","Principal component analysis (PCA) is one of the most fundamental procedures in exploratory data analysis and is the basic step in applications ranging from quantitative finance and bioinformatics to image analysis and neuroscience. However, it is well-documented that the applicability of PCA in many real scenarios could be constrained by an ""immune deficiency"" to outliers such as corrupted observations. We consider the following algorithmic question about the PCA with outliers. For a set of nnn points in Rd\mathbb{R}^{d}, how to learn a subset of points, say 1% of the total number of points, such that the remaining part of the points is best fit into some unknown rr-dimensional subspace? We provide a rigorous algorithmic analysis of the problem. We show that the problem is solvable in time nO(d2)n^{O(d^2)}. In particular, for constant dimension the problem is solvable in polynomial time. We complement the algorithmic result by the lower bound, showing that unless Exponential Time Hypothesis fails, in time f(d)no(d)f(d)n^{o(d)}, for any function ff of dd, it is impossible not only to solve the problem exactly but even to approximate it within a constant factor.",http://proceedings.mlr.press/v97/simonov19a.html,http://proceedings.mlr.press/v97/simonov19a/simonov19a.pdf,ICML
213,2019,On The Power of Curriculum Learning in Training Deep Networks,"Guy Hacohen,         Daphna Weinshall","Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive “teacher"" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function.",http://proceedings.mlr.press/v97/hacohen19a.html,http://proceedings.mlr.press/v97/hacohen19a/hacohen19a.pdf,ICML
214,2019,Dropout as a Structured Shrinkage Prior,"Eric Nalisnick,         Jose Miguel Hernandez-Lobato,         Padhraic Smyth","Dropout regularization of deep neural networks has been a mysterious yet effective tool to prevent overfitting. Explanations for its success range from the prevention of ""co-adapted"" weights to it being a form of cheap Bayesian inference. We propose a novel framework for understanding multiplicative noise in neural networks, considering continuous distributions as well as Bernoulli noise (i.e. dropout). We show that multiplicative noise induces structured shrinkage priors on a network’s weights. We derive the equivalence through reparametrization properties of scale mixtures and without invoking any approximations. Given the equivalence, we then show that dropout’s Monte Carlo training objective approximates marginal MAP estimation. We leverage these insights to propose a novel shrinkage framework for resnets, terming the prior ’automatic depth determination’ as it is the natural analog of automatic relevance determination for network depth. Lastly, we investigate two inference strategies that improve upon the aforementioned MAP approximation in regression benchmarks.",http://proceedings.mlr.press/v97/nalisnick19a.html,http://proceedings.mlr.press/v97/nalisnick19a/nalisnick19a.pdf,ICML
215,2019,Conditional Independence in Testing Bayesian Networks,"Yujia Shen,         Haiying Huang,         Arthur Choi,         Adnan Darwiche","Testing Bayesian Networks (TBNs) were introduced recently to represent a set of distributions, one of which is selected based on the given evidence and used for reasoning. TBNs are more expressive than classical Bayesian Networks (BNs): Marginal queries correspond to multi-linear functions in BNs and to piecewise multi-linear functions in TBNs. Moreover, TBN queries are universal approximators, like neural networks. In this paper, we study conditional independence in TBNs, showing that it can be inferred from d-separation as in BNs. We also study the role of TBN expressiveness and independence in dealing with the problem of learning with incomplete models (i.e., ones that miss nodes or edges from the data-generating model). Finally, we illustrate our results on a number of concrete examples, including a case study on Hidden Markov Models.",http://proceedings.mlr.press/v97/shen19a.html,http://proceedings.mlr.press/v97/shen19a/shen19a.pdf,ICML
216,2019,Adaptive Sensor Placement for Continuous Spaces,"James Grant,         Alexis Boukouvalas,         Ryan-Rhys Griffiths,         David Leslie,         Sattar Vakili,         Enrique Munoz De Cote",We consider the problem of adaptively placing sensors along an interval to detect stochastically-generated events. We present a new formulation of the problem as a continuum-armed bandit problem with feedback in the form of partial observations of realisations of an inhomogeneous Poisson process. We design a solution method by combining Thompson sampling with nonparametric inference via increasingly granular Bayesian histograms and derive an O~(T2/3)O~(T2/3)\tilde{O}(T^{2/3}) bound on the Bayesian regret in TTT rounds. This is coupled with the design of an efficent optimisation approach to select actions in polynomial time. In simulations we demonstrate our approach to have substantially lower and less variable regret than competitor algorithms.,http://proceedings.mlr.press/v97/grant19a.html,http://proceedings.mlr.press/v97/grant19a/grant19a.pdf,ICML
217,2019,Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations,"Daniel Brown,         Wonjoon Goo,         Prabhat Nagarajan,         Scott Niekum","A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",http://proceedings.mlr.press/v97/brown19a.html,http://proceedings.mlr.press/v97/brown19a/brown19a.pdf,ICML
218,2019,Pareto Optimal Streaming Unsupervised Classification,"Soumya Basu,         Steven Gutstein,         Brent Lance,         Sanjay Shakkottai","We study an online and streaming unsupervised classification system. Our setting consists of a collection of classifiers (with unknown confusion matrices) each of which can classify one sample per unit time, and which are accessed by a stream of unlabeled samples. Each sample is dispatched to one or more classifiers, and depending on the labels collected from these classifiers, may be sent to other classifiers to collect additional labels. The labels are continually aggregated. Once the aggregated label has high enough accuracy (a pre-specified threshold for accuracy) or the sample is sent to all the classifiers, the now labeled sample is ejected from the system. For any given pre-specified threshold for accuracy, the objective is to sustain the maximum possible rate of arrival of new samples, such that the number of samples in memory does not grow unbounded. In this paper, we characterize the Pareto-optimal region of accuracy and arrival rate, and develop an algorithm that can operate at any point within this region. Our algorithm uses queueing-based routing and scheduling approaches combined with novel online tensor decomposition method to learn the hidden parameters, to Pareto-optimality guarantees. We finally verify our theoretical results through simulations on two ensembles formed using AlexNet, VGG, and ResNet deep image classifiers.",http://proceedings.mlr.press/v97/basu19a.html,http://proceedings.mlr.press/v97/basu19a/basu19a.pdf,ICML
219,2019,LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning,"Huaiyu Li,         Weiming Dong,         Xing Mei,         Chongyang Ma,         Feiyue Huang,         Bao-Gang Hu","In this work, we propose a novel meta-learning approach for few-shot classification, which learns transferable prior knowledge across tasks and directly produces network parameters for similar unseen tasks with training samples. Our approach, called LGM-Net, includes two key modules, namely, TargetNet and MetaNet. The TargetNet module is a neural network for solving a specific task and the MetaNet module aims at learning to generate functional weights for TargetNet by observing training samples. We also present an intertask normalization strategy for the training process to leverage common information shared across different tasks. The experimental results on Omniglot and miniImageNet datasets demonstrate that LGM-Net can effectively adapt to similar unseen tasks and achieve competitive performance, and the results on synthetic datasets show that transferable prior knowledge is learned by the MetaNet module via mapping training data to functional weights. LGM-Net enables fast learning and adaptation since no further tuning steps are required compared to other meta-learning approaches",http://proceedings.mlr.press/v97/li19c.html,http://proceedings.mlr.press/v97/li19c/li19c.pdf,ICML
220,2019,CAB: Continuous Adaptive Blending for Policy Evaluation and Learning,"Yi Su,         Lequn Wang,         Michele Santacatterina,         Thorsten Joachims","The ability to perform offline A/B-testing and off-policy learning using logged contextual bandit feedback is highly desirable in a broad range of applications, including recommender systems, search engines, ad placement, and personalized health care. Both offline A/B-testing and off-policy learning require a counterfactual estimator that evaluates how some new policy would have performed, if it had been used instead of the logging policy. In this paper, we identify a family of counterfactual estimators which subsumes most such estimators proposed to date. Our analysis of this family identifies a new estimator - called Continuous Adaptive Blending (CAB) - which enjoys many advantageous theoretical and practical properties. In particular, it can be substantially less biased than clipped Inverse Propensity Score (IPS) weighting and the Direct Method, and it can have less variance than Doubly Robust and IPS estimators. In addition, it is sub-differentiable such that it can be used for learning, unlike the SWITCH estimator. Experimental results show that CAB provides excellent evaluation accuracy and outperforms other counterfactual estimators in terms of learning performance.",http://proceedings.mlr.press/v97/su19a.html,http://proceedings.mlr.press/v97/su19a/su19a.pdf,ICML
221,2019,Learning to Optimize Multigrid PDE Solvers,"Daniel Greenfeld,         Meirav Galun,         Ronen Basri,         Irad Yavneh,         Ron Kimmel","Constructing fast numerical solvers for partial differential equations (PDEs) is crucial for many scientific disciplines. A leading technique for solving large-scale PDEs is using multigrid methods. At the core of a multigrid solver is the prolongation matrix, which relates between different scales of the problem. This matrix is strongly problem-dependent, and its optimal construction is critical to the efficiency of the solver. In practice, however, devising multigrid algorithms for new problems often poses formidable challenges. In this paper we propose a framework for learning multigrid solvers. Our method learns a (single) mapping from discretized PDEs to prolongation operators for a broad class of 2D diffusion problems. We train a neural network once for the entire class of PDEs, using an efficient and unsupervised loss function. Our tests demonstrate improved convergence rates compared to the widely used Black-Box multigrid scheme, suggesting that our method successfully learned rules for constructing prolongation matrices.",http://proceedings.mlr.press/v97/greenfeld19a.html,http://proceedings.mlr.press/v97/greenfeld19a/greenfeld19a.pdf,ICML
222,2019,Learning to Clear the Market,"Weiran Shen,         Sebastien Lahaie,         Renato Paes Leme","The problem of market clearing is to set a price for an item such that quantity demanded equals quantity supplied. In this work, we cast the problem of predicting clearing prices into a learning framework and use the resulting models to perform revenue optimization in auctions and markets with contextual information. The economic intuition behind market clearing allows us to obtain fine-grained control over the aggressiveness of the resulting pricing policy, grounded in theory. To evaluate our approach, we fit a model of clearing prices over a massive dataset of bids in display ad auctions from a major ad exchange. The learned prices outperform other modeling techniques in the literature in terms of revenue and efficiency trade-offs. Because of the convex nature of the clearing loss function, the convergence rate of our method is as fast as linear regression.",http://proceedings.mlr.press/v97/shen19b.html,http://proceedings.mlr.press/v97/shen19b/shen19b.pdf,ICML
223,2019,Leveraging Low-Rank Relations Between Surrogate Tasks in Structured Prediction,"Giulia Luise,         Dimitrios Stamos,         Massimiliano Pontil,         Carlo Ciliberto","We study the interplay between surrogate methods for structured prediction and techniques from multitask learning designed to leverage relationships between surrogate outputs. We propose an efficient algorithm based on trace norm regularization which, differently from previous methods, does not require explicit knowledge of the coding/decoding functions of the surrogate framework. As a result, our algorithm can be applied to the broad class of problems in which the surrogate space is large or even infinite dimensional. We study excess risk bounds for trace norm regularized structured prediction proving the consistency and learning rates for our estimator. We also identify relevant regimes in which our approach can enjoy better generalization performance than previous methods. Numerical experiments on ranking problems indicate that enforcing low-rank relations among surrogate outputs may indeed provide a significant advantage in practice.",http://proceedings.mlr.press/v97/luise19a.html,http://proceedings.mlr.press/v97/luise19a/luise19a.pdf,ICML
224,2019,CoT: Cooperative Training for Generative Modeling of Discrete Data,"Sidi Lu,         Lantao Yu,         Siyuan Feng,         Yaoming Zhu,         Weinan Zhang","In this paper, we study the generative models of sequential discrete data. To tackle the exposure bias problem inherent in maximum likelihood estimation (MLE), generative adversarial networks (GANs) are introduced to penalize the unrealistic generated samples. To exploit the supervision signal from the discriminator, most previous models leverage REINFORCE to address the non-differentiable problem of sequential discrete data. However, because of the unstable property of the training signal during the dynamic process of adversarial training, the effectiveness of REINFORCE, in this case, is hardly guaranteed. To deal with such a problem, we propose a novel approach called Cooperative Training (CoT) to improve the training of sequence generative models. CoT transforms the min-max game of GANs into a joint maximization framework and manages to explicitly estimate and optimize Jensen-Shannon divergence. Moreover, CoT works without the necessity of pre-training via MLE, which is crucial to the success of previous methods. In the experiments, compared to existing state-of-the-art methods, CoT shows superior or at least competitive performance on sample quality, diversity, as well as training stability.",http://proceedings.mlr.press/v97/lu19d.html,http://proceedings.mlr.press/v97/lu19d/lu19d.pdf,ICML
225,2019,Predicate Exchange: Inference with Declarative Knowledge,"Zenna Tavares,         Javier Burroni,         Edgar Minasyan,         Armando Solar-Lezama,         Rajesh Ranganath","Programming languages allow us to express complex predicates, but existing inference methods are unable to condition probabilistic models on most of them. To support a broader class of predicates, we develop an inference procedure called predicate exchange, which softens predicates. A soft predicate quantifies the extent to which values of model variables are consistent with its hard counterpart. We substitute the likelihood term in the Bayesian posterior with a soft predicate, and develop a variant of replica exchange MCMC to draw posterior samples. We implement predicate exchange as a language agnostic tool which performs a nonstandard execution of a probabilistic program. We demonstrate the approach on sequence models of health and inverse rendering.",http://proceedings.mlr.press/v97/tavares19a.html,http://proceedings.mlr.press/v97/tavares19a/tavares19a.pdf,ICML
226,2019,DAG-GNN: DAG Structure Learning with Graph Neural Networks,"Yue Yu,         Jie Chen,         Tian Gao,         Mo Yu","Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at \url{https://github.com/fishmoon1234/DAG-GNN}.",http://proceedings.mlr.press/v97/yu19a.html,http://proceedings.mlr.press/v97/yu19a/yu19a.pdf,ICML
227,2019,Robust Learning from Untrusted Sources,"Nikola Konstantinov,         Christoph Lampert","Modern machine learning methods often require more data for training than a single expert can provide. Therefore, it has become a standard procedure to collect data from multiple external sources, \eg via crowdsourcing. Unfortunately, the quality of these sources is not always guaranteed. As further complications, the data might be stored in a distributed way, or might even have to remain private. In this work, we address the question of how to learn robustly in such scenarios. Studying the problem through the lens of statistical learning theory, we derive a procedure that allows for learning from all available sources, yet automatically suppresses irrelevant or corrupted data. We show by extensive experiments that our method provides significant improvements over alternative approaches from robust statistics and distributed optimization.",http://proceedings.mlr.press/v97/konstantinov19a.html,http://proceedings.mlr.press/v97/konstantinov19a/konstantinov19a.pdf,ICML
228,2019,Open-ended learning in symmetric zero-sum games,"David Balduzzi,         Marta Garnelo,         Yoram Bachrach,         Wojciech Czarnecki,         Julien Perolat,         Max Jaderberg,         Thore Graepel","Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them ‘winner’ and ‘loser’. If the game is approximately transitive, then self-play generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective – we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield open-ended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectified Nash response, PSRO_rN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSRO_rN to two highly nontransitive resource allocation games and find that PSRO_rN consistently outperforms the existing alternatives.",http://proceedings.mlr.press/v97/balduzzi19a.html,http://proceedings.mlr.press/v97/balduzzi19a/balduzzi19a.pdf,ICML
229,2019,Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules,"Daniel Ho,         Eric Liang,         Xi Chen,         Ion Stoica,         Pieter Abbeel","A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at https://github.com/arcelien/pba.",http://proceedings.mlr.press/v97/ho19b.html,http://proceedings.mlr.press/v97/ho19b/ho19b.pdf,ICML
230,2019,Sensitivity Analysis of Linear Structural Causal Models,"Carlos Cinelli,         Daniel Kumor,         Bryant Chen,         Judea Pearl,         Elias Bareinboim","Causal inference requires assumptions about the data generating process, many of which are unverifiable from the data. Given that some causal assumptions might be uncertain or disputed, formal methods are needed to quantify how sensitive research conclusions are to violations of those assumptions. Although an extensive literature exists on the topic, most results are limited to specific model structures, while a general-purpose algorithmic framework for sensitivity analysis is still lacking. In this paper, we develop a formal, systematic approach to sensitivity analysis for arbitrary linear Structural Causal Models (SCMs). We start by formalizing sensitivity analysis as a constrained identification problem. We then develop an efficient, graph-based identification algorithm that exploits non-zero constraints on both directed and bidirected edges. This allows researchers to systematically derive sensitivity curves for a target causal quantity with an arbitrary set of path coefficients and error covariances as sensitivity parameters. These results can be used to display the degree to which violations of causal assumptions affect the target quantity of interest, and to judge, on scientific grounds, whether problematic degrees of violations are plausible.",http://proceedings.mlr.press/v97/cinelli19a.html,http://proceedings.mlr.press/v97/cinelli19a/cinelli19a.pdf,ICML
231,2019,Combining parametric and nonparametric models for off-policy evaluation,"Omer Gottesman,         Yao Liu,         Scott Sussex,         Emma Brunskill,         Finale Doshi-Velez","We consider a model-based approach to perform batch off-policy evaluation in reinforcement learning. Our method takes a mixture-of-experts approach to combine parametric and non-parametric models of the environment such that the final value estimate has the least expected error. We do so by first estimating the local accuracy of each model and then using a planner to select which model to use at every time step as to minimize the return error estimate along entire trajectories. Across a variety of domains, our mixture-based approach outperforms the individual models alone as well as state-of-the-art importance sampling-based estimators.",http://proceedings.mlr.press/v97/gottesman19a.html,http://proceedings.mlr.press/v97/gottesman19a/gottesman19a.pdf,ICML
232,2019,ARSM: Augment-REINFORCE-Swap-Merge Estimator for Gradient Backpropagation Through Categorical Variables,"Mingzhang Yin,         Yuguang Yue,         Mingyuan Zhou","To address the challenge of backpropagating the gradient through categorical variables, we propose the augment-REINFORCE-swap-merge (ARSM) gradient estimator that is unbiased and has low variance. ARSM first uses variable augmentation, REINFORCE, and Rao-Blackwellization to re-express the gradient as an expectation under the Dirichlet distribution, then uses variable swapping to construct differently expressed but equivalent expectations, and finally shares common random numbers between these expectations to achieve significant variance reduction. Experimental results show ARSM closely resembles the performance of the true gradient for optimization in univariate settings; outperforms existing estimators by a large margin when applied to categorical variational auto-encoders; and provides a ""try-and-see self-critic"" variance reduction method for discrete-action policy gradient, which removes the need of estimating baselines by generating a random number of pseudo actions and estimating their action-value functions.",http://proceedings.mlr.press/v97/yin19c.html,http://proceedings.mlr.press/v97/yin19c/yin19c.pdf,ICML
233,2019,Understanding the Origins of Bias in Word Embeddings,"Marc-Etienne Brunet,         Colleen Alkalay-Houlihan,         Ashton Anderson,         Richard Zemel","Popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems can amplify stereotypes in important contexts. Although some methods have been developed to mitigate this problem, how word embedding biases arise during training is poorly understood. In this work we develop a technique to address this question. Given a word embedding, our method reveals how perturbing the training corpus would affect the resulting embedding bias. By tracing the origins of word embedding bias back to the original training documents, one can identify subsets of documents whose removal would most reduce bias. We demonstrate our methodology on Wikipedia and New York Times corpora, and find it to be very accurate.",http://proceedings.mlr.press/v97/brunet19a.html,http://proceedings.mlr.press/v97/brunet19a/brunet19a.pdf,ICML
234,2019,Exploiting structure of uncertainty for efficient matroid semi-bandits,"Pierre Perrault,         Vianney Perchet,         Michal Valko","We improve the efficiency of algorithms for stochastic combinatorial semi-bandits. In most interesting problems, state-of-the-art algorithms take advantage of structural properties of rewards, such as independence. However, while being minimax optimal in terms of regret, these algorithms are intractable. In our paper, we first reduce their implementation to a specific submodular maximization. Then, in case of matroid constraints, we design adapted approximation routines, thereby providing the first efficient algorithms that exploit the reward structure. In particular, we improve the state-of-the-art efficient gap-free regret bound by a factor sqrt(k), where k is the maximum action size. Finally, we show how our improvement translates to more general budgeted combinatorial semi-bandits.",http://proceedings.mlr.press/v97/perrault19a.html,http://proceedings.mlr.press/v97/perrault19a/perrault19a.pdf,ICML
235,2019,On discriminative learning of prediction uncertainty,"Vojtech Franc,         Daniel Prusa","In classification with a reject option, the classifier is allowed in uncertain cases to abstain from prediction. The classical cost based model of an optimal classifier with a reject option requires the cost of rejection to be defined explicitly. An alternative bounded-improvement model, avoiding the notion of the reject cost, seeks for a classifier with a guaranteed selective risk and maximal cover. We prove that both models share the same class of optimal strategies, and we provide an explicit relation between the reject cost and the target risk being the parameters of the two models. An optimal rejection strategy for both models is based on thresholding the conditional risk defined by posterior probabilities which are usually unavailable. We propose a discriminative algorithm learning an uncertainty function which preserves ordering of the input space induced by the conditional risk, and hence can be used to construct optimal rejection strategies.",http://proceedings.mlr.press/v97/franc19a.html,http://proceedings.mlr.press/v97/franc19a/franc19a.pdf,ICML
236,2019,Bayesian Joint Spike-and-Slab Graphical Lasso,"Zehang Li,         Tyler Mccormick,         Samuel Clark","In this article, we propose a new class of priors for Bayesian inference with multiple Gaussian graphical models. We introduce Bayesian treatments of two popular procedures, the group graphical lasso and the fused graphical lasso, and extend them to a continuous spike-and-slab framework to allow self-adaptive shrinkage and model selection simultaneously. We develop an EM algorithm that performs fast and dynamic explorations of posterior modes. Our approach selects sparse models efficiently and automatically with substantially smaller bias than would be induced by alternative regularization procedures. The performance of the proposed methods are demonstrated through simulation and two real data examples.",http://proceedings.mlr.press/v97/li19h.html,http://proceedings.mlr.press/v97/li19h/li19h.pdf,ICML
237,2019,How does Disagreement Help Generalization against Label Corruption?,"Xingrui Yu,         Bo Han,         Jiangchao Yao,         Gang Niu,         Ivor Tsang,         Masashi Sugiyama","Learning with noisy labels is one of the hottest problems in weakly-supervised learning. Based on memorization effects of deep neural networks, training on small-loss instances becomes very promising for handling noisy labels. This fosters the state-of-the-art approach ""Co-teaching"" that cross-trains two deep neural networks using the small-loss trick. However, with the increase of epochs, two networks converge to a consensus and Co-teaching reduces to the self-training MentorNet. To tackle this issue, we propose a robust learning paradigm called Co-teaching+, which bridges the ""Update by Disagreement” strategy with the original Co-teaching. First, two networks feed forward and predict all data, but keep prediction disagreement data only. Then, among such disagreement data, each network selects its small-loss data, but back propagates the small-loss data from its peer network and updates its own parameters. Empirical results on benchmark datasets demonstrate that Co-teaching+ is much superior to many state-of-the-art methods in the robustness of trained models.",http://proceedings.mlr.press/v97/yu19b.html,http://proceedings.mlr.press/v97/yu19b/yu19b.pdf,ICML
238,2019,A Kernel Perspective for Regularizing Deep Neural Networks,"Alberto Bietti,         Grégoire Mialon,         Dexiong Chen,         Julien Mairal","We propose a new point of view for regularizing deep neural networks by using the norm of a reproducing kernel Hilbert space (RKHS). Even though this norm cannot be computed, it admits upper and lower approximations leading to various practical strategies. Specifically, this perspective (i) provides a common umbrella for many existing regularization principles, including spectral norm and gradient penalties, or adversarial training, (ii) leads to new effective regularization penalties, and (iii) suggests hybrid strategies combining lower and upper bounds to get better approximations of the RKHS norm. We experimentally show this approach to be effective when learning on small datasets, or to obtain adversarially robust models.",http://proceedings.mlr.press/v97/bietti19a.html,http://proceedings.mlr.press/v97/bietti19a/bietti19a.pdf,ICML
239,2019,Adaptive Neural Trees,"Ryutaro Tanno,         Kai Arulkumaran,         Daniel Alexander,         Antonio Criminisi,         Aditya Nori","Deep neural networks and decision trees operate on largely separate paradigms; typically, the former performs representation learning with pre-specified architectures, while the latter is characterised by learning hierarchies over pre-specified features with data-driven architectures. We unite the two via adaptive neural trees (ANTs), a model that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving competitive performance on classification and regression datasets, ANTs benefit from (i) lightweight inference via conditional computation, (ii) hierarchical separation of features useful to the predictive task e.g. learning meaningful class associations, such as separating natural vs. man-made objects, and (iii) a mechanism to adapt the architecture to the size and complexity of the training dataset.",http://proceedings.mlr.press/v97/tanno19a.html,http://proceedings.mlr.press/v97/tanno19a/tanno19a.pdf,ICML
240,2019,GMNN: Graph Markov Neural Networks,"Meng Qu,         Yoshua Bengio,         Jian Tang","This paper studies semi-supervised object classification in relational data, which is a fundamental problem in relational data modeling. The problem has been extensively studied in the literature of both statistical relational learning (e.g. relational Markov networks) and graph neural networks (e.g. graph convolutional networks). Statistical relational learning methods can effectively model the dependency of object labels through conditional random fields for collective classification, whereas graph neural networks learn effective object representations for classification through end-to-end training. In this paper, we propose the Graph Markov Neural Network (GMNN) that combines the advantages of both worlds. A GMNN models the joint distribution of object labels with a conditional random field, which can be effectively trained with the variational EM algorithm. In the E-step, one graph neural network learns effective object representations for approximating the posterior distributions of object labels. In the M-step, another graph neural network is used to model the local label dependency. Experiments on object classification, link classification, and unsupervised node representation learning show that GMNN achieves state-of-the-art results.",http://proceedings.mlr.press/v97/qu19a.html,http://proceedings.mlr.press/v97/qu19a/qu19a.pdf,ICML
241,2019,Projection onto Minkowski Sums with Application to Constrained Learning,"Joong-Ho Won,         Jason Xu,         Kenneth Lange","We introduce block descent algorithms for projecting onto Minkowski sums of sets. Projection onto such sets is a crucial step in many statistical learning problems, and may regularize complexity of solutions to an optimization problem or arise in dual formulations of penalty methods. We show that projecting onto the Minkowski sum admits simple, efficient algorithms when complications such as overlapping constraints pose challenges to existing methods. We prove that our algorithm converges linearly when sets are strongly convex or satisfy an error bound condition, and extend the theory and methods to encompass non-convex sets as well. We demonstrate empirical advantages in runtime and accuracy over competitors in applications to ℓ1,pℓ1,p\ell_{1,p}-regularized learning, constrained lasso, and overlapping group lasso.",http://proceedings.mlr.press/v97/lange19a.html,http://proceedings.mlr.press/v97/lange19a/lange19a.pdf,ICML
242,2019,Weakly-Supervised Temporal Localization via Occurrence Count Learning,"Julien Schroeter,         Kirill Sidorov,         David Marshall","We propose a novel model for temporal detection and localization which allows the training of deep neural networks using only counts of event occurrences as training labels. This powerful weakly-supervised framework alleviates the burden of the imprecise and time consuming process of annotating event locations in temporal data. Unlike existing methods, in which localization is explicitly achieved by design, our model learns localization implicitly as a byproduct of learning to count instances. This unique feature is a direct consequence of the model’s theoretical properties. We validate the effectiveness of our approach in a number of experiments (drum hit and piano onset detection in audio, digit detection in images) and demonstrate performance comparable to that of fully-supervised state-of-the-art methods, despite much weaker training requirements.",http://proceedings.mlr.press/v97/schroeter19a.html,http://proceedings.mlr.press/v97/schroeter19a/schroeter19a.pdf,ICML
243,2019,Online Control with Adversarial Disturbances,"Naman Agarwal,         Brian Bullins,         Elad Hazan,         Sham Kakade,         Karan Singh","We study the control of linear dynamical systems with adversarial disturbances, as opposed to statistical noise. We present an efficient algorithm that achieves nearly-tight regret bounds in this setting. Our result generalizes upon previous work in two main aspects: the algorithm can accommodate adversarial noise in the dynamics, and can handle general convex costs.",http://proceedings.mlr.press/v97/agarwal19c.html,http://proceedings.mlr.press/v97/agarwal19c/agarwal19c.pdf,ICML
244,2019,"Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning","Casey Chu,         Jose Blanchet,         Peter Glynn","The goal of this paper is to provide a unifying view of a wide range of problems of interest in machine learning by framing them as the minimization of functionals defined on the space of probability measures. In particular, we show that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be seen through the lens of our framework. We then discuss a generic optimization algorithm for our formulation, called probability functional descent (PFD), and show how this algorithm recovers existing methods developed independently in the settings mentioned earlier.",http://proceedings.mlr.press/v97/chu19a.html,http://proceedings.mlr.press/v97/chu19a/chu19a.pdf,ICML
245,2019,AUCμ: A Performance Metric for Multi-Class Machine Learning Models,"Ross Kleiman,         David Page","The area under the receiver operating characteristic curve (AUC) is arguably the most common metric in machine learning for assessing the quality of a two-class classification model. As the number and complexity of machine learning applications grows, so too does the need for measures that can gracefully extend to classification models trained for more than two classes. Prior work in this area has proven computationally intractable and/or inconsistent with known properties of AUC, and thus there is still a need for an improved multi-class efficacy metric. We provide in this work a multi-class extension of AUC that we call AUC{\textmu} that is derived from first principles of the binary class AUC. AUC{\textmu} has similar computational complexity to AUC and maintains the properties of AUC critical to its interpretation and use.",http://proceedings.mlr.press/v97/kleiman19a.html,http://proceedings.mlr.press/v97/kleiman19a/kleiman19a.pdf,ICML
246,2019,The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study,"Daniel Park,         Jascha Sohl-Dickstein,         Quoc Le,         Samuel Smith","We investigate how the final parameters found by stochastic gradient descent are influenced by over-parameterization. We generate families of models by increasing the number of channels in a base network, and then perform a large hyper-parameter search to study how the test error depends on learning rate, batch size, and network width. We find that the optimal SGD hyper-parameters are determined by a ""normalized noise scale,"" which is a function of the batch size, learning rate, and initialization conditions. In the absence of batch normalization, the optimal normalized noise scale is directly proportional to width. Wider networks, with their higher optimal noise scale, also achieve higher test accuracy. These observations hold for MLPs, ConvNets, and ResNets, and for two different parameterization schemes (""Standard"" and ""NTK""). We observe a similar trend with batch normalization for ResNets. Surprisingly, since the largest stable learning rate is bounded, the largest batch size consistent with the optimal normalized noise scale decreases as the width increases.",http://proceedings.mlr.press/v97/park19b.html,http://proceedings.mlr.press/v97/park19b/park19b.pdf,ICML
247,2019,Diagnosing Bottlenecks in Deep Q-learning Algorithms,"Justin Fu,         Aviral Kumar,         Matthew Soh,         Sergey Levine","Q-learning methods are a common class of algorithms used in reinforcement learning (RL). However, their behavior with function approximation, especially with neural networks, is poorly understood theoretically and empirically. In this work, we aim to experimentally investigate potential issues in Q-learning, by means of a ""unit testing"" framework where we can utilize oracles to disentangle sources of error. Specifically, we investigate questions related to function approximation, sampling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with deep RL methods. We find that large neural network architectures have many benefits with regards to learning stability; offer several practical compensations for overfitting; and develop a novel sampling method based on explicitly compensating for function approximation error that yields fair improvement on high-dimensional continuous control domains.",http://proceedings.mlr.press/v97/fu19a.html,http://proceedings.mlr.press/v97/fu19a/fu19a.pdf,ICML
248,2019,Overcoming Mean-Field Approximations in Recurrent Gaussian Process Models,"Alessandro Davide Ialongo,         Mark Van Der Wilk,         James Hensman,         Carl Edward Rasmussen","We identify a new variational inference scheme for dynamical systems whose transition function is modelled by a Gaussian process. Inference in this setting has either employed computationally intensive MCMC methods, or relied on factorisations of the variational posterior. As we demonstrate in our experiments, the factorisation between latent system states and transition function can lead to a miscalibrated posterior and to learning unnecessarily large noise terms. We eliminate this factorisation by explicitly modelling the dependence between state trajectories and the low-rank representation of our Gaussian process posterior. Samples of the latent states can then be tractably generated by conditioning on this representation. The method we obtain gives better predictive performance and more calibrated estimates of the transition function, yet maintains the same time and space complexities as mean-field methods.",http://proceedings.mlr.press/v97/ialongo19a.html,http://proceedings.mlr.press/v97/ialongo19a/ialongo19a.pdf,ICML
249,2019,Learning to Groove with Inverse Sequence Transformations,"Jon Gillick,         Adam Roberts,         Jesse Engel,         Douglas Eck,         David Bamman","We explore models for translating abstract musical ideas (scores, rhythms) into expressive performances using seq2seq and recurrent variational information bottleneck (VIB) models. Though seq2seq models usually require painstakingly aligned corpora, we show that it is possible to adapt an approach from the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix, Vid2Vid) to sequences, creating large volumes of paired data by performing simple transformations and training generative models to plausibly invert these transformations. Music, and drumming in particular, provides a strong test case for this approach because many common transformations (quantization, removing voices) have clear semantics, and learning to invert them has real-world applications. Focusing on the case of drum set players, we create and release a new dataset for this purpose, containing over 13 hours of recordings by professional drummers aligned with fine-grained timing and dynamics information. We also explore some of the creative potential of these models, demonstrating improvements on state-of-the-art methods for Humanization (instantiating a performance from a musical score).",http://proceedings.mlr.press/v97/gillick19a.html,http://proceedings.mlr.press/v97/gillick19a/gillick19a.pdf,ICML
250,2019,IMEXnet A Forward Stable Deep Neural Network,"Eldad Haber,         Keegan Lensink,         Eran Treister,         Lars Ruthotto","Deep convolutional neural networks have revolutionized many machine learning and computer vision tasks, however, some remaining key challenges limit their wider use. These challenges include improving the network’s robustness to perturbations of the input image and the limited “field of view” of convolution operators. We introduce the IMEXnet that addresses these challenges by adapting semi-implicit methods for partial differential equations. Compared to similar explicit networks, such as residual networks, our network is more stable, which has recently shown to reduce the sensitivity to small changes in the input features and improve generalization. The addition of an implicit step connects all pixels in each channel of the image and therefore addresses the field of view problem while still being comparable to standard convolutions in terms of the number of parameters and computational complexity. We also present a new dataset for semantic segmentation and demonstrate the effectiveness of our architecture using the NYU Depth dataset.",http://proceedings.mlr.press/v97/haber19a.html,http://proceedings.mlr.press/v97/haber19a/haber19a.pdf,ICML
251,2019,Competing Against Nash Equilibria in Adversarially Changing Zero-Sum Games,"Adrian Rivera Cardoso,         Jacob Abernethy,         He Wang,         Huan Xu","We study the problem of repeated play in a zero-sum game in which the payoff matrix may change, in a possibly adversarial fashion, on each round; we call these Online Matrix Games. Finding the Nash Equilibrium (NE) of a two player zero-sum game is core to many problems in statistics, optimization, and economics, and for a fixed game matrix this can be easily reduced to solving a linear program. But when the payoff matrix evolves over time our goal is to find a sequential algorithm that can compete with, in a certain sense, the NE of the long-term-averaged payoff matrix. We design an algorithm with small NE regret–that is, we ensure that the long-term payoff of both players is close to minimax optimum in hindsight. Our algorithm achieves near-optimal dependence with respect to the number of rounds and depends poly-logarithmically on the number of available actions of the players. Additionally, we show that the naive reduction, where each player simply minimizes its own regret, fails to achieve the stated objective regardless of which algorithm is used. Lastly, we consider the so-called bandit setting, where the feedback is significantly limited, and we provide an algorithm with small NE regret using one-point estimates of each payoff matrix.",http://proceedings.mlr.press/v97/cardoso19a.html,http://proceedings.mlr.press/v97/cardoso19a/cardoso19a.pdf,ICML
252,2019,Fairness-Aware Learning for Continuous Attributes and Treatments,"Jeremie Mary,         Clément Calauzènes,         Noureddine El Karoui","We address the problem of algorithmic fairness: ensuring that the outcome of a classifier is not biased towards certain values of sensitive variables such as age, race or gender. As common fairness metrics can be expressed as measures of (conditional) independence between variables, we propose to use the Rényi maximum correlation coefficient to generalize fairness measurement to continuous variables. We exploit Witsenhausen’s characterization of the Rényi correlation coefficient to propose a differentiable implementation linked to fff-divergences. This allows us to generalize fairness-aware learning to continuous variables by using a penalty that upper bounds this coefficient. Theses allows fairness to be extented to variables such as mixed ethnic groups or financial status without thresholds effects. This penalty can be estimated on mini-batches allowing to use deep nets. Experiments show favorable comparisons to state of the art on binary variables and prove the ability to protect continuous ones",http://proceedings.mlr.press/v97/mary19a.html,http://proceedings.mlr.press/v97/mary19a/mary19a.pdf,ICML
253,2019,Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation,"Kaichao You,         Ximei Wang,         Mingsheng Long,         Michael Jordan","Deep unsupervised domain adaptation (Deep UDA) methods successfully leverage rich labeled data in a source domain to boost the performance on related but unlabeled data in a target domain. However, algorithm comparison is cumbersome in Deep UDA due to the absence of accurate and standardized model selection method, posing an obstacle to further advances in the field. Existing model selection methods for Deep UDA are either highly biased, restricted, unstable, or even controversial (requiring labeled target data). To this end, we propose Deep Embedded Validation (DEV), which embeds adapted feature representation into the validation procedure to obtain unbiased estimation of the target risk with bounded variance. The variance is further reduced by the technique of control variate. The efficacy of the method has been justified both theoretically and empirically.",http://proceedings.mlr.press/v97/you19a.html,http://proceedings.mlr.press/v97/you19a/you19a.pdf,ICML
254,2019,POPQORN: Quantifying Robustness of Recurrent Neural Networks,"Ching-Yun Ko,         Zhaoyang Lyu,         Lily Weng,         Luca Daniel,         Ngai Wong,         Dahua Lin","The vulnerability to adversarial attacks has been a critical issue for deep neural networks. Addressing this issue requires a reliable way to evaluate the robustness of a network. Recently, several methods have been developed to compute robustness quantification for neural networks, namely, certified lower bounds of the minimum adversarial perturbation. Such methods, however, were devised for feed-forward networks, e.g. multi-layer perceptron or convolutional networks. It remains an open problem to quantify robustness for recurrent networks, especially LSTM and GRU. For such networks, there exist additional challenges in computing the robustness quantification, such as handling the inputs at multiple steps and the interaction between gates and states. In this work, we propose POPQORN (Propagated-output Quantified Robustness for RNNs), a general algorithm to quantify robustness of RNNs, including vanilla RNNs, LSTMs, and GRUs. We demonstrate its effectiveness on different network architectures and show that the robustness quantification on individual steps can lead to new insights.",http://proceedings.mlr.press/v97/ko19a.html,http://proceedings.mlr.press/v97/ko19a/ko19a.pdf,ICML
255,2019,MASS: Masked Sequence to Sequence Pre-training for Language Generation,"Kaitao Song,         Xu Tan,         Tao Qin,         Jianfeng Lu,         Tie-Yan Liu","Pre-training and fine-tuning, e.g., BERT \citep{devlin2018bert}, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Especially, we achieve the state-of-the-art accuracy (30.02 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model \citep{bahdanau2015neural}.",http://proceedings.mlr.press/v97/song19d.html,http://proceedings.mlr.press/v97/song19d/song19d.pdf,ICML
256,2019,Learning to Generalize from Sparse and Underspecified Rewards,"Rishabh Agarwal,         Chen Liang,         Dale Schuurmans,         Mohammad Norouzi","We consider the problem of learning from sparse and underspecified rewards, where an agent receives a complex input, such as a natural language instruction, and needs to generate a complex response, such as an action sequence, while only receiving binary success-failure feedback. Such success-failure rewards are often underspecified: they do not distinguish between purposeful and accidental success. Generalization from underspecified rewards hinges on discounting spurious trajectories that attain accidental success, while learning from sparse feedback requires effective exploration. We address exploration by using a mode covering direction of KL divergence to collect a diverse set of successful trajectories, followed by a mode seeking KL divergence to train a robust policy. We propose Meta Reward Learning (MeRL) to construct an auxiliary reward function that provides more refined feedback for learning. The parameters of the auxiliary reward function are optimized with respect to the validation performance of a trained policy. The MeRL approach outperforms an alternative method for reward learning based on Bayesian Optimization, and achieves the state-of-the-art on weakly-supervised semantic parsing. It improves previous work by 1.2% and 2.4% on WikiTableQuestions and WikiSQL datasets respectively.",http://proceedings.mlr.press/v97/agarwal19e.html,http://proceedings.mlr.press/v97/agarwal19e/agarwal19e.pdf,ICML
257,2019,Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning,"Jakob Foerster,         Francis Song,         Edward Hughes,         Neil Burch,         Iain Dunning,         Shimon Whiteson,         Matthew Botvinick,         Michael Bowling","When observing the actions of others, humans make inferences about why they acted as they did, and what this implies about the world; humans also use the fact that their actions will be interpreted in this manner, allowing them to act informatively and thereby communicate efficiently with others. Although learning algorithms have recently achieved superhuman performance in a number of two-player, zero-sum games, scalable multi-agent reinforcement learning algorithms that can discover effective strategies and conventions in complex, partially observable settings have proven elusive. We present the Bayesian action decoder (BAD), a new multi-agent learning method that uses an approximate Bayesian update to obtain a public belief that conditions on the actions taken by all agents in the environment. BAD introduces a new Markov decision process, the public belief MDP, in which the action space consists of all deterministic partial policies, and exploits the fact that an agent acting only on this public belief state can still learn to use its private information if the action space is augmented to be over all partial policies mapping private information into environment actions. The Bayesian update is closely related to the theory of mind reasoning that humans carry out when observing others’ actions. We first validate BAD on a proof-of-principle two-step matrix game, where it outperforms policy gradient methods; we then evaluate BAD on the challenging, cooperative partial-information card game Hanabi, where, in the two-player setting, it surpasses all previously published learning and hand-coded approaches, establishing a new state of the art.",http://proceedings.mlr.press/v97/foerster19a.html,http://proceedings.mlr.press/v97/foerster19a/foerster19a.pdf,ICML
258,2019,Learning and Data Selection in Big Datasets,"Hossein Shokri Ghadikolaei,         Hadi Ghauch,         Carlo Fischione,         Mikael Skoglund","Finding a dataset of minimal cardinality to characterize the optimal parameters of a model is of paramount importance in machine learning and distributed optimization over a network. This paper investigates the compressibility of large datasets. More specifically, we propose a framework that jointly learns the input-output mapping as well as the most representative samples of the dataset (sufficient dataset). Our analytical results show that the cardinality of the sufficient dataset increases sub-linearly with respect to the original dataset size. Numerical evaluations of real datasets reveal a large compressibility, up to 95%, without a noticeable drop in the learnability performance, measured by the generalization error.",http://proceedings.mlr.press/v97/ghadikolaei19a.html,http://proceedings.mlr.press/v97/ghadikolaei19a/ghadikolaei19a.pdf,ICML
259,2019,Deep Generative Learning via Variational Gradient Flow,"Yuan Gao,         Yuling Jiao,         Yang Wang,         Yao Wang,         Can Yang,         Shunkang Zhang","We propose a framework to learn deep generative models via \textbf{V}ariational \textbf{Gr}adient Fl\textbf{ow} (VGrow) on probability spaces. The evolving distribution that asymptotically converges to the target distribution is governed by a vector field, which is the negative gradient of the first variation of the ff-divergence between them. We prove that the evolving distribution coincides with the pushforward distribution through the infinitesimal time composition of residual maps that are perturbations of the identity map along the vector field. The vector field depends on the density ratio of the pushforward distribution and the target distribution, which can be consistently learned from a binary classification problem. Connections of our proposed VGrow method with other popular methods, such as VAE, GAN and flow-based methods, have been established in this framework, gaining new insights of deep generative learning. We also evaluated several commonly used divergences, including Kullback-Leibler, Jensen-Shannon, Jeffreys divergences as well as our newly discovered “logD” divergence which serves as the objective function of the logD-trick GAN. Experimental results on benchmark datasets demonstrate that VGrow can generate high-fidelity images in a stable and efficient manner, achieving competitive performance with state-of-the-art GANs.",http://proceedings.mlr.press/v97/gao19b.html,http://proceedings.mlr.press/v97/gao19b/gao19b.pdf,ICML
260,2019,Repairing without Retraining: Avoiding Disparate Impact with Counterfactual Distributions,"Hao Wang,         Berk Ustun,         Flavio Calmon","When the performance of a machine learning model varies over groups defined by sensitive attributes (e.g., gender or ethnicity), the performance disparity can be expressed in terms of the probability distributions of the input and output variables over each group. In this paper, we exploit this fact to reduce the disparate impact of a fixed classification model over a population of interest. Given a black-box classifier, we aim to eliminate the performance gap by perturbing the distribution of input variables for the disadvantaged group. We refer to the perturbed distribution as a counterfactual distribution, and characterize its properties for common fairness criteria. We introduce a descent algorithm to learn a counterfactual distribution from data. We then discuss how the estimated distribution can be used to build a data preprocessor that can reduce disparate impact without training a new model. We validate our approach through experiments on real-world datasets, showing that it can repair different forms of disparity without a significant drop in accuracy.",http://proceedings.mlr.press/v97/wang19l.html,http://proceedings.mlr.press/v97/wang19l/wang19l.pdf,ICML
261,2019,Contextual Multi-armed Bandit Algorithm for Semiparametric Reward Model,"Gi-Soo Kim,         Myunghee Cho Paik","Contextual multi-armed bandit (MAB) algorithms have been shown promising for maximizing cumulative rewards in sequential decision tasks such as news article recommendation systems, web page ad placement algorithms, and mobile health. However, most of the proposed contextual MAB algorithms assume linear relationships between the reward and the context of the action. This paper proposes a new contextual MAB algorithm for a relaxed, semiparametric reward model that supports nonstationarity. The proposed method is less restrictive, easier to implement and faster than two alternative algorithms that consider the same model, while achieving a tight regret upper bound. We prove that the high-probability upper bound of the regret incurred by the proposed algorithm has the same order as the Thompson sampling algorithm for linear reward models. The proposed and existing algorithms are evaluated via simulation and also applied to Yahoo! news article recommendation log data.",http://proceedings.mlr.press/v97/kim19d.html,http://proceedings.mlr.press/v97/kim19d/kim19d.pdf,ICML
262,2019,Efficient Nonconvex Regularized Tensor Completion with Structure-aware Proximal Iterations,"Quanming Yao,         James Tin-Yau Kwok,         Bo Han","Nonconvex regularizers have been successfully used in low-rank matrix learning. In this paper, we extend this to the more challenging problem of low-rank tensor completion. Based on the proximal average algorithm, we develop an efficient solver that avoids expensive tensor folding and unfolding. A special “sparse plus low-rank"" structure, which is essential for fast computation of individual proximal steps, is maintained throughout the iterations. We also incorporate adaptive momentum to further speed up empirical convergence. Convergence results to critical points are provided under smoothness and Kurdyka-Lojasiewicz conditions. Experimental results on a number of synthetic and real-world data sets show that the proposed algorithm is more efficient in both time and space, and is also more accurate than existing approaches.",http://proceedings.mlr.press/v97/yao19a.html,http://proceedings.mlr.press/v97/yao19a/yao19a.pdf,ICML
263,2019,A Framework for Bayesian Optimization in Embedded Subspaces,"Amin Nayebi,         Alexander Munteanu,         Matthias Poloczek","We present a theoretically founded approach for high-dimensional Bayesian optimization based on low-dimensional subspace embeddings. We prove that the error in the Gaussian process model is bounded tightly when going from the original high-dimensional search domain to the low-dimensional embedding. This implies that the optimization process in the low-dimensional embedding proceeds essentially as if it were run directly on an unknown active subspace of low dimensionality. The argument applies to a large class of algorithms and GP models, including non-stationary kernels. Moreover, we provide an efficient implementation based on hashing and demonstrate empirically that this subspace embedding achieves considerably better results than the previously proposed methods for high-dimensional BO based on Gaussian matrix projections and structure-learning.",http://proceedings.mlr.press/v97/nayebi19a.html,http://proceedings.mlr.press/v97/nayebi19a/nayebi19a.pdf,ICML
264,2019,Neural Network Attributions: A Causal Perspective,"Aditya Chattopadhyay,         Piyushi Manupriya,         Anirban Sarkar,         Vineeth N Balasubramanian","We propose a new attribution method for neural networks developed using ﬁrst principles of causality (to the best of our knowledge, the ﬁrst such). The neural network architecture is viewed as a Structural Causal Model, and a methodology to compute the causal effect of each feature on the output is presented. With reasonable assumptions on the causal structure of the input data, we propose algorithms to efﬁciently compute the causal effects, as well as scale the approach to data with large dimensionality. We also show how this method can be used for recurrent neural networks. We report experimental results on both simulated and real datasets showcasing the promise and usefulness of the proposed algorithm.",http://proceedings.mlr.press/v97/chattopadhyay19a.html,http://proceedings.mlr.press/v97/chattopadhyay19a/chattopadhyay19a.pdf,ICML
265,2019,Submodular Observation Selection and Information Gathering for Quadratic Models,"Abolfazl Hashemi,         Mahsa Ghasemi,         Haris Vikalo,         Ufuk Topcu","We study the problem of selecting most informative subset of a large observation set to enable accurate estimation of unknown parameters. This problem arises in a variety of settings in machine learning and signal processing including feature selection, phase retrieval, and target localization. Since for quadratic measurement models the moment matrix of the optimal estimator is generally unknown, majority of prior work resorts to approximation techniques such as linearization of the observation model to optimize the alphabetical optimality criteria of an approximate moment matrix. Conversely, by exploiting a connection to the classical Van Trees’ inequality, we derive new alphabetical optimality criteria without distorting the relational structure of the observation model. We further show that under certain conditions on parameters of the problem these optimality criteria are monotone and (weak) submodular set functions. These results enable us to develop an efficient greedy observation selection algorithm uniquely tailored for quadratic models, and provide theoretical bounds on its achievable utility.",http://proceedings.mlr.press/v97/hashemi19a.html,http://proceedings.mlr.press/v97/hashemi19a/hashemi19a.pdf,ICML
266,2019,Efficient learning of smooth probability functions from Bernoulli tests with guarantees,"Paul Rolland,         Ali Kavis,         Alexander Immer,         Adish Singla,         Volkan Cevher","We study the fundamental problem of learning an unknown, smooth probability function via point-wise Bernoulli tests. We provide a scalable algorithm for efficiently solving this problem with rigorous guarantees. In particular, we prove the convergence rate of our posterior update rule to the true probability function in L2-norm. Moreover, we allow the Bernoulli tests to depend on contextual features, and provide a modified inference engine with provable guarantees for this novel setting. Numerical results show that the empirical convergence rates match the theory, and illustrate the superiority of our approach in handling contextual features over the state-of-the-art.",http://proceedings.mlr.press/v97/rolland19a.html,http://proceedings.mlr.press/v97/rolland19a/rolland19a.pdf,ICML
267,2019,DeepMDP: Learning Continuous Latent Space Models for Representation Learning,"Carles Gelada,         Saurabh Kumar,         Jacob Buckman,         Ofir Nachum,         Marc G. Bellemare","Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a \texit{DeepMDP}, a parameterized latent space model that is trained via the minimization of two tractable latent space losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees (1) the quality of the embedding function as a representation of the state space and (2) the quality of the DeepMDP as a model of the environment. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.",http://proceedings.mlr.press/v97/gelada19a.html,http://proceedings.mlr.press/v97/gelada19a/gelada19a.pdf,ICML
268,2019,Discriminative Regularization for Latent Variable Models with Applications to Electrocardiography,"Andrew Miller,         Ziad Obermeyer,         John Cunningham,         Sendhil Mullainathan","Generative models often use latent variables to represent structured variation in high-dimensional data, such as images and medical waveforms. However, these latent variables may ignore subtle, yet meaningful features in the data. Some features may predict an outcome of interest (e.g. heart attack) but account for only a small fraction of variation in the data. We propose a generative model training objective that uses a black-box discriminative model as a regularizer to learn representations that preserve this predictive variation. With these discriminatively regularized latent variable models, we visualize and measure variation in the data that influence a black-box predictive model, enabling an expert to better understand each prediction. With this technique, we study models that use electrocardiograms to predict outcomes of clinical interest. We measure our approach on synthetic and real data with statistical summaries and an experiment carried out by a physician.",http://proceedings.mlr.press/v97/miller19a.html,http://proceedings.mlr.press/v97/miller19a/miller19a.pdf,ICML
269,2019,Poission Subsampled Rényi Differential Privacy,"Yuqing Zhu,         Yu-Xiang Wang","We consider the problem of privacy-amplification by under the Renyi Differential Privacy framework. This is the main technique underlying the moments accountants (Abadi et al., 2016) for differentially private deep learning. Unlike previous attempts on this problem which deals with Sampling with Replacement, we consider the Poisson subsampling scheme which selects each data point independently with a coin toss. This allows us to significantly simplify and tighten the bounds for the RDP of subsampled mechanisms and derive numerically stable approximation schemes. In particular, for subsampled Gaussian mechanism and subsampled Laplace mechanism, we prove an analytical formula of their RDP that exactly matches the lower bound. The result is the first of its kind and we numerically demonstrate an order of magnitude improvement in the privacy-utility tradeoff.",http://proceedings.mlr.press/v97/zhu19c.html,http://proceedings.mlr.press/v97/zhu19c/zhu19c.pdf,ICML
270,2019,The Natural Language of Actions,"Guy Tennenholtz,         Shie Mannor","We introduce Act2Vec, a general framework for learning context-based action representation for Reinforcement Learning. Representing actions in a vector space help reinforcement learning algorithms achieve better performance by grouping similar actions and utilizing relations between different actions. We show how prior knowledge of an environment can be extracted from demonstrations and injected into action vector representations that encode natural compatible behavior. We then use these for augmenting state representations as well as improving function approximation of Q-values. We visualize and test action embeddings in three domains including a drawing task, a high dimensional navigation task, and the large action space domain of StarCraft II.",http://proceedings.mlr.press/v97/tennenholtz19a.html,http://proceedings.mlr.press/v97/tennenholtz19a/tennenholtz19a.pdf,ICML
271,2019,Nonconvex Variance Reduced Optimization with Arbitrary Sampling,"Samuel Horváth,         Peter Richtarik","We provide the first importance sampling variants of variance reduced algorithms for empirical risk minimization with non-convex loss functions. In particular, we analyze non-convex versions of \texttt{SVRG}, \texttt{SAGA} and \texttt{SARAH}. Our methods have the capacity to speed up the training process by an order of magnitude compared to the state of the art on real datasets. Moreover, we also improve upon current mini-batch analysis of these methods by proposing importance sampling for minibatches in this setting. Surprisingly, our approach can in some regimes lead to superlinear speedup with respect to the minibatch size, which is not usually present in stochastic optimization. All the above results follow from a general analysis of the methods which works with arbitrary sampling, i.e., fully general randomized strategy for the selection of subsets of examples to be sampled in each iteration. Finally, we also perform a novel importance sampling analysis of \texttt{SARAH} in the convex setting.",http://proceedings.mlr.press/v97/horvath19a.html,http://proceedings.mlr.press/v97/horvath19a/horvath19a.pdf,ICML
272,2019,Formal Privacy for Functional Data with Gaussian Perturbations,"Ardalan Mirshani,         Matthew Reimherr,         Aleksandra Slavković","Motivated by the rapid rise in statistical tools in Functional Data Analysis, we consider the Gaussian mechanism for achieving differential privacy (DP) with parameter estimates taking values in a, potentially infinite-dimensional, separable Banach space. Using classic results from probability theory, we show how densities over function spaces can be utilized to achieve the desired DP bounds. This extends prior results of Hall et al (2013) to a much broader class of statistical estimates and summaries, including “path level"" summaries, nonlinear functionals, and full function releases. By focusing on Banach spaces, we provide a deeper picture of the challenges for privacy with complex data, especially the role regularization plays in balancing utility and privacy. Using an application to penalized smoothing, we highlight this balance in the context of mean function estimation. Simulations and an application to {diffusion tensor imaging} are briefly presented, with extensive additions included in a supplement.",http://proceedings.mlr.press/v97/mirshani19a.html,http://proceedings.mlr.press/v97/mirshani19a/mirshani19a.pdf,ICML
273,2019,Feature-Critic Networks for Heterogeneous Domain Generalization,"Yiying Li,         Yongxin Yang,         Wei Zhou,         Timothy Hospedales","The well known domain shift issue causes model performance to degrade when deployed to a new target domain with different statistics to training. Domain adaptation techniques alleviate this, but need some instances from the target domain to drive adaptation. Domain generalisation is the recently topical problem of learning a model that generalises to unseen domains out of the box, and various approaches aim to train a domain-invariant feature extractor, typically by adding some manually designed losses. In this work, we propose a learning to learn approach, where the auxiliary loss that helps generalisation is itself learned. Beyond conventional domain generalisation, we consider a more challenging setting of heterogeneous domain generalisation, where the unseen domains do not share label space with the seen ones, and the goal is to train a feature representation that is useful off-the-shelf for novel data and novel categories. Experimental evaluation demonstrates that our method outperforms state-of-the-art solutions in both settings.",http://proceedings.mlr.press/v97/li19l.html,http://proceedings.mlr.press/v97/li19l/li19l.pdf,ICML
274,2019,The Odds are Odd: A Statistical Test for Detecting Adversarial Examples,"Kevin Roth,         Yannic Kilcher,         Thomas Hofmann","We investigate conditions under which test statistics exist that can reliably detect examples, which have been adversarially manipulated in a white-box attack. These statistics can be easily computed and calibrated by randomly corrupting inputs. They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints. Access to the log-odds is the only requirement to defend models. We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our experiments, we show that it is even possible to correct test time predictions for adversarial attacks with high accuracy.",http://proceedings.mlr.press/v97/roth19a.html,http://proceedings.mlr.press/v97/roth19a/roth19a.pdf,ICML
275,2019,Acceleration of SVRG and Katyusha X by Inexact Preconditioning,"Yanli Liu,         Fei Feng,         Wotao Yin","Empirical risk minimization is an important class of optimization problems with many popular machine learning applications, and stochastic variance reduction methods are popular choices for solving them. Among these methods, SVRG and Katyusha X (a Nesterov accelerated SVRG) achieve fast convergence without substantial memory requirement. In this paper, we propose to accelerate these two algorithms by inexact preconditioning, the proposed methods employ fixed preconditioners, although the subproblem in each epoch becomes harder, it suffices to apply fixed number of simple subroutines to solve it inexactly, without losing the overall convergence. As a result, this inexact preconditioning strategy gives provably better iteration complexity and gradient complexity over SVRG and Katyusha X. We also allow each function in the finite sum to be nonconvex while the sum is strongly convex. In our numerical experiments, we observe an on average 8×8×8\times speedup on the number of iterations and 7×7×7\times speedup on runtime.",http://proceedings.mlr.press/v97/liu19a.html,http://proceedings.mlr.press/v97/liu19a/liu19a.pdf,ICML
276,2019,Understanding and Controlling Memory in Recurrent Neural Networks,"Doron Haviv,         Alexander Rivkind,         Omri Barak","To be effective in sequential data processing, Recurrent Neural Networks (RNNs) are required to keep track of past events by creating memories. While the relation between memories and the network’s hidden state dynamics was established over the last decade, previous works in this direction were of a predominantly descriptive nature focusing mainly on locating the dynamical objects of interest. In particular, it remained unclear how dynamical observables affect the performance, how they form and whether they can be manipulated. Here, we utilize different training protocols, datasets and architectures to obtain a range of networks solving a delayed classification task with similar performance, alongside substantial differences in their ability to extrapolate for longer delays. We analyze the dynamics of the network’s hidden state, and uncover the reasons for this difference. Each memory is found to be associated with a nearly steady state of the dynamics which we refer to as a ’slow point’. Slow point speeds predict extrapolation performance across all datasets, protocols and architectures tested. Furthermore, by tracking the formation of the slow points we are able to understand the origin of differences between training protocols. Finally, we propose a novel regularization technique that is based on the relation between hidden state speeds and memory longevity. Our technique manipulates these speeds, thereby leading to a dramatic improvement in memory robustness over time, and could pave the way for a new class of regularization methods.",http://proceedings.mlr.press/v97/haviv19a.html,http://proceedings.mlr.press/v97/haviv19a/haviv19a.pdf,ICML
277,2019,Graph Matching Networks for Learning the Similarity of Graph Structured Objects,"Yujia Li,         Chenjie Gu,         Thomas Dullien,         Oriol Vinyals,         Pushmeet Kohli","This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions. First, we demonstrate how Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning. Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism. We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems. The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain specific baseline systems that have been carefully hand-engineered for these problems.",http://proceedings.mlr.press/v97/li19d.html,http://proceedings.mlr.press/v97/li19d/li19d.pdf,ICML
278,2019,A Block Coordinate Descent Proximal Method for Simultaneous Filtering and Parameter Estimation,"Ramin Raziperchikolaei,         Harish Bhat","We propose and analyze a block coordinate descent proximal algorithm (BCD-prox) for simultaneous filtering and parameter estimation of ODE models. As we show on ODE systems with up to d=40 dimensions, as compared to state-of-the-art methods, BCD-prox exhibits increased robustness (to noise, parameter initialization, and hyperparameters), decreased training times, and improved accuracy of both filtered states and estimated parameters. We show how BCD-prox can be used with multistep numerical discretizations, and we establish convergence of BCD-prox under hypotheses that include real systems of interest.",http://proceedings.mlr.press/v97/raziperchikolaei19a.html,http://proceedings.mlr.press/v97/raziperchikolaei19a/raziperchikolaei19a.pdf,ICML
279,2019,Adaptive Antithetic Sampling for Variance Reduction,"Hongyu Ren,         Shengjia Zhao,         Stefano Ermon","Variance reduction is crucial in stochastic estimation and optimization problems. Antithetic sampling reduces the variance of a Monte Carlo estimator by drawing correlated, rather than independent, samples. However, designing an effective correlation structure is challenging and application specific, thus limiting the practical applicability of these methods. In this paper, we propose a general-purpose adaptive antithetic sampling framework. We provide gradient-based and gradient-free methods to train the samplers such that they reduce variance while ensuring that the underlying Monte Carlo estimator is provably unbiased. We demonstrate the effectiveness of our approach on Bayesian inference and generative model training, where it reduces variance and improves task performance with little computational overhead.",http://proceedings.mlr.press/v97/ren19b.html,http://proceedings.mlr.press/v97/ren19b/ren19b.pdf,ICML
280,2019,The Value Function Polytope in Reinforcement Learning,"Robert Dadashi,         Adrien Ali Taiga,         Nicolas Le Roux,         Dale Schuurmans,         Marc G. Bellemare","We establish geometric and topological properties of the space of value functions in finite state-action Markov decision processes. Our main contribution is the characterization of the nature of its shape: a general polytope (Aigner et al., 2010). To demonstrate this result, we exhibit several properties of the structural relationship between policies and value functions including the line theorem, which shows that the value functions of policies constrained on all but one state describe a line segment. Finally, we use this novel perspective and introduce visualizations to enhance the understanding of the dynamics of reinforcement learning algorithms.",http://proceedings.mlr.press/v97/dadashi19a.html,http://proceedings.mlr.press/v97/dadashi19a/dadashi19a.pdf,ICML
281,2019,End-to-End Probabilistic Inference for Nonstationary Audio Analysis,"William Wilkinson,         Michael Andersen,         Joshua D. Reiss,         Dan Stowell,         Arno Solin","A typical audio signal processing pipeline includes multiple disjoint analysis stages, including calculation of a time-frequency representation followed by spectrogram-based feature analysis. We show how time-frequency analysis and nonnegative matrix factorisation can be jointly formulated as a spectral mixture Gaussian process model with nonstationary priors over the amplitude variance parameters. Further, we formulate this nonlinear model’s state space representation, making it amenable to infinite-horizon Gaussian process regression with approximate inference via expectation propagation, which scales linearly in the number of time steps and quadratically in the state dimensionality. By doing so, we are able to process audio signals with hundreds of thousands of data points. We demonstrate, on various tasks with empirical data, how this inference scheme outperforms more standard techniques that rely on extended Kalman filtering.",http://proceedings.mlr.press/v97/wilkinson19a.html,http://proceedings.mlr.press/v97/wilkinson19a/wilkinson19a.pdf,ICML
282,2019,Towards a Unified Analysis of Random Fourier Features,"Zhu Li,         Jean-Francois Ton,         Dino Oglic,         Dino Sejdinovic","Random Fourier features is a widely used, simple, and effective technique for scaling up kernel methods. The existing theoretical analysis of the approach, however, remains focused on specific learning tasks and typically gives pessimistic bounds which are at odds with the empirical results. We tackle these problems and provide the first unified risk analysis of learning with random Fourier features using the squared error and Lipschitz continuous loss functions. In our bounds, the trade-off between the computational cost and the expected risk convergence rate is problem specific and expressed in terms of the regularization parameter and the number of effective degrees of freedom. We study both the standard random Fourier features method for which we improve the existing bounds on the number of features required to guarantee the corresponding minimax risk convergence rate of kernel ridge regression, as well as a data-dependent modification which samples features proportional to ridge leverage scores and further reduces the required number of features. As ridge leverage scores are expensive to compute, we devise a simple approximation scheme which provably reduces the computational cost without loss of statistical efficiency.",http://proceedings.mlr.press/v97/li19k.html,http://proceedings.mlr.press/v97/li19k/li19k.pdf,ICML
283,2019,Adversarial Examples Are a Natural Consequence of Test Error in Noise,"Justin Gilmer,         Nicolas Ford,         Nicholas Carlini,         Ekin Cubuk","Over the last few years, the phenomenon of adversarial examples — maliciously constructed inputs that fool trained machine learning models — has captured the attention of the research community, especially when restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon, and therefore the adversarial robustness and corruption robustness research programs are closely related. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.",http://proceedings.mlr.press/v97/gilmer19a.html,http://proceedings.mlr.press/v97/gilmer19a/gilmer19a.pdf,ICML
284,2019,Noisy Dual Principal Component Pursuit,"Tianyu Ding,         Zhihui Zhu,         Tianjiao Ding,         Yunchen Yang,         Rene Vidal,         Manolis Tsakiris,         Daniel Robinson","Dual Principal Component Pursuit (DPCP) is a recently proposed non-convex optimization based method for learning subspaces of high relative dimension from noiseless datasets contaminated by as many outliers as the square of the number of inliers. Experimentally, DPCP has proved to be robust to noise and outperform the popular RANSAC on 3D vision tasks such as road plane detection and relative poses estimation from three views. This paper extends the global optimality and convergence theory of DPCP to the case of data corrupted by noise, and further demonstrates its robustness using synthetic and real data.",http://proceedings.mlr.press/v97/ding19b.html,http://proceedings.mlr.press/v97/ding19b/ding19b.pdf,ICML
285,2019,Safe Policy Improvement with Baseline Bootstrapping,"Romain Laroche,         Paul Trichelair,         Remi Tachet Des Combes","This paper considers Safe Policy Improvement (SPI) in Batch Reinforcement Learning (Batch RL): from a fixed dataset and without direct access to the true environment, train a policy that is guaranteed to perform at least as well as the baseline policy used to collect the data. 	 Our approach, called SPI with Baseline Bootstrapping (SPIBB), is inspired by the knows-what-it-knows paradigm: it bootstraps the trained policy with the baseline when the uncertainty is high. 	 Our first algorithm, ΠbΠb\Pi_b-SPIBB, comes with SPI theoretical guarantees. 	 We also implement a variant, Π≤bΠ≤b\Pi_{\leq b}-SPIBB, that is even more efficient in practice. 	 We apply our algorithms to a motivational stochastic gridworld domain and further demonstrate on randomly generated MDPs the superiority of SPIBB with respect to existing algorithms, not only in safety but also in mean performance. 	 Finally, we implement a model-free version of SPIBB and show its benefits on a navigation task with deep RL implementation called SPIBB-DQN, which is, to the best of our knowledge, the first RL algorithm relying on a neural network representation able to train efficiently and reliably from batch data, without any interaction with the environment.",http://proceedings.mlr.press/v97/laroche19a.html,http://proceedings.mlr.press/v97/laroche19a/laroche19a.pdf,ICML
286,2019,Deep Residual Output Layers for Neural Language Generation,"Nikolaos Pappas,         James Henderson","Many tasks, including language generation, benefit from learning the structure of the output space, particularly when the space of output labels is large and the data is sparse. State-of-the-art neural language models indirectly capture the output space structure in their classifier weights since they lack parameter sharing across output labels. Learning shared output label mappings helps, but existing methods have limited expressivity and are prone to overfitting. In this paper, we investigate the usefulness of more powerful shared mappings for output labels, and propose a deep residual output mapping with dropout between layers to better capture the structure of the output space and avoid overfitting. Evaluations on three language generation tasks show that our output label mapping can match or improve state-of-the-art recurrent and self-attention architectures, and suggest that the classifier does not necessarily need to be high-rank to better model natural language if it is better at capturing the structure of the output space.",http://proceedings.mlr.press/v97/pappas19a.html,http://proceedings.mlr.press/v97/pappas19a/pappas19a.pdf,ICML
287,2019,Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,"Sanjeev Arora,         Simon Du,         Wei Hu,         Zhiyuan Li,         Ruosong Wang","Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR’17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.",http://proceedings.mlr.press/v97/arora19a.html,http://proceedings.mlr.press/v97/arora19a/arora19a.pdf,ICML
288,2019,Fast Direct Search in an Optimally Compressed Continuous Target Space for Efficient Multi-Label Active Learning,"Weishi Shi,         Qi Yu","Active learning for multi-label classification poses fundamental challenges given the complex label correlations and a potentially large and sparse label space. We propose a novel CS-BPCA process that integrates compressed sensing and Bayesian principal component analysis to perform a two-level label transformation, resulting in an optimally compressed continuous target space. Besides leveraging correlation and sparsity of a large label space for effective compression, an optimal compressing rate and the relative importance of the resultant targets are automatically determined through Bayesian inference. Furthermore, the orthogonality of the transformed space completely decouples the correlations among targets, which significantly simplifies multi-label sampling in the target space. We define a novel sampling function that leverages a multi-output Gaussian Process (MOGP). Gradient-free optimization strategies are developed to achieve fast online hyper-parameter learning and model retraining for active learning. Experimental results over multiple real-world datasets and comparison with competitive multi-label active learning models demonstrate the effectiveness of the proposed framework.",http://proceedings.mlr.press/v97/shi19b.html,http://proceedings.mlr.press/v97/shi19b/shi19b.pdf,ICML
289,2019,Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation,"Samuel Wiqvist,         Pierre-Alexandre Mattei,         Umberto Picchini,         Jes Frellsen","We present a novel family of deep neural architectures, named partially exchangeable networks (PENs) that leverage probabilistic symmetries. By design, PENs are invariant to block-switch transformations, which characterize the partial exchangeability properties of conditionally Markovian processes. Moreover, we show that any block-switch invariant function has a PEN-like representation. The DeepSets architecture is a special case of PEN and we can therefore also target fully exchangeable data. We employ PENs to learn summary statistics in approximate Bayesian computation (ABC). When comparing PENs to previous deep learning methods for learning summary statistics, our results are highly competitive, both considering time series and static models. Indeed, PENs provide more reliable posterior samples even when using less training data.",http://proceedings.mlr.press/v97/wiqvist19a.html,http://proceedings.mlr.press/v97/wiqvist19a/wiqvist19a.pdf,ICML
290,2019,Transferable Clean-Label Poisoning Attacks on Deep Neural Nets,"Chen Zhu,         W. Ronny Huang,         Hengduo Li,         Gavin Taylor,         Christoph Studer,         Tom Goldstein","In this paper, we explore clean-label poisoning attacks on deep convolutional networks with access to neither the network’s output nor its architecture or parameters. Our goal is to ensure that after injecting the poisons into the training data, a model with unknown architecture and parameters trained on that data will misclassify the target image into a specific class. To achieve this goal, we generate multiple poison images from the base class by adding small perturbations which cause the poison images to trap the target image within their convex polytope in feature space. We also demonstrate that using Dropout during crafting of the poisons and enforcing this objective in multiple layers enhances transferability, enabling attacks against both the transfer learning and end-to-end training settings. We demonstrate transferable attack success rates of over 50% by poisoning only 1% of the training set.",http://proceedings.mlr.press/v97/zhu19a.html,http://proceedings.mlr.press/v97/zhu19a/zhu19a.pdf,ICML
291,2019,Distributed Learning over Unreliable Networks,"Chen Yu,         Hanlin Tang,         Cedric Renggli,         Simon Kassing,         Ankit Singla,         Dan Alistarh,         Ce Zhang,         Ji Liu","Most of today’s distributed machine learning systems assume reliable networks: whenever two machines exchange information (e.g., gradients or models), the network should guarantee the delivery of the message. At the same time, recent work exhibits the impressive tolerance of machine learning algorithms to errors or noise arising from relaxed communication or synchronization. In this paper, we connect these two trends, and consider the following question: Can we design machine learning systems that are tolerant to network unreliability during training? With this motivation, we focus on a theoretical problem of independent interest—given a standard distributed parameter server architecture, if every communication between the worker and the server has a non-zero probability ppp of being dropped, does there exist an algorithm that still converges, and at what speed? In the context of prior art, this problem can be phrased as distributed learning over random topologies. The technical contribution of this paper is a novel theoretical analysis proving that distributed learning over random topologies can achieve comparable convergence rate to centralized or distributed learning over reliable networks. Further, we prove that the influence of the packet drop rate diminishes with the growth of the number of parameter servers. We map this theoretical result onto a real-world scenario, training deep neural networks over an unreliable network layer, and conduct network simulation to validate the system improvement by allowing the networks to be unreliable.",http://proceedings.mlr.press/v97/yu19f.html,http://proceedings.mlr.press/v97/yu19f/yu19f.pdf,ICML
292,2019,Projections for Approximate Policy Iteration Algorithms,"Riad Akrour,         Joni Pajarinen,         Jan Peters,         Gerhard Neumann","Approximate policy iteration is a class of reinforcement learning (RL) algorithms where the policy is encoded using a function approximator and which has been especially prominent in RL with continuous action spaces. In this class of RL algorithms, ensuring increase of the policy return during policy update often requires to constrain the change in action distribution. Several approximations exist in the literature to solve this constrained policy update problem. In this paper, we propose to improve over such solutions by introducing a set of projections that transform the constrained problem into an unconstrained one which is then solved by standard gradient descent. Using these projections, we empirically demonstrate that our approach can improve the policy update solution and the control over exploration of existing approximate policy iteration algorithms.",http://proceedings.mlr.press/v97/akrour19a.html,http://proceedings.mlr.press/v97/akrour19a/akrour19a.pdf,ICML
293,2019,"Fast Incremental von Neumann Graph Entropy Computation: Theory, Algorithm, and Applications","Pin-Yu Chen,         Lingfei Wu,         Sijia Liu,         Indika Rajapakse","The von Neumann graph entropy (VNGE) facilitates measurement of information divergence and distance between graphs in a graph sequence. It has been successfully applied to various learning tasks driven by network-based data. While effective, VNGE is computationally demanding as it requires the full eigenspectrum of the graph Laplacian matrix. In this paper, we propose a new computational framework, Fast Incremental von Neumann Graph EntRopy (FINGER), which approaches VNGE with a performance guarantee. FINGER reduces the cubic complexity of VNGE to linear complexity in the number of nodes and edges, and thus enables online computation based on incremental graph changes. We also show asymptotic equivalence of FINGER to the exact VNGE, and derive its approximation error bounds. Based on FINGER, we propose efficient algorithms for computing Jensen-Shannon distance between graphs. Our experimental results on different random graph models demonstrate the computational efficiency and the asymptotic equivalence of FINGER. In addition, we apply FINGER to two real-world applications and one synthesized anomaly detection dataset, and corroborate its superior performance over seven baseline graph similarity methods.",http://proceedings.mlr.press/v97/chen19j.html,http://proceedings.mlr.press/v97/chen19j/chen19j.pdf,ICML
294,2019,On the Design of Estimators for Bandit Off-Policy Evaluation,"Nikos Vlassis,         Aurelien Bibaut,         Maria Dimakopoulou,         Tony Jebara","Off-policy evaluation is the problem of estimating the value of a target policy using data collected under a different policy. Given a base estimator for bandit off-policy evaluation and a parametrized class of control variates, we address the problem of computing a control variate in that class that reduces the risk of the base estimator. We derive the population risk as a function of the class parameters and we establish conditions that guarantee risk improvement. We present our main results in the context of multi-armed bandits, and we propose a simple design for contextual bandits that gives rise to an estimator that is shown to perform well in multi-class cost-sensitive classification datasets.",http://proceedings.mlr.press/v97/vlassis19a.html,http://proceedings.mlr.press/v97/vlassis19a/vlassis19a.pdf,ICML
295,2019,Scalable Fair Clustering,"Arturs Backurs,         Piotr Indyk,         Krzysztof Onak,         Baruch Schieber,         Ali Vakilian,         Tal Wagner","We study the fair variant of the classic k-median problem introduced by (Chierichetti et al., NeurIPS 2017) in which the points are colored, and the goal is to minimize the same average distance objective as in the standard kkk-median problem while ensuring that all clusters have an “approximately equal” number of points of each color. (Chierichetti et al., NeurIPS 2017) proposed a two-phase algorithm for fair kkk-clustering. In the first step, the pointset is partitioned into subsets called fairlets that satisfy the fairness requirement and approximately preserve the k-median objective. In the second step, fairlets are merged into k clusters by one of the existing k-median algorithms. The running time of this algorithm is dominated by the first step, which takes super-quadratic time. In this paper, we present a practical approximate fairlet decomposition algorithm that runs in nearly linear time.",http://proceedings.mlr.press/v97/backurs19a.html,http://proceedings.mlr.press/v97/backurs19a/backurs19a.pdf,ICML
296,2019,Learning to Route in Similarity Graphs,"Dmitry Baranchuk,         Dmitry Persiyanov,         Anton Sinitsin,         Artem Babenko","Recently similarity graphs became the leading paradigm for efficient nearest neighbor search, outperforming traditional tree-based and LSH-based methods. Similarity graphs perform the search via greedy routing: a query traverses the graph and in each vertex moves to the adjacent vertex that is the closest to this query. In practice, similarity graphs are often susceptible to local minima, when queries do not reach its nearest neighbors, getting stuck in suboptimal vertices. In this paper we propose to learn the routing function that overcomes local minima via incorporating information about the graph global structure. In particular, we augment the vertices of a given graph with additional representations that are learned to provide the optimal routing from the start vertex to the query nearest neighbor. By thorough experiments, we demonstrate that the proposed learnable routing successfully diminishes the local minima problem and significantly improves the overall search performance.",http://proceedings.mlr.press/v97/baranchuk19a.html,http://proceedings.mlr.press/v97/baranchuk19a/baranchuk19a.pdf,ICML
297,2019,Subspace Robust Wasserstein Distances,"François-Pierre Paty,         Marco Cuturi","Making sense of Wasserstein distances between discrete measures in high-dimensional settings remains a challenge. Recent work has advocated a two-step approach to improve robustness and facilitate the computation of optimal transport, using for instance projections on random real lines, or a preliminary quantization of the measures to reduce the size of their support. We propose in this work a “max-min” robust variant of the Wasserstein distance by considering the maximal possible distance that can be realized between two measures, assuming they can be projected orthogonally on a lower k-dimensional subspace. Alternatively, we show that the corresponding “min-max” OT problem has a tight convex relaxation which can be cast as that of finding an optimal transport plan with a low transportation cost, where the cost is alternatively defined as the sum of the k largest eigenvalues of the second order moment matrix of the displacements (or matchings) corresponding to that plan (the usual OT definition only considers the trace of that matrix). We show that both quantities inherit several favorable properties from the OT geometry. We propose two algorithms to compute the latter formulation using entropic regularization, and illustrate the interest of this approach empirically.",http://proceedings.mlr.press/v97/paty19a.html,http://proceedings.mlr.press/v97/paty19a/paty19a.pdf,ICML
298,2019,Learning Action Representations for Reinforcement Learning,"Yash Chandak,         Georgios Theocharous,         James Kostas,         Scott Jordan,         Philip Thomas","Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems.",http://proceedings.mlr.press/v97/chandak19a.html,http://proceedings.mlr.press/v97/chandak19a/chandak19a.pdf,ICML
299,2019,Traditional and Heavy Tailed Self Regularization in Neural Network Models,"Michael Mahoney,         Charles Martin","Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet. Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints. Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization. For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a “size scale” separating signal from noise. For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems. This implicit Self-Regularization can depend strongly on the many knobs of the training process. By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.",http://proceedings.mlr.press/v97/mahoney19a.html,http://proceedings.mlr.press/v97/mahoney19a/mahoney19a.pdf,ICML
300,2019,Online Meta-Learning,"Chelsea Finn,         Aravind Rajeswaran,         Sham Kakade,         Sergey Levine","A central capability of intelligent systems is the ability to continuously build upon previous experiences to speed up and enhance learning of new tasks. Two distinct research paradigms have studied this question. Meta-learning views this problem as learning a prior over model parameters that is amenable for fast adaptation on a new task, but typically assumes the tasks are available together as a batch. In contrast, online (regret based) learning considers a setting where tasks are revealed one after the other, but conventionally trains a single model without task-specific adaptation. This work introduces an online meta-learning setting, which merges ideas from both paradigms to better capture the spirit and practice of continual lifelong learning. We propose the follow the meta leader (FTML) algorithm which extends the MAML algorithm to this setting. Theoretically, this work provides an O(log T) regret guarantee with one additional higher order smoothness assumption (in comparison to the standard online setting). Our experimental evaluation on three different large-scale problems suggest that the proposed algorithm significantly outperforms alternatives based on traditional online learning approaches.",http://proceedings.mlr.press/v97/finn19a.html,http://proceedings.mlr.press/v97/finn19a/finn19a.pdf,ICML
301,2019,The Wasserstein Transform,"Facundo Memoli,         Zane Smith,         Zhengchao Wan","We introduce the Wasserstein transform, a method for enhancing and denoising datasets defined on general metric spaces. The construction draws inspiration from Optimal Transportation ideas. We establish the stability of our method under data perturbation and, when the dataset is assumed to be Euclidean, we also exhibit a precise connection between the Wasserstein transform and the mean shift family of algorithms. We then use this connection to prove that mean shift also inherits stability under perturbations. We study the performance of the Wasserstein transform method on different datasets as a preprocessing step prior to clustering and classification tasks.",http://proceedings.mlr.press/v97/memoli19a.html,http://proceedings.mlr.press/v97/memoli19a/memoli19a.pdf,ICML
302,2019,Scalable Nonparametric Sampling from Multimodal Posteriors with the Posterior Bootstrap,"Edwin Fong,         Simon Lyddon,         Chris Holmes","Increasingly complex datasets pose a number of challenges for Bayesian inference. Conventional posterior sampling based on Markov chain Monte Carlo can be too computationally intensive, is serial in nature and mixes poorly between posterior modes. Furthermore, all models are misspecified, which brings into question the validity of the conventional Bayesian update. We present a scalable Bayesian nonparametric learning routine that enables posterior sampling through the optimization of suitably randomized objective functions. A Dirichlet process prior on the unknown data distribution accounts for model misspecification, and admits an embarrassingly parallel posterior bootstrap algorithm that generates independent and exact samples from the nonparametric posterior distribution. Our method is particularly adept at sampling from multimodal posterior distributions via a random restart mechanism, and we demonstrate this on Gaussian mixture model and sparse logistic regression examples.",http://proceedings.mlr.press/v97/fong19a.html,http://proceedings.mlr.press/v97/fong19a/fong19a.pdf,ICML
303,2019,Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization,"Hesham Mostafa,         Xin Wang","Modern deep neural networks are typically highly overparameterized. Pruning techniques are able to remove a significant fraction of network parameters with little loss in accuracy. Recently, techniques based on dynamic reallocation of non-zero parameters have emerged, allowing direct training of sparse networks without having to pre-train a large dense model. Here we present a novel dynamic sparse reparameterization method that addresses the limitations of previous techniques such as high computational cost and the need for manual configuration of the number of free parameters allocated to each layer. We evaluate the performance of dynamic reallocation methods in training deep convolutional networks and show that our method outperforms previous static and dynamic reparameterization methods, yielding the best accuracy for a fixed parameter budget, on par with accuracies obtained by iteratively pruning a pre-trained dense model. We further investigated the mechanisms underlying the superior generalization performance of the resultant sparse networks. We found that neither the structure, nor the initialization of the non-zero parameters were sufficient to explain the superior performance. Rather, effective learning crucially depended on the continuous exploration of the sparse network structure space during training. Our work suggests that exploring structural degrees of freedom during training is more effective than adding extra parameters to the network.",http://proceedings.mlr.press/v97/mostafa19a.html,http://proceedings.mlr.press/v97/mostafa19a/mostafa19a.pdf,ICML
304,2019,Particle Flow Bayes’ Rule,"Xinshi Chen,         Hanjun Dai,         Le Song","We present a particle flow realization of Bayes’ rule, where an ODE-based neural operator is used to transport particles from a prior to its posterior after a new observation. We prove that such an ODE operator exists. Its neural parameterization can be trained in a meta-learning framework, allowing this operator to reason about the effect of an individual observation on the posterior, and thus generalize across different priors, observations and to sequential Bayesian inference. We demonstrated the generalization ability of our particle flow Bayes operator in several canonical and high dimensional examples.",http://proceedings.mlr.press/v97/chen19c.html,http://proceedings.mlr.press/v97/chen19c/chen19c.pdf,ICML
305,2019,Self-Attention Generative Adversarial Networks,"Han Zhang,         Ian Goodfellow,         Dimitris Metaxas,         Augustus Odena","In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN performs better than prior work, boosting the best published Inception score from 36.8 to 52.52 and reducing Fréchet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.",http://proceedings.mlr.press/v97/zhang19d.html,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf,ICML
306,2019,Unifying Orthogonal Monte Carlo Methods,"Krzysztof Choromanski,         Mark Rowland,         Wenyu Chen,         Adrian Weller","Many machine learning methods making use of Monte Carlo sampling in vector spaces have been shown to be improved by conditioning samples to be mutually orthogonal. Exact orthogonal coupling of samples is computationally intensive, hence approximate methods have been of great interest. In this paper, we present a unifying perspective of many approximate methods by considering Givens transformations, propose new approximate methods based on this framework, and demonstrate the ﬁrst statistical guarantees for families of approximate methods in kernel approximation. We provide extensive empirical evaluations with guidance for practitioners.",http://proceedings.mlr.press/v97/choromanski19a.html,http://proceedings.mlr.press/v97/choromanski19a/choromanski19a.pdf,ICML
307,2019,Decentralized Exploration in Multi-Armed Bandits,"Raphael Feraud,         Reda Alami,         Romain Laroche","We consider the decentralized exploration problem: a set of players collaborate to identify the best arm by asynchronously interacting with the same stochastic environment. The objective is to insure privacy in the best arm identification problem between asynchronous, collaborative, and thrifty players. In the context of a digital service, we advocate that this decentralized approach allows a good balance between conflicting interests: the providers optimize their services, while protecting privacy of users and saving resources. We define the privacy level as the amount of information an adversary could infer by intercepting all the messages concerning a single user. We provide a generic algorithm DECENTRALIZED ELIMINATION, which uses any best arm identification algorithm as a subroutine. We prove that this algorithm insures privacy, with a low communication cost, and that in comparison to the lower bound of the best arm identification problem, its sample complexity suffers from a penalty depending on the inverse of the probability of the most frequent players. Then, thanks to the genericity of the approach, we extend the proposed algorithm to the non-stationary bandits. Finally, experiments illustrate and complete the analysis.",http://proceedings.mlr.press/v97/feraud19a.html,http://proceedings.mlr.press/v97/feraud19a/feraud19a.pdf,ICML
308,2019,Proportionally Fair Clustering,"Xingyu Chen,         Brandon Fain,         Liang Lyu,         Kamesh Munagala","We extend the fair machine learning literature by considering the problem of proportional centroid clustering in a metric context. For clustering n points with k centers, we define fairness as proportionality to mean that any n/k points are entitled to form their own cluster if there is another center that is closer in distance for all n/k points. We seek clustering solutions to which there are no such justified complaints from any subsets of agents, without assuming any a priori notion of protected subsets. We present and analyze algorithms to efficiently compute, optimize, and audit proportional solutions. We conclude with an empirical examination of the tradeoff between proportional solutions and the k-means objective.",http://proceedings.mlr.press/v97/chen19d.html,http://proceedings.mlr.press/v97/chen19d/chen19d.pdf,ICML
309,2019,Per-Decision Option Discounting,"Anna Harutyunyan,         Peter Vrancx,         Philippe Hamel,         Ann Nowe,         Doina Precup","In order to solve complex problems an agent must be able to reason over a sufficiently long horizon. Temporal abstraction, commonly modeled through options, offers the ability to reason at many timescales, but the horizon length is still determined by the discount factor of the underlying Markov Decision Process. We propose a modification to the options framework that naturally scales the agent’s horizon with option length. We show that the proposed option-step discount controls a bias-variance trade-off, with larger discounts (counter-intuitively) leading to less estimation variance.",http://proceedings.mlr.press/v97/harutyunyan19a.html,http://proceedings.mlr.press/v97/harutyunyan19a/harutyunyan19a.pdf,ICML
310,2019,State-Reification Networks: Improving Generalization by Modeling the Distribution of Hidden Representations,"Alex Lamb,         Jonathan Binas,         Anirudh Goyal,         Sandeep Subramanian,         Ioannis Mitliagkas,         Yoshua Bengio,         Michael Mozer","Machine learning promises methods that generalize well from finite labeled data. However, the brittleness of existing neural net approaches is revealed by notable failures, such as the existence of adversarial examples that are misclassified despite being nearly identical to a training example, or the inability of recurrent sequence-processing nets to stay on track without teacher forcing. We introduce a method, which we refer to as _state reification_, that involves modeling the distribution of hidden states over the training data and then projecting hidden states observed during testing toward this distribution. Our intuition is that if the network can remain in a familiar manifold of hidden space, subsequent layers of the net should be well trained to respond appropriately. We show that this state-reification method helps neural nets to generalize better, especially when labeled data are sparse, and also helps overcome the challenge of achieving robust generalization with adversarial training.",http://proceedings.mlr.press/v97/lamb19a.html,http://proceedings.mlr.press/v97/lamb19a/lamb19a.pdf,ICML
311,2019,High-Fidelity Image Generation With Fewer Labels,"Mario Lučić,         Michael Tschannen,         Marvin Ritter,         Xiaohua Zhai,         Olivier Bachem,         Sylvain Gelly","Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, high-dimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform the state of the art on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the-art conditional model BigGAN on ImageNet using only 10% of the labels and outperform it using 20% of the labels.",http://proceedings.mlr.press/v97/lucic19a.html,http://proceedings.mlr.press/v97/lucic19a/lucic19a.pdf,ICML
312,2019,Loss Landscapes of Regularized Linear Autoencoders,"Daniel Kunin,         Jonathan Bloom,         Aleksandrina Goeva,         Cotton Seed","Autoencoders are a deep learning model for representation learning. When trained to minimize the distance between the data and its reconstruction, linear autoencoders (LAEs) learn the subspace spanned by the top principal directions but cannot learn the principal directions themselves. In this paper, we prove that L2L2L_2-regularized LAEs are symmetric at all critical points and learn the principal directions as the left singular vectors of the decoder. We smoothly parameterize the critical manifold and relate the minima to the MAP estimate of probabilistic PCA. We illustrate these results empirically and consider implications for PCA algorithms, computational neuroscience, and the algebraic topology of learning.",http://proceedings.mlr.press/v97/kunin19a.html,http://proceedings.mlr.press/v97/kunin19a/kunin19a.pdf,ICML
313,2019,Self-Attention Graph Pooling,"Junhyun Lee,         Inyeop Lee,         Jaewoo Kang","Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.",http://proceedings.mlr.press/v97/lee19c.html,http://proceedings.mlr.press/v97/lee19c/lee19c.pdf,ICML
314,2019,Differential Inclusions for Modeling Nonsmooth ADMM Variants: A Continuous Limit Theory,"Huizhuo Yuan,         Yuren Zhou,         Chris Junchi Li,         Qingyun Sun","Recently, there has been a great deal of research attention on understanding the convergence behavior of first-order methods. One line of this research focuses on analyzing the convergence behavior of first-order methods using tools from continuous dynamical systems such as ordinary differential equations and differential inclusions. These research results shed lights on better understanding first-order methods from a non-optimization point of view. The alternating direction method of multipliers (ADMM) is a widely used first-order method for solving optimization problems arising from machine learning and statistics, and it is important to investigate its behavior using these new techniques from dynamical systems. Existing works along this line have been mainly focusing on problems with smooth objective functions, which exclude many important applications that are traditionally solved by ADMM variants. In this paper, we analyze some well-known and widely used ADMM variants for nonsmooth optimization problems using tools of differential inclusions. In particular, we analyze the convergence behavior of linearized ADMM, gradient-based ADMM, generalized ADMM and accelerated generalized ADMM for nonsmooth problems and show their connections with dynamical systems. We anticipate that these results will provide new insights on understanding ADMM for solving nonsmooth problems.",http://proceedings.mlr.press/v97/yuan19c.html,http://proceedings.mlr.press/v97/yuan19c/yuan19c.pdf,ICML
315,2019,Taming MAML: Efficient unbiased meta-reinforcement learning,"Hao Liu,         Richard Socher,         Caiming Xiong","While meta reinforcement learning (Meta-RL) methods have achieved remarkable success, obtaining correct and low variance estimates for policy gradients remains a significant challenge. In particular, estimating a large Hessian, poor sample efficiency and unstable training continue to make Meta-RL difficult. We propose a surrogate objective function named, Taming MAML (TMAML), that adds control variates into gradient estimation via automatic differentiation. TMAML improves the quality of gradient estimation by reducing variance without introducing bias. We further propose a version of our method that extends the meta-learning framework to learning the control variates themselves, enabling efficient and scalable learning from a distribution of MDPs. We empirically compare our approach with MAML and other variance-bias trade-off methods including DICE, LVC, and action-dependent control variates. Our approach is easy to implement and outperforms existing methods in terms of the variance and accuracy of gradient estimation, ultimately yielding higher performance across a variety of challenging Meta-RL environments.",http://proceedings.mlr.press/v97/liu19g.html,http://proceedings.mlr.press/v97/liu19g/liu19g.pdf,ICML
316,2019,TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing,"Augustus Odena,         Catherine Olsson,         David Andersen,         Ian Goodfellow","Neural networks are difficult to interpret and debug. We introduce testing techniques for neural networks that can discover errors occurring only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how approximate nearest neighbor (ANN) algorithms can provide this coverage metric for neural networks. We then combine these methods with techniques for property-based testing (PBT). In PBT, one asserts properties that a function should satisfy and the system automatically generates tests exercising those properties. We then apply this system to practical goals including (but not limited to) surfacing broken loss functions in popular GitHub repositories and making performance improvements to TensorFlow. Finally, we release an open source library called TensorFuzz that implements the described techniques.",http://proceedings.mlr.press/v97/odena19a.html,http://proceedings.mlr.press/v97/odena19a/odena19a.pdf,ICML
317,2019,Iterative Linearized Control: Stable Algorithms and Complexity Guarantees,"Vincent Roulet,         Siddhartha Srinivasa,         Dmitriy Drusvyatskiy,         Zaid Harchaoui","We examine popular gradient-based algorithms for nonlinear control in the light of the modern complexity analysis of first-order optimization algorithms. The examination reveals that the complexity bounds can be clearly stated in terms of calls to a computational oracle related to dynamic programming and implementable by gradient back-propagation using machine learning software libraries such as PyTorch or TensorFlow. Finally, we propose a regularized Gauss-Newton algorithm enjoying worst-case complexity bounds and improved convergence behavior in practice. The software library based on PyTorch is publicly available.",http://proceedings.mlr.press/v97/roulet19a.html,http://proceedings.mlr.press/v97/roulet19a/roulet19a.pdf,ICML
318,2019,Online Learning with Sleeping Experts and Feedback Graphs,"Corinna Cortes,         Giulia Desalvo,         Claudio Gentile,         Mehryar Mohri,         Scott Yang","We consider the scenario of online learning with sleeping experts, where not all experts are available at each round, and analyze the general framework of learning with feedback graphs, where the loss observations associated with each expert are characterized by a graph. A critical assumption in this framework is that the loss observations and the set of sleeping experts at each round are independent. We first extend the classical sleeping experts algorithm of Kleinberg et al. 2008 to the feedback graphs scenario, and prove matching upper and lower bounds for the sleeping regret of the resulting algorithm under the independence assumption. Our main contribution is then to relax this assumption, present a more general notion of sleeping regret, and derive a general algorithm with strong theoretical guarantees. We apply this new framework to the important scenario of online learning with abstention, where a learner can elect to abstain from making a prediction at the price of a certain cost. We empirically validate our algorithm against multiple online abstention algorithms on several real-world datasets, showing substantial performance improvements.",http://proceedings.mlr.press/v97/cortes19a.html,http://proceedings.mlr.press/v97/cortes19a/cortes19a.pdf,ICML
319,2019,Exploring the Landscape of Spatial Robustness,"Logan Engstrom,         Brandon Tran,         Dimitris Tsipras,         Ludwig Schmidt,         Aleksander Madry","The study of adversarial robustness has so far largely focused on perturbations bound in ℓpℓp\ell_p-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network–based classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and test-time input aggregation to significantly improve robustness. Finally we find that, in contrast to the ℓpℓp\ell_p-norm case, first-order methods cannot reliably find worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring additional study.",http://proceedings.mlr.press/v97/engstrom19a.html,http://proceedings.mlr.press/v97/engstrom19a/engstrom19a.pdf,ICML
320,2019,Adversarial Online Learning with noise,"Alon Resler,         Yishay Mansour","We present and study models of adversarial online learning where the feedback observed by the learner is noisy, and the feedback is either full information feedback or bandit feedback. Specifically, we consider binary losses xored with the noise, which is a Bernoulli random variable. We consider both a constant noise rate and a variable noise rate. Our main results are tight regret bounds for learning with noise in the adversarial online learning model.",http://proceedings.mlr.press/v97/resler19a.html,http://proceedings.mlr.press/v97/resler19a/resler19a.pdf,ICML
321,2019,PROVEN: Verifying Robustness of Neural Networks with a Probabilistic Approach,"Lily Weng,         Pin-Yu Chen,         Lam Nguyen,         Mark Squillante,         Akhilan Boopathy,         Ivan Oseledets,         Luca Daniel","We propose a novel framework PROVEN to \textbf{PRO}babilistically \textbf{VE}rify \textbf{N}eural network’s robustness with statistical guarantees. PROVEN provides probability certificates of neural network robustness when the input perturbation follow distributional characterization. Notably, PROVEN is derived from current state-of-the-art worst-case neural network robustness verification frameworks, and therefore it can provide probability certificates with little computational overhead on top of existing methods such as Fast-Lin, CROWN and CNN-Cert. Experiments on small and large MNIST and CIFAR neural network models demonstrate our probabilistic approach can tighten up robustness certificate to around 1.8×1.8×1.8 \times and 3.5×3.5×3.5 \times with at least a 99.9999.9999.99% confidence compared with the worst-case robustness certificate by CROWN and CNN-Cert.",http://proceedings.mlr.press/v97/weng19a.html,http://proceedings.mlr.press/v97/weng19a/weng19a.pdf,ICML
322,2019,Composable Core-sets for Determinant Maximization: A Simple Near-Optimal Algorithm,"Sepideh Mahabadi,         Piotr Indyk,         Shayan Oveis Gharan,         Alireza Rezaei","“Composable core-sets” are an efficient framework for solving optimization problems in massive data models. In this work, we consider efficient construction of composable core-sets for the determinant maximization problem. This can also be cast as the MAP inference task for “determinantal point processes"", that have recently gained a lot of interest for modeling diversity and fairness. The problem was recently studied in \cite{indyk2018composable}, where they designed composable core-sets with the optimal approximation bound of O(k)kO(k)kO(k)^k. On the other hand, the more practical “Greedy"" algorithm has been previously used in similar contexts. In this work, first we provide a theoretical approximation guarantee of Ck2Ck2C^{k^2} for the Greedy algorithm in the context of composable core-sets; Further, we propose to use a “Local Search"" based algorithm that while being still practical, achieves a nearly optimal approximation bound of O(k)2kO(k)2kO(k)^{2k}; Finally, we implement all three algorithms and show the effectiveness of our proposed algorithm on standard data sets.",http://proceedings.mlr.press/v97/mahabadi19a.html,http://proceedings.mlr.press/v97/mahabadi19a/mahabadi19a.pdf,ICML
323,2019,TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning,"Tameem Adel,         Adrian Weller","One of the challenges to reinforcement learning (RL) is scalable transferability among complex tasks. Incorporating a graphical model (GM), along with the rich family of related methods, as a basis for RL frameworks provides potential to address issues such as transferability, generalisation and exploration. Here we propose a flexible GM-based RL framework which leverages efficient inference procedures to enhance generalisation and transfer power. In our proposed transferable and information-based graphical model framework ‘TibGM’, we show the equivalence between our mutual information-based objective in the GM, and an RL consolidated objective consisting of a standard reward maximisation target and a generalisation/transfer objective. In settings where there is a sparse or deceptive reward signal, our TibGM framework is flexible enough to incorporate exploration bonuses depicting intrinsic rewards. We empirically verify improved performance and exploration power.",http://proceedings.mlr.press/v97/adel19a.html,http://proceedings.mlr.press/v97/adel19a/adel19a.pdf,ICML
324,2019,Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning,"Thinh Doan,         Siva Maguluri,         Justin Romberg","We study the policy evaluation problem in multi-agent reinforcement learning. In this problem, a group of agents works cooperatively to evaluate the value function for the global discounted accumulative reward problem, which is composed of local rewards observed by the agents. Over a series of time steps, the agents act, get rewarded, update their local estimate of the value function, then communicate with their neighbors. The local update at each agent can be interpreted as a distributed consensus-based variant of the popular temporal difference learning algorithm TD(0). While distributed reinforcement learning algorithms have been presented in the literature, almost nothing is known about their convergence rate. Our main contribution is providing a finite-time analysis for the convergence of the distributed TD(0) algorithm. We do this when the communication network between the agents is time-varying in general. We obtain an explicit upper bound on the rate of convergence of this algorithm as a function of the network topology and the discount factor. Our results mirror what we would expect from using distributed stochastic gradient descent for solving convex optimization problems.",http://proceedings.mlr.press/v97/doan19a.html,http://proceedings.mlr.press/v97/doan19a/doan19a.pdf,ICML
325,2019,Accelerated Linear Convergence of Stochastic Momentum Methods in Wasserstein Distances,"Bugra Can,         Mert Gurbuzbalaban,         Lingjiong Zhu","Momentum methods such as Polyak’s heavy ball (HB) method, Nesterov’s accelerated gradient (AG) as well as accelerated projected gradient (APG) method have been commonly used in machine learning practice, but their performance is quite sensitive to noise in the gradients. We study these methods under a first-order stochastic oracle model where noisy estimates of the gradients are available. For strongly convex problems, we show that the distribution of the iterates of AG converges with the accelerated O(κ−−√log(1/ε))O(κlog⁡(1/ε))O(\sqrt{\kappa}\log(1/\varepsilon)) linear rate to a ball of radius εε\varepsilon centered at a unique invariant distribution in the 1-Wasserstein metric where κκ\kappa is the condition number as long as the noise variance is smaller than an explicit upper bound we can provide. Our analysis also certifies linear convergence rates as a function of the stepsize, momentum parameter and the noise variance; recovering the accelerated rates in the noiseless case and quantifying the level of noise that can be tolerated to achieve a given performance. To the best of our knowledge, these are the first linear convergence results for stochastic momentum methods under the stochastic oracle model. We also develop finer results for the special case of quadratic objectives, extend our results to the APG method and weakly convex functions showing accelerated rates when the noise magnitude is sufficiently small.",http://proceedings.mlr.press/v97/can19a.html,http://proceedings.mlr.press/v97/can19a/can19a.pdf,ICML
326,2019,Area Attention,"Yang Li,         Lukasz Kaiser,         Samy Bengio,         Si Si","Existing attention mechanisms are trained to attend to individual items in a collection (the memory) with a predefined, fixed granularity, e.g., a word token or an image grid. We propose area attention: a way to attend to areas in the memory, where each area contains a group of items that are structurally adjacent, e.g., spatially for a 2D memory such as images, or temporally for a 1D memory such as natural language sentences. Importantly, the shape and the size of an area are dynamically determined via learning, which enables a model to attend to information with varying granularity. Area attention can easily work with existing model architectures such as multi-head attention for simultaneously attending to multiple areas in the memory. We evaluate area attention on two tasks: neural machine translation (both character and token-level) and image captioning, and improve upon strong (state-of-the-art) baselines in all the cases. These improvements are obtainable with a basic form of area attention that is parameter free.",http://proceedings.mlr.press/v97/li19e.html,http://proceedings.mlr.press/v97/li19e/li19e.pdf,ICML
327,2019,Efficient On-Device Models using Neural Projections,Sujith Ravi,"Many applications involving visual and language understanding can be effectively solved using deep neural networks. Even though these techniques achieve state-of-the-art results, it is very challenging to apply them on devices with limited memory and computational capacity such as mobile phones, smart watches and IoT. We propose a neural projection approach for training compact on-device neural networks. We introduce ""projection"" networks that use locality-sensitive projections to generate compact binary representations and learn small neural networks with computationally efficient operations. We design a joint optimization framework where the projection network can be trained from scratch or leverage existing larger neural networks such as feed-forward NNs, CNNs or RNNs. The trained neural projection network can be directly used for inference on device at low memory and computation cost. We demonstrate the effectiveness of this as a general-purpose approach for significantly shrinking memory requirements of different types of neural networks while preserving good accuracy on multiple visual and text classification tasks.",http://proceedings.mlr.press/v97/ravi19a.html,http://proceedings.mlr.press/v97/ravi19a/ravi19a.pdf,ICML
328,2019,Lipschitz Generative Adversarial Nets,"Zhiming Zhou,         Jiadong Liang,         Yuxuan Song,         Lantao Yu,         Hongwei Wang,         Weinan Zhang,         Yong Yu,         Zhihua Zhang","In this paper we show that generative adversarial networks (GANs) without restriction on the discriminative function space commonly suffer from the problem that the gradient produced by the discriminator is uninformative to guide the generator. By contrast, Wasserstein GAN (WGAN), where the discriminative function is restricted to 1-Lipschitz, does not suffer from such a gradient uninformativeness problem. We further show in the paper that the model with a compact dual form of Wasserstein distance, where the Lipschitz condition is relaxed, may also theoretically suffer from this issue. This implies the importance of Lipschitz condition and motivates us to study the general formulation of GANs with Lipschitz constraint, which leads to a new family of GANs that we call Lipschitz GANs (LGANs). We show that LGANs guarantee the existence and uniqueness of the optimal discriminative function as well as the existence of a unique Nash equilibrium. We prove that LGANs are generally capable of eliminating the gradient uninformativeness problem. According to our empirical analysis, LGANs are more stable and generate consistently higher quality samples compared with WGAN.",http://proceedings.mlr.press/v97/zhou19c.html,http://proceedings.mlr.press/v97/zhou19c/zhou19c.pdf,ICML
329,2019,Batch Policy Learning under Constraints,"Hoang Le,         Cameron Voloshin,         Yisong Yue","When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. As part of off-policy learning, we propose a simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting.",http://proceedings.mlr.press/v97/le19a.html,http://proceedings.mlr.press/v97/le19a/le19a.pdf,ICML
330,2019,Context-Aware Zero-Shot Learning for Object Recognition,"Eloi Zablocki,         Patrick Bordes,         Laure Soulier,         Benjamin Piwowarski,         Patrick Gallinari","Zero-Shot Learning (ZSL) aims at classifying unlabeled objects by leveraging auxiliary knowledge, such as semantic representations. A limitation of previous approaches is that only intrinsic properties of objects, e.g. their visual appearance, are taken into account while their context, e.g. the surrounding objects in the image, is ignored. Following the intuitive principle that objects tend to be found in certain contexts but not others, we propose a new and challenging approach, context-aware ZSL, that leverages semantic representations in a new way to model the conditional likelihood of an object to appear in a given context. Finally, through extensive experiments conducted on Visual Genome, we show that contextual information can substantially improve the standard ZSL approach and is robust to unbalanced classes.",http://proceedings.mlr.press/v97/zablocki19a.html,http://proceedings.mlr.press/v97/zablocki19a/zablocki19a.pdf,ICML
331,2019,DL2: Training and Querying Neural Networks with Logic,"Marc Fischer,         Mislav Balunovic,         Dana Drachsler-Cohen,         Timon Gehr,         Ce Zhang,         Martin Vechev","We present DL2, a system for training and querying neural networks with logical constraints. Using DL2, one can declaratively specify domain knowledge constraints to be enforced during training, as well as pose queries on the model to find inputs that satisfy a set of constraints. DL2 works by translating logical constraints into a loss function with desirable mathematical properties. The loss is then minimized with standard gradient-based methods. We evaluate DL2 by training networks with interesting constraints in unsupervised, semi-supervised and supervised settings. Our experimental evaluation demonstrates that DL2 is more expressive than prior approaches combining logic and neural networks, and its loss functions are better suited for optimization. Further, we show that for a number of queries, DL2 can find the desired inputs in seconds (even for large models such as ResNet-50 on ImageNet).",http://proceedings.mlr.press/v97/fischer19a.html,http://proceedings.mlr.press/v97/fischer19a/fischer19a.pdf,ICML
332,2019,Near optimal finite time identification of arbitrary linear dynamical systems,"Tuhin Sarkar,         Alexander Rakhlin","We derive finite time error bounds for estimating general linear time-invariant (LTI) systems from a single observed trajectory using the method of least squares. We provide the first analysis of the general case when eigenvalues of the LTI system are arbitrarily distributed in three regimes: stable, marginally stable, and explosive. Our analysis yields sharp upper bounds for each of these cases separately. We observe that although the underlying process behaves quite differently in each of these three regimes, the systematic analysis of a self–normalized martingale difference term helps bound identification error up to logarithmic factors of the lower bound. On the other hand, we demonstrate that the least squares solution may be statistically inconsistent under certain conditions even when the signal-to-noise ratio is high.",http://proceedings.mlr.press/v97/sarkar19a.html,http://proceedings.mlr.press/v97/sarkar19a/sarkar19a.pdf,ICML
333,2019,Katalyst: Boosting Convex Katayusha for Non-Convex Problems with a Large Condition Number,"Zaiyi Chen,         Yi Xu,         Haoyuan Hu,         Tianbao Yang","An important class of non-convex objectives that has wide applications in machine learning consists of a sum of nnn smooth functions and a non-smooth convex function. Tremendous studies have been devoted to conquering these problems by leveraging one of the two types of variance reduction techniques, i.e., SVRG-type that computes a full gradient occasionally and SAGA-type that maintains nnn stochastic gradients at every iteration. In practice, SVRG-type is preferred to SAGA-type due to its potentially less memory costs. An interesting question that has been largely ignored is how to improve the complexity of variance reduction methods for problems with a large condition number that measures the degree to which the objective is close to a convex function. In this paper, we present a simple but non-trivial boosting of a state-of-the-art SVRG-type method for convex problems (namely Katyusha) to enjoy an improved complexity for solving non-convex problems with a large condition number (that is close to a convex function). To the best of our knowledge, its complexity has the best dependence on nnn and the degree of non-convexity, and also matches that of a recent SAGA-type accelerated stochastic algorithm for a constrained non-convex smooth optimization problem.",http://proceedings.mlr.press/v97/chen19k.html,http://proceedings.mlr.press/v97/chen19k/chen19k.pdf,ICML
334,2019,Dynamic Measurement Scheduling for Event Forecasting using Deep RL,"Chun-Hao Chang,         Mingjie Mai,         Anna Goldenberg","Imagine a patient in critical condition. What and when should be measured to forecast detrimental events, especially under the budget constraints? We answer this question by deep reinforcement learning (RL) that jointly minimizes the measurement cost and maximizes predictive gain, by scheduling strategically-timed measurements. We learn our policy to be dynamically dependent on the patient’s health history. To scale our framework to exponentially large action space, we distribute our reward in a sequential setting that makes the learning easier. In our simulation, our policy outperforms heuristic-based scheduling with higher predictive gain and lower cost. In a real-world ICU mortality prediction task (MIMIC3), our policies reduce the total number of measurements by 31% or improve predictive gain by a factor of 3 as compared to physicians, under the off-policy policy evaluation.",http://proceedings.mlr.press/v97/chang19a.html,http://proceedings.mlr.press/v97/chang19a/chang19a.pdf,ICML
335,2019,Stable-Predictive Optimistic Counterfactual Regret Minimization,"Gabriele Farina,         Christian Kroer,         Noam Brown,         Tuomas Sandholm","The CFR framework has been a powerful tool for solving large-scale extensive-form games in practice. However, the theoretical rate at which past CFR-based algorithms converge to the Nash equilibrium is on the order of O(T−1/2)O(T−1/2)O(T^{-1/2}), where TTT is the number of iterations. In contrast, first-order methods can be used to achieve a O(T−1)O(T−1)O(T^{-1}) dependence on iterations, yet these methods have been less successful in practice. In this work we present the first CFR variant that breaks the square-root dependence on iterations. By combining and extending recent advances on predictive and stable regret minimizers for the matrix-game setting we show that it is possible to leverage “optimistic” regret minimizers to achieve a O(T−3/4)O(T−3/4)O(T^{-3/4}) convergence rate within CFR. This is achieved by introducing a new notion of stable-predictivity, and by setting the stability of each counterfactual regret minimizer relative to its location in the decision tree. Experiments show that this method is faster than the original CFR algorithm, although not as fast as newer variants, in spite of their worst-case O(T−1/2)O(T−1/2)O(T^{-1/2}) dependence on iterations.",http://proceedings.mlr.press/v97/farina19a.html,http://proceedings.mlr.press/v97/farina19a/farina19a.pdf,ICML
336,2019,Shallow-Deep Networks: Understanding and Mitigating Network Overthinking,"Yigitcan Kaya,         Sanghyun Hong,         Tudor Dumitras","We characterize a prevalent weakness of deep neural networks (DNNs), ’overthinking’, which occurs when a DNN can reach correct predictions before its final layer. Overthinking is computationally wasteful, and it can also be destructive when, by the final layer, a correct prediction changes into a misclassification. Understanding overthinking requires studying how each prediction evolves during a DNN’s forward pass, which conventionally is opaque. For prediction transparency, we propose the Shallow-Deep Network (SDN), a generic modification to off-the-shelf DNNs that introduces internal classifiers. We apply SDN to four modern architectures, trained on three image classification tasks, to characterize the overthinking problem. We show that SDNs can mitigate the wasteful effect of overthinking with confidence-based early exits, which reduce the average inference cost by more than 50% and preserve the accuracy. We also find that the destructive effect occurs for 50% of misclassifications on natural inputs and that it can be induced, adversarially, with a recent backdooring attack. To mitigate this effect, we propose a new confusion metric to quantify the internal disagreements that will likely to lead to misclassifications.",http://proceedings.mlr.press/v97/kaya19a.html,http://proceedings.mlr.press/v97/kaya19a/kaya19a.pdf,ICML
337,2019,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,"Natasha Jaques,         Angeliki Lazaridou,         Edward Hughes,         Caglar Gulcehre,         Pedro Ortega,         Dj Strouse,         Joel Z. Leibo,         Nando De Freitas","We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents’ actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents’ behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area.",http://proceedings.mlr.press/v97/jaques19a.html,http://proceedings.mlr.press/v97/jaques19a/jaques19a.pdf,ICML
338,2019,Compositional Fairness Constraints for Graph Embeddings,"Avishek Bose,         William Hamilton","Learning high-quality node embeddings is a key building block for machine learning models that operate on graph data, such as social networks and recommender systems. However, existing graph embedding techniques are unable to cope with fairness constraints, e.g., ensuring that the learned representations do not correlate with certain attributes, such as age or gender. Here, we introduce an adversarial framework to enforce fairness constraints on graph embeddings. Our approach is compositional—meaning that it can flexibly accommodate different combinations of fairness constraints during inference. For instance, in the context of social recommendations, our framework would allow one user to request that their recommendations are invariant to both their age and gender, while also allowing another user to request invariance to just their age. Experiments on standard knowledge graph and recommender system benchmarks highlight the utility of our proposed framework.",http://proceedings.mlr.press/v97/bose19a.html,http://proceedings.mlr.press/v97/bose19a/bose19a.pdf,ICML
339,2019,Fingerprint Policy Optimisation for Robust Reinforcement Learning,"Supratik Paul,         Michael A. Osborne,         Shimon Whiteson","Policy gradient methods ignore the potential value of adjusting environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but are controllable in a simulator. This can lead to slow learning, or convergence to suboptimal policies, if the environment variable has a large impact on the transition dynamics. In this paper, we present fingerprint policy optimisation (FPO), which finds a policy that is optimal in expectation across the distribution of environment variables. The central idea is to use Bayesian optimisation (BO) to actively select the distribution of the environment variable that maximises the improvement generated by each iteration of the policy gradient method. To make this BO practical, we contribute two easy-to-compute low-dimensional fingerprints of the current policy. Our experiments show that FPO can efficiently learn policies that are robust to significant rare events, which are unlikely to be observable under random sampling, but are key to learning good policies.",http://proceedings.mlr.press/v97/paul19a.html,http://proceedings.mlr.press/v97/paul19a/paul19a.pdf,ICML
340,2019,Understanding MCMC Dynamics as Flows on the Wasserstein Space,"Chang Liu,         Jingwei Zhuo,         Jun Zhu","It is known that the Langevin dynamics used in MCMC is the gradient flow of the KL divergence on the Wasserstein space, which helps convergence analysis and inspires recent particle-based variational inference methods (ParVIs). But no more MCMC dynamics is understood in this way. In this work, by developing novel concepts, we propose a theoretical framework that recognizes a general MCMC dynamics as the fiber-gradient Hamiltonian flow on the Wasserstein space of a fiber-Riemannian Poisson manifold. The ""conservation + convergence"" structure of the flow gives a clear picture on the behavior of general MCMC dynamics. The framework also enables ParVI simulation of MCMC dynamics, which enriches the ParVI family with more efficient dynamics, and also adapts ParVI advantages to MCMCs. We develop two ParVI methods for a particular MCMC dynamics and demonstrate the benefits in experiments.",http://proceedings.mlr.press/v97/liu19j.html,http://proceedings.mlr.press/v97/liu19j/liu19j.pdf,ICML
341,2019,On the Generalization Gap in Reparameterizable Reinforcement Learning,"Huan Wang,         Stephan Zheng,         Caiming Xiong,         Richard Socher","Understanding generalization in reinforcement learning (RL) is a significant challenge, as many common assumptions of traditional supervised learning theory do not apply. We focus on the special class of reparameterizable RL problems, where the trajectory distribution can be decomposed using the reparametrization trick. For this problem class, estimating the expected return is efficient and the trajectory can be computed deterministically given peripheral random variables, which enables us to study reparametrizable RL using supervised learning and transfer learning theory. Through these relationships, we derive guarantees on the gap between the expected and empirical return for both intrinsic and external errors, based on Rademacher complexity as well as the PAC-Bayes bound. Our bound suggests the generalization capability of reparameterizable RL is related to multiple factors including “smoothness” of the environment transition, reward and agent policy function class. We also empirically verify the relationship between the generalization gap and these factors through simulations.",http://proceedings.mlr.press/v97/wang19o.html,http://proceedings.mlr.press/v97/wang19o/wang19o.pdf,ICML
342,2019,Feature Grouping as a Stochastic Regularizer for High-Dimensional Structured Data,"Sergul Aydore,         Bertrand Thirion,         Gael Varoquaux","In many applications where collecting data is expensive, for example neuroscience or medical imaging, the sample size is typically small compared to the feature dimension. These datasets call for intelligent regularization that exploits known structure, such as correlations between the features arising from the measurement device. However, existing structured regularizers need specially crafted solvers, which are difficult to apply to complex models. We propose a new regularizer specifically designed to leverage structure in the data in a way that can be applied efficiently to complex models. Our approach relies on feature grouping, using a fast clustering algorithm inside a stochastic gradient descent loop: given a family of feature groupings that capture feature covariations, we randomly select these groups at each iteration. Experiments on two real-world datasets demonstrate that the proposed approach produces models that generalize better than those trained with conventional regularizers, and also improves convergence speed, and has a linear computational cost.",http://proceedings.mlr.press/v97/aydore19a.html,http://proceedings.mlr.press/v97/aydore19a/aydore19a.pdf,ICML
343,2019,Toward Controlling Discrimination in Online Ad Auctions,"Elisa Celis,         Anay Mehrotra,         Nisheeth Vishnoi","Online advertising platforms are thriving due to the customizable audiences they offer advertisers. However, recent studies show that advertisements can be discriminatory with respect to the gender or race of the audience that sees the ad, and may inadvertently cross ethical and/or legal boundaries. To prevent this, we propose a constrained ad auction framework that maximizes the platform’s revenue conditioned on ensuring that the audience seeing an advertiser’s ad is distributed appropriately across sensitive types such as gender or race. Building upon Myerson’s classic work, we first present an optimal auction mechanism for a large class of fairness constraints. Finding the parameters of this optimal auction, however, turns out to be a non-convex problem. We show that this non-convex problem can be reformulated as a more structured non-convex problem with no saddle points or local-maxima; this allows us to develop a gradient-descent-based algorithm to solve it. Our empirical results on the A1 Yahoo! dataset demonstrate that our algorithm can obtain uniform coverage across different user types for each advertiser at a minor loss to the revenue of the platform, and a small change to the size of the audience each advertiser reaches.",http://proceedings.mlr.press/v97/mehrotra19a.html,http://proceedings.mlr.press/v97/mehrotra19a/mehrotra19a.pdf,ICML
344,2019,"A Gradual, Semi-Discrete Approach to Generative Network Training via Explicit Wasserstein Minimization","Yucheng Chen,         Matus Telgarsky,         Chao Zhang,         Bolton Bailey,         Daniel Hsu,         Jian Peng","This paper provides a simple procedure to fit generative networks to target distributions, with the goal of a small Wasserstein distance (or other optimal transport costs). The approach is based on two principles: (a) if the source randomness of the network is a continuous distribution (the ""semi-discrete"" setting), then the Wasserstein distance is realized by a deterministic optimal transport mapping; (b) given an optimal transport mapping between a generator network and a target distribution, the Wasserstein distance may be decreased via a regression between the generated data and the mapped target points. The procedure here therefore alternates these two steps, forming an optimal transport and regressing against it, gradually adjusting the generator network towards the target distribution. Mathematically, this approach is shown to minimize the Wasserstein distance to both the empirical target distribution, and also its underlying population counterpart. Empirically, good performance is demonstrated on the training and testing sets of the MNIST and Thin-8 data. The paper closes with a discussion of the unsuitability of the Wasserstein distance for certain tasks, as has been identified in prior work (Arora et al., 2017; Huang et al., 2017).",http://proceedings.mlr.press/v97/chen19h.html,http://proceedings.mlr.press/v97/chen19h/chen19h.pdf,ICML
345,2019,Learning from Delayed Outcomes via Proxies with Applications to Recommender Systems,"Timothy Arthur Mann,         Sven Gowal,         Andras Gyorgy,         Huiyi Hu,         Ray Jiang,         Balaji Lakshminarayanan,         Prav Srinivasan","Predicting delayed outcomes is an important problem in recommender systems (e.g., if customers will finish reading an ebook). We formalize the problem as an adversarial, delayed online learning problem and consider how a proxy for the delayed outcome (e.g., if customers read a third of the book in 24 hours) can help minimize regret, even though the proxy is not available when making a prediction. Motivated by our regret analysis, we propose two neural network architectures: Factored Forecaster (FF) which is ideal if the proxy is informative of the outcome in hindsight, and Residual Factored Forecaster (RFF) that is robust to a non-informative proxy. Experiments on two real-world datasets for predicting human behavior show that RFF outperforms both FF and a direct forecaster that does not make use of the proxy. Our results suggest that exploiting proxies by factorization is a promising way to mitigate the impact of long delays in human-behavior prediction tasks.",http://proceedings.mlr.press/v97/mann19a.html,http://proceedings.mlr.press/v97/mann19a/mann19a.pdf,ICML
346,2019,Characterizing Well-Behaved vs. Pathological Deep Neural Networks,Antoine Labatie,"We introduce a novel approach, requiring only mild assumptions, for the characterization of deep neural networks at initialization. Our approach applies both to fully-connected and convolutional networks and easily incorporates batch normalization and skip-connections. Our key insight is to consider the evolution with depth of statistical moments of signal and noise, thereby characterizing the presence or absence of pathologies in the hypothesis space encoded by the choice of hyperparameters. We establish: (i) for feedforward networks, with and without batch normalization, the multiplicativity of layer composition inevitably leads to ill-behaved moments and pathologies; (ii) for residual networks with batch normalization, on the other hand, skip-connections induce power-law rather than exponential behaviour, leading to well-behaved moments and no pathology.",http://proceedings.mlr.press/v97/labatie19a.html,http://proceedings.mlr.press/v97/labatie19a/labatie19a.pdf,ICML
347,2019,Compressing Gradient Optimizers via Count-Sketches,"Ryan Spring,         Anastasios Kyrillidis,         Vijai Mohan,         Anshumali Shrivastava","Many popular first-order optimization methods accelerate the convergence rate of deep learning models. However, these algorithms require auxiliary variables, which cost additional memory proportional to the number of parameters in the model. The problem is becoming more severe as models grow larger to learn from complex, large-scale datasets. Our proposed solution is to maintain a linear sketch to compress the auxiliary variables. Our approach has the same performance as the full-sized baseline, while using less space for the auxiliary variables. Theoretically, we prove that count-sketch optimization maintains the SGD convergence rate, while gracefully reducing memory usage for large-models. We show a rigorous evaluation on popular architectures such as ResNet-18 and Transformer-XL. On the 1-Billion Word dataset, we save 25% of the memory used during training (7.7 GB instead of 10.8 GB) with minimal accuracy and performance loss. For an Amazon extreme classification task with over 49.5 million classes, we also reduce the training time by 38%, by increasing the mini-batch size 3.5x using our count-sketch optimizer.",http://proceedings.mlr.press/v97/spring19a.html,http://proceedings.mlr.press/v97/spring19a/spring19a.pdf,ICML
348,2019,Topological Data Analysis of Decision Boundaries with Application to Model Selection,"Karthikeyan Natesan Ramamurthy,         Kush Varshney,         Krishnan Mody","We propose the labeled Cech complex, the plain labeled Vietoris-Rips complex, and the locally scaled labeled Vietoris-Rips complex to perform persistent homology inference of decision boundaries in classification tasks. We provide theoretical conditions and analysis for recovering the homology of a decision boundary from samples. Our main objective is quantification of deep neural network complexity to enable matching of datasets to pre-trained models to facilitate the functioning of AI marketplaces; we report results for experiments using MNIST, FashionMNIST, and CIFAR10.",http://proceedings.mlr.press/v97/ramamurthy19a.html,http://proceedings.mlr.press/v97/ramamurthy19a/ramamurthy19a.pdf,ICML
349,2019,Convolutional Poisson Gamma Belief Network,"Chaojie Wang,         Bo Chen,         Sucheng Xiao,         Mingyuan Zhou","For text analysis, one often resorts to a lossy representation that either completely ignores word order or embeds each word as a low-dimensional dense feature vector. In this paper, we propose convolutional Poisson factor analysis (CPFA) that directly operates on a lossless representation that processes the words in each document as a sequence of high-dimensional one-hot vectors. To boost its performance, we further propose the convolutional Poisson gamma belief network (CPGBN) that couples CPFA with the gamma belief network via a novel probabilistic pooling layer. CPFA forms words into phrases and captures very specific phrase-level topics, and CPGBN further builds a hierarchy of increasingly more general phrase-level topics. For efficient inference, we develop both a Gibbs sampler and a Weibull distribution based convolutional variational auto-encoder. Experimental results demonstrate that CPGBN can extract high-quality text latent representations that capture the word order information, and hence can be leveraged as a building block to enrich a wide variety of existing latent variable models that ignore word order.",http://proceedings.mlr.press/v97/wang19b.html,http://proceedings.mlr.press/v97/wang19b/wang19b.pdf,ICML
350,2019,Autoregressive Energy Machines,"Conor Durkan,         Charlie Nash","Neural density estimators are flexible families of parametric models which have seen widespread use in unsupervised machine learning in recent years. Maximum-likelihood training typically dictates that these models be constrained to specify an explicit density. However, this limitation can be overcome by instead using a neural network to specify an energy function, or unnormalized density, which can subsequently be normalized to obtain a valid distribution. The challenge with this approach lies in accurately estimating the normalizing constant of the high-dimensional energy function. We propose the Autoregressive Energy Machine, an energy-based model which simultaneously learns an unnormalized density and computes an importance-sampling estimate of the normalizing constant for each conditional in an autoregressive decomposition. The Autoregressive Energy Machine achieves state-of-the-art performance on a suite of density-estimation tasks.",http://proceedings.mlr.press/v97/durkan19a.html,http://proceedings.mlr.press/v97/durkan19a/durkan19a.pdf,ICML
351,2019,A fully differentiable beam search decoder,"Ronan Collobert,         Awni Hannun,         Gabriel Synnaeve","We introduce a new beam search decoder that is fully differentiable, making it possible to optimize at training time through the inference procedure. Our decoder allows us to combine models which operate at different granularities (e.g. acoustic and language models). It can be used when target sequences are not aligned to input sequences by considering all possible alignments between the two. We demonstrate our approach scales by applying it to speech recognition, jointly training acoustic and word-level language models. The system is end-to-end, with gradients flowing through the whole architecture from the word-level transcriptions. Recent research efforts have shown that deep neural networks with attention-based mechanisms can successfully train an acoustic model from the final transcription, while implicitly learning a language model. Instead, we show that it is possible to discriminatively train an acoustic model jointly with an explicit and possibly pre-trained language model.",http://proceedings.mlr.press/v97/collobert19a.html,http://proceedings.mlr.press/v97/collobert19a/collobert19a.pdf,ICML
352,2019,Training CNNs with Selective Allocation of Channels,"Jongheon Jeong,         Jinwoo Shin","Recent progress in deep convolutional neural networks (CNNs) have enabled a simple paradigm of architecture design: larger models typically achieve better accuracy. Due to this, in modern CNN architectures, it becomes more important to design models that generalize well under certain resource constraints, e.g. the number of parameters. In this paper, we propose a simple way to improve the capacity of any CNN model having large-scale features, without adding more parameters. In particular, we modify a standard convolutional layer to have a new functionality of channel-selectivity, so that the layer is trained to select important channels to re-distribute their parameters. Our experimental results under various CNN architectures and datasets demonstrate that the proposed new convolutional layer allows new optima that generalize better via efficient resource utilization, compared to the baseline.",http://proceedings.mlr.press/v97/jeong19c.html,http://proceedings.mlr.press/v97/jeong19c/jeong19c.pdf,ICML
353,2019,"HyperGAN: A Generative Model for Diverse, Performant Neural Networks","Neale Ratzlaff,         Li Fuxin","We introduce HyperGAN, a generative model that learns to generate all the parameters of a deep neural network. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of generated samples with a classification loss. This is equivalent to minimizing the KL-divergence between the distribution of generated parameters, and the unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning, while also generating a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty estimates than standard ensembles. This is evidenced by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples.",http://proceedings.mlr.press/v97/ratzlaff19a.html,http://proceedings.mlr.press/v97/ratzlaff19a/ratzlaff19a.pdf,ICML
354,2019,A Persistent Weisfeiler-Lehman Procedure for Graph Classification,"Bastian Rieck,         Christian Bock,         Karsten Borgwardt","The Weisfeiler–Lehman graph kernel exhibits competitive performance in many graph classification tasks. However, its subtree features are not able to capture connected components and cycles, topological features known for characterising graphs. To extract such features, we leverage propagated node label information and transform unweighted graphs into metric ones. This permits us to augment the subtree features with topological information obtained using persistent homology, a concept from topological data analysis. Our method, which we formalise as a generalisation of Weisfeiler–Lehman subtree features, exhibits favourable classification accuracy and its improvements in predictive performance are mainly driven by including cycle information.",http://proceedings.mlr.press/v97/rieck19a.html,http://proceedings.mlr.press/v97/rieck19a/rieck19a.pdf,ICML
355,2019,Scale-free adaptive planning for deterministic dynamics & discounted rewards,"Peter Bartlett,         Victor Gabillon,         Jennifer Healey,         Michal Valko","We address the problem of planning in an environment with deterministic dynamics and stochastic discounted rewards under a limited numerical budget where the ranges of both rewards and noise are unknown. We introduce PlaTypOOS, an adaptive, robust, and efficient alternative to the OLOP (open-loop optimistic planning) algorithm. Whereas OLOP requires a priori knowledge of the ranges of both rewards and noise, PlaTypOOS dynamically adapts its behavior to both. This allows PlaTypOOS to be immune to two vulnerabilities of OLOP: failure when given underestimated ranges of noise and rewards and inefficiency when these are overestimated. PlaTypOOS additionally adapts to the global smoothness of the value function. PlaTypOOS acts in a provably more efficient manner vs. OLOP when OLOP is given an overestimated reward and show that in the case of no noise, PlaTypOOS learns exponentially faster.",http://proceedings.mlr.press/v97/bartlett19a.html,http://proceedings.mlr.press/v97/bartlett19a/bartlett19a.pdf,ICML
356,2019,Model Function Based Conditional Gradient Method with Armijo-like Line Search,"Peter Ochs,         Yura Malitsky","The Conditional Gradient Method is generalized to a class of non-smooth non-convex optimization problems with many applications in machine learning. The proposed algorithm iterates by minimizing so-called model functions over the constraint set. Complemented with an Armijo line search procedure, we prove that subsequences converge to a stationary point. The abstract framework of model functions provides great flexibility in the design of concrete algorithms. As special cases, for example, we develop an algorithm for additive composite problems and an algorithm for non-linear composite problems which leads to a Gauss-Newton-type algorithm. Both instances are novel in non-smooth non-convex optimization and come with numerous applications in machine learning. We perform an experiment on a non-linear robust regression problem and discuss the flexibility of the proposed framework in several matrix factorization formulations.",http://proceedings.mlr.press/v97/ochs19a.html,http://proceedings.mlr.press/v97/ochs19a/ochs19a.pdf,ICML
357,2019,Orthogonal Random Forest for Causal Inference,"Miruna Oprescu,         Vasilis Syrgkanis,         Zhiwei Steven Wu","We propose the orthogonal random forest, an algorithm that combines Neyman-orthogonality to reduce sensitivity with respect to estimation error of nuisance parameters with generalized random forests (Athey et al., 2017)—a flexible non-parametric method for statistical estimation of conditional moment models using random forests. We provide a consistency rate and establish asymptotic normality for our estimator. We show that under mild assumptions on the consistency rate of the nuisance estimator, we can achieve the same error rate as an oracle with a priori knowledge of these nuisance parameters. We show that when the nuisance functions have a locally sparse parametrization, then a local ell_1-penalized regression achieves the required rate. We apply our method to estimate heterogeneous treatment effects from observational data with discrete treatments or continuous treatments, and we show that, unlike prior work, our method provably allows to control for a high-dimensional set of variables under standard sparsity conditions. We also provide a comprehensive empirical evaluation of our algorithm on both synthetic and real data.",http://proceedings.mlr.press/v97/oprescu19a.html,http://proceedings.mlr.press/v97/oprescu19a/oprescu19a.pdf,ICML
358,2019,Separating value functions across time-scales,"Joshua Romoff,         Peter Henderson,         Ahmed Touati,         Emma Brunskill,         Joelle Pineau,         Yann Ollivier","In many finite horizon episodic reinforcement learning (RL) settings, it is desirable to optimize for the undiscounted return - in settings like Atari, for instance, the goal is to collect the most points while staying alive in the long run. Yet, it may be difficult (or even intractable) mathematically to learn with this target. As such, temporal discounting is often applied to optimize over a shorter effective planning horizon. This comes at the cost of potentially biasing the optimization target away from the undiscounted goal. In settings where this bias is unacceptable - where the system must optimize for longer horizons at higher discounts - the target of the value function approximator may increase in variance leading to difficulties in learning. We present an extension of temporal difference (TD) learning, which we call TD(ΔΔ\Delta), that breaks down a value function into a series of components based on the differences between value functions with smaller discount factors. The separation of a longer horizon value function into these components has useful properties in scalability and performance. We discuss these properties and show theoretic and empirical improvements over standard TD learning in certain settings.",http://proceedings.mlr.press/v97/romoff19a.html,http://proceedings.mlr.press/v97/romoff19a/romoff19a.pdf,ICML
359,2019,An Instability in Variational Inference for Topic Models,"Behrooz Ghorbani,         Hamid Javadi,         Andrea Montanari","Naive mean field variational methods are the state of-the-art approach to inference in topic modeling. We show that these methods suffer from an instability that can produce misleading conclusions. Namely, for certain regimes of the model parameters, variational inference outputs a non-trivial decomposition into topics. However -for the same parameter values- the data contain no actual information about the true topic decomposition, and the output of the algorithm is uncorrelated with it. In particular, the estimated posterior mean is wrong, and estimated credible regions do not achieve the nominal coverage. We discuss how this instability is remedied by more accurate mean field approximations.",http://proceedings.mlr.press/v97/ghorbani19a.html,http://proceedings.mlr.press/v97/ghorbani19a/ghorbani19a.pdf,ICML
360,2019,MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization,"Eric Chu,         Peter Liu","Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, paired document-summary examples. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some progress has been made in learning sequence-to-sequence mappings with only unpaired examples. In our work, we consider the setting where there are only documents (product or business reviews) with no summaries provided, and propose an end-to-end, neural model architecture to perform unsupervised abstractive summarization. Our proposed model consists of an auto-encoder where the mean of the representations of the input reviews decodes to a reasonable summary-review. We consider variants of the proposed architecture and perform an ablation study to show the importance of specific components. We show through metrics and human evaluation that the generated summaries are highly abstractive, fluent, relevant, and representative of the average sentiment of the input reviews. Finally, we collect a ground-truth evaluation dataset and show that our model outperforms a strong extractive baseline.",http://proceedings.mlr.press/v97/chu19b.html,http://proceedings.mlr.press/v97/chu19b/chu19b.pdf,ICML
361,2019,Task-Agnostic Dynamics Priors for Deep Reinforcement Learning,"Yilun Du,         Karthic Narasimhan","While model-based deep reinforcement learning (RL) holds great promise for sample efficiency and generalization, learning an accurate dynamics model is often challenging and requires substantial interaction with the environment. A wide variety of domains have dynamics that share common foundations like the laws of classical mechanics, which are rarely exploited by existing algorithms. In fact, humans continuously acquire and use such dynamics priors to easily adapt to operating in new environments. In this work, we propose an approach to learn task-agnostic dynamics priors from videos and incorporate them into an RL agent. Our method involves pre-training a frame predictor on task-agnostic physics videos to initialize dynamics models (and fine-tune them) for unseen target environments. Our frame prediction architecture, SpatialNet, is designed specifically to capture localized physical phenomena and interactions. Our approach allows for both faster policy learning and convergence to better policies, outperforming competitive approaches on several different environments. We also demonstrate that incorporating this prior allows for more effective transfer between environments.",http://proceedings.mlr.press/v97/du19e.html,http://proceedings.mlr.press/v97/du19e/du19e.pdf,ICML
362,2019,On the statistical rate of nonlinear recovery in generative models with heavy-tailed data,"Xiaohan Wei,         Zhuoran Yang,         Zhaoran Wang","We consider estimating a high-dimensional vector from non-linear measurements where the unknown vector is represented by a generative model G:Rk→RdG:\mathbb{R}^k\rightarrow\mathbb{R}^d with k≪dk\ll d. Such a model poses structural priors on the unknown vector without having a dedicated basis, and in particular allows new and efficient approaches solving recovery problems with number of measurements far less than the ambient dimension of the vector. While progresses have been made recently regarding theoretical understandings on the linear Gaussian measurements, much less is known when the model is possibly misspecified and the measurements are non-Gaussian. In this paper, we make a step towards such a direction by considering the scenario where the measurements are non-Gaussian, subject to possibly unknown nonlinear transformations and the responses are heavy-tailed. We then propose new estimators via score functions based on the first and second order Stein’s identity, and prove the sample size bound of m=O(kε−2log(L/ε))m=\mathcal{O}(k\varepsilon^{-2}\log(L/\varepsilon)) achieving an ε\varepsilon error in the form of exponential concentration inequalities. Furthermore, for the special case of multi-layer ReLU generative model, we improve the sample bound by a logarithm factor to m=O(kε−2log(d))m=\mathcal{O}(k\varepsilon^{-2}\log(d)), matching the state-of-art statistical rate in compressed sensing for estimating kk-sparse vectors. On the technical side, we develop new chaining methods bounding heavy-tailed processes, which could be of independent interest.",http://proceedings.mlr.press/v97/wei19b.html,http://proceedings.mlr.press/v97/wei19b/wei19b.pdf,ICML
363,2019,Voronoi Boundary Classification: A High-Dimensional Geometric Approach via Weighted Monte Carlo Integration,"Vladislav Polianskii,         Florian T. Pokorny","Voronoi cell decompositions provide a classical avenue to classification. Typical approaches however only utilize point-wise cell-membership information by means of nearest neighbor queries and do not utilize further geometric information about Voronoi cells since the computation of Voronoi diagrams is prohibitively expensive in high dimensions. We propose a Monte-Carlo integration based approach that instead computes a weighted integral over the boundaries of Voronoi cells, thus incorporating additional information about the Voronoi cell structure. We demonstrate the scalability of our approach in up to 3072 dimensional spaces and analyze convergence based on the number of Monte Carlo samples and choice of weight functions. Experiments comparing our approach to Nearest Neighbors, SVM and Random Forests indicate that while our approach performs similarly to Random Forests for large data sizes, the algorithm exhibits non-trivial data-dependent performance characteristics for smaller datasets and can be analyzed in terms of a geometric confidence measure, thus adding to the repertoire of geometric approaches to classification while having the benefit of not requiring any model changes or retraining as new training samples or classes are added.",http://proceedings.mlr.press/v97/polianskii19a.html,http://proceedings.mlr.press/v97/polianskii19a/polianskii19a.pdf,ICML
364,2019,On Medians of (Randomized) Pairwise Means,"Pierre Laforgue,         Stephan Clemencon,         Patrice Bertail","Tournament procedures, recently introduced in the literature, offer an appealing alternative, from a theoretical perspective at least, to the principle of Empirical Risk Minimization in machine learning. Statistical learning by Median-of-Means (MoM) basically consists in segmenting the training data into blocks of equal size and comparing the statistical performance of every pair of candidate decision rules on each data block: that with highest performance on the majority of the blocks is declared as the winner. In the context of nonparametric regression, functions having won all their duels have been shown to outperform empirical risk minimizers w.r.t. the mean squared error under minimal assumptions, while exhibiting robustness properties. It is the purpose of this paper to extend this approach, in order to address other learning problems in particular, for which the performance criterion takes the form of an expectation over pairs of observations rather than over one single observation, as may be the case in pairwise ranking, clustering or metric learning. Precisely, it is proved here that the bounds achieved by MoM are essentially conserved when the blocks are built by means of independent sampling without replacement schemes instead of a simple segmentation. These results are next extended to situations where the risk is related to a pairwise loss function and its empirical counterpart is of the form of a UUU-statistic. Beyond theoretical results guaranteeing the performance of the learning/estimation methods proposed, some numerical experiments provide empirical evidence of their relevance in practice.",http://proceedings.mlr.press/v97/clemencon19a.html,http://proceedings.mlr.press/v97/clemencon19a/clemencon19a.pdf,ICML
365,2019,Exploration Conscious Reinforcement Learning Revisited,"Lior Shani,         Yonathan Efroni,         Shie Mannor","The Exploration-Exploitation tradeoff arises in Reinforcement Learning when one cannot tell if a policy is optimal. Then, there is a constant need to explore new actions instead of exploiting past experience. In practice, it is common to resolve the tradeoff by using a fixed exploration mechanism, such as ϵϵ\epsilon-greedy exploration or by adding Gaussian noise, while still trying to learn an optimal policy. In this work, we take a different approach and study exploration-conscious criteria, that result in optimal policies with respect to the exploration mechanism. Solving these criteria, as we establish, amounts to solving a surrogate Markov Decision Process. We continue and analyze properties of exploration-conscious optimal policies and characterize two general approaches to solve such criteria. Building on the approaches, we apply simple changes in existing tabular and deep Reinforcement Learning algorithms and empirically demonstrate superior performance relatively to their non-exploration-conscious counterparts, both for discrete and continuous action spaces.",http://proceedings.mlr.press/v97/shani19a.html,http://proceedings.mlr.press/v97/shani19a/shani19a.pdf,ICML
366,2019,Doubly-Competitive Distribution Estimation,"Yi Hao,         Alon Orlitsky","Distribution estimation is a statistical-learning cornerstone. Its classical min-max formulation minimizes the estimation error for the worst distribution, hence under-performs for practical distributions that, like power-law, are often rather simple. Modern research has therefore focused on two frameworks: structural estimation that improves learning accuracy by assuming a simple structure of the underlying distribution; and competitive, or instance-optimal, estimation that achieves the performance of a genie aided estimator up to a small excess error that vanishes as the sample size grows, regardless of the distribution. This paper combines and strengthens the two frameworks. It designs a single estimator whose excess error vanishes both at a universal rate as the sample size grows, as well as when the (unknown) distribution gets simpler. We show that the resulting algorithm significantly improves the performance guarantees for numerous competitive- and structural-estimation results. The algorithm runs in near-linear time and is robust to model misspecification and domain-symbol permutations.",http://proceedings.mlr.press/v97/hao19a.html,http://proceedings.mlr.press/v97/hao19a/hao19a.pdf,ICML
367,2019,Sample-Optimal Parametric Q-Learning Using Linearly Additive Features,"Lin Yang,         Mengdi Wang","Consider a Markov decision process (MDP) that admits a set of state-action features, which can linearly express the process’s probabilistic transition model. We propose a parametric Q-learning algorithm that finds an approximate-optimal policy using a sample size proportional to the feature dimension KKK and invariant with respect to the size of the state space. To further improve its sample efficiency, we exploit the monotonicity property and intrinsic noise structure of the Bellman operator, provided the existence of anchor state-actions that imply implicit non-negativity in the feature space. We augment the algorithm using techniques of variance reduction, monotonicity preservation, and confidence bounds. It is proved to find a policy which is ϵϵ\epsilon-optimal from any initial state with high probability using O˜(K/ϵ2(1−γ)3)O~(K/ϵ2(1−γ)3)\widetilde{O}(K/\epsilon^2(1-\gamma)^3) sample transitions for arbitrarily large-scale MDP with a discount factor γ∈(0,1)γ∈(0,1)\gamma\in(0,1). A matching information-theoretical lower bound is proved, confirming the sample optimality of the proposed method with respect to all parameters (up to polylog factors).",http://proceedings.mlr.press/v97/yang19b.html,http://proceedings.mlr.press/v97/yang19b/yang19b.pdf,ICML
368,2019,Learning Structured Decision Problems with Unawareness,"Craig Innes,         Alex Lascarides","Structured models of decision making often assume an agent is aware of all possible states and actions in advance. This assumption is sometimes untenable. In this paper, we learn Bayesian Decision Networks from both domain exploration and expert assertions in a way which guarantees convergence to optimal behaviour, even when the agent starts unaware of actions or belief variables that are critical to success. Our experiments show that our agent learns optimal behaviour on both small and large decision problems, and that allowing an agent to conserve information upon making new discoveries results in faster convergence.",http://proceedings.mlr.press/v97/innes19a.html,http://proceedings.mlr.press/v97/innes19a/innes19a.pdf,ICML
369,2019,EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis,"Chaoqi Wang,         Roger Grosse,         Sanja Fidler,         Guodong Zhang","Reducing the test time resource requirements of a neural network while preserving test accuracy is crucial for running inference on resource-constrained devices. To achieve this goal, we introduce a novel network reparameterization based on the Kronecker-factored eigenbasis (KFE), and then apply Hessian-based structured pruning methods in this basis. As opposed to existing Hessian-based pruning algorithms which do pruning in parameter coordinates, our method works in the KFE where different weights are approximately independent, enabling accurate pruning and fast computation. We demonstrate empirically the effectiveness of the proposed method through extensive experiments. In particular, we highlight that the improvements are especially significant for more challenging datasets and networks. With negligible loss of accuracy, an iterative-pruning version gives a 10x reduction in model size and a 8x reduction in FLOPs on wide ResNet32.",http://proceedings.mlr.press/v97/wang19g.html,http://proceedings.mlr.press/v97/wang19g/wang19g.pdf,ICML
370,2019,A Multitask Multiple Kernel Learning Algorithm for Survival Analysis with Application to Cancer Biology,"Onur Dereli,         Ceyda Oğuz,         Mehmet Gönen","Predictive performance of machine learning algorithms on related problems can be improved using multitask learning approaches. Rather than performing survival analysis on each data set to predict survival times of cancer patients, we developed a novel multitask approach based on multiple kernel learning (MKL). Our multitask MKL algorithm both works on multiple cancer data sets and integrates cancer-related pathways/gene sets into survival analysis. We tested our algorithm, which is named as Path2MSurv, on the Cancer Genome Atlas data sets analyzing gene expression profiles of 7,655 patients from 20 cancer types together with cancer-specific pathway/gene set collections. Path2MSurv obtained better or comparable predictive performance when benchmarked against random survival forest, survival support vector machine, and single-task variant of our algorithm. Path2MSurv has the ability to identify key pathways/gene sets in predicting survival times of patients from different cancer types.",http://proceedings.mlr.press/v97/dereli19a.html,http://proceedings.mlr.press/v97/dereli19a/dereli19a.pdf,ICML
371,2019,Statistical Foundations of Virtual Democracy,"Anson Kahng,         Min Kyung Lee,         Ritesh Noothigattu,         Ariel Procaccia,         Christos-Alexandros Psomas","Virtual democracy is an approach to automating decisions, by learning models of the preferences of individual people, and, at runtime, aggregating the predicted preferences of those people on the dilemma at hand. One of the key questions is which aggregation method – or voting rule – to use; we offer a novel statistical viewpoint that provides guidance. Specifically, we seek voting rules that are robust to prediction errors, in that their output on people’s true preferences is likely to coincide with their output on noisy estimates thereof. We prove that the classic Borda count rule is robust in this sense, whereas any voting rule belonging to the wide family of pairwise-majority consistent rules is not. Our empirical results further support, and more precisely measure, the robustness of Borda count.",http://proceedings.mlr.press/v97/kahng19a.html,http://proceedings.mlr.press/v97/kahng19a/kahng19a.pdf,ICML
372,2019,Bilinear Bandits with Low-rank Structure,"Kwang-Sung Jun,         Rebecca Willett,         Stephen Wright,         Robert Nowak","We introduce the bilinear bandit problem with low-rank structure in which an action takes the form of a pair of arms from two different entity types, and the reward is a bilinear function of the known feature vectors of the arms. The unknown in the problem is a d1d_1 by d2d_2 matrix Θ∗\mathbf{\Theta}^* that defines the reward, and has low rank r≪min{d1,d2}r \ll \min\{d_1,d_2\}. Determination of Θ∗\mathbf{\Theta}^* with this low-rank structure poses a significant challenge in finding the right exploration-exploitation tradeoff. In this work, we propose a new two-stage algorithm called “Explore-Subspace-Then-Refine” (ESTR). The first stage is an explicit subspace exploration, while the second stage is a linear bandit algorithm called “almost-low-dimensional OFUL” (LowOFUL) that exploits and further refines the estimated subspace via a regularization technique. We show that the regret of ESTR is ˜O((d1+d2)3/2√rT)\widetilde{\mathcal{O}}((d_1+d_2)^{3/2} \sqrt{r T}) where ˜O\widetilde{\mathcal{O}} hides logarithmic factors and TT is the time horizon, which improves upon the regret of ˜O(d1d2√T)\widetilde{\mathcal{O}}(d_1d_2\sqrt{T}) attained for a naïve linear bandit reduction. We conjecture that the regret bound of ESTR is unimprovable up to polylogarithmic factors, and our preliminary experiment shows that ESTR outperforms a naïve linear bandit reduction.",http://proceedings.mlr.press/v97/jun19a.html,http://proceedings.mlr.press/v97/jun19a/jun19a.pdf,ICML
373,2019,Coresets for Ordered Weighted Clustering,"Vladimir Braverman,         Shaofeng H.-C. Jiang,         Robert Krauthgamer,         Xuan Wu","We design coresets for Ordered k-Median, a generalization of classical clustering problems such as k-Median and k-Center. Its objective function is defined via the Ordered Weighted Averaging (OWA) paradigm of Yager (1988), where data points are weighted according to a predefined weight vector, but in order of their contribution to the objective (distance from the centers). A powerful data-reduction technique, called a coreset, is to summarize a point set XXX in RdRd\mathbb{R}^d into a small (weighted) point set X′X′X’, such that for every set of kkk potential centers, the objective value of the coreset X′X′X’ approximates that of XXX within factor 1±ϵ1±ϵ1\pm \epsilon. When there are multiple objectives (weights), the above standard coreset might have limited usefulness, whereas in a simultaneous coreset, the above approximation holds for all weights (in addition to all centers). Our main result is a construction of a simultaneous coreset of size Oϵ,d(k2log2|X|)Oϵ,d(k2log2|X|)O_{\epsilon, d}(k^2 \log^2 |X|) for Ordered k-Median. We validate our algorithm on a real geographical data set, and we find our coreset leads to a massive speedup of clustering computations, while maintaining high accuracy for a range of weights.",http://proceedings.mlr.press/v97/braverman19a.html,http://proceedings.mlr.press/v97/braverman19a/braverman19a.pdf,ICML
374,2019,"Look Ma, No Latent Variables: Accurate Cutset Networks via Compilation","Tahrima Rahman,         Shasha Jin,         Vibhav Gogate","Tractable probabilistic models obviate the need for unreliable approximate inference approaches and as a result often yield accurate query answers in practice. However, most tractable models that achieve state-of-the-art generalization performance (measured using test set likelihood score) use latent variables. Such models admit poly-time marginal (MAR) inference but do not admit poly-time (full) maximum-a-posteriori (MAP) inference. To address this problem, in this paper, we propose a novel approach for inducing cutset networks, a well-known tractable, highly interpretable representation that does not use latent variables and admits linear time MAR as well as MAP inference. Our approach addresses a major limitation of existing techniques that learn cutset networks from data in that their accuracy is quite low as compared to latent variable models such as ensembles of cutset networks and sum-product networks. The key idea in our approach is to construct deep cutset networks by not only learning them from data but also compiling them from a more accurate latent tractable model. We show experimentally that our new approach yields more accurate MAP estimates as compared with existing approaches and significantly improves the test set log-likelihood score of cutset networks bringing them closer in terms of generalization performance to latent variable models.",http://proceedings.mlr.press/v97/rahman19a.html,http://proceedings.mlr.press/v97/rahman19a/rahman19a.pdf,ICML
375,2019,Breaking the gridlock in Mixture-of-Experts: Consistent and Efficient Algorithms,"Ashok Makkuva,         Pramod Viswanath,         Sreeram Kannan,         Sewoong Oh","Mixture-of-Experts (MoE) is a widely popular model for ensemble learning and is a basic building block of highly successful modern neural networks as well as a component in Gated Recurrent Units (GRU) and Attention networks. However, present algorithms for learning MoE, including the EM algorithm and gradient descent, are known to get stuck in local optima. From a theoretical viewpoint, finding an efficient and provably consistent algorithm to learn the parameters remains a long standing open problem for more than two decades. In this paper, we introduce the first algorithm that learns the true parameters of a MoE model for a wide class of non-linearities with global consistency guarantees. While existing algorithms jointly or iteratively estimate the expert parameters and the gating parameters in the MoE, we propose a novel algorithm that breaks the deadlock and can directly estimate the expert parameters by sensing its echo in a carefully designed cross-moment tensor between the inputs and the output. Once the experts are known, the recovery of gating parameters still requires an EM algorithm; however, we show that the EM algorithm for this simplified problem, unlike the joint EM algorithm, converges to the true parameters. We empirically validate our algorithm on both the synthetic and real data sets in a variety of settings, and show superior performance to standard baselines.",http://proceedings.mlr.press/v97/makkuva19a.html,http://proceedings.mlr.press/v97/makkuva19a/makkuva19a.pdf,ICML
376,2019,Regret Circuits: Composability of Regret Minimizers,"Gabriele Farina,         Christian Kroer,         Tuomas Sandholm","Regret minimization is a powerful tool for solving large-scale problems; it was recently used in breakthrough results for large-scale extensive-form game solving. This was achieved by composing simplex regret minimizers into an overall regret-minimization framework for extensive-form game strategy spaces. In this paper we study the general composability of regret minimizers. We derive a calculus for constructing regret minimizers for composite convex sets that are obtained from convexity-preserving operations on simpler convex sets. We show that local regret minimizers for the simpler sets can be combined with additional regret minimizers into an aggregate regret minimizer for the composite set. As one application, we show that the CFR framework can be constructed easily from our framework. We also show ways to include curtailing (constraining) operations into our framework. For one, they enable the construction of CFR generalization for extensive-form games with general convex strategy constraints that can cut across decision points.",http://proceedings.mlr.press/v97/farina19b.html,http://proceedings.mlr.press/v97/farina19b/farina19b.pdf,ICML
377,2019,Variational Russian Roulette for Deep Bayesian Nonparametrics,"Kai Xu,         Akash Srivastava,         Charles Sutton","Bayesian nonparametric models provide a principled way to automatically adapt the complexity of a model to the amount of the data available, but computation in such models is difficult. Amortized variational approximations are appealing because of their computational efficiency, but current methods rely on a fixed finite truncation of the infinite model. This truncation level can be difficult to set, and also interacts poorly with amortized methods due to the over-pruning problem. Instead, we propose a new variational approximation, based on a method from statistical physics called Russian roulette sampling. This allows the variational distribution to adapt its complexity during inference, without relying on a fixed truncation level, and while still obtaining an unbiased estimate of the gradient of the original variational objective. We demonstrate this method on infinite sized variational auto-encoders using a Beta-Bernoulli (Indian buffet process) prior.",http://proceedings.mlr.press/v97/xu19e.html,http://proceedings.mlr.press/v97/xu19e/xu19e.pdf,ICML
378,2019,Fairness without Harm: Decoupled Classifiers with Preference Guarantees,"Berk Ustun,         Yang Liu,         David Parkes","In domains such as medicine, it can be acceptable for machine learning models to include sensitive attributes such as gender and ethnicity. In this work, we argue that when there is this kind of treatment disparity, then it should be in the best interest of each group. Drawing on ethical principles such as beneficence (""do the best"") and non-maleficence (""do no harm""), we show how to use sensitive attributes to train decoupled classifiers that satisfy preference guarantees. These guarantees ensure the majority of individuals in each group prefer their assigned classifier to (i) a pooled model that ignores group membership (rationality), and (ii) the model assigned to any other group (envy-freeness). We introduce a recursive procedure that adaptively selects group attributes for decoupling, and present formal conditions to ensure preference guarantees in terms of generalization error. We validate the effectiveness of the procedure on real-world datasets, showing that it improves accuracy without violating preference guarantees on test data.",http://proceedings.mlr.press/v97/ustun19a.html,http://proceedings.mlr.press/v97/ustun19a/ustun19a.pdf,ICML
379,2019,LegoNet: Efficient Convolutional Neural Networks with Lego Filters,"Zhaohui Yang,         Yunhe Wang,         Chuanjian Liu,         Hanting Chen,         Chunjing Xu,         Boxin Shi,         Chao Xu,         Chang Xu","This paper aims to build efficient convolutional neural networks using a set of Lego filters. Many successful building blocks, e.g., inception and residual modules, have been designed to refresh state-of-the-art records of CNNs on visual recognition tasks. Beyond these high-level modules, we suggest that an ordinary filter in the neural network can be upgraded to a sophisticated module as well. Filter modules are established by assembling a shared set of Lego filters that are often of much lower dimensions. Weights in Lego filters and binary masks to stack Lego filters for these filter modules can be simultaneously optimized in an end-to-end manner as usual. Inspired by network engineering, we develop a split-transform-merge strategy for an efficient convolution by exploiting intermediate Lego feature maps. The compression and acceleration achieved by Lego Networks using the proposed Lego filters have been theoretically discussed. Experimental results on benchmark datasets and deep models demonstrate the advantages of the proposed Lego filters and their potential real-world applications on mobile devices.",http://proceedings.mlr.press/v97/yang19c.html,http://proceedings.mlr.press/v97/yang19c/yang19c.pdf,ICML
380,2019,Simple Stochastic Gradient Methods for Non-Smooth Non-Convex Regularized Optimization,"Michael Metel,         Akiko Takeda","Our work focuses on stochastic gradient methods for optimizing a smooth non-convex loss function with a non-smooth non-convex regularizer. Research on this class of problem is quite limited, and until recently no non-asymptotic convergence results have been reported. We present two simple stochastic gradient algorithms, for finite-sum and general stochastic optimization problems, which have superior convergence complexities compared to the current state-of-the-art. We also compare our algorithms’ performance in practice for empirical risk minimization.",http://proceedings.mlr.press/v97/metel19a.html,http://proceedings.mlr.press/v97/metel19a/metel19a.pdf,ICML
381,2019,Learning Generative Models across Incomparable Spaces,"Charlotte Bunne,         David Alvarez-Melis,         Andreas Krause,         Stefanie Jegelka","Generative Adversarial Networks have shown remarkable success in learning a distribution that faithfully recovers a reference distribution in its entirety. However, in some cases, we may want to only learn some aspects (e.g., cluster or manifold structure), while modifying others (e.g., style, orientation or dimension). In this work, we propose an approach to learn generative models across such incomparable spaces, and demonstrate how to steer the learned distribution towards target properties. A key component of our model is the Gromov-Wasserstein distance, a notion of discrepancy that compares distributions relationally rather than absolutely. While this framework subsumes current generative models in identically reproducing distributions, its inherent flexibility allows application to tasks in manifold learning, relational learning and cross-domain learning.",http://proceedings.mlr.press/v97/bunne19a.html,http://proceedings.mlr.press/v97/bunne19a/bunne19a.pdf,ICML
382,2019,White-box vs Black-box: Bayes Optimal Strategies for Membership Inference,"Alexandre Sablayrolles,         Matthijs Douze,         Cordelia Schmid,         Yann Ollivier,         Herve Jegou","Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet.",http://proceedings.mlr.press/v97/sablayrolles19a.html,http://proceedings.mlr.press/v97/sablayrolles19a/sablayrolles19a.pdf,ICML
383,2019,Adversarial Attacks on Node Embeddings via Graph Poisoning,"Aleksandar Bojchevski,         Stephan Günnemann","The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.",http://proceedings.mlr.press/v97/bojchevski19a.html,http://proceedings.mlr.press/v97/bojchevski19a/bojchevski19a.pdf,ICML
384,2019,EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE,"Chao Ma,         Sebastian Tschiatschek,         Konstantina Palla,         Jose Miguel Hernandez-Lobato,         Sebastian Nowozin,         Cheng Zhang","Many real-life decision making situations allow further relevant information to be acquired at a specific cost, for example, in assessing the health status of a patient we may decide to take additional measurements such as diagnostic tests or imaging scans before making a final assessment. Acquiring more relevant information enables better decision making, but may be costly. How can we trade off the desire to make good decisions by acquiring further information with the cost of performing that acquisition? To this end, we propose a principled framework, named EDDI (Efficient Dynamic Discovery of high-value Information), based on the theory of Bayesian experimental design. In EDDI, we propose a novel partial variational autoencoder (Partial VAE) to predict missing data entries problematically given any subset of the observed ones, and combine it with an acquisition function that maximizes expected information gain on a set of target variables. We show cost reduction at the same decision quality and improved decision quality at the same cost in multiple machine learning benchmarks and two real-world health-care applications.",http://proceedings.mlr.press/v97/ma19c.html,http://proceedings.mlr.press/v97/ma19c/ma19c.pdf,ICML
385,2019,Improving Model Selection by Employing the Test Data,"Max Westphal,         Werner Brannath","Model selection and evaluation are usually strictly separated by means of data splitting to enable an unbiased estimation and a simple statistical inference for the unknown generalization performance of the final prediction model. We investigate the properties of novel evaluation strategies, namely when the final model is selected based on empirical performances on the test data. To guard against selection induced overoptimism, we employ a parametric multiple test correction based on the approximate multivariate distribution of performance estimates. Our numerical experiments involve training common machine learning algorithms (EN, CART, SVM, XGB) on various artificial classification tasks. At its core, our proposed approach improves model selection in terms of the expected final model performance without introducing overoptimism. We furthermore observed a higher probability for a successful evaluation study, making it easier in practice to empirically demonstrate a sufficiently high predictive performance.",http://proceedings.mlr.press/v97/westphal19a.html,http://proceedings.mlr.press/v97/westphal19a/westphal19a.pdf,ICML
386,2019,"On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference","Rohin Shah,         Noah Gundotra,         Pieter Abbeel,         Anca Dragan","Our goal is for agents to optimize the right reward function, despite how difficult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with specific assumptions, and instead use a purely data-driven approach. We decided to put this to the test – rather than relying on assumptions about which specific bias the demonstrator has when planning, we instead learn the demonstrator’s planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed findings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this benefit is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. Code is available at https://tinyurl.com/learningbiases.",http://proceedings.mlr.press/v97/shah19a.html,http://proceedings.mlr.press/v97/shah19a/shah19a.pdf,ICML
387,2019,On the Universality of Invariant Networks,"Haggai Maron,         Ethan Fetaya,         Nimrod Segol,         Yaron Lipman","Constraining linear layers in neural networks to respect symmetry transformations from a group GGG is a common design principle for invariant networks that has found many applications in machine learning. 		 In this paper, we consider a fundamental question that has received very little attention to date: Can these networks approximate any (continuous) invariant function? 		 We tackle the rather general case where G≤SnG≤SnG\leq S_n (an arbitrary subgroup of the symmetric group) that acts on \Rn\Rn\R^n by permuting coordinates. This setting includes several recent popular invariant networks. We present two main results: First, GGG-invariant networks are universal if high-order tensors are allowed. Second, there are groups GGG for which higher-order tensors are unavoidable for obtaining universality. 		 GGG-invariant networks consisting of only first-order tensors are of special interest due to their practical value. We conclude the paper by proving a necessary condition for the universality of GGG-invariant networks that incorporate only first-order tensors. Lastly, we propose a conjecture stating that this condition is also sufficient.",http://proceedings.mlr.press/v97/maron19a.html,http://proceedings.mlr.press/v97/maron19a/maron19a.pdf,ICML
388,2019,Functional Transparency for Structured Data: a Game-Theoretic Approach,"Guang-He Lee,         Wengong Jin,         David Alvarez-Melis,         Tommi Jaakkola","We provide a new approach to training neural models to exhibit transparency in a well-defined, functional manner. Our approach naturally operates over structured data and tailors the predictor, functionally, towards a chosen family of (local) witnesses. The estimation problem is setup as a co-operative game between an unrestricted predictor such as a neural network, and a set of witnesses chosen from the desired transparent family. The goal of the witnesses is to highlight, locally, how well the predictor conforms to the chosen family of functions, while the predictor is trained to minimize the highlighted discrepancy. We emphasize that the predictor remains globally powerful as it is only encouraged to agree locally with locally adapted witnesses. We analyze the effect of the proposed approach, provide example formulations in the context of deep graph and sequence models, and empirically illustrate the idea in chemical property prediction, temporal modeling, and molecule representation learning.",http://proceedings.mlr.press/v97/lee19b.html,http://proceedings.mlr.press/v97/lee19b/lee19b.pdf,ICML
389,2019,Humor in Word Embeddings: Cockamamie Gobbledegook for Nincompoops,"Limor Gultchin,         Genevieve Patterson,         Nancy Baym,         Nathaniel Swinger,         Adam Kalai","While humor is often thought to be beyond the reach of Natural Language Processing, we show that several aspects of single-word humor correlate with simple linear directions in Word Embeddings. In particular: (a) the word vectors capture multiple aspects discussed in humor theories from various disciplines; (b) each individual’s sense of humor can be represented by a vector, which can predict differences in people’s senses of humor on new, unrated, words; and (c) upon clustering humor ratings of multiple demographic groups, different humor preferences emerge across the different groups. Humor ratings are taken from the work of Engelthaler and Hills (2017) as well as from an original crowdsourcing study of 120,000 words. Our dataset further includes annotations for the theoretically-motivated humor features we identify.",http://proceedings.mlr.press/v97/gultchin19a.html,http://proceedings.mlr.press/v97/gultchin19a/gultchin19a.pdf,ICML
390,2019,Robust Inference via Generative Classifiers for Handling Noisy Labels,"Kimin Lee,         Sukmin Yun,         Kibok Lee,         Honglak Lee,         Bo Li,         Jinwoo Shin","Large-scale datasets may contain significant proportions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks (DNNs) poorly generalize from such noisy training datasets. To mitigate the issue, we propose a novel inference method, termed Robust Generative classifier (RoG), applicable to any discriminative (e.g., softmax) neural classifier pre-trained on noisy datasets. In particular, we induce a generative classifier on top of hidden feature spaces of the pre-trained DNNs, for obtaining a more robust decision boundary. By estimating the parameters of generative classifier using the minimum covariance determinant estimator, we significantly improve the classification accuracy with neither re-training of the deep model nor changing its architectures. With the assumption of Gaussian distribution for features, we prove that RoG generalizes better than baselines under noisy labels. Finally, we propose the ensemble version of RoG to improve its performance by investigating the layer-wise characteristics of DNNs. Our extensive experimental results demonstrate the superiority of RoG given different learning models optimized by several training techniques to handle diverse scenarios of noisy labels.",http://proceedings.mlr.press/v97/lee19f.html,http://proceedings.mlr.press/v97/lee19f/lee19f.pdf,ICML
391,2019,Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations,"Francesco Locatello,         Stefan Bauer,         Mario Lucic,         Gunnar Raetsch,         Sylvain Gelly,         Bernhard Schölkopf,         Olivier Bachem","The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 120001200012000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.",http://proceedings.mlr.press/v97/locatello19a.html,http://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf,ICML
392,2019,Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models,"Eldan Cohen,         Christopher Beck","Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, work on a number of applications has found that the quality of the highest probability hypothesis found by beam search degrades with large beam widths. We perform an empirical study of the behavior of beam search across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early, very low probability tokens that are followed by a sequence of tokens with higher (conditional) probability. We show that, empirically, such sequences are more likely to have a lower evaluation score than lower probability sequences without this pattern. Using the notion of search discrepancies from heuristic search, we hypothesize that large discrepancies are the cause of the performance degradation. We show that this hypothesis generalizes the previous ones in machine translation and image captioning. To validate our hypothesis, we show that constraining beam search to avoid large discrepancies eliminates the performance degradation.",http://proceedings.mlr.press/v97/cohen19a.html,http://proceedings.mlr.press/v97/cohen19a/cohen19a.pdf,ICML
393,2019,Adversarial camera stickers: A physical camera-based attack on deep learning systems,"Juncheng Li,         Frank Schmidt,         Zico Kolter","Recent work has documented the susceptibility of deep learning systems to adversarial examples, but most such attacks directly manipulate the digital input to a classifier. Although a smaller line of work considers physical adversarial attacks, in all cases these involve manipulating the object of interest, e.g., putting a physical sticker on an object to misclassify it, or manufacturing an object specifically intended to be misclassified. In this work, we consider an alternative question: is it possible to fool deep classifiers, over all perceived objects of a certain type, by physically manipulating the camera itself? We show that by placing a carefully crafted and mainly-translucent sticker over the lens of a camera, one can create universal perturbations of the observed images that are inconspicuous, yet misclassify target objects as a different (targeted) class. To accomplish this, we propose an iterative procedure for both updating the attack perturbation (to make it adversarial for a given classifier), and the threat model itself (to ensure it is physically realizable). For example, we show that we can achieve physically-realizable attacks that fool ImageNet classifiers in a targeted fashion 49.6% of the time. This presents a new class of physically-realizable threat models to consider in the context of adversarially robust machine learning. Our demo video can be viewed at: https://youtu.be/wUVmL33Fx54",http://proceedings.mlr.press/v97/li19j.html,http://proceedings.mlr.press/v97/li19j/li19j.pdf,ICML
394,2019,Relational Pooling for Graph Representations,"Ryan Murphy,         Balasubramaniam Srinivasan,         Vinayak Rao,         Bruno Ribeiro","This work generalizes graph neural networks (GNNs) beyond those based on the Weisfeiler-Lehman (WL) algorithm, graph Laplacians, and diffusions. Our approach, denoted Relational Pooling (RP), draws from the theory of finite partial exchangeability to provide a framework with maximal representation power for graphs. RP can work with existing graph representation models and, somewhat counterintuitively, can make them even more powerful than the original WL isomorphism test. Additionally, RP allows architectures like Recurrent Neural Networks and Convolutional Neural Networks to be used in a theoretically sound approach for graph classification. We demonstrate improved performance of RP-based graph representations over state-of-the-art methods on a number of tasks.",http://proceedings.mlr.press/v97/murphy19a.html,http://proceedings.mlr.press/v97/murphy19a/murphy19a.pdf,ICML
395,2019,More Efficient Off-Policy Evaluation through Regularized Targeted Learning,"Aurelien Bibaut,         Ivana Malenica,         Nikos Vlassis,         Mark Van Der Laan","We study the problem of off-policy evaluation (OPE) in Reinforcement Learning (RL), where the aim is to estimate the performance of a new policy given historical data that may have been generated by a different policy, or policies. In particular, we introduce a novel doubly-robust estimator for the OPE problem in RL, based on the Targeted Maximum Likelihood Estimation principle from the statistical causal inference literature. We also introduce several variance reduction techniques that lead to impressive performance gains in off-policy evaluation. We show empirically that our estimator uniformly wins over existing off-policy evaluation methods across multiple RL environments and various levels of model misspecification. Finally, we further the existing theoretical analysis of estimators for the RL off-policy estimation problem by showing their OP(1/√n)O_P(1/\sqrt{n}) rate of convergence and characterizing their asymptotic distribution.",http://proceedings.mlr.press/v97/bibaut19a.html,http://proceedings.mlr.press/v97/bibaut19a/bibaut19a.pdf,ICML
396,2019,Screening rules for Lasso with non-convex Sparse Regularizers,"Alain Rakotomamonjy,         Gilles Gasso,         Joseph Salmon","Leveraging on the convexity of the Lasso problem, screening rules help in accelerating solvers by discarding irrelevant variables, during the optimization process. However, because they provide better theoretical guarantees in identifying relevant	variables, several non-convex regularizers for the Lasso have been proposed in the literature. This work is the first that introduces a screening rule strategy into a non-convex Lasso solver. The approach we propose is based on a iterative majorization-minimization (MM) strategy that includes a screening rule in the inner solver and a condition for propagating screened variables between iterations of MM. In addition to improve efficiency of solvers, we also provide guarantees that the inner solver is able to identify the zeros components of its critical point in finite time. Our experimental analysis illustrates the significant computational gain brought by the new screening rule compared to classical coordinate-descent or proximal gradient descent methods.",http://proceedings.mlr.press/v97/rakotomamonjy19a.html,http://proceedings.mlr.press/v97/rakotomamonjy19a/rakotomamonjy19a.pdf,ICML
397,2019,Generalized No Free Lunch Theorem for Adversarial Robustness,Elvis Dohmatob,"This manuscript presents some new impossibility results on adversarial robustness in machine learning, a very important yet largely open problem. We show that if conditioned on a class label the data distribution satisfies the W2W2W_2 Talagrand transportation-cost inequality (for example, this condition is satisfied if the conditional distribution has density which is log-concave; is the uniform measure on a compact Riemannian manifold with positive Ricci curvature, any classifier can be adversarially fooled with high probability once the perturbations are slightly greater than the natural noise level in the problem. We call this result The Strong ""No Free Lunch"" Theorem as some recent results (Tsipras et al. 2018, Fawzi et al. 2018, etc.) on the subject can be immediately recovered as very particular cases. Our theoretical bounds are demonstrated on both simulated and real data (MNIST). We conclude the manuscript with some speculation on possible future research directions.",http://proceedings.mlr.press/v97/dohmatob19a.html,http://proceedings.mlr.press/v97/dohmatob19a/dohmatob19a.pdf,ICML
398,2019,Bridging Theory and Algorithm for Domain Adaptation,"Yuchen Zhang,         Tianle Liu,         Mingsheng Long,         Michael Jordan","This paper addresses the problem of unsupervised domain adaption from theoretical and algorithmic perspectives. Existing domain adaptation theories naturally imply minimax optimization algorithms, which connect well with the domain adaptation methods based on adversarial learning. However, several disconnections still exist and form the gap between theory and algorithm. We extend previous theories (Mansour et al., 2009c; Ben-David et al., 2010) to multiclass classification in domain adaptation, where classifiers based on the scoring functions and margin loss are standard choices in algorithm design. We introduce Margin Disparity Discrepancy, a novel measurement with rigorous generalization bounds, tailored to the distribution comparison with the asymmetric margin loss, and to the minimax optimization for easier training. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adaptation, successfully bridging the gap between theory and algorithm. A series of empirical studies show that our algorithm achieves the state of the art accuracies on challenging domain adaptation tasks.",http://proceedings.mlr.press/v97/zhang19i.html,http://proceedings.mlr.press/v97/zhang19i/zhang19i.pdf,ICML
399,2019,Learning to select for a predefined ranking,"Aleksei Ustimenko,         Aleksandr Vorobev,         Gleb Gusev,         Pavel Serdyukov","In this paper, we formulate a novel problem of learning to select a set of items maximizing the quality of their ordered list, where the order is predefined by some explicit rule. Unlike the classic information retrieval problem, in our setting, the predefined order of items in the list may not correspond to their quality in general. For example, this is a dominant scenario in personalized news and social media feeds, where items are ordered by publication time in a user interface. We propose new theoretically grounded algorithms based on direct optimization of the resulting list quality. Our offline and online experiments with a large-scale product search engine demonstrate the overwhelming advantage of our methods over the baselines in terms of all key quality metrics.",http://proceedings.mlr.press/v97/vorobev19a.html,http://proceedings.mlr.press/v97/vorobev19a/vorobev19a.pdf,ICML
400,2019,Emerging Convolutions for Generative Normalizing Flows,"Emiel Hoogeboom,         Rianne Van Den Berg,         Max Welling","Generative flows are attractive because they admit exact likelihood optimization and efficient image synthesis. Recently, Kingma & Dhariwal (2018) demonstrated with Glow that generative flows are capable of generating high quality images. We generalize the 1 {\texttimes} 1 convolutions proposed in Glow to invertible d {\texttimes} d convolutions, which are more flexible since they operate on both channel and spatial axes. We propose two methods to produce invertible convolutions, that have receptive fields identical to standard convolutions: Emerging convolutions are obtained by chaining specific autoregressive convolutions, and periodic convolutions are decoupled in the frequency domain. Our experiments show that the flexibility of d {\texttimes} d convolutions significantly improves the performance of generative flow models on galaxy images, CIFAR10 and ImageNet.",http://proceedings.mlr.press/v97/hoogeboom19a.html,http://proceedings.mlr.press/v97/hoogeboom19a/hoogeboom19a.pdf,ICML
401,2019,A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks,"Umut Simsekli,         Levent Sagun,         Mert Gurbuzbalaban","The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumption is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed αα\alpha-stable random variable. Accordingly, we propose to analyze SGD as an SDE driven by a Lévy motion. Such SDEs can incur ‘jumps’, which force the SDE transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the αα\alpha-stable assumption, we conduct experiments on common deep learning scenarios and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that SGD prefers wide minima.",http://proceedings.mlr.press/v97/simsekli19a.html,http://proceedings.mlr.press/v97/simsekli19a/simsekli19a.pdf,ICML
402,2019,Accelerated Flow for Probability Distributions,"Amirhossein Taghvaei,         Prashant Mehta","This paper presents a methodology and numerical algorithms for constructing accelerated gradient flows on the space of probability distributions. In particular, we extend the recent variational formulation of accelerated methods in (Wibisono et al., 2016) from vector valued variables to probability distributions. The variational problem is modeled as a mean-field optimal control problem. A quantitative estimate on the asymptotic convergence rate is provided based on a Lyapunov function construction, when the objective functional is displacement convex. An important special case is considered where the objective functional is the relative entropy. For this case, two numerical approximations are presented to implement the Hamilton’s equations as a system of N interacting particles. The algorithm is numerically illustrated and compared with the MCMC and Hamiltonian MCMC algorithms.",http://proceedings.mlr.press/v97/taghvaei19a.html,http://proceedings.mlr.press/v97/taghvaei19a/taghvaei19a.pdf,ICML
403,2019,Finding Options that Minimize Planning Time,"Yuu Jinnai,         David Abel,         David Hershkowitz,         Michael Littman,         George Konidaris","We formalize the problem of selecting the optimal set of options for planning as that of computing the smallest set of options so that planning converges in less than a given maximum of value-iteration passes. We first show that the problem is \NP\NP\NP-hard, even if the task is constrained to be deterministic—the first such complexity result for option discovery. We then present the first polynomial-time boundedly suboptimal approximation algorithm for this setting, and empirically evaluate it against both the optimal options and a representative collection of heuristic approaches in simple grid-based domains.",http://proceedings.mlr.press/v97/jinnai19a.html,http://proceedings.mlr.press/v97/jinnai19a/jinnai19a.pdf,ICML
404,2019,Fast Algorithm for Generalized Multinomial Models with Ranking Data,"Jiaqi Gu,         Guosheng Yin","We develop a framework of generalized multinomial models, which includes both the popular Plackett–Luce model and Bradley–Terry model as special cases. From a theoretical perspective, we prove that the maximum likelihood estimator (MLE) under generalized multinomial models corresponds to the stationary distribution of an inhomogeneous Markov chain uniquely. Based on this property, we propose an iterative algorithm that is easy to implement and interpret, and is guaranteed to converge. Numerical experiments on synthetic data and real data demonstrate the advantages of our Markov chain based algorithm over existing ones. Our algorithm converges to the MLE with fewer iterations and at a faster convergence rate. The new algorithm is readily applicable to problems such as page ranking or sports ranking data.",http://proceedings.mlr.press/v97/gu19a.html,http://proceedings.mlr.press/v97/gu19a/gu19a.pdf,ICML
405,2019,A Composite Randomized Incremental Gradient Method,"Junyu Zhang,         Lin Xiao","We consider the problem of minimizing the composition of a smooth function (which can be nonconvex) and a smooth vector mapping, where both of them can be express as the average of a large number of components. We propose a composite randomized incremental gradient method by extending the SAGA framework. The gradient sample complexity of our method matches that of several recently developed methods based on SVRG in the general case. However, for structured problems where linear convergence rates can be obtained, our method can be much better for ill-conditioned problems. In addition, when the finite-sum structure only appear for the inner mapping, the sample complexity of our method is the same as that of SAGA for minimizing finite sum of smooth nonconvex functions, despite the additional outer composition and the stochastic composite gradients being biased in our case.",http://proceedings.mlr.press/v97/zhang19n.html,http://proceedings.mlr.press/v97/zhang19n/zhang19n.pdf,ICML
406,2019,Categorical Feature Compression via Submodular Optimization,"Mohammadhossein Bateni,         Lin Chen,         Hossein Esfandiari,         Thomas Fu,         Vahab Mirrokni,         Afshin Rostamizadeh","In the era of big data, learning from categorical features with very large vocabularies (e.g., 28 million for the Criteo click prediction dataset) has become a practical challenge for machine learning researchers and practitioners. We design a highly-scalable vocabulary compression algorithm that seeks to maximize the mutual information between the compressed categorical feature and the target binary labels and we furthermore show that its solution is guaranteed to be within a 1−1/e≈631−1/e≈631-1/e \approx 63% factor of the global optimal solution. Although in some settings, entropy-based set functions are known to be submodular, this is not the case for the mutual information objective we consider (mutual information with respect to the target labels). To address this, we introduce a novel re-parametrization of the mutual information objective, which we prove is submodular, and also design a data structure to query the submodular function in amortized O(logn)O(log⁡n)O(\log n ) time (where nnn is the input vocabulary size). Our complete algorithm is shown to operate in O(nlogn)O(nlog⁡n)O(n \log n ) time. Additionally, we design a distributed implementation in which the query data structure is decomposed across O(k)O(k)O(k) machines such that each machine only requires O(nk)O(nk)O(\frac n k) space, while still preserving the approximation guarantee and using only logarithmic rounds of computation. We also provide analysis of simple alternative heuristic compression methods to demonstrate they cannot achieve any approximation guarantee. Using the large-scale Criteo learning task, we demonstrate better performance in retaining mutual information and also verify competitive learning performance compared to other baseline methods.",http://proceedings.mlr.press/v97/bateni19a.html,http://proceedings.mlr.press/v97/bateni19a/bateni19a.pdf,ICML
407,2019,RaFM: Rank-Aware Factorization Machines,"Xiaoshuang Chen,         Yin Zheng,         Jiaxing Wang,         Wenye Ma,         Junzhou Huang","Fatorization machines (FM) are a popular model class to learn pairwise interactions by a low-rank approximation. Different from existing FM-based approaches which use a fixed rank for all features, this paper proposes a Rank-Aware FM (RaFM) model which adopts pairwise interactions from embeddings with different ranks. The proposed model achieves a better performance on real-world datasets where different features have significantly varying frequencies of occurrences. Moreover, we prove that the RaFM model can be stored, evaluated, and trained as efficiently as one single FM, and under some reasonable conditions it can be even significantly more efficient than FM. RaFM improves the performance of FMs in both regression tasks and classification tasks while incurring less computational burden, therefore also has attractive potential in industrial applications.",http://proceedings.mlr.press/v97/chen19n.html,http://proceedings.mlr.press/v97/chen19n/chen19n.pdf,ICML
408,2019,Rehashing Kernel Evaluation in High Dimensions,"Paris Siminelakis,         Kexin Rong,         Peter Bailis,         Moses Charikar,         Philip Levis","Kernel methods are effective but do not scale well to large scale data, especially in high dimensions where the geometric data structures used to accelerate kernel evaluation suffer from the curse of dimensionality. Recent theoretical advances have proposed fast kernel evaluation algorithms leveraging hashing techniques with worst-case asymptotic improvements. However, these advances are largely confined to the theoretical realm due to concerns such as super-linear preprocessing time and diminishing gains in non-worst case datasets. In this paper, we close the gap between theory and practice by addressing these challenges via provable and practical procedures for adaptive sample size selection, preprocessing time reduction, and refined variance bounds that quantify the data-dependent performance of random sampling and hashing-based kernel evaluation methods. Our experiments show that these new tools offer up to 10×10×10\times improvement in evaluation time on a range of synthetic and real-world datasets.",http://proceedings.mlr.press/v97/siminelakis19a.html,http://proceedings.mlr.press/v97/siminelakis19a/siminelakis19a.pdf,ICML
409,2019,Robust Decision Trees Against Adversarial Examples,"Hongge Chen,         Huan Zhang,         Duane Boning,         Cho-Jui Hsieh","Although adversarial examples and model robust-ness have been extensively studied in the context of neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversarial examples is still limited. In this paper, we show that tree-based models are also vulnerable to adversarial examples and develop a novel algorithm to learn robust trees. At its core, our method aims to optimize the performance under the worst-case perturbation of input features, which leads to a max-min saddle point problem. Incorporating this saddle point objective into the decision tree building procedure is non-trivial due to the discrete nature of trees{—}a naive approach to finding the best split according to this saddle point objective will take exponential time. To make our approach practical and scalable, we propose efficient tree building algorithms by approximating the inner minimizer in the saddlepoint problem, and present efficient implementations for classical information gain based trees as well as state-of-the-art tree boosting systems such as XGBoost. Experimental results on real world datasets demonstrate that the proposed algorithms can significantly improve the robustness of tree-based models against adversarial examples.",http://proceedings.mlr.press/v97/chen19m.html,http://proceedings.mlr.press/v97/chen19m/chen19m.pdf,ICML
410,2019,Demystifying Dropout,"Hongchang Gao,         Jian Pei,         Heng Huang","Dropout is a popular technique to train large-scale deep neural networks to alleviate the overfitting problem. To disclose the underlying reasons for its gain, numerous works have tried to explain it from different perspectives. In this paper, unlike existing works, we explore it from a new perspective to provide new insight into this line of research. In detail, we disentangle the forward and backward pass of dropout. Then, we find that these two passes need different levels of noise to improve the generalization performance of deep neural networks. Based on this observation, we propose the augmented dropout which employs different dropping strategies in the forward and backward pass. Experimental results have verified the effectiveness of our proposed method.",http://proceedings.mlr.press/v97/gao19d.html,http://proceedings.mlr.press/v97/gao19d/gao19d.pdf,ICML
411,2019,Calibrated Model-Based Deep Reinforcement Learning,"Ali Malik,         Volodymyr Kuleshov,         Jiaming Song,         Danny Nemer,         Harlan Seymour,         Stefano Ermon","Estimates of predictive uncertainty are important for accurate model-based planning and reinforcement learning. However, predictive uncertainties — especially ones derived from modern deep learning systems — can be inaccurate and impose a bottleneck on performance. This paper explores which uncertainties are needed for model-based reinforcement learning and argues that ideal uncertainties should be calibrated, i.e. their probabilities should match empirical frequencies of predicted events. We describe a simple way to augment any model-based reinforcement learning agent with a calibrated model and show that doing so consistently improves planning, sample complexity, and exploration. On the \textsc{HalfCheetah} MuJoCo task, our system achieves state-of-the-art performance using 50% fewer samples than the current leading approach. Our findings suggest that calibration can improve the performance of model-based reinforcement learning with minimal computational and implementation overhead.",http://proceedings.mlr.press/v97/malik19a.html,http://proceedings.mlr.press/v97/malik19a/malik19a.pdf,ICML
412,2019,SGD: General Analysis and Improved Rates,"Robert Mansel Gower,         Nicolas Loizou,         Xun Qian,         Alibek Sailanbayev,         Egor Shulgin,         Peter Richtárik","We propose a general yet simple theorem describing the convergence of SGD under the arbitrary sampling paradigm. Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a specific probability law governing the data selection rule used to form minibatches. This is the first time such an analysis is performed, and most of our variants of SGD were never explicitly considered in the literature before. Our analysis relies on the recently introduced notion of expected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By specializing our theorem to different mini-batching strategies, such as sampling with replacement and independent sampling, we derive exact expressions for the stepsize as a function of the mini-batch size. With this we can also determine the mini-batch size that optimizes the total complexity, and show explicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the optimal mini-batch size. For zero variance, the optimal mini-batch size is one. Moreover, we prove insightful stepsize-switching rules which describe when one should switch from a constant to a decreasing stepsize regime.",http://proceedings.mlr.press/v97/qian19b.html,http://proceedings.mlr.press/v97/qian19b/qian19b.pdf,ICML
413,2019,Training Neural Networks with Local Error Signals,"Arild Nøkland,         Lars Hiller Eidnes","Supervised training of neural networks for classification is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is back-propagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss functions. In this paper we demonstrate, for the first time, that layer-wise training can approach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses help with optimization in the context of local learning. Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be transported back to hidden layers. A completely backprop free variant outperforms previously reported results among methods aiming for higher biological plausibility.",http://proceedings.mlr.press/v97/nokland19a.html,http://proceedings.mlr.press/v97/nokland19a/nokland19a.pdf,ICML
414,2019,Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians,Vardan Papyan,"We expose a structure in deep classifying neural networks in the derivative of the logits with respect to the parameters of the model, which is used to explain the existence of outliers in the spectrum of the Hessian. Previous works decomposed the Hessian into two components, attributing the outliers to one of them, the so-called Covariance of gradients. We show this term is not a Covariance but a second moment matrix, i.e., it is influenced by means of gradients. These means possess an additive two-way structure that is the source of the outliers in the spectrum. This structure can be used to approximate the principal subspace of the Hessian using certain ""averaging"" operations, avoiding the need for high-dimensional eigenanalysis. We corroborate this claim across different datasets, architectures and sample sizes.",http://proceedings.mlr.press/v97/papyan19a.html,http://proceedings.mlr.press/v97/papyan19a/papyan19a.pdf,ICML
415,2019,Active Learning with Disagreement Graphs,"Corinna Cortes,         Giulia Desalvo,         Mehryar Mohri,         Ningshan Zhang,         Claudio Gentile","We present two novel enhancements of an online importance-weighted active learning algorithm IWAL, using the properties of disagreements among hypotheses. The first enhancement, IWALD, prunes the hypothesis set with a more aggressive strategy based on the disagreement graph. We show that IWAL-D improves the generalization performance and the label complexity of the original IWAL, and quantify the improvement in terms of the disagreement graph coefficient. The second enhancement, IZOOM, further improves IWAL-D by adaptively zooming into the current version space and thus reducing the best-in-class error. We show that IZOOM admits favorable theoretical guarantees with the changing hypothesis set. We report experimental results on multiple datasets and demonstrate that the proposed algorithms achieve better test performances than IWAL given the same amount of labeling budget.",http://proceedings.mlr.press/v97/cortes19b.html,http://proceedings.mlr.press/v97/cortes19b/cortes19b.pdf,ICML
416,2019,Discovering Context Effects from Raw Choice Data,"Arjun Seshadri,         Alex Peysakhovich,         Johan Ugander","Many applications in preference learning assume that decisions come from the maximization of a stable utility function. Yet a large experimental literature shows that individual choices and judgements can be affected by “irrelevant” aspects of the context in which they are made. An important class of such contexts is the composition of the choice set. In this work, our goal is to discover such choice set effects from raw choice data. We introduce an extension of the Multinomial Logit (MNL) model, called the context dependent random utility model (CDM), which allows for a particular class of choice set effects. We show that the CDM can be thought of as a second-order approximation to a general choice system, can be inferred optimally using maximum likelihood and, importantly, is easily interpretable. We apply the CDM to both real and simulated choice data to perform principled exploratory analyses for the presence of choice set effects.",http://proceedings.mlr.press/v97/seshadri19a.html,http://proceedings.mlr.press/v97/seshadri19a/seshadri19a.pdf,ICML
417,2019,Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data,"Vatsal Sharan,         Kai Sheng Tai,         Peter Bailis,         Gregory Valiant","What learning algorithms can be run directly on compressively-sensed data? In this work, we consider the question of accurately and efficiently computing low-rank matrix or tensor factorizations given data compressed via random projections. We examine the approach of first performing factorization in the compressed domain, and then reconstructing the original high-dimensional factors from the recovered (compressed) factors. In both the matrix and tensor settings, we establish conditions under which this natural approach will provably recover the original factors. While it is well-known that random projections preserve a number of geometric properties of a dataset, our work can be viewed as showing that they can also preserve certain solutions of non-convex, NP-Hard problems like non-negative matrix factorization. We support these theoretical results with experiments on synthetic data and demonstrate the practical applicability of compressed factorization on real-world gene expression and EEG time series datasets.",http://proceedings.mlr.press/v97/sharan19a.html,http://proceedings.mlr.press/v97/sharan19a/sharan19a.pdf,ICML
418,2019,Learning Optimal Linear Regularizers,Matthew Streeter,"We present algorithms for efficiently learning regularizers that improve generalization. Our approach is based on the insight that regularizers can be viewed as upper bounds on the generalization gap, and that reducing the slack in the bound can improve performance on test data. For a broad class of regularizers, the hyperparameters that give the best upper bound can be computed using linear programming. Under certain Bayesian assumptions, solving the LP lets us ""jump"" to the optimal hyperparameters given very limited data. This suggests a natural algorithm for tuning regularization hyperparameters, which we show to be effective on both real and synthetic data.",http://proceedings.mlr.press/v97/streeter19a.html,http://proceedings.mlr.press/v97/streeter19a/streeter19a.pdf,ICML
419,2019,Making Decisions that Reduce Discriminatory Impacts,"Matt Kusner,         Chris Russell,         Joshua Loftus,         Ricardo Silva","As machine learning algorithms move into real-world settings, it is crucial to ensure they are aligned with societal values. There has been much work on one aspect of this, namely the discriminatory prediction problem: How can we reduce discrimination in the predictions themselves? While an important question, solutions to this problem only apply in a restricted setting, as we have full control over the predictions. Often we care about the non-discrimination of quantities we do not have full control over. Thus, we describe another key aspect of this challenge, the discriminatory impact problem: How can we reduce discrimination arising from the real-world impact of decisions? To address this, we describe causal methods that model the relevant parts of the real-world system in which the decisions are made. Unlike previous approaches, these models not only allow us to map the causal pathway of a single decision, but also to model the effect of interference–how the impact on an individual depends on decisions made about other people. Often, the goal of decision policies is to maximize a beneficial impact overall. To reduce the discrimination of these benefits, we devise a constraint inspired by recent work in counterfactual fairness, and give an efficient procedure to solve the constrained optimization problem. We demonstrate our approach with an example: how to increase students taking college entrance exams in New York City public schools.",http://proceedings.mlr.press/v97/kusner19a.html,http://proceedings.mlr.press/v97/kusner19a/kusner19a.pdf,ICML
420,2019,Jumpout : Improved Dropout for Deep Neural Networks with ReLUs,"Shengjie Wang,         Tianyi Zhou,         Jeff Bilmes","We discuss three novel insights about dropout for DNNs with ReLUs: 1) dropout encourages each local linear piece of a DNN to be trained on data points from nearby regions; 2) the same dropout rate results in different (effective) deactivation rates for layers with different portions of ReLU-deactivated neurons; and 3) the rescaling factor of dropout causes a normalization inconsistency between training and test when used together with batch normalization. The above leads to three simple but nontrivial modifications resulting in our method “jumpout.” Jumpout samples the dropout rate from a monotone decreasing distribution (e.g., the right half of a Gaussian), so each local linear piece is trained, with high probability, to work better for data points from nearby than more distant regions. Jumpout moreover adaptively normalizes the dropout rate at each layer and every training batch, so the effective deactivation rate on the activated neurons is kept the same. Furthermore, it rescales the outputs for a better trade-off that keeps both the variance and mean of neurons more consistent between training and test phases, thereby mitigating the incompatibility between dropout and batch normalization. Jumpout significantly improves the performance of different neural nets on CIFAR10, CIFAR100, Fashion-MNIST, STL10, SVHN, ImageNet-1k, etc., while introducing negligible additional memory and computation costs.",http://proceedings.mlr.press/v97/wang19q.html,http://proceedings.mlr.press/v97/wang19q/wang19q.pdf,ICML
421,2019,Exploring interpretable LSTM neural networks over multi-variable data,"Tian Guo,         Tao Lin,         Nino Antulov-Fantulin","For recurrent neural networks trained on time series with target and exogenous variables, in addition to accurate prediction, it is also desired to provide interpretable insights into the data. In this paper, we explore the structure of LSTM recurrent neural networks to learn variable-wise hidden states, with the aim to capture different dynamics in multi-variable time series and distinguish the contribution of variables to the prediction. With these variable-wise hidden states, a mixture attention mechanism is proposed to model the generative process of the target. Then we develop associated training methods to jointly learn network parameters, variable and temporal importance w.r.t the prediction of the target variable. Extensive experiments on real datasets demonstrate enhanced prediction performance by capturing the dynamics of different variables. Meanwhile, we evaluate the interpretation results both qualitatively and quantitatively. It exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variable data.",http://proceedings.mlr.press/v97/guo19b.html,http://proceedings.mlr.press/v97/guo19b/guo19b.pdf,ICML
422,2019,Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs,"Lingbing Guo,         Zequn Sun,         Wei Hu","We study the problem of knowledge graph (KG) embedding. A widely-established assumption to this problem is that similar entities are likely to have similar relational roles. However, existing related methods derive KG embeddings mainly based on triple-level learning, which lack the capability of capturing long-term relational dependencies of entities. Moreover, triple-level learning is insufficient for the propagation of semantic information among entities, especially for the case of cross-KG embedding. In this paper, we propose recurrent skipping networks (RSNs), which employ a skipping mechanism to bridge the gaps between entities. RSNs integrate recurrent neural networks (RNNs) with residual learning to efficiently capture the long-term relational dependencies within and between KGs. We design an end-to-end framework to support RSNs on different tasks. Our experimental results showed that RSNs outperformed state-of-the-art embedding-based methods for entity alignment and achieved competitive performance for KG completion.",http://proceedings.mlr.press/v97/guo19c.html,http://proceedings.mlr.press/v97/guo19c/guo19c.pdf,ICML
423,2019,Anomaly Detection With Multiple-Hypotheses Predictions,"Duc Tam Nguyen,         Zhongyu Lou,         Michael Klar,         Thomas Brox","In one-class-learning tasks, only the normal case (foreground) can be modeled with data, whereas the variation of all possible anomalies is too erratic to be described by samples. Thus, due to the lack of representative data, the wide-spread discriminative approaches cannot cover such learning tasks, and rather generative models,which attempt to learn the input density of the foreground, are used. However, generative models suffer from a large input dimensionality (as in images) and are typically inefficient learners.We propose to learn the data distribution of the foreground more efficiently with a multi-hypotheses autoencoder. Moreover, the model is criticized by a discriminator, which prevents artificial data modes not supported by data, and which enforces diversity across hypotheses. Our multiple-hypotheses-based anomaly detection framework allows the reliable identification of out-of-distribution samples. For anomaly detection on CIFAR-10, it yields up to 3.9% points improvement over previously reported results. On a real anomaly detection task, the approach reduces the error of the baseline models from 6.8% to 1.5%.",http://proceedings.mlr.press/v97/nguyen19b.html,http://proceedings.mlr.press/v97/nguyen19b/nguyen19b.pdf,ICML
424,2019,Remember and Forget for Experience Replay,"Guido Novati,         Petros Koumoutsakos","Experience replay (ER) is a fundamental component of off-policy deep reinforcement learning (RL). ER recalls experiences from past iterations to compute gradient estimates for the current policy, increasing data-efficiency. However, the accuracy of such updates may deteriorate when the policy diverges from past behaviors and can undermine the performance of ER. Many algorithms mitigate this issue by tuning hyper-parameters to slow down policy changes. An alternative is to actively enforce the similarity between policy and the experiences in the replay memory. We introduce Remember and Forget Experience Replay (ReF-ER), a novel method that can enhance RL algorithms with parameterized policies. ReF-ER (1) skips gradients computed from experiences that are too unlikely with the current policy and (2) regulates policy changes within a trust region of the replayed behaviors. We couple ReF-ER with Q-learning, deterministic policy gradient and off-policy gradient methods. We find that ReF-ER consistently improves the performance of continuous-action, off-policy RL on fully observable benchmarks and partially observable flow control problems.",http://proceedings.mlr.press/v97/novati19a.html,http://proceedings.mlr.press/v97/novati19a/novati19a.pdf,ICML
425,2019,Submodular Cost Submodular Cover with an Approximate Oracle,"Victoria Crawford,         Alan Kuhnle,         My Thai","In this work, we study the Submodular Cost Submodular Cover problem, which is to minimize the submodular cost required to ensure that the submodular benefit function exceeds a given threshold. Existing approximation ratios for the greedy algorithm assume a value oracle to the benefit function. However, access to a value oracle is not a realistic assumption for many applications of this problem, where the benefit function is difficult to compute. We present two incomparable approximation ratios for this problem with an approximate value oracle and demonstrate that the ratios take on empirically relevant values through a case study with the Influence Threshold problem in online social networks.",http://proceedings.mlr.press/v97/crawford19a.html,http://proceedings.mlr.press/v97/crawford19a/crawford19a.pdf,ICML
426,2019,Hierarchically Structured Meta-learning,"Huaxiu Yao,         Ying Wei,         Junzhou Huang,         Zhenhui Li","In order to learn quickly with few samples, meta-learning utilizes prior knowledge learned from previous tasks. However, a critical challenge in meta-learning is task uncertainty and heterogeneity, which can not be handled via globally sharing knowledge among tasks. In this paper, based on gradient-based meta-learning, we propose a hierarchically structured meta-learning (HSML) algorithm that explicitly tailors the transferable knowledge to different clusters of tasks. Inspired by the way human beings organize knowledge, we resort to a hierarchical task clustering structure to cluster tasks. As a result, the proposed approach not only addresses the challenge via the knowledge customization to different clusters of tasks, but also preserves knowledge generalization among a cluster of similar tasks. To tackle the changing of task relationship, in addition, we extend the hierarchical structure to a continual learning environment. The experimental results show that our approach can achieve state-of-the-art performance in both toy-regression and few-shot image classification problems.",http://proceedings.mlr.press/v97/yao19b.html,http://proceedings.mlr.press/v97/yao19b/yao19b.pdf,ICML
427,2019,Dynamic Weights in Multi-Objective Deep Reinforcement Learning,"Axel Abels,         Diederik Roijers,         Tom Lenaerts,         Ann Nowé,         Denis Steckelmacher","Many real-world decision problems are characterized by multiple conflicting objectives which must be balanced based on their relative importance. In the dynamic weights setting the relative importance changes over time and specialized algorithms that deal with such change, such as a tabular Reinforcement Learning (RL) algorithm by Natarajan and Tadepalli (2005), are required. However, this earlier work is not feasible for RL settings that necessitate the use of function approximators. We generalize across weight changes and high-dimensional inputs by proposing a multi-objective Q-network whose outputs are conditioned on the relative importance of objectives and we introduce Diverse Experience Replay (DER) to counter the inherent non-stationarity of the Dynamic Weights setting. We perform an extensive experimental evaluation and compare our methods to adapted algorithms from Deep Multi-Task/Multi-Objective Reinforcement Learning and show that our proposed network in combination with DER dominates these adapted algorithms across weight change scenarios and problem domains.",http://proceedings.mlr.press/v97/abels19a.html,http://proceedings.mlr.press/v97/abels19a/abels19a.pdf,ICML
428,2019,Shape Constraints for Set Functions,"Andrew Cotter,         Maya Gupta,         Heinrich Jiang,         Erez Louidor,         James Muller,         Tamann Narayan,         Serena Wang,         Tao Zhu","Set functions predict a label from a permutation-invariant variable-size collection of feature vectors. We propose making set functions more understandable and regularized by capturing domain knowledge through shape constraints. We show how prior work in monotonic constraints can be adapted to set functions, and then propose two new shape constraints designed to generalize the conditioning role of weights in a weighted mean. We show how one can train standard functions and set functions that satisfy these shape constraints with a deep lattice network. We propose a nonlinear estimation strategy we call the semantic feature engine that uses set functions with the proposed shape constraints to estimate labels for compound sparse categorical features. Experiments on real-world data show the achieved accuracy is similar to deep sets or deep neural networks, but provides guarantees on the model behavior, which makes it easier to explain and debug.",http://proceedings.mlr.press/v97/cotter19a.html,http://proceedings.mlr.press/v97/cotter19a/cotter19a.pdf,ICML
429,2019,A Wrapped Normal Distribution on Hyperbolic Space for Gradient-Based Learning,"Yoshihiro Nagano,         Shoichiro Yamaguchi,         Yasuhiro Fujita,         Masanori Koyama","Hyperbolic space is a geometry that is known to be well-suited for representation learning of data with an underlying hierarchical structure. In this paper, we present a novel hyperbolic distribution called hyperbolic wrapped distribution, a wrapped normal distribution on hyperbolic space whose density can be evaluated analytically and differentiated with respect to the parameters. Our distribution enables the gradient-based learning of the probabilistic models on hyperbolic space that could never have been considered before. Also, we can sample from this hyperbolic probability distribution without resorting to auxiliary means like rejection sampling. As applications of our distribution, we develop a hyperbolic-analog of variational autoencoder and a method of probabilistic word embedding on hyperbolic space. We demonstrate the efficacy of our distribution on various datasets including MNIST, Atari 2600 Breakout, and WordNet.",http://proceedings.mlr.press/v97/nagano19a.html,http://proceedings.mlr.press/v97/nagano19a/nagano19a.pdf,ICML
430,2019,Adversarial examples from computational constraints,"Sebastien Bubeck,         Yin Tat Lee,         Eric Price,         Ilya Razenshteyn","Why are classifiers in high dimension vulnerable to “adversarial” perturbations? We show that it is likely not due to information theoretic limitations, but rather it could be due to computational constraints. First we prove that, for a broad set of classification tasks, the mere existence of a robust classifier implies that it can be found by a possibly exponential-time algorithm with relatively few training examples. Then we give two particular classification tasks where learning a robust classifier is computationally intractable. More precisely we construct two binary classifications task in high dimensional space which are (i) information theoretically easy to learn robustly for large perturbations, (ii) efficiently learnable (non-robustly) by a simple linear separator, (iii) yet are not efficiently robustly learnable, even for small perturbations. Specifically, for the first task hardness holds for any efficient algorithm in the statistical query (SQ) model, while for the second task we rule out any efficient algorithm under a cryptographic assumption. These examples give an exponential separation between classical learning and robust learning in the statistical query model or under a cryptographic assumption. It suggests that adversarial examples may be an unavoidable byproduct of computational limitations of learning algorithms.",http://proceedings.mlr.press/v97/bubeck19a.html,http://proceedings.mlr.press/v97/bubeck19a/bubeck19a.pdf,ICML
431,2019,AdaGrad Stepsizes: Sharp Convergence Over Nonconvex Landscapes,"Rachel Ward,         Xiaoxia Wu,         Leon Bottou","Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine-tune parameters such as the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization. We bridge this gap by providing strong theoretical guarantees for the convergence of AdaGrad over smooth, nonconvex landscapes. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to a stationary point at the O(log(N)/N−−√)O(log⁡(N)/N)\mathcal{O}(\log(N)/\sqrt{N}) rate in the stochastic setting, and at the optimal O(1/N)O(1/N)\mathcal{O}(1/N) rate in the batch (non-stochastic) setting – in this sense, our convergence guarantees are “sharp”. In particular, both our theoretical results and extensive numerical experiments imply that AdaGrad-Norm is robust to the unknown Lipschitz constant and level of stochastic noise on the gradient.",http://proceedings.mlr.press/v97/ward19a.html,http://proceedings.mlr.press/v97/ward19a/ward19a.pdf,ICML
432,2019,QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning,"Kyunghwan Son,         Daewoo Kim,         Wan Ju Kang,         David Earl Hostallero,         Yung Yi","We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate QTRAN’s superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively.",http://proceedings.mlr.press/v97/son19a.html,http://proceedings.mlr.press/v97/son19a/son19a.pdf,ICML
433,2019,Co-manifold learning with missing data,"Gal Mishne,         Eric Chi,         Ronald Coifman","Representation learning is typically applied to only one mode of a data matrix, either its rows or columns. Yet in many applications, there is an underlying geometry to both the rows and the columns. We propose utilizing this coupled structure to perform co-manifold learning: uncovering the underlying geometry of both the rows and the columns of a given matrix, where we focus on a missing data setting. Our unsupervised approach consists of three components. We first solve a family of optimization problems to estimate a complete matrix at multiple scales of smoothness. We then use this collection of smooth matrix estimates to compute pairwise distances on the rows and columns based on a new multi-scale metric that implicitly introduces a coupling between the rows and the columns. Finally, we construct row and column representations from these multi-scale metrics. We demonstrate that our approach outperforms competing methods in both data visualization and clustering.",http://proceedings.mlr.press/v97/mishne19a.html,http://proceedings.mlr.press/v97/mishne19a/mishne19a.pdf,ICML
434,2019,Simple Black-box Adversarial Attacks,"Chuan Guo,         Jacob Gardner,         Yurong You,         Andrew Gordon Wilson,         Kilian Weinberger","We propose an intriguingly simple method for the construction of adversarial images in the black-box setting. In constrast to the white-box scenario, constructing black-box adversarial images has the additional constraint on query budget, and efficient attacks remain an open problem to date. With only the mild assumption of requiring continuous-valued confidence scores, our highly query-efficient algorithm utilizes the following simple iterative principle: we randomly sample a vector from a predefined orthonormal basis and either add or subtract it to the target image. Despite its simplicity, the proposed method can be used for both untargeted and targeted attacks – resulting in previously unprecedented query efficiency in both settings. We demonstrate the efficacy and efficiency of our algorithm on several real world settings including the Google Cloud Vision API. We argue that our proposed algorithm should serve as a strong baseline for future black-box attacks, in particular because it is extremely fast and its implementation requires less than 20 lines of PyTorch code.",http://proceedings.mlr.press/v97/guo19a.html,http://proceedings.mlr.press/v97/guo19a/guo19a.pdf,ICML
435,2019,Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem,"Alon Brutzkus,         Amir Globerson","Empirical evidence suggests that neural networks with ReLU activations generalize better with over-parameterization. However, there is currently no theoretical analysis that explains this observation. In this work, we provide theoretical and empirical evidence that, in certain cases, overparameterized convolutional networks generalize better than small networks because of an interplay between weight clustering and feature exploration at initialization. We demonstrate this theoretically for a 3-layer convolutional neural network with max-pooling, in a novel setting which extends the XOR problem. We show that this interplay implies that with overparamterization, gradient descent converges to global minima with better generalization performance compared to global minima of small networks. Empirically, we demonstrate these phenomena for a 3-layer convolutional neural network in the MNIST task.",http://proceedings.mlr.press/v97/brutzkus19b.html,http://proceedings.mlr.press/v97/brutzkus19b/brutzkus19b.pdf,ICML
436,2019,Sever: A Robust Meta-Algorithm for Stochastic Optimization,"Ilias Diakonikolas,         Gautam Kamath,         Daniel Kane,         Jerry Li,         Jacob Steinhardt,         Alistair Stewart","In high dimensions, most machine learning methods are brittle to even a small fraction of structured outliers. To address this, we introduce a new meta-algorithm that can take in a base learner such as least squares or stochastic gradient descent, and harden the learner to be resistant to outliers. Our method, Sever, possesses strong theoretical guarantees yet is also highly scalable – beyond running the base learner itself, it only requires computing the top singular vector of a certain n{\texttimes}d matrix. We apply Sever on a drug design dataset and a spam classification dataset, and find that in both cases it has substantially greater robustness than several baselines. On the spam dataset, with 1% corruptions, we achieved 7.4% test error, compared to 13.4%-20.5% for the baselines, and 3% error on the uncorrupted dataset. Similarly, on the drug design dataset, with 10% corruptions, we achieved 1.42 mean-squared error test error, compared to 1.51-2.33 for the baselines, and 1.23 error on the uncorrupted dataset.",http://proceedings.mlr.press/v97/diakonikolas19a.html,http://proceedings.mlr.press/v97/diakonikolas19a/diakonikolas19a.pdf,ICML
437,2019,Kernel-Based Reinforcement Learning in Robust Markov Decision Processes,"Shiau Hong Lim,         Arnaud Autef","The robust Markov decision processes (MDP) framework aims to address the problem of parameter uncertainty due to model mismatch, approximation errors or even adversarial behaviors. It is especially relevant when deploying the learned policies in real-world applications. Scaling up the robust MDP framework to large or continuous state space remains a challenging problem. The use of function approximation in this case is usually inevitable and this can only amplify the problem of model mismatch and parameter uncertainties. It has been previously shown that, in the case of MDPs with state aggregation, the robust policies enjoy a tighter performance bound compared to standard solutions due to its reduced sensitivity to approximation errors. We extend these results to the much larger class of kernel-based approximators and show, both analytically and empirically that the robust policies can significantly outperform the non-robust counterpart.",http://proceedings.mlr.press/v97/lim19a.html,http://proceedings.mlr.press/v97/lim19a/lim19a.pdf,ICML
438,2019,Model-Based Active Exploration,"Pranav Shyam,         Wojciech Jaśkowski,         Faustino Gomez","Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.",http://proceedings.mlr.press/v97/shyam19a.html,http://proceedings.mlr.press/v97/shyam19a/shyam19a.pdf,ICML
439,2019,Generative Adversarial User Model for Reinforcement Learning Based Recommendation System,"Xinshi Chen,         Shuang Li,         Hui Li,         Shaohua Jiang,         Yuan Qi,         Le Song","There are great interests as well as many challenges in applying reinforcement learning (RL) to recommendation systems. In this setting, an online user is the environment; neither the reward function nor the environment dynamics are clearly defined, making the application of RL challenging. In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we develop a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel Cascading DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently. In our experiments with real data, we show this generative adversarial user model can better explain user behavior than alternatives, and the RL policy based on this model can lead to a better long-term reward for the user and higher click rate for the system.",http://proceedings.mlr.press/v97/chen19f.html,http://proceedings.mlr.press/v97/chen19f/chen19f.pdf,ICML
440,2019,Learning Dependency Structures for Weak Supervision Models,"Paroma Varma,         Frederic Sala,         Ann He,         Alexander Ratner,         Christopher Re","Labeling training data is a key bottleneck in the modern machine learning pipeline. Recent weak supervision approaches combine labels from multiple noisy sources by estimating their accuracies without access to ground truth labels; however, estimating the dependencies among these sources is a critical challenge. We focus on a robust PCA-based algorithm for learning these dependency structures, establish improved theoretical recovery rates, and outperform existing methods on various real-world tasks. Under certain conditions, we show that the amount of unlabeled data needed can scale sublinearly or even logarithmically with the number of sources m, improving over previous efforts that ignore the sparsity pattern in the dependency structure and scale linearly in m. We provide an information-theoretic lower bound on the minimum sample complexity of the weak supervision setting. Our method outperforms weak supervision approaches that assume conditionally-independent sources by up to 4.64 F1 points and previous structure learning approaches by up to 4.41 F1 points on real-world relation extraction and image classification tasks.",http://proceedings.mlr.press/v97/varma19a.html,http://proceedings.mlr.press/v97/varma19a/varma19a.pdf,ICML
441,2019,Optimal Auctions through Deep Learning,"Paul Duetting,         Zhe Feng,         Harikrishna Narasimhan,         David Parkes,         Sai Srivatsa Ravindranath","Designing an incentive compatible auction that maximizes expected revenue is an intricate task. The single-item case was resolved in a seminal piece of work by Myerson in 1981. Even after 30-40 years of intense research the problem remains unsolved for seemingly simple multi-bidder, multi-item settings. In this work, we initiate the exploration of the use of tools from deep learning for the automated design of optimal auctions. We model an auction as a multi-layer neural network, frame optimal auction design as a constrained learning problem, and show how it can be solved using standard pipelines. We prove generalization bounds and present extensive experiments, recovering essentially all known analytical solutions for multi-item settings, and obtaining novel mechanisms for settings in which the optimal mechanism is unknown.",http://proceedings.mlr.press/v97/duetting19a.html,http://proceedings.mlr.press/v97/duetting19a/duetting19a.pdf,ICML
442,2019,Beating Stochastic and Adversarial Semi-bandits Optimally and Simultaneously,"Julian Zimmert,         Haipeng Luo,         Chen-Yu Wei","We develop the first general semi-bandit algorithm that simultaneously achieves O(logT)O(log⁡T)\mathcal{O}(\log T) regret for stochastic environments and O(T−−√)O(T)\mathcal{O}(\sqrt{T}) regret for adversarial environments without knowledge of the regime or the number of rounds TTT. The leading problem-dependent constants of our bounds are not only optimal in some worst-case sense studied previously, but also optimal for two concrete instances of semi-bandit problems. Our algorithm and analysis extend the recent work of (Zimmert & Seldin, 2019) for the special case of multi-armed bandits, but importantly requires a novel hybrid regularizer designed specifically for semi-bandit. Experimental results on synthetic data show that our algorithm indeed performs well uniformly over different environments. We finally provide a preliminary extension of our results to the full bandit feedback.",http://proceedings.mlr.press/v97/zimmert19a.html,http://proceedings.mlr.press/v97/zimmert19a/zimmert19a.pdf,ICML
443,2019,Spectral Approximate Inference,"Sejun Park,         Eunho Yang,         Se-Young Yun,         Jinwoo Shin","Given a graphical model (GM), computing its partition function is the most essential inference task, but it is computationally intractable in general. To address the issue, iterative approximation algorithms exploring certain local structure/consistency of GM have been investigated as popular choices in practice. However, due to their local/iterative nature, they often output poor approximations or even do not converge, e.g., in low-temperature regimes (hard instances of large parameters). To overcome the limitation, we propose a novel approach utilizing the global spectral feature of GM. Our contribution is two-fold: (a) we first propose a fully polynomial-time approximation scheme (FPTAS) for approximating the partition function of GM associating with a low-rank coupling matrix; (b) for general high-rank GMs, we design a spectral mean-field scheme utilizing (a) as a subroutine, where it approximates a high-rank GM into a product of rank-1 GMs for an efficient approximation of the partition function. The proposed algorithm is more robust in its running time and accuracy than prior methods, i.e., neither suffers from the convergence issue nor depends on hard local structures, as demonstrated in our experiments.",http://proceedings.mlr.press/v97/park19c.html,http://proceedings.mlr.press/v97/park19c/park19c.pdf,ICML
444,2019,Differentiable Linearized ADMM,"Xingyu Xie,         Jianlong Wu,         Guangcan Liu,         Zhisheng Zhong,         Zhouchen Lin","Recently, a number of learning-based optimization methods that combine data-driven architectures with the classical optimization algorithms have been proposed and explored, showing superior empirical performance in solving various ill-posed inverse problems, but there is still a scarcity of rigorous analysis about the convergence behaviors of learning-based optimization. In particular, most existing analyses are specific to unconstrained problems but cannot apply to the more general cases where some variables of interest are subject to certain constraints. In this paper, we propose Differentiable Linearized ADMM (D-LADMM) for solving the problems with linear constraints. Specifically, D-LADMM is a K-layer LADMM inspired deep neural network, which is obtained by firstly introducing some learnable weights in the classical Linearized ADMM algorithm and then generalizing the proximal operator to some learnable activation function. Notably, we rigorously prove that there exist a set of learnable parameters for D-LADMM to generate globally converged solutions, and we show that those desired parameters can be attained by training D-LADMM in a proper way. To the best of our knowledge, we are the first to provide the convergence analysis for the learning-based optimization method on constrained problems.",http://proceedings.mlr.press/v97/xie19c.html,http://proceedings.mlr.press/v97/xie19c/xie19c.pdf,ICML
445,2019,Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints,"Andrew Cotter,         Maya Gupta,         Heinrich Jiang,         Nathan Srebro,         Karthik Sridharan,         Serena Wang,         Blake Woodworth,         Seungil You","Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization performance for such constrained optimization problems, in terms of how well the constraints are satisfied at evaluation time, given that they are satisfied at training time. To improve generalization, we frame the problem as a two-player game where one player optimizes the model parameters on a training dataset, and the other player enforces the constraints on an independent validation dataset. We build on recent work in two-player constrained optimization to show that if one uses this two-dataset approach, then constraint generalization can be significantly improved. As we illustrate experimentally, this approach works not only in theory, but also in practice.",http://proceedings.mlr.press/v97/cotter19b.html,http://proceedings.mlr.press/v97/cotter19b/cotter19b.pdf,ICML
446,2019,Revisiting the Softmax Bellman Operator: New Benefits and New Perspective,"Zhao Song,         Ron Parr,         Lawrence Carin","The impact of softmax on the value function itself in reinforcement learning (RL) is often viewed as problematic because it leads to sub-optimal value (or Q) functions and interferes with the contraction properties of the Bellman operator. Surprisingly, despite these concerns, and independent of its effect on exploration, the softmax Bellman operator when combined with Deep Q-learning, leads to Q-functions with superior policies in practice, even outperforming its double Q-learning counterpart. To better understand how and why this occurs, we revisit theoretical properties of the softmax Bellman operator, and prove that (i) it converges to the standard Bellman operator exponentially fast in the inverse temperature parameter, and (ii) the distance of its Q function from the optimal one can be bounded. These alone do not explain its superior performance, so we also show that the softmax operator can reduce the overestimation error, which may give some insight into why a sub-optimal operator leads to better performance in the presence of value function approximation. A comparison among different Bellman operators is then presented, showing the trade-offs when selecting them.",http://proceedings.mlr.press/v97/song19c.html,http://proceedings.mlr.press/v97/song19c/song19c.pdf,ICML
447,2019,Wasserstein Adversarial Examples via Projected Sinkhorn Iterations,"Eric Wong,         Frank Schmidt,         Zico Kolter","A rapidly growing area of work has studied the existence of adversarial examples, datapoints which have been perturbed to fool a classifier, but the vast majority of these works have focused primarily on threat models defined by ℓpℓp\ell_p norm-bounded perturbations. In this paper, we propose a new threat model for adversarial attacks based on the Wasserstein distance. In the image classification setting, such distances measure the cost of moving pixel mass, which can naturally represent “standard” image manipulations such as scaling, rotation, translation, and distortion (and can potentially be applied to other settings as well). To generate Wasserstein adversarial examples, we develop a procedure for approximate projection onto the Wasserstein ball, based upon a modified version of the Sinkhorn iteration. The resulting algorithm can successfully attack image classification models, bringing traditional CIFAR10 models down to 3% accuracy within a Wasserstein ball with radius 0.1 (i.e., moving 10% of the image mass 1 pixel), and we demonstrate that PGD-based adversarial training can improve this adversarial accuracy to 76%. In total, this work opens up a new direction of study in adversarial robustness, more formally considering convex metrics that accurately capture the invariances that we typically believe should exist in classifiers, and code for all experiments in the paper is available at https://github.com/locuslab/projected_sinkhorn.",http://proceedings.mlr.press/v97/wong19a.html,http://proceedings.mlr.press/v97/wong19a/wong19a.pdf,ICML
448,2019,Provable Guarantees for Gradient-Based Meta-Learning,"Maria-Florina Balcan,         Mikhail Khodak,         Ameet Talwalkar","We study the problem of meta-learning through the lens of online convex optimization, developing a meta-algorithm bridging the gap between popular gradient-based meta-learning and classical regularization-based multi-task transfer methods. Our method is the first to simultaneously satisfy good sample efficiency guarantees in the convex setting, with generalization bounds that improve with task-similarity, while also being computationally scalable to modern deep learning architectures and the many-task setting. Despite its simplicity, the algorithm matches, up to a constant factor, a lower bound on the performance of any such parameter-transfer method under natural task similarity assumptions. We use experiments in both convex and deep learning settings to verify and demonstrate the applicability of our theory.",http://proceedings.mlr.press/v97/balcan19a.html,http://proceedings.mlr.press/v97/balcan19a/balcan19a.pdf,ICML
449,2019,The information-theoretic value of unlabeled data in semi-supervised learning,"Alexander Golovnev,         David Pal,         Balazs Szorenyi","We quantify the separation between the numbers of labeled examples required to learn in two settings: Settings with and without the knowledge of the distribution of the unlabeled data. More specifically, we prove a separation by Θ(logn)Θ(log⁡n)\Theta(\log n) multiplicative factor for the class of projections over the Boolean hypercube of dimension nnn. We prove that there is no separation for the class of all functions on domain of any size. Learning with the knowledge of the distribution (a.k.a. fixed-distribution learning) can be viewed as an idealized scenario of semi-supervised learning where the number of unlabeled data points is so great that the unlabeled distribution is known exactly. For this reason, we call the separation the value of unlabeled data.",http://proceedings.mlr.press/v97/golovnev19a.html,http://proceedings.mlr.press/v97/golovnev19a/golovnev19a.pdf,ICML
450,2019,Mallows ranking models: maximum likelihood estimate and regeneration,Wenpin Tang,"This paper is concerned with various Mallows ranking models. We study the statistical properties of the MLE of Mallows’ ϕϕ\phi model. We also make connections of various Mallows ranking models, encompassing recent progress in mathematics. Motivated by the infinite top-ttt ranking model, we propose an algorithm to select the model size ttt automatically. The key idea relies on the renewal property of such an infinite random permutation. Our algorithm shows good performance on several data sets.",http://proceedings.mlr.press/v97/tang19a.html,http://proceedings.mlr.press/v97/tang19a/tang19a.pdf,ICML
451,2019,Optimistic Policy Optimization via Multiple Importance Sampling,"Matteo Papini,         Alberto Maria Metelli,         Lorenzo Lupo,         Marcello Restelli","Policy Search (PS) is an effective approach to Reinforcement Learning (RL) for solving control tasks with continuous state-action spaces. In this paper, we address the exploration-exploitation trade-off in PS by proposing an approach based on Optimism in the Face of Uncertainty. We cast the PS problem as a suitable Multi Armed Bandit (MAB) problem, defined over the policy parameter space, and we propose a class of algorithms that effectively exploit the problem structure, by leveraging Multiple Importance Sampling to perform an off-policy estimation of the expected return. We show that the regret of the proposed approach is bounded by O˜(T−−√)O~(T)\widetilde{\mathcal{O}}(\sqrt{T}) for both discrete and continuous parameter spaces. Finally, we evaluate our algorithms on tasks of varying difficulty, comparing them with existing MAB and RL algorithms.",http://proceedings.mlr.press/v97/papini19a.html,http://proceedings.mlr.press/v97/papini19a/papini19a.pdf,ICML
452,2019,Metric-Optimized Example Weights,"Sen Zhao,         Mahdi Milani Fard,         Harikrishna Narasimhan,         Maya Gupta","Real-world machine learning applications often have complex test metrics, and may have training and test data that are not identically distributed. Motivated by known connections between complex test metrics and cost-weighted learning, we propose addressing these issues by using a weighted loss function with a standard loss, where the weights on the training examples are learned to optimize the test metric on a validation set. These metric-optimized example weights can be learned for any test metric, including black box and customized ones for specific applications. We illustrate the performance of the proposed method on diverse public benchmark datasets and real-world applications. We also provide a generalization bound for the method.",http://proceedings.mlr.press/v97/zhao19b.html,http://proceedings.mlr.press/v97/zhao19b/zhao19b.pdf,ICML
453,2019,Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation,"Marco Ancona,         Cengiz Oztireli,         Markus Gross","The problem of explaining the behavior of deep neural networks has recently gained a lot of attention. While several attribution methods have been proposed, most come without strong theoretical foundations, which raises questions about their reliability. On the other hand, the literature on cooperative game theory suggests Shapley values as a unique way of assigning relevance scores such that certain desirable properties are satisfied. Unfortunately, the exact evaluation of Shapley values is prohibitively expensive, exponential in the number of input features. In this work, by leveraging recent results on uncertainty propagation, we propose a novel, polynomial-time approximation of Shapley values in deep neural networks. We show that our method produces significantly better approximations of Shapley values than existing state-of-the-art attribution methods.",http://proceedings.mlr.press/v97/ancona19a.html,http://proceedings.mlr.press/v97/ancona19a/ancona19a.pdf,ICML
454,2019,Differentially Private Fair Learning,"Matthew Jagielski,         Michael Kearns,         Jieming Mao,         Alina Oprea,         Aaron Roth,         Saeed Sharifi -Malvajerdi,         Jonathan Ullman","Motivated by settings in which predictive models may be required to be non-discriminatory with respect to certain attributes (such as race), but even collecting the sensitive attribute may be forbidden or restricted, we initiate the study of fair learning under the constraint of differential privacy. Our first algorithm is a private implementation of the equalized odds post-processing approach of (Hardt et al., 2016). This algorithm is appealingly simple, but must be able to use protected group membership explicitly at test time, which can be viewed as a form of “disparate treatment”. Our second algorithm is a differentially private version of the oracle-efficient in-processing approach of (Agarwal et al., 2018) which is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three properties, and show that these tradeoffs can be milder if group membership may be used at test time. We conclude with a brief experimental evaluation.",http://proceedings.mlr.press/v97/jagielski19a.html,http://proceedings.mlr.press/v97/jagielski19a/jagielski19a.pdf,ICML
455,2019,When Samples Are Strategically Selected,"Hanrui Zhang,         Yu Cheng,         Vincent Conitzer","In standard classification problems, the assumption is that the entity making the decision (the principal) has access to all the samples. However, in many contexts, she either does not have direct access to the samples, or can inspect only a limited set of samples and does not know which are the most relevant ones. In such cases, she must rely on another party (the agent) to either provide the samples or point out the most relevant ones. If the agent has a different objective, then the principal cannot trust the submitted samples to be representative. She must set a policy for how she makes decisions, keeping in mind the agent’s incentives. In this paper, we introduce a theoretical framework for this problem and provide key structural and computational results.",http://proceedings.mlr.press/v97/zhang19c.html,http://proceedings.mlr.press/v97/zhang19c/zhang19c.pdf,ICML
456,2019,Theoretically Principled Trade-off between Robustness and Accuracy,"Hongyang Zhang,         Yaodong Yu,         Jiantao Jiao,         Eric Xing,         Laurent El Ghaoui,         Michael Jordan","We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of  2,000 submissions, surpassing the runner-up approach by 11.41% in terms of mean L_2 perturbation distance.",http://proceedings.mlr.press/v97/zhang19p.html,http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf,ICML
457,2019,Neural Logic Reinforcement Learning,"Zhengyao Jiang,         Shan Luo","Deep reinforcement learning (DRL) has achieved significant breakthroughs in various tasks. However, most DRL algorithms suffer a problem of generalising the learned policy, which makes the policy performance largely affected even by minor modifications of the training environment. Except that, the use of deep neural networks makes the learned policies hard to be interpretable. To address these two challenges, we propose a novel algorithm named Neural Logic Reinforcement Learning (NLRL) to represent the policies in reinforcement learning by first-order logic. NLRL is based on policy gradient methods and differentiable inductive logic programming that have demonstrated significant advantages in terms of interpretability and generalisability in supervised tasks. Extensive experiments conducted on cliff-walking and blocks manipulation tasks demonstrate that NLRL can induce interpretable policies achieving near-optimal performance while showing good generalisability to environments of different initial states and problem sizes.",http://proceedings.mlr.press/v97/jiang19a.html,http://proceedings.mlr.press/v97/jiang19a/jiang19a.pdf,ICML
458,2019,Geometric Losses for Distributional Learning,"Arthur Mensch,         Mathieu Blondel,         Gabriel Peyré","Building upon recent advances in entropy-regularized optimal transport, and upon Fenchel duality between measures and continuous functions, we propose a generalization of the logistic loss that incorporates a metric or cost between classes. Unlike previous attempts to use optimal transport distances for learning, our loss results in unconstrained convex objective functions, supports infinite (or very large) class spaces, and naturally defines a geometric generalization of the softmax operator. The geometric properties of this loss make it suitable for predicting sparse and singular distributions, for instance supported on curves or hyper-surfaces. We study the theoretical properties of our loss and showcase its effectiveness on two applications: ordinal regression and drawing generation.",http://proceedings.mlr.press/v97/mensch19a.html,http://proceedings.mlr.press/v97/mensch19a/mensch19a.pdf,ICML
459,2019,CHiVE: Varying Prosody in Speech Synthesis with a Linguistically Driven Dynamic Hierarchical Conditional Variational Network,"Tom Kenter,         Vincent Wan,         Chun-An Chan,         Rob Clark,         Jakub Vit","The prosodic aspects of speech signals produced by current text-to-speech systems are typically averaged over training material, and as such lack the variety and liveliness found in natural speech. To avoid monotony and averaged prosody contours, it is desirable to have a way of modeling the variation in the prosodic aspects of speech, so audio signals can be synthesized in multiple ways for a given text. We present a new, hierarchically structured conditional variational auto-encoder to generate prosodic features (fundamental frequency, energy and duration) suitable for use with a vocoder or a generative model like WaveNet. At inference time, an embedding representing the prosody of a sentence may be sampled from the variational layer to allow for prosodic variation. To efficiently capture the hierarchical nature of the linguistic input (words, syllables and phones), both the encoder and decoder parts of the auto-encoder are hierarchical, in line with the linguistic structure, with layers being clocked dynamically at the respective rates. We show in our experiments that our dynamic hierarchical network outperforms a non-hierarchical state-of-the-art baseline, and, additionally, that prosody transfer across sentences is possible by employing the prosody embedding of one sentence to generate the speech signal of another.",http://proceedings.mlr.press/v97/kenter19a.html,http://proceedings.mlr.press/v97/kenter19a/kenter19a.pdf,ICML
460,2019,Automatic Classifiers as Scientific Instruments: One Step Further Away from Ground-Truth,"Jacob Whitehill,         Anand Ramakrishnan","Automatic machine learning-based detectors of various psychological and social phenomena (e.g., emotion, stress, engagement) have great potential to advance basic science. However, when a detector d is trained to approximate an existing measurement tool (e.g., a questionnaire, observation protocol), then care must be taken when interpreting measurements collected using d since they are one step further removed from the under- lying construct. We examine how the accuracy of d, as quantified by the correlation q of d’s out- puts with the ground-truth construct U, impacts the estimated correlation between U (e.g., stress) and some other phenomenon V (e.g., academic performance). In particular: (1) We show that if the true correlation between U and V is r, then the expected sample correlation, over all vectors T n whose correlation with U is q, is qr. (2) We derive a formula for the probability that the sample correlation (over n subjects) using d is positive given that the true correlation is negative (and vice-versa); this probability can be substantial (around 20 - 30%) for values of n and q that have been used in recent affective computing studies. (3) With the goal to reduce the variance of correlations estimated by an automatic detector, we show that training multiple neural networks d(1) , . . . , d(m) using different training architectures and hyperparameters for the same detection task provides only limited “coverage” of T^n.",http://proceedings.mlr.press/v97/whitehill19a.html,http://proceedings.mlr.press/v97/whitehill19a/whitehill19a.pdf,ICML
461,2019,Multi-Agent Adversarial Inverse Reinforcement Learning,"Lantao Yu,         Jiaming Song,         Stefano Ermon","Reinforcement learning agents are prone to undesired behaviors due to reward mis-specification. Finding a set of reward functions to properly guide agent behaviors is particularly challenging in multi-agent scenarios. Inverse reinforcement learning provides a framework to automatically acquire suitable reward functions from expert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more complex notions of rational behaviors. In this paper, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. We derive our algorithm based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. In the experiments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with the ground truth rewards, while significantly outperforms prior methods in terms of policy imitation.",http://proceedings.mlr.press/v97/yu19e.html,http://proceedings.mlr.press/v97/yu19e/yu19e.pdf,ICML
462,2019,Kernel Mean Matching for Content Addressability of GANs,"Wittawat Jitkrittum,         Patsorn Sangkloy,         Muhammad Waleed Gondal,         Amit Raj,         James Hays,         Bernhard Schölkopf","We propose a novel procedure which adds ""content-addressability"" to any given unconditional implicit model e.g., a generative adversarial network (GAN). The procedure allows users to control the generative process by specifying a set (arbitrary size) of desired examples based on which similar samples are generated from the model. The proposed approach, based on kernel mean matching, is applicable to any generative models which transform latent vectors to samples, and does not require retraining of the model. Experiments on various high-dimensional image generation problems (CelebA-HQ, LSUN bedroom, bridge, tower) show that our approach is able to generate images which are consistent with the input set, while retaining the image quality of the original model. To our knowledge, this is the first work that attempts to construct, at test time, a content-addressable generative model from a trained marginal model.",http://proceedings.mlr.press/v97/jitkrittum19a.html,http://proceedings.mlr.press/v97/jitkrittum19a/jitkrittum19a.pdf,ICML
463,2019,Locally Private Bayesian Inference for Count Models,"Aaron Schein,         Zhiwei Steven Wu,         Alexandra Schofield,         Mingyuan Zhou,         Hanna Wallach","We present a general and modular method for privacy-preserving Bayesian inference for Poisson factorization, a broad class of models that includes some of the most widely used models in the social sciences. Our method satisfies limited-precision local privacy, a generalization of local differential privacy that we introduce to formulate appropriate privacy guarantees for sparse count data. We present an MCMC algorithm that approximates the posterior distribution over the latent variables conditioned on data that has been locally privatized by the geometric mechanism. Our method is based on two insights: 1) a novel reinterpretation of the geometric mechanism in terms of the Skellam distribution and 2) a general theorem that relates the Skellam and Bessel distributions. We demonstrate our method’s utility using two case studies that involve real-world email data. We show that our method consistently outperforms the commonly used naive approach, wherein inference proceeds as usual, treating the locally privatized data as if it were not privatized.",http://proceedings.mlr.press/v97/schein19a.html,http://proceedings.mlr.press/v97/schein19a/schein19a.pdf,ICML
464,2019,Generative Modeling of Infinite Occluded Objects for Compositional Scene Representation,"Jinyang Yuan,         Bin Li,         Xiangyang Xue","We present a deep generative model which explicitly models object occlusions for compositional scene representation. Latent representations of objects are disentangled into location, size, shape, and appearance, and the visual scene can be generated compositionally by integrating these representations and an infinite-dimensional binary vector indicating presences of objects in the scene. By training the model to learn spatial dependences of pixels in the unsupervised setting, the number of objects, pixel-level segregation of objects, and presences of objects in overlapping regions can be estimated through inference of latent variables. Extensive experiments conducted on a series of specially designed datasets demonstrate that the proposed method outperforms two state-of-the-art methods when object occlusions exist.",http://proceedings.mlr.press/v97/yuan19b.html,http://proceedings.mlr.press/v97/yuan19b/yuan19b.pdf,ICML
465,2019,Combating Label Noise in Deep Learning using Abstention,"Sunil Thulasidasan,         Tanmoy Bhattacharya,         Jeff Bilmes,         Gopinath Chennupati,         Jamal Mohd-Yusof","We introduce a novel method to combat label noise when training deep neural networks for classification. We propose a loss function that permits abstention during training thereby allowing the DNN to abstain on confusing samples while continuing to learn and improve classification performance on the non-abstained samples. We show how such a deep abstaining classifier (DAC) can be used for robust learning in the presence of different types of label noise. In the case of structured or systematic label noise {–} where noisy training labels or confusing examples are correlated with underlying features of the data{–} training with abstention enables representation learning for features that are associated with unreliable labels. In the case of unstructured (arbitrary) label noise, abstention during training enables the DAC to be used as an effective data cleaner by identifying samples that are likely to have label noise. We provide analytical results on the loss function behavior that enable dynamic adaption of abstention rates based on learning progress during training. We demonstrate the utility of the deep abstaining classifier for various image classification tasks under different types of label noise; in the case of arbitrary label noise, we show significant im- provements over previously published results on multiple image benchmarks.",http://proceedings.mlr.press/v97/thulasidasan19a.html,http://proceedings.mlr.press/v97/thulasidasan19a/thulasidasan19a.pdf,ICML
466,2019,The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects,"Zhanxing Zhu,         Jingfeng Wu,         Bing Yu,         Lei Wu,         Jinwen Ma","Understanding the behavior of stochastic gradient descent (SGD) in the context of deep neural networks has raised lots of concerns recently. Along this line, we study a general form of gradient based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. Through investigating this general optimization dynamics, we analyze the behavior of SGD on escaping from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function. Based on this indicator, two conditions are established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency. We further show that the anisotropic noise in SGD satisfies the two conditions, and thus helps to escape from sharp and poor minima effectively, towards more stable and flat minima that typically generalize well. We systematically design various experiments to verify the benefits of the anisotropic noise, compared with full gradient descent plus isotropic diffusion (i.e. Langevin dynamics).",http://proceedings.mlr.press/v97/zhu19e.html,http://proceedings.mlr.press/v97/zhu19e/zhu19e.pdf,ICML
467,2019,Invariant-Equivariant Representation Learning for Multi-Class Data,Ilya Feige,"Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode. We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations). This approach is based primarily on the strategic routing of data through the two latent variables, and thus is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data). We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.",http://proceedings.mlr.press/v97/feige19a.html,http://proceedings.mlr.press/v97/feige19a/feige19a.pdf,ICML
468,2019,Random Walks on Hypergraphs with Edge-Dependent Vertex Weights,"Uthsav Chitra,         Benjamin Raphael","Hypergraphs are used in machine learning to model higher-order relationships in data. While spectral methods for graphs are well-established, spectral theory for hypergraphs remains an active area of research. In this paper, we use random walks to develop a spectral theory for hypergraphs with edge-dependent vertex weights: hypergraphs where every vertex v has a weight γe(v)γe(v)\gamma_e(v) for each incident hyperedge e that describes the contribution of v to the hyperedge e. We derive a random walk-based hypergraph Laplacian, and bound the mixing time of random walks on such hypergraphs. Moreover, we give conditions under which random walks on such hypergraphs are equivalent to random walks on graphs. As a corollary, we show that current machine learning methods that rely on Laplacians derived from random walks on hypergraphs with edge-independent vertex weights do not utilize higher-order relationships in the data. Finally, we demonstrate the advantages of hypergraphs with edge-dependent vertex weights on ranking applications using real-world datasets.",http://proceedings.mlr.press/v97/chitra19a.html,http://proceedings.mlr.press/v97/chitra19a/chitra19a.pdf,ICML
469,2019,Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers,"Hong Liu,         Mingsheng Long,         Jianmin Wang,         Michael Jordan","Domain adaptation enables knowledge transfer from a labeled source domain to an unlabeled target domain. A mainstream approach is adversarial feature adaptation, which learns domain-invariant representations through aligning the feature distributions of both domains. However, a theoretical prerequisite of domain adaptation is the adaptability measured by the expected risk of an ideal joint hypothesis over the source and target domains. In this respect, adversarial feature adaptation may potentially deteriorate the adaptability, since it distorts the original feature distributions when suppressing domain-specific variations. To this end, we propose Transferable Adversarial Training (TAT) to enable the adaptation of deep classifiers. The approach generates transferable examples to fill in the gap between the source and target domains, and adversarially trains the deep classifiers to make consistent predictions over the transferable examples. Without learning domain-invariant representations at the expense of distorting the feature distributions, the adaptability in the theoretical learning bound is algorithmically guaranteed. A series of experiments validate that our approach advances the state of the arts on a variety of domain adaptation tasks in vision and NLP, including object recognition, learning from synthetic to real data, and sentiment classification.",http://proceedings.mlr.press/v97/liu19b.html,http://proceedings.mlr.press/v97/liu19b/liu19b.pdf,ICML
470,2019,Understanding Geometry of Encoder-Decoder CNNs,"Jong Chul Ye,         Woon Kyoung Sung","Encoder-decoder networks using convolutional neural network (CNN) architecture have been extensively used in deep learning literatures thanks to its excellent performance for various inverse problems in computer vision, medical imaging, etc. However, it is still difficult to obtain coherent geometric view why such an architecture gives the desired performance. Inspired by recent theoretical understanding on generalizability, expressivity and optimization landscape of neural networks, as well as the theory of convolutional framelets, here we provide a unified theoretical framework that leads to a better understanding of geometry of encoder-decoder CNNs. Our unified mathematical framework shows that encoder-decoder CNN architecture is closely related to nonlinear basis representation using combinatorial convolution frames, whose expressibility increases exponentially with the network depth. We also demonstrate the importance of skipped connection in terms of expressibility, and optimization landscape.",http://proceedings.mlr.press/v97/ye19a.html,http://proceedings.mlr.press/v97/ye19a/ye19a.pdf,ICML
471,2019,On Variational Bounds of Mutual Information,"Ben Poole,         Sherjil Ozair,         Aaron Van Den Oord,         Alex Alemi,         George Tucker","Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning, but bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks. However, the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of these new bounds for estimation and representation learning.",http://proceedings.mlr.press/v97/poole19a.html,http://proceedings.mlr.press/v97/poole19a/poole19a.pdf,ICML
472,2019,NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks,"Yandong Li,         Lijun Li,         Liqiang Wang,         Tong Zhang,         Boqing Gong","Powerful adversarial attack methods are vital for understanding how to construct robust deep neural networks (DNNs) and for thoroughly testing defense techniques. In this paper, we propose a black-box adversarial attack algorithm that can defeat both vanilla DNNs and those generated by various defense techniques developed recently. Instead of searching for an ""optimal"" adversarial example for a benign input to a targeted DNN, our algorithm finds a probability density distribution over a small region centered around the input, such that a sample drawn from this distribution is likely an adversarial example, without the need of accessing the DNN’s internal layers or weights. Our approach is universal as it can successfully attack different neural networks by a single algorithm. It is also strong; according to the testing against 2 vanilla DNNs and 13 defended ones, it outperforms state-of-the-art black-box or white-box attack methods for most test cases. Additionally, our results reveal that adversarial training remains one of the best defense techniques, and the adversarial examples are not as transferable across defended DNNs as them across vanilla DNNs.",http://proceedings.mlr.press/v97/li19g.html,http://proceedings.mlr.press/v97/li19g/li19g.pdf,ICML
473,2019,Validating Causal Inference Models via Influence Functions,"Ahmed Alaa,         Mihaela Van Der Schaar","The problem of estimating causal effects of treatments from observational data falls beyond the realm of supervised learning {—} because counterfactual data is inaccessible, we can never observe the true causal effects. In the absence of ""supervision"", how can we evaluate the performance of causal inference methods? In this paper, we use influence functions {—} the functional derivatives of a loss function {—} to develop a model validation procedure that estimates the estimation error of causal inference methods. Our procedure utilizes a Taylor-like expansion to approximate the loss function of a method on a given dataset in terms of the influence functions of its loss on a ""synthesized"", proximal dataset with known causal effects. Under minimal regularity assumptions, we show that our procedure is consistent and efficient. Experiments on 77 benchmark datasets show that using our procedure, we can accurately predict the comparative performances of state-of-the-art causal inference methods applied to a given observational study.",http://proceedings.mlr.press/v97/alaa19a.html,http://proceedings.mlr.press/v97/alaa19a/alaa19a.pdf,ICML
474,2019,SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver,"Po-Wei Wang,         Priya Donti,         Bryan Wilder,         Zico Kolter","Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a “visual Sudoku” problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.",http://proceedings.mlr.press/v97/wang19e.html,http://proceedings.mlr.press/v97/wang19e/wang19e.pdf,ICML
475,2019,Blended Conditonal Gradients,"Gábor Braun,         Sebastian Pokutta,         Dan Tu,         Stephen Wright","We present a blended conditional gradient approach for minimizing a smooth convex function over a polytope P, combining the Frank{–}Wolfe algorithm (also called conditional gradient) with gradient-based steps, different from away steps and pairwise steps, but still achieving linear convergence for strongly convex functions, along with good practical performance. Our approach retains all favorable properties of conditional gradient algorithms, notably avoidance of projections onto P and maintenance of iterates as sparse convex combinations of a limited number of extreme points of P. The algorithm is lazy, making use of inexpensive inexact solutions of the linear programming subproblem that characterizes the conditional gradient approach. It decreases measures of optimality (primal and dual gaps) rapidly, both in the number of iterations and in wall-clock time, outperforming even the lazy conditional gradient algorithms of Braun et al. 2017. We also present a streamlined version of the algorithm that applies when P is the probability simplex.",http://proceedings.mlr.press/v97/braun19a.html,http://proceedings.mlr.press/v97/braun19a/braun19a.pdf,ICML
476,2019,"Classification from Positive, Unlabeled and Biased Negative Data","Yu-Guan Hsieh,         Gang Niu,         Masashi Sugiyama","In binary classification, there are situations where negative (N) data are too diverse to be fully labeled and we often resort to positive-unlabeled (PU) learning in these scenarios. However, collecting a non-representative N set that contains only a small portion of all possible N data can often be much easier in practice. This paper studies a novel classification framework which incorporates such biased N (bN) data in PU learning. We provide a method based on empirical risk minimization to address this PUbN classification problem. Our approach can be regarded as a novel example-weighting algorithm, with the weight of each example computed through a preliminary step that draws inspiration from PU learning. We also derive an estimation error bound for the proposed method. Experimental results demonstrate the effectiveness of our algorithm in not only PUbN learning scenarios but also ordinary PU learning scenarios on several benchmark datasets.",http://proceedings.mlr.press/v97/hsieh19c.html,http://proceedings.mlr.press/v97/hsieh19c/hsieh19c.pdf,ICML
477,2019,Improving Neural Language Modeling via Adversarial Training,"Dilin Wang,         Chengyue Gong,         Qiang Liu","Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.65, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.",http://proceedings.mlr.press/v97/wang19f.html,http://proceedings.mlr.press/v97/wang19f/wang19f.pdf,ICML
478,2019,Doubly Robust Joint Learning for Recommendation on Data Missing Not at Random,"Xiaojie Wang,         Rui Zhang,         Yu Sun,         Jianzhong Qi","In recommender systems, usually the ratings of a user to most items are missing and a critical problem is that the missing ratings are often missing not at random (MNAR) in reality. It is widely acknowledged that MNAR ratings make it difficult to accurately predict the ratings and unbiasedly estimate the performance of rating prediction. Recent approaches use imputed errors to recover the prediction errors for missing ratings, or weight observed ratings with the propensities of being observed. These approaches can still be severely biased in performance estimation or suffer from the variance of the propensities. To overcome these limitations, we first propose an estimator that integrates the imputed errors and propensities in a doubly robust way to obtain unbiased performance estimation and alleviate the effect of the propensity variance. To achieve good performance guarantees, based on this estimator, we propose joint learning of rating prediction and error imputation, which outperforms the state-of-the-art approaches on four real-world datasets.",http://proceedings.mlr.press/v97/wang19n.html,http://proceedings.mlr.press/v97/wang19n/wang19n.pdf,ICML
479,2019,First-Order Algorithms Converge Faster than O(1/k)O(1/k)O(1/k) on Convex Problems,"Ching-Pei Lee,         Stephen Wright","It is well known that both gradient descent and stochastic coordinate descent achieve a global convergence rate of O(1/k)O(1/k)O(1/k) in the objective value, when applied to a scheme for minimizing a Lipschitz-continuously differentiable, unconstrained convex function. In this work, we improve this rate to o(1/k)o(1/k)o(1/k). We extend the result to proximal gradient and proximal coordinate descent on regularized problems to show similar o(1/k)o(1/k)o(1/k) convergence rates. The result is tight in the sense that a rate of O(1/k1+ϵ)O(1/k1+ϵ)O(1/k^{1+\epsilon}) is not generally attainable for any ϵ>0ϵ>0\epsilon>0, for any of these methods.",http://proceedings.mlr.press/v97/lee19e.html,http://proceedings.mlr.press/v97/lee19e/lee19e.pdf,ICML
480,2019,Analogies Explained: Towards Understanding Word Embeddings,"Carl Allen,         Timothy Hospedales","Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy “woman is to queen as man is to king” approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of “wxwxw_x is to wywyw_y”. From these concepts we prove existence of linear relationship between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.",http://proceedings.mlr.press/v97/allen19a.html,http://proceedings.mlr.press/v97/allen19a/allen19a.pdf,ICML
481,2019,Bayesian Deconditional Kernel Mean Embeddings,"Kelvin Hsu,         Fabio Ramos","Conditional kernel mean embeddings form an attractive nonparametric framework for representing conditional means of functions, describing the observation processes for many complex models. However, the recovery of the original underlying function of interest whose conditional mean was observed is a challenging inference task. We formalize deconditional kernel mean embeddings as a solution to this inverse problem, and show that it can be naturally viewed as a nonparametric Bayes' rule. Critically, we introduce the notion of task transformed Gaussian processes and establish deconditional kernel means as their posterior predictive mean. This connection provides Bayesian interpretations and uncertainty estimates for deconditional kernel mean embeddings, explains their regularization hyperparameters, and reveals a marginal likelihood for kernel hyperparameter learning. These revelations further enable practical applications such as likelihood-free inference and learning sparse representations for big data.",http://proceedings.mlr.press/v97/hsu19a.html,http://proceedings.mlr.press/v97/hsu19a/hsu19a.pdf,ICML
482,2019,HOList: An Environment for Machine Learning of Higher Order Logic Theorem Proving,"Kshitij Bansal,         Sarah Loos,         Markus Rabe,         Christian Szegedy,         Stewart Wilcox","We present an environment, benchmark, and deep learning driven automated theorem prover for higher-order logic. Higher-order interactive theorem provers enable the formalization of arbitrary mathematical theories and thereby present an interesting challenge for deep learning. We provide an open-source framework based on the HOL Light theorem prover that can be used as a reinforcement learning environment. HOL Light comes with a broad coverage of basic mathematical theorems on calculus and the formal proof of the Kepler conjecture, from which we derive a challenging benchmark for automated reasoning approaches. We also present a deep reinforcement learning driven automated theorem prover, DeepHOL, that gives strong initial results on this benchmark.",http://proceedings.mlr.press/v97/bansal19a.html,http://proceedings.mlr.press/v97/bansal19a/bansal19a.pdf,ICML
483,2019,Random Function Priors for Correlation Modeling,"Aonan Zhang,         John Paisley","The likelihood model of high dimensional data XnXnX_n can often be expressed as p(Xn|Zn,θ)p(Xn|Zn,θ)p(X_n|Z_n,\theta), where θ:=(θk)k∈[K]θ:=(θk)k∈[K]\theta\mathrel{\mathop:}=(\theta_k)_{k\in[K]} is a collection of hidden features shared across objects, indexed by nnn, and ZnZnZ_n is a non-negative factor loading vector with KKK entries where ZnkZnkZ_{nk} indicates the strength of θkθk\theta_k used to express XnXnX_n. In this paper, we introduce random function priors for ZnZnZ_n for modeling correlations among its KKK dimensions Zn1Zn1Z_{n1} through ZnKZnKZ_{nK}, which we call population random measure embedding (PRME). Our model can be viewed as a generalized paintbox model \cite{Broderick13} using random functions, and can be learned efficiently with neural networks via amortized variational inference. We derive our Bayesian nonparametric method by applying a representation theorem on separately exchangeable discrete random measures.",http://proceedings.mlr.press/v97/zhang19k.html,http://proceedings.mlr.press/v97/zhang19k/zhang19k.pdf,ICML
484,2019,Policy Consolidation for Continual Reinforcement Learning,"Christos Kaplanis,         Murray Shanahan,         Claudia Clopath","We propose a method for tackling catastrophic forgetting in deep reinforcement learning that is agnostic to the timescale of changes in the distribution of experiences, does not require knowledge of task boundaries and can adapt in continuously changing environments. In our policy consolidation model, the policy network interacts with a cascade of hidden networks that simultaneously remember the agent’s policy at a range of timescales and regularise the current policy by its own history, thereby improving its ability to learn without forgetting. We find that the model improves continual learning relative to baselines on a number of continuous control tasks in single-task, alternating two-task, and multi-agent competitive self-play settings.",http://proceedings.mlr.press/v97/kaplanis19a.html,http://proceedings.mlr.press/v97/kaplanis19a/kaplanis19a.pdf,ICML
485,2019,"Submodular Streaming in All Its Glory: Tight Approximation, Minimum Memory and Low Adaptive Complexity","Ehsan Kazemi,         Marko Mitrovic,         Morteza Zadimoghaddam,         Silvio Lattanzi,         Amin Karbasi","Streaming algorithms are generally judged by the quality of their solution, memory footprint, and computational complexity. In this paper, we study the problem of maximizing a monotone submodular function in the streaming setting with a cardinality constraint kkk. We first propose SIEVE-STREAMING++, which requires just one pass over the data, keeps only O(k)O(k)O(k) elements and achieves the tight 1212\frac{1}{2}-approximation guarantee. The best previously known streaming algorithms either achieve a suboptimal 1414\frac{1}{4}-approximation with Θ(k)Θ(k)\Theta(k) memory or the optimal 1212\frac{1}{2}-approximation with O(klogk)O(klog⁡k)O(k\log k) memory. Next, we show that by buffering a small fraction of the stream and applying a careful filtering procedure, one can heavily reduce the number of adaptive computational rounds, thus substantially lowering the computational complexity of SIEVE-STREAMING++. We then generalize our results to the more challenging multi-source streaming setting. We show how one can achieve the tight 1212\frac{1}{2}-approximation guarantee with O(k)O(k)O(k) shared memory, while minimizing not only the rounds of computations but also the total number of communicated bits. Finally, we demonstrate the efficiency of our algorithms on real-world data summarization tasks for multi-source streams of tweets and of YouTube videos.",http://proceedings.mlr.press/v97/kazemi19a.html,http://proceedings.mlr.press/v97/kazemi19a/kazemi19a.pdf,ICML
486,2019,Learning deep kernels for exponential family densities,"Li Wenliang,         Dougal Sutherland,         Heiko Strathmann,         Arthur Gretton","The kernel exponential family is a rich class of distributions, which can be fit efficiently and with statistical guarantees by score matching. Being required to choose a priori a simple kernel such as the Gaussian, however, limits its practical applicability. We provide a scheme for learning a kernel parameterized by a deep network, which can find complex location-dependent local features of the data geometry. This gives a very rich class of density models, capable of fitting complex structures on moderate-dimensional problems. Compared to deep density models fit via maximum likelihood, our approach provides a complementary set of strengths and tradeoffs: in empirical studies, the former can yield higher likelihoods, whereas the latter gives better estimates of the gradient of the log density, the score, which describes the distribution’s shape.",http://proceedings.mlr.press/v97/wenliang19a.html,http://proceedings.mlr.press/v97/wenliang19a/wenliang19a.pdf,ICML
487,2019,Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation,"Shani Gamrian,         Yoav Goldberg","Despite the remarkable success of Deep RL in learning control policies from raw pixels, the resulting models do not generalize. We demonstrate that a trained agent fails completely when facing small visual changes, and that fine-tuning—the common transfer learning paradigm—fails to adapt to these changes, to the extent that it is faster to re-train the model from scratch. We show that by separating the visual transfer task from the control policy we achieve substantially better sample efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual mapping from the target to the source domain is performed using unaligned GANs, resulting in a control policy that can be further improved using imitation learning from imperfect demonstrations. We demonstrate the approach on synthetic visual variants of the Breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our approach can be seen in \url{https://youtu.be/4mnkzYyXMn4} and \url{https://youtu.be/KCGTrQi6Ogo}.",http://proceedings.mlr.press/v97/gamrian19a.html,http://proceedings.mlr.press/v97/gamrian19a/gamrian19a.pdf,ICML
488,2019,HexaGAN: Generative Adversarial Nets for Real World Classification,"Uiwon Hwang,         Dahuin Jung,         Sungroh Yoon","Most deep learning classification studies assume clean data. However, when dealing with the real world data, we encounter three problems such as 1) missing data, 2) class imbalance, and 3) missing label problems. These problems undermine the performance of a classifier. Various preprocessing techniques have been proposed to mitigate one of these problems, but an algorithm that assumes and resolves all three problems together has not been proposed yet. In this paper, we propose HexaGAN, a generative adversarial network framework that shows promising classification performance for all three problems. We interpret the three problems from a single perspective to solve them jointly. To enable this, the framework consists of six components, which interact with each other. We also devise novel loss functions corresponding to the architecture. The designed loss functions allow us to achieve state-of-the-art imputation performance, with up to a 14% improvement, and to generate high-quality class-conditional data. We evaluate the classification performance (F1-score) of the proposed method with 20% missingness and confirm up to a 5% improvement in comparison with the performance of combinations of state-of-the-art methods.",http://proceedings.mlr.press/v97/hwang19a.html,http://proceedings.mlr.press/v97/hwang19a/hwang19a.pdf,ICML
489,2019,Mixture Models for Diverse Machine Translation: Tricks of the Trade,"Tianxiao Shen,         Myle Ott,         Michael Auli,         Marc’Aurelio Ranzato","Mixture models trained via EM are among the simplest, most widely used and well understood latent variable models in the machine learning literature. Surprisingly, these models have been hardly explored in text generation applications such as machine translation. In principle, they provide a latent variable to control generation and produce a diverse set of hypotheses. In practice, however, mixture models are prone to degeneracies—often only one component gets trained or the latent variable is simply ignored. We find that disabling dropout noise in responsibility computation is critical to successful training. In addition, the design choices of parameterization, prior distribution, hard versus soft EM and online versus offline assignment can dramatically affect model performance. We develop an evaluation protocol to assess both quality and diversity of generations against multiple references, and provide an extensive empirical study of several mixture model variants. Our analysis shows that certain types of mixture models are more robust and offer the best trade-off between translation quality and diversity compared to variational models and diverse decoding approaches.\footnote{Code to reproduce the results in this paper is available at \url{https://github.com/pytorch/fairseq}}",http://proceedings.mlr.press/v97/shen19c.html,http://proceedings.mlr.press/v97/shen19c/shen19c.pdf,ICML
490,2019,Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness,"Raphael Suter,         Djordje Miladinovic,         Bernhard Schölkopf,         Stefan Bauer","The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size.",http://proceedings.mlr.press/v97/suter19a.html,http://proceedings.mlr.press/v97/suter19a/suter19a.pdf,ICML
491,2019,Online learning with kernel losses,"Niladri Chatterji,         Aldo Pacchiano,         Peter Bartlett","We present a generalization of the adversarial linear bandits framework, where the underlying losses are kernel functions (with an associated reproducing kernel Hilbert space) rather than linear functions. We study a version of the exponential weights algorithm and bound its regret in this setting. Under conditions on the eigen-decay of the kernel we provide a sharp characterization of the regret for this algorithm. When we have polynomial eigen-decay (μj≤O(j−β)μj≤O(j−β)\mu_j \le \mathcal{O}(j^{-\beta})), we find that the regret is bounded by Rn≤O(nβ/2(β−1))Rn≤O(nβ/2(β−1))\mathcal{R}_n \le \mathcal{O}(n^{\beta/2(\beta-1)}). While under the assumption of exponential eigen-decay (μj≤O(e−βj)μj≤O(e−βj)\mu_j \le \mathcal{O}(e^{-\beta j })) we get an even tighter bound on the regret Rn≤O~(n1/2)Rn≤O~(n1/2)\mathcal{R}_n \le \tilde{\mathcal{O}}(n^{1/2}). When the eigen-decay is polynomial we also show a non-matching minimax lower bound on the regret of Rn≥Ω(n(β+1)/2β)Rn≥Ω(n(β+1)/2β)\mathcal{R}_n \ge \Omega(n^{(\beta+1)/2\beta}) and a lower bound of Rn≥Ω(n1/2)Rn≥Ω(n1/2)\mathcal{R}_n \ge \Omega(n^{1/2}) when the decay in the eigen-values is exponentially fast. We also study the full information setting when the underlying losses are kernel functions and present an adapted exponential weights algorithm and a conditional gradient descent algorithm.",http://proceedings.mlr.press/v97/chatterji19a.html,http://proceedings.mlr.press/v97/chatterji19a/chatterji19a.pdf,ICML
492,2019,Learning to Prove Theorems via Interacting with Proof Assistants,"Kaiyu Yang,         Jia Deng","Humans prove theorems by relying on substantial high-level reasoning and problem-specific insights. Proof assistants offer a formalism that resembles human mathematical reasoning, representing theorems in higher-order logic and proofs as high-level tactics. However, human experts have to construct proofs manually by entering tactics into the proof assistant. In this paper, we study the problem of using machine learning to automate the interaction with proof assistants. We construct CoqGym, a large-scale dataset and learning environment containing 71K human-written proofs from 123 projects developed with the Coq proof assistant. We develop ASTactic, a deep learning-based model that generates tactics as programs in the form of abstract syntax trees (ASTs). Experiments show that ASTactic trained on CoqGym can generate effective tactics and can be used to prove new theorems not previously provable by automated methods. Code is available at https://github.com/princeton-vl/CoqGym.",http://proceedings.mlr.press/v97/yang19a.html,http://proceedings.mlr.press/v97/yang19a/yang19a.pdf,ICML
493,2019,Optimal Continuous DR-Submodular Maximization and Applications to Provable Mean Field Inference,"Yatao Bian,         Joachim Buhmann,         Andreas Krause","Mean field inference for discrete graphical models is generally a highly nonconvex problem, which also holds for the class of probabilistic log-submodular models. Existing optimization methods, e.g., coordinate ascent algorithms, typically only find local optima. In this work we propose provable mean filed methods for probabilistic log-submodular models and its posterior agreement (PA) with strong approximation guarantees. The main algorithmic technique is a new Double Greedy scheme, termed DR-DoubleGreedy, for continuous DR-submodular maximization with box-constraints. It is a one-pass algorithm with linear time complexity, reaching the optimal 1/2 approximation ratio, which may be of independent interest. We validate the superior performance of our algorithms against baselines on both synthetic and real-world datasets.",http://proceedings.mlr.press/v97/bian19a.html,http://proceedings.mlr.press/v97/bian19a/bian19a.pdf,ICML
494,2019,Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff,"Yochai Blau,         Tomer Michaeli","Lossy compression algorithms are typically designed and analyzed through the lens of Shannon’s rate-distortion theory, where the goal is to achieve the lowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate. However, in recent years, it has become increasingly accepted that ""low distortion"" is not a synonym for ""high perceptual quality"", and in fact optimization of one often comes at the expense of the other. In light of this understanding, it is natural to seek for a generalization of rate-distortion theory which takes perceptual quality into account. In this paper, we adopt the mathematical definition of perceptual quality recently proposed by Blau & Michaeli (2018), and use it to study the three-way tradeoff between rate, distortion, and perception. We show that restricting the perceptual quality to be high, generally leads to an elevation of the rate-distortion curve, thus necessitating a sacrifice in either rate or distortion. We prove several fundamental properties of this triple-tradeoff, calculate it in closed form for a Bernoulli source, and illustrate it visually on a toy MNIST example.",http://proceedings.mlr.press/v97/blau19a.html,http://proceedings.mlr.press/v97/blau19a/blau19a.pdf,ICML
495,2019,Learning Classifiers for Target Domain with Limited or No Labels,"Pengkai Zhu,         Hanxiao Wang,         Venkatesh Saligrama","In computer vision applications, such as domain adaptation (DA), few shot learning (FSL) and zero-shot learning (ZSL), we encounter new objects and environments, for which insufficient examples exist to allow for training “models from scratch,” and methods that adapt existing models, trained on the presented training environment, to the new scenario are required. We propose a novel visual attribute encoding method that encodes each image as a low-dimensional probability vector composed of prototypical part-type probabilities. The prototypes are learnt to be representative of all training data. At test-time we utilize this encoding as an input to a classifier. At test-time we freeze the encoder and only learn/adapt the classifier component to limited annotated labels in FSL; new semantic attributes in ZSL. We conduct extensive experiments on benchmark datasets. Our method outperforms state-of-art methods trained for the specific contexts (ZSL, FSL, DA).",http://proceedings.mlr.press/v97/zhu19d.html,http://proceedings.mlr.press/v97/zhu19d/zhu19d.pdf,ICML
496,2019,SWALP : Stochastic Weight Averaging in Low Precision Training,"Guandao Yang,         Tianyi Zhang,         Polina Kirichenko,         Junwen Bai,         Andrew Gordon Wilson,         Chris De Sa","Low precision operations can provide scalability, memory savings, portability, and energy efficiency. This paper proposes SWALP, an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule. SWALP is easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits, including the gradient accumulators. Additionally, we show that SWALP converges arbitrarily close to the optimal solution for quadratic objectives, and to a noise ball asymptotically smaller than low precision SGD in strongly convex settings.",http://proceedings.mlr.press/v97/yang19d.html,http://proceedings.mlr.press/v97/yang19d/yang19d.pdf,ICML
497,2019,Do ImageNet Classifiers Generalize to ImageNet?,"Benjamin Recht,         Rebecca Roelofs,         Ludwig Schmidt,         Vaishaal Shankar","We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models’ inability to generalize to slightly ""harder"" images than those found in the original test sets.",http://proceedings.mlr.press/v97/recht19a.html,http://proceedings.mlr.press/v97/recht19a/recht19a.pdf,ICML
498,2019,Learning Linear-Quadratic Regulators Efficiently with only \sqrtT\sqrtT\sqrtT Regret,"Alon Cohen,         Tomer Koren,         Yishay Mansour","We present the first computationally-efficient algorithm with O˜(T−−√)O~(T)\widetilde{O}(\sqrt{T}) regret for learning in Linear Quadratic Control systems with unknown dynamics. By that, we resolve an open question of Abbasi-Yadkori and Szepesvari (2011) and Dean,Mania, Matni, Recht, and Tu (2018).",http://proceedings.mlr.press/v97/cohen19b.html,http://proceedings.mlr.press/v97/cohen19b/cohen19b.pdf,ICML
499,2019,A Contrastive Divergence for Combining Variational Inference and MCMC,"Francisco Ruiz,         Michalis Titsias","We develop a method to combine Markov chain Monte Carlo (MCMC) and variational inference (VI), leveraging the advantages of both inference approaches. Specifically, we improve the variational distribution by running a few MCMC steps. To make inference tractable, we introduce the variational contrastive divergence (VCD), a new divergence that replaces the standard Kullback-Leibler (KL) divergence used in VI. The VCD captures a notion of discrepancy between the initial variational distribution and its improved version (obtained after running the MCMC steps), and it converges asymptotically to the symmetrized KL divergence between the variational distribution and the posterior of interest. The VCD objective can be optimized efficiently with respect to the variational parameters via stochastic optimization. We show experimentally that optimizing the VCD leads to better predictive performance on two latent variable models: logistic matrix factorization and variational autoencoders (VAEs).",http://proceedings.mlr.press/v97/ruiz19a.html,http://proceedings.mlr.press/v97/ruiz19a/ruiz19a.pdf,ICML
500,2019,Hierarchical Decompositional Mixtures of Variational Autoencoders,"Ping Liang Tan,         Robert Peharz","Variational autoencoders (VAEs) have received considerable attention, since they allow us to learn expressive neural density estimators effectively and efficiently. However, learning and inference in VAEs is still problematic due to the sensitive interplay between the generative model and the inference network. Since these problems become generally more severe in high dimensions, we propose a novel hierarchical mixture model over low-dimensional VAE experts. Our model decomposes the overall learning problem into many smaller problems, which are coordinated by the hierarchical mixture, represented by a sum-product network. In experiments we show that our models outperform classical VAEs on almost all of our experimental benchmarks. Moreover, we show that our model is highly data efficient and degrades very gracefully in extremely low data regimes.ow data regimes.",http://proceedings.mlr.press/v97/tan19b.html,http://proceedings.mlr.press/v97/tan19b/tan19b.pdf,ICML
501,2019,Wasserstein of Wasserstein Loss for Learning Generative Models,"Yonatan Dukler,         Wuchen Li,         Alex Lin,         Guido Montufar","The Wasserstein distance serves as a loss function for unsupervised learning which depends on the choice of a ground metric on sample space. We propose to use the Wasserstein distance itself as the ground metric on the sample space of images. This ground metric is known as an effective distance for image retrieval, that correlates with human perception. We derive the Wasserstein ground metric on pixel space and define a Riemannian Wasserstein gradient penalty to be used in the Wasserstein Generative Adversarial Network (WGAN) framework. The new gradient penalty is computed efficiently via convolutions on the L2L2L^2 gradients with negligible additional computational cost. The new formulation is more robust to the natural variability of the data and provides for a more continuous discriminator in sample space.",http://proceedings.mlr.press/v97/dukler19a.html,http://proceedings.mlr.press/v97/dukler19a/dukler19a.pdf,ICML
502,2019,FloWaveNet : A Generative Flow for Raw Audio,"Sungwon Kim,         Sang-Gil Lee,         Jongyoon Song,         Jaehyeon Kim,         Sungroh Yoon","Most modern text-to-speech architectures use a WaveNet vocoder for synthesizing high-fidelity waveform audio, but there have been limitations, such as high inference time, in practical applications due to its ancestral sampling scheme. The recently suggested Parallel WaveNet and ClariNet has achieved real-time audio synthesis capability by incorporating inverse autoregressive flow (IAF) for parallel sampling. However, these approaches require a two-stage training pipeline with a well-trained teacher network and can only produce natural sound by using probability distillation along with heavily-engineered auxiliary loss terms. We propose FloWaveNet, a flow-based generative model for raw audio synthesis. FloWaveNet requires only a single-stage training procedure and a single maximum likelihood loss, without any additional auxiliary terms, and it is inherently parallel due to the characteristics of generative flow. The model can efficiently sample raw audio in real-time, with clarity comparable to previous two-stage parallel models. The code and samples for all models, including our FloWaveNet, are available on GitHub.",http://proceedings.mlr.press/v97/kim19b.html,http://proceedings.mlr.press/v97/kim19b/kim19b.pdf,ICML
503,2019,Non-Parametric Priors For Generative Adversarial Networks,"Rajhans Singh,         Pavan Turaga,         Suren Jayasuriya,         Ravi Garg,         Martin Braun","The advent of generative adversarial networks (GAN) has enabled new capabilities in synthesis, interpolation, and data augmentation heretofore considered very challenging. However, one of the common assumptions in most GAN architectures is the assumption of simple parametric latent-space distributions. While easy to implement, a simple latent-space distribution can be problematic for uses such as interpolation. This is due to distributional mismatches when samples are interpolated in the latent space. We present a straightforward formalization of this problem; using basic results from probability theory and off-the-shelf-optimization tools, we develop ways to arrive at appropriate non-parametric priors. The obtained prior exhibits unusual qualitative properties in terms of its shape, and quantitative benefits in terms of lower divergence with its mid-point distribution. We demonstrate that our designed prior helps improve image generation along any Euclidean straight line during interpolation, both qualitatively and quantitatively, without any additional training or architectural modifications. The proposed formulation is quite flexible, paving the way to impose newer constraints on the latent-space statistics.",http://proceedings.mlr.press/v97/singh19a.html,http://proceedings.mlr.press/v97/singh19a/singh19a.pdf,ICML
504,2019,GDPP: Learning Diverse Generations using Determinantal Point Processes,"Mohamed Elfeki,         Camille Couprie,         Morgane Riviere,         Mohamed Elhoseiny","Generative models have proven to be an outstanding tool for representing high-dimensional probability distributions and generating realistic looking images. An essential characteristic of generative models is their ability to produce multi-modal outputs. However, while training, they are often susceptible to mode collapse, that is models are limited in mapping input noise to only a few modes of the true data distribution. In this work, we draw inspiration from Determinantal Point Process (DPP) to propose an unsupervised penalty loss that alleviates mode collapse while producing higher quality samples. DPP is an elegant probabilistic measure used to model negative correlations within a subset and hence quantify its diversity. We use DPP kernel to model the diversity in real data as well as in synthetic data. Then, we devise an objective term that encourages generator to synthesize data with a similar diversity to real data. In contrast to previous state-of-the-art generative models that tend to use additional trainable parameters or complex training paradigms, our method does not change the original training scheme. Embedded in an adversarial training and variational autoencoder, our Generative DPP approach shows a consistent resistance to mode-collapse on a wide-variety of synthetic data and natural image datasets including MNIST, CIFAR10, and CelebA, while outperforming state-of-the-art methods for data-efficiency, generation quality, and convergence-time whereas being 5.8x faster than its closest competitor.",http://proceedings.mlr.press/v97/elfeki19a.html,http://proceedings.mlr.press/v97/elfeki19a/elfeki19a.pdf,ICML
505,2019,Flexibly Fair Representation Learning by Disentanglement,"Elliot Creager,         David Madras,         Joern-Henrik Jacobsen,         Marissa Weis,         Kevin Swersky,         Toniann Pitassi,         Richard Zemel","We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also flexibly fair, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder—which does not require the sensitive attributes for inference—allows for the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.",http://proceedings.mlr.press/v97/creager19a.html,http://proceedings.mlr.press/v97/creager19a/creager19a.pdf,ICML
506,2019,Causal Identification under Markov Equivalence: Completeness Results,"Amin Jaber,         Jiji Zhang,         Elias Bareinboim","Causal effect identification is the task of determining whether a causal distribution is computable from the combination of an observational distribution and substantive knowledge about the domain under investigation. One of the most studied versions of this problem assumes that knowledge is articulated in the form of a fully known causal diagram, which is arguably a strong assumption in many settings. In this paper, we relax this requirement and consider that the knowledge is articulated in the form of an equivalence class of causal diagrams, in particular, a partial ancestral graph (PAG). This is attractive because a PAG can be learned directly from data, and the scientist does not need to commit to a particular, unique diagram. There are different sufficient conditions for identification in PAGs, but none is complete. We derive a complete algorithm for identification given a PAG. This implies that whenever the causal effect is identifiable, the algorithm returns a valid identification expression; alternatively, it will throw a failure condition, which means that the effect is provably not identifiable. We further provide a graphical characterization of non-identifiability of causal effects in PAGs.",http://proceedings.mlr.press/v97/jaber19a.html,http://proceedings.mlr.press/v97/jaber19a/jaber19a.pdf,ICML
507,2019,Automated Model Selection with Bayesian Quadrature,"Henry Chai,         Jean-Francois Ton,         Michael A. Osborne,         Roman Garnett","We present a novel technique for tailoring Bayesian quadrature (BQ) to model selection. The state-of-the-art for comparing the evidence of multiple models relies on Monte Carlo methods, which converge slowly and are unreliable for computationally expensive models. Although previous research has shown that BQ offers sample efficiency superior to Monte Carlo in computing the evidence of an individual model, applying BQ directly to model comparison may waste computation producing an overly-accurate estimate for the evidence of a clearly poor model. We propose an automated and efficient algorithm for computing the most-relevant quantity for model selection: the posterior model probability. Our technique maximizes the mutual information between this quantity and observations of the models’ likelihoods, yielding efficient sample acquisition across disparate model spaces when likelihood observations are limited. Our method produces more-accurate posterior estimates using fewer likelihood evaluations than standard Bayesian quadrature and Monte Carlo estimators, as we demonstrate on synthetic and real-world examples.",http://proceedings.mlr.press/v97/chai19a.html,http://proceedings.mlr.press/v97/chai19a/chai19a.pdf,ICML
508,2019,Off-Policy Deep Reinforcement Learning without Exploration,"Scott Fujimoto,         David Meger,         Doina Precup","Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.",http://proceedings.mlr.press/v97/fujimoto19a.html,http://proceedings.mlr.press/v97/fujimoto19a/fujimoto19a.pdf,ICML
509,2019,Ladder Capsule Network,"Taewon Jeong,         Youngmin Lee,         Heeyoung Kim","We propose a new architecture of the capsule network called the ladder capsule network, which has an alternative building block to the dynamic routing algorithm in the capsule network (Sabour et al., 2017). Motivated by the need for using only important capsules during training for robust performance, we first introduce a new layer called the pruning layer, which removes irrelevant capsules. Based on the selected capsules, we construct higher-level capsule outputs. Subsequently, to capture the part-whole spatial relationships, we introduce another new layer called the ladder layer, the outputs of which are regressed lower-level capsule outputs from higher-level capsules. Unlike the capsule network adopting the routing-by-agreement, the ladder capsule network uses backpropagation from a loss function to reconstruct the lower-level capsule outputs from higher-level capsules; thus, the ladder layer implements the reverse directional inference of the agreement/disagreement mechanism of the capsule network. The experiments on MNIST demonstrate that the ladder capsule network learns an equivariant representation and improves the capability to extrapolate or generalize to pose variations.",http://proceedings.mlr.press/v97/jeong19b.html,http://proceedings.mlr.press/v97/jeong19b/jeong19b.pdf,ICML
510,2019,Statistics and Samples in Distributional Reinforcement Learning,"Mark Rowland,         Robert Dadashi,         Saurabh Kumar,         Remi Munos,         Marc G. Bellemare,         Will Dabney","We present a unifying framework for designing and analysing distributional reinforcement learning (DRL) algorithms in terms of recursively estimating statistics of the return distribution. Our key insight is that DRL algorithms can be decomposed as the combination of some statistical estimator and a method for imputing a return distribution consistent with that set of statistics. With this new understanding, we are able to provide improved analyses of existing DRL algorithms as well as construct a new algorithm (EDRL) based upon estimation of the expectiles of the return distribution. We compare EDRL with existing methods on a variety of MDPs to illustrate concrete aspects of our analysis, and develop a deep RL variant of the algorithm, ER-DQN, which we evaluate on the Atari-57 suite of games.",http://proceedings.mlr.press/v97/rowland19a.html,http://proceedings.mlr.press/v97/rowland19a/rowland19a.pdf,ICML
511,2019,Multivariate-Information Adversarial Ensemble for Scalable Joint Distribution Matching,"Ziliang Chen,         Zhanfu Yang,         Xiaoxi Wang,         Xiaodan Liang,         Xiaopeng Yan,         Guanbin Li,         Liang Lin","A broad range of cross-mmm-domain generation researches boil down to matching a joint distribution by deep generative models (DGMs). Hitherto algorithms excel in pairwise domains while as mmm increases, remain struggling to scale themselves to ﬁt a joint distribution. In this paper, we propose a domain-scalable DGM, i.e., MMI-ALI for mmm-domain joint distribution matching. As an mmm-domain ensemble model of ALIs (Dumoulin et al., 2016), MMI-ALI is adversarially trained with maximizing Multivariate Mutual Information (MMI) w.r.t. joint variables of each pair of domains and their shared feature. The negative MMIs are upper bounded by a series of feasible losses provably leading to matching mmm-domain joint distributions. MMI-ALI linearly scales as mmm increases and thus, strikes a right balance between efﬁcacy and scalability. We evaluate MMI-ALI in diverse challenging mmm-domain scenarios and verify its superiority.",http://proceedings.mlr.press/v97/chen19l.html,http://proceedings.mlr.press/v97/chen19l/chen19l.pdf,ICML
512,2019,Collaborative Evolutionary Reinforcement Learning,"Shauharda Khadka,         Somdeb Majumdar,         Tarek Nassar,         Zach Dwiel,         Evren Tumer,         Santiago Miret,         Yinyin Liu,         Kagan Tumer","Deep reinforcement learning algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically struggle with achieving effective exploration and are extremely sensitive to the choice of hyperparameters. One reason is that most approaches use a noisy version of their operating policy to explore - thereby limiting the range of exploration. In this paper, we introduce Collaborative Evolutionary Reinforcement Learning (CERL), a scalable framework that comprises a portfolio of policies that simultaneously explore and exploit diverse regions of the solution space. A collection of learners - typically proven algorithms like TD3 - optimize over varying time-horizons leading to this diverse portfolio. All learners contribute to and use a shared replay buffer to achieve greater sample efficiency. Computational resources are dynamically distributed to favor the best learners as a form of online algorithm selection. Neuroevolution binds this entire process to generate a single emergent learner that exceeds the capabilities of any individual learner. Experiments in a range of continuous control benchmarks demonstrate that the emergent learner significantly outperforms its composite learners while remaining overall more sample-efficient - notably solving the Mujoco Humanoid benchmark where all of its composite learners (TD3) fail entirely in isolation.",http://proceedings.mlr.press/v97/khadka19a.html,http://proceedings.mlr.press/v97/khadka19a/khadka19a.pdf,ICML
513,2019,GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects,"Edward Smith,         Scott Fujimoto,         Adriana Romero,         David Meger","Mesh models are a promising approach for encoding the structure of 3D objects. Current mesh reconstruction systems predict uniformly distributed vertex locations of a predetermined graph through a series of graph convolutions, leading to compromises with respect to performance or resolution. In this paper, we argue that the graph representation of geometric objects allows for additional structure, which should be leveraged for enhanced reconstruction. Thus, we propose a system which properly benefits from the advantages of the geometric structure of graph-encoded objects by introducing (1) a graph convolutional update preserving vertex information; (2) an adaptive splitting heuristic allowing detail to emerge; and (3) a training objective operating both on the local surfaces defined by vertices as well as the global structure defined by the mesh. Our proposed method is evaluated on the task of 3D object reconstruction from images with the ShapeNet dataset, where we demonstrate state of the art performance, both visually and numerically, while having far smaller space requirements by generating adaptive meshes.",http://proceedings.mlr.press/v97/smith19a.html,http://proceedings.mlr.press/v97/smith19a/smith19a.pdf,ICML
514,2019,Distributed Weighted Matching via Randomized Composable Coresets,"Sepehr Assadi,         Mohammadhossein Bateni,         Vahab Mirrokni","Maximum weight matching is one of the most fundamental combinatorial optimization problems with a wide range of applications in data mining and bioinformatics. Developing distributed weighted matching algorithms has been challenging due to the sequential nature of efficient algorithms for this problem. In this paper, we develop a simple distributed algorithm for the problem on general graphs with approximation guarantee of 2 + eps that (nearly) matches that of the sequential greedy algorithm. A key advantage of this algorithm is that it can be easily implemented in only two rounds of computation in modern parallel computation frameworks such as MapReduce. We also demonstrate the efficiency of our algorithm in practice on various graphs (some with half a trillion edges) by achieving objective values always close to what is achievable in the centralized setting.",http://proceedings.mlr.press/v97/assadi19a.html,http://proceedings.mlr.press/v97/assadi19a/assadi19a.pdf,ICML
515,2019,TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning,"Sung Whan Yoon,         Jun Seo,         Jaekyun Moon","Handling previously unseen tasks after given only a few training examples continues to be a tough challenge in machine learning. We propose TapNets, neural networks augmented with task-adaptive projection for improved few-shot learning. Here, employing a meta-learning strategy with episode-based training, a network and a set of per-class reference vectors are learned across widely varying tasks. At the same time, for every episode, features in the embedding space are linearly projected into a new space as a form of quick task-specific conditioning. The training loss is obtained based on a distance metric between the query and the reference vectors in the projection space. Excellent generalization results in this way. When tested on the Omniglot, miniImageNet and tieredImageNet datasets, we obtain state of the art classification accuracies under various few-shot scenarios.",http://proceedings.mlr.press/v97/yoon19a.html,http://proceedings.mlr.press/v97/yoon19a/yoon19a.pdf,ICML
516,2019,Faster Attend-Infer-Repeat with Tractable Probabilistic Models,"Karl Stelzner,         Robert Peharz,         Kristian Kersting","The recent Attend-Infer-Repeat (AIR) framework marks a milestone in structured probabilistic modeling, as it tackles the challenging problem of unsupervised scene understanding via Bayesian inference. AIR expresses the composition of visual scenes from individual objects, and uses variational autoencoders to model the appearance of those objects. However, inference in the overall model is highly intractable, which hampers its learning speed and makes it prone to suboptimal solutions. In this paper, we show that the speed and robustness of learning in AIR can be considerably improved by replacing the intractable object representations with tractable probabilistic models. In particular, we opt for sum-product networks (SPNs), expressive deep probabilistic models with a rich set of tractable inference routines. The resulting model, called SuPAIR, learns an order of magnitude faster than AIR, treats object occlusions in a consistent manner, and allows for the inclusion of a background noise model, improving the robustness of Bayesian scene understanding.",http://proceedings.mlr.press/v97/stelzner19a.html,http://proceedings.mlr.press/v97/stelzner19a/stelzner19a.pdf,ICML
517,2019,Sparse Extreme Multi-label Learning with Oracle Property,"Weiwei Liu,         Xiaobo Shen","The pioneering work of sparse local embeddings for extreme classification (SLEEC) (Bhatia et al., 2015) has shown great promise in multi-label learning. Unfortunately, the statistical rate of convergence and oracle property of SLEEC are still not well understood. To fill this gap, we present a unified framework for SLEEC with nonconvex penalty. Theoretically, we rigorously prove that our proposed estimator enjoys oracle property (i.e., performs as well as if the underlying model were known beforehand), and obtains a desirable statistical convergence rate. Moreover, we show that under a mild condition on the magnitude of the entries in the underlying model, we are able to obtain an improved convergence rate. Extensive numerical experiments verify our theoretical findings and the superiority of our proposed estimator.",http://proceedings.mlr.press/v97/liu19d.html,http://proceedings.mlr.press/v97/liu19d/liu19d.pdf,ICML
518,2019,Exploiting Worker Correlation for Label Aggregation in Crowdsourcing,"Yuan Li,         Benjamin Rubinstein,         Trevor Cohn","Crowdsourcing has emerged as a core component of data science pipelines. From collected noisy worker labels, aggregation models that incorporate worker reliability parameters aim to infer a latent true annotation. In this paper, we argue that existing crowdsourcing approaches do not sufficiently model worker correlations observed in practical settings; we propose in response an enhanced Bayesian classifier combination (EBCC) model, with inference based on a mean-field variational approach. An introduced mixture of intra-class reliabilities—connected to tensor decomposition and item clustering—induces inter-worker correlation. EBCC does not suffer the limitations of existing correlation models: intractable marginalisation of missing labels and poor scaling to large worker cohorts. Extensive empirical comparison on 17 real-world datasets sees EBCC achieving the highest mean accuracy across 10 benchmark crowdsourcing methods.",http://proceedings.mlr.press/v97/li19i.html,http://proceedings.mlr.press/v97/li19i/li19i.pdf,ICML
519,2019,NAS-Bench-101: Towards Reproducible Neural Architecture Search,"Chris Ying,         Aaron Klein,         Eric Christiansen,         Esteban Real,         Kevin Murphy,         Frank Hutter","Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research. To build NAS-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.",http://proceedings.mlr.press/v97/ying19a.html,http://proceedings.mlr.press/v97/ying19a/ying19a.pdf,ICML
520,2019,Multi-Frequency Vector Diffusion Maps,"Yifeng Fan,         Zhizhen Zhao","We introduce multi-frequency vector diffusion maps (MFVDM), a new framework for organizing and analyzing high dimensional data sets. The new method is a mathematical and algorithmic generalization of vector diffusion maps (VDM) and other non-linear dimensionality reduction methods. The idea of MFVDM is to incorporates multiple unitary irreducible representations of the alignment group which introduces robustness to noise. We illustrate the efficacy of MFVDM on synthetic and cryo-EM image datasets, achieving better nearest neighbors search and alignment estimation than other baselines as VDM and diffusion maps (DM), especially on extremely noisy data.",http://proceedings.mlr.press/v97/fan19a.html,http://proceedings.mlr.press/v97/fan19a/fan19a.pdf,ICML
521,2019,Optimal Algorithms for Lipschitz Bandits with Heavy-tailed Rewards,"Shiyin Lu,         Guanghui Wang,         Yao Hu,         Lijun Zhang","We study Lipschitz bandits, where a learner repeatedly plays one arm from an infinite arm set and then receives a stochastic reward whose expectation is a Lipschitz function of the chosen arm. Most of existing work assume the reward distributions are bounded or at least sub-Gaussian, and thus do not apply to heavy-tailed rewards arising in many real-world scenarios such as web advertising and financial markets. To address this limitation, in this paper we relax the assumption on rewards to allow arbitrary distributions that have finite (1+ϵ)(1+ϵ)(1+\epsilon)-th moments for some ϵ∈(0,1]ϵ∈(0,1]\epsilon \in (0, 1], and propose algorithms that enjoy a sublinear regret of O˜(T(dzϵ+1)/(dzϵ+ϵ+1))O~(T(dzϵ+1)/(dzϵ+ϵ+1))\widetilde{O}(T^{(d_z\epsilon + 1)/(d_z \epsilon + \epsilon + 1)}) where TTT is the time horizon and dzdzd_z is the zooming dimension. The key idea is to exploit the Lipschitz property of the expected reward function by adaptively discretizing the arm set, and employ upper confidence bound policies with robust mean estimators designed for heavy-tailed distributions. Furthermore, we provide a lower bound for Lipschitz bandits with heavy-tailed rewards, and show that our algorithms are optimal in terms of TTT. Finally, we conduct numerical experiments to demonstrate the effectiveness of our algorithms.",http://proceedings.mlr.press/v97/lu19c.html,http://proceedings.mlr.press/v97/lu19c/lu19c.pdf,ICML
522,2019,PA-GD: On the Convergence of Perturbed Alternating Gradient Descent to Second-Order Stationary Points for Structured Nonconvex Optimization,"Songtao Lu,         Mingyi Hong,         Zhengdao Wang","Alternating gradient descent (A-GD) is a simple but popular algorithm in machine learning, which updates two blocks of variables in an alternating manner using gradient descent steps. In this paper, we consider a smooth unconstrained nonconvex optimization problem, and propose a perturbed A-GD (PA-GD) which is able to converge (with high probability) to the second-order stationary points (SOSPs) with a global sublinear rate. Existing analysis on A-GD type algorithm either only guarantees convergence to first-order solutions, or converges to second-order solutions asymptotically (without rates). To the best of our knowledge, this is the first alternating type algorithm that takes O(polylog(d)/ϵ2)O(polylog(d)/ϵ2)\mathcal{O}(\text{polylog}(d)/\epsilon^2) iterations to achieve an (ϵ,ϵ√ϵ,ϵ\epsilon,\sqrt{\epsilon})-SOSP with high probability, where polylog(d)(d)(d) denotes the polynomial of the logarithm with respect to problem dimension ddd.",http://proceedings.mlr.press/v97/lu19a.html,http://proceedings.mlr.press/v97/lu19a/lu19a.pdf,ICML
523,2019,Beyond Backprop: Online Alternating Minimization with Auxiliary Variables,"Anna Choromanska,         Benjamin Cowen,         Sadhana Kumaravel,         Ronny Luss,         Mattia Rigotti,         Irina Rish,         Paolo Diachille,         Viatcheslav Gurev,         Brian Kingsbury,         Ravi Tejwani,         Djallel Bouneffouf","Despite significant recent advances in deep neural networks, training them remains a challenge due to the highly non-convex nature of the objective function. State-of-the-art methods rely on error backpropagation, which suffers from several well-known issues, such as vanishing and exploding gradients, inability to handle non-differentiable nonlinearities and to parallelize weight-updates across layers, and biological implausibility. These limitations continue to motivate exploration of alternative training algorithms, including several recently proposed auxiliary-variable methods which break the complex nested objective function into local subproblems. However, those techniques are mainly offline (batch), which limits their applicability to extremely large datasets, as well as to online, continual or reinforcement learning. The main contribution of our work is a novel online (stochastic/mini-batch) alternating minimization (AM) approach for training deep neural networks, together with the first theoretical convergence guarantees for AM in stochastic settings and promising empirical results on a variety of architectures and datasets.",http://proceedings.mlr.press/v97/choromanska19a.html,http://proceedings.mlr.press/v97/choromanska19a/choromanska19a.pdf,ICML
524,2019,Approximation and non-parametric estimation of ResNet-type convolutional neural networks,"Kenta Oono,         Taiji Suzuki","Convolutional neural networks (CNNs) have been shown to achieve optimal approximation and estimation error rates (in minimax sense) in several function classes. However, previous analyzed optimal CNNs are unrealistically wide and difficult to obtain via optimization due to sparse constraints in important function classes, including the Hölder class. We show a ResNet-type CNN can attain the minimax optimal error rates in these classes in more plausible situations – it can be dense, and its width, channel size, and filter size are constant with respect to sample size. The key idea is that we can replicate the learning ability of Fully-connected neural networks (FNNs) by tailored CNNs, as long as the FNNs have block-sparse structures. Our theory is general in a sense that we can automatically translate any approximation rate achieved by block-sparse FNNs into that by CNNs. As an application, we derive approximation and estimation error rates of the aformentioned type of CNNs for the Barron and Hölder classes with the same strategy.",http://proceedings.mlr.press/v97/oono19a.html,http://proceedings.mlr.press/v97/oono19a/oono19a.pdf,ICML
525,2019,Deep Compressed Sensing,"Yan Wu,         Mihaela Rosca,         Timothy Lillicrap","Compressed sensing (CS) provides an elegant framework for recovering sparse signals from compressed measurements. For example, CS can exploit the structure of natural images and recover an image from only a few random measurements. Unlike popular autoencoding models, reconstruction in CS is posed as an optimisation problem that is separate from sensing. CS is flexible and data efficient, but its application has been restricted by the strong assumption of sparsity and costly reconstruction process. A recent approach that combines CS with neural network generators has removed the constraint of sparsity, but reconstruction remains slow. Here we propose a novel framework that significantly improves both the performance and speed of signal recovery by jointly training a generator and the optimisation process for reconstruction via meta-learning. We explore training the measurements with different objectives, and derive a family of models based on minimising measurement errors. We show that Generative Adversarial Nets (GANs) can be viewed as a special case in this family of models. Borrowing insights from the CS perspective, we develop a novel way of improving GANs using gradient information from the discriminator.",http://proceedings.mlr.press/v97/wu19d.html,http://proceedings.mlr.press/v97/wu19d/wu19d.pdf,ICML
526,2019,DoubleSqueeze: Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression,"Hanlin Tang,         Chen Yu,         Xiangru Lian,         Tong Zhang,         Ji Liu","A standard approach in large scale machine learning is distributed stochastic gradient training, which requires the computation of aggregated stochastic gradients over multiple nodes on a network. Communication is a major bottleneck in such applications, and in recent years, compressed stochastic gradient methods such as QSGD (quantized SGD) and sparse SGD have been proposed to reduce communication. It was also shown that error compensation can be combined with compression to achieve better convergence in a scheme that each node compresses its local stochastic gradient and broadcast the result to all other nodes over the network in a single pass. However, such a single pass broadcast approach is not realistic in many practical implementations. For example, under the popular parameter-server model for distributed learning, the worker nodes need to send the compressed local gradients to the parameter server, which performs the aggregation. The parameter server has to compress the aggregated stochastic gradient again before sending it back to the worker nodes. In this work, we provide a detailed analysis on this two-pass communication model, with error-compensated compression both on the worker nodes and on the parameter server. We show that the error-compensated stochastic gradient algorithm admits three very nice properties: 1) it is compatible with an arbitrary compression technique; 2) it admits an improved convergence rate than the non error-compensated stochastic gradient method such as QSGD and sparse SGD; 3) it admits linear speedup with respect to the number of workers. The empirical study is also conducted to validate our theoretical results.",http://proceedings.mlr.press/v97/tang19d.html,http://proceedings.mlr.press/v97/tang19d/tang19d.pdf,ICML
527,2019,Flat Metric Minimization with Applications in Generative Modeling,"Thomas Möllenhoff,         Daniel Cremers","We take the novel perspective to view data not as a probability distribution but rather as a current. Primarily studied in the field of geometric measure theory, k-currents are continuous linear functionals acting on compactly supported smooth differential forms and can be understood as a generalized notion of oriented k-dimensional manifold. By moving from distributions (which are 0-currents) to k-currents, we can explicitly orient the data by attaching a k-dimensional tangent plane to each sample point. Based on the flat metric which is a fundamental distance between currents, we derive FlatGAN, a formulation in the spirit of generative adversarial networks but generalized to k-currents. In our theoretical contribution we prove that the flat metric between a parametrized current and a reference current is Lipschitz continuous in the parameters. In experiments, we show that the proposed shift to k>0 leads to interpretable and disentangled latent representations which behave equivariantly to the specified oriented tangent planes.",http://proceedings.mlr.press/v97/mollenhoff19a.html,http://proceedings.mlr.press/v97/mollenhoff19a/mollenhoff19a.pdf,ICML
528,2019,Multi-objective training of Generative Adversarial Networks with multiple discriminators,"Isabela Albuquerque,         Joao Monteiro,         Thang Doan,         Breandan Considine,         Tiago Falk,         Ioannis Mitliagkas","Recent literature has demonstrated promising results for training Generative Adversarial Networks by employing a set of discriminators, in contrast to the traditional game involving one generator against a single adversary. Such methods perform single-objective optimization on some simple consolidation of the losses, e.g. an arithmetic average. In this work, we revisit the multiple-discriminator setting by framing the simultaneous minimization of losses provided by different models as a multi-objective optimization problem. Specifically, we evaluate the performance of multiple gradient descent and the hypervolume maximization algorithm on a number of different datasets. Moreover, we argue that the previously proposed methods and hypervolume maximization can all be seen as variations of multiple gradient descent in which the update direction can be computed efficiently. Our results indicate that hypervolume maximization presents a better compromise between sample quality and computational cost than previous methods.",http://proceedings.mlr.press/v97/albuquerque19a.html,http://proceedings.mlr.press/v97/albuquerque19a/albuquerque19a.pdf,ICML
529,2019,A Conditional-Gradient-Based Augmented Lagrangian Framework,"Alp Yurtsever,         Olivier Fercoq,         Volkan Cevher","This paper considers a generic convex minimization template with affine constraints over a compact domain, which covers key semidefinite programming applications. The existing conditional gradient methods either do not apply to our template or are too slow in practice. To this end, we propose a new conditional gradient method, based on a unified treatment of smoothing and augmented Lagrangian frameworks. The proposed method maintains favorable properties of the classical conditional gradient method, such as cheap linear minimization oracle calls and sparse representation of the decision variable. We prove O(1/k−−√)O(1/k)O(1/\sqrt{k}) convergence rate for our method in the objective residual and the feasibility gap. This rate is essentially the same as the state of the art CG-type methods for our problem template, but the proposed method is arguably superior in practice compared to existing methods in various applications.",http://proceedings.mlr.press/v97/yurtsever19a.html,http://proceedings.mlr.press/v97/yurtsever19a/yurtsever19a.pdf,ICML
530,2019,Dirichlet Simplex Nest and Geometric Inference,"Mikhail Yurochkin,         Aritra Guha,         Yuekai Sun,         Xuanlong Nguyen","We propose Dirichlet Simplex Nest, a class of probabilistic models suitable for a variety of data types, and develop fast and provably accurate inference algorithms by accounting for the model’s convex geometry and low dimensional simplicial structure. By exploiting the connection to Voronoi tessellation and properties of Dirichlet distribution, the proposed inference algorithm is shown to achieve consistency and strong error bound guarantees on a range of model settings and data distributions. The effectiveness of our model and the learning algorithm is demonstrated by simulations and by analyses of text and financial data.",http://proceedings.mlr.press/v97/yurochkin19b.html,http://proceedings.mlr.press/v97/yurochkin19b/yurochkin19b.pdf,ICML
531,2019,Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions,"Antoine Liutkus,         Umut Simsekli,         Szymon Majewski,         Alain Durmus,         Fabian-Robert Stöter","By building upon the recent theory that established the connection between implicit generative modeling (IGM) and optimal transport, in this study, we propose a novel parameter-free algorithm for learning the underlying distributions of complicated datasets and sampling from them. The proposed algorithm is based on a functional optimization problem, which aims at finding a measure that is close to the data distribution as much as possible and also expressive enough for generative modeling purposes. We formulate the problem as a gradient flow in the space of probability measures. The connections between gradient flows and stochastic differential equations let us develop a computationally efficient algorithm for solving the optimization problem. We provide formal theoretical analysis where we prove finite-time error guarantees for the proposed algorithm. To the best of our knowledge, the proposed algorithm is the first nonparametric IGM algorithm with explicit theoretical guarantees. Our experimental results support our theory and show that our algorithm is able to successfully capture the structure of different types of data distributions.",http://proceedings.mlr.press/v97/liutkus19a.html,http://proceedings.mlr.press/v97/liutkus19a/liutkus19a.pdf,ICML
532,2019,A Baseline for Any Order Gradient Estimation in Stochastic Computation Graphs,"Jingkai Mao,         Jakob Foerster,         Tim Rocktäschel,         Maruan Al-Shedivat,         Gregory Farquhar,         Shimon Whiteson","By enabling correct differentiation in Stochastic Computation Graphs (SCGs), the infinitely differentiable Monte-Carlo estimator (DiCE) can generate correct estimates for the higher order gradients that arise in, e.g., multi-agent reinforcement learning and meta-learning. However, the baseline term in DiCE that serves as a control variate for reducing variance applies only to first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient estimate. It reuses the same baseline function (e.g., the state-value function in reinforcement learning) already used for the first order baseline. We provide theoretical analysis and numerical evaluations of this new baseline, which demonstrate that it can dramatically reduce the variance of DiCE’s second order gradient estimators and also show empirically that it reduces the variance of third and fourth order gradients. This computational tool can be easily used to estimate higher order gradients with unprecedented efficiency and simplicity wherever automatic differentiation is utilised, and it has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.",http://proceedings.mlr.press/v97/mao19a.html,http://proceedings.mlr.press/v97/mao19a/mao19a.pdf,ICML
533,2019,Approximated Oracle Filter Pruning for Destructive CNN Width Optimization,"Xiaohan Ding,         Guiguang Ding,         Yuchen Guo,         Jungong Han,         Chenggang Yan","It is not easy to design and run Convolutional Neural Networks (CNNs) due to: 1) finding the optimal number of filters (i.e., the width) at each layer is tricky, given an architecture; and 2) the computational intensity of CNNs impedes the deployment on computationally limited devices. Oracle Pruning is designed to remove the unimportant filters from a well-trained CNN, which estimates the filters’ importance by ablating them in turn and evaluating the model, thus delivers high accuracy but suffers from intolerable time complexity, and requires a given resulting width but cannot automatically find it. To address these problems, we propose Approximated Oracle Filter Pruning (AOFP), which keeps searching for the least important filters in a binary search manner, makes pruning attempts by masking out filters randomly, accumulates the resulting errors, and finetunes the model via a multi-path framework. As AOFP enables simultaneous pruning on multiple layers, we can prune an existing very deep CNN with acceptable time cost, negligible accuracy drop, and no heuristic knowledge, or re-design a model which exerts higher accuracy and faster inference.",http://proceedings.mlr.press/v97/ding19a.html,http://proceedings.mlr.press/v97/ding19a/ding19a.pdf,ICML
534,2019,Dimension-Wise Importance Sampling Weight Clipping for Sample-Efficient Reinforcement Learning,"Seungyul Han,         Youngchul Sung","In importance sampling (IS)-based reinforcement learning algorithms such as Proximal Policy Optimization (PPO), IS weights are typically clipped to avoid large variance in learning. However, policy update from clipped statistics induces large bias in tasks with high action dimensions, and bias from clipping makes it difficult to reuse old samples with large IS weights. In this paper, we consider PPO, a representative on-policy algorithm, and propose its improvement by dimension-wise IS weight clipping which separately clips the IS weight of each action dimension to avoid large bias and adaptively controls the IS weight to bound policy update from the current policy. This new technique enables efficient learning for high action-dimensional tasks and reusing of old samples like in off-policy learning to increase the sample efficiency. Numerical results show that the proposed new algorithm outperforms PPO and other RL algorithms in various Open AI Gym tasks.",http://proceedings.mlr.press/v97/han19b.html,http://proceedings.mlr.press/v97/han19b/han19b.pdf,ICML
535,2019,"Graph Element Networks: adaptive, structured computation and memory","Ferran Alet,         Adarsh Keshav Jeewajee,         Maria Bauza Villalonga,         Alberto Rodriguez,         Tomas Lozano-Perez,         Leslie Kaelbling","We explore the use of graph neural networks (GNNs) to model spatial processes in which there is no a priori graphical structure. Similar to finite element analysis, we assign nodes of a GNN to spatial locations and use a computational process defined on the graph to model the relationship between an initial function defined over a space and a resulting function in the same space. We use GNNs as a computational substrate, and show that the locations of the nodes in space as well as their connectivity can be optimized to focus on the most complex parts of the space. Moreover, this representational strategy allows the learned input-output relationship to generalize over the size of the underlying space and run the same model at different levels of precision, trading computation for accuracy. We demonstrate this method on a traditional PDE problem, a physical prediction problem from robotics, and learning to predict scene images from novel viewpoints.",http://proceedings.mlr.press/v97/alet19a.html,http://proceedings.mlr.press/v97/alet19a/alet19a.pdf,ICML
536,2019,Learning Optimal Fair Policies,"Razieh Nabi,         Daniel Malinsky,         Ilya Shpitser","Systematic discriminatory biases present in our society influence the way data is collected and stored, the way variables are defined, and the way scientific findings are put into practice as policy. Automated decision procedures and learning algorithms applied to such data may serve to perpetuate existing injustice or unfairness in our society. In this paper, we consider how to make optimal but fair decisions, which “break the cycle of injustice” by correcting for the unfair dependence of both decisions and outcomes on sensitive features (e.g., variables that correspond to gender, race, disability, or other protected attributes). We use methods from causal inference and constrained optimization to learn optimal policies in a way that addresses multiple potential biases which afflict data analysis in sensitive contexts, extending the approach of Nabi & Shpitser (2018). Our proposal comes equipped with the theoretical guarantee that the chosen fair policy will induce a joint distribution for new instances that satisfies given fairness constraints. We illustrate our approach with both synthetic data and real criminal justice data.",http://proceedings.mlr.press/v97/nabi19a.html,http://proceedings.mlr.press/v97/nabi19a/nabi19a.pdf,ICML
537,2019,DBSCAN++: Towards fast and scalable density clustering,"Jennifer Jang,         Heinrich Jiang","DBSCAN is a classical density-based clustering procedure with tremendous practical relevance. However, DBSCAN implicitly needs to compute the empirical density for each sample point, leading to a quadratic worst-case time complexity, which is too slow on large datasets. We propose DBSCAN++, a simple modification of DBSCAN which only requires computing the densities for a chosen subset of points. We show empirically that, compared to traditional DBSCAN, DBSCAN++ can provide not only competitive performance but also added robustness in the bandwidth hyperparameter while taking a fraction of the runtime. We also present statistical consistency guarantees showing the trade-off between computational cost and estimation rates. Surprisingly, up to a certain point, we can enjoy the same estimation rates while lowering computational cost, showing that DBSCAN++ is a sub-quadratic algorithm that attains minimax optimal rates for level-set estimation, a quality that may be of independent interest.",http://proceedings.mlr.press/v97/jang19a.html,http://proceedings.mlr.press/v97/jang19a/jang19a.pdf,ICML
538,2019,Learning Discrete Structures for Graph Neural Networks,"Luca Franceschi,         Mathias Niepert,         Massimiliano Pontil,         Xiao He","Graph neural networks (GNNs) are a popular class of machine learning models that have been successfully applied to a range of problems. Their major advantage lies in their ability to explicitly incorporate a sparse and discrete dependency structure between data points. Unfortunately, GNNs can only be used when such a graph-structure is available. In practice, however, real-world graphs are often noisy and incomplete or might not be available at all. With this work, we propose to jointly learn the graph structure and the parameters of graph convolutional networks (GCNs) by approximately solving a bilevel program that learns a discrete probability distribution on the edges of the graph. This allows one to apply GCNs not only in scenarios where the given graph is incomplete or corrupted but also in those where a graph is not available. We conduct a series of experiments that analyze the behavior of the proposed method and demonstrate that it outperforms related methods by a significant margin.",http://proceedings.mlr.press/v97/franceschi19a.html,http://proceedings.mlr.press/v97/franceschi19a/franceschi19a.pdf,ICML
539,2019,Correlated Variational Auto-Encoders,"Da Tang,         Dawen Liang,         Tony Jebara,         Nicholas Ruozzi","Variational Auto-Encoders (VAEs) are capable of learning latent representations for high dimensional data. However, due to the i.i.d. assumption, VAEs only optimize the singleton variational distributions and fail to account for the correlations between data points, which might be crucial for learning latent representations from dataset where a priori we know correlations exist. We propose Correlated Variational Auto-Encoders (CVAEs) that can take the correlation structure into consideration when learning latent representations with VAEs. CVAEs apply a prior based on the correlation structure. To address the intractability introduced by the correlated prior, we develop an approximation by average of a set of tractable lower bounds over all maximal acyclic subgraphs of the undirected correlation graph. Experimental results on matching and link prediction on public benchmark rating datasets and spectral clustering on a synthetic dataset show the effectiveness of the proposed method over baseline algorithms.",http://proceedings.mlr.press/v97/tang19b.html,http://proceedings.mlr.press/v97/tang19b/tang19b.pdf,ICML
540,2019,Scalable Learning in Reproducing Kernel Krein Spaces,"Dino Oglic,         Thomas Gärtner","We provide the first mathematically complete derivation of the Nystr{ö}m method for low-rank approximation of indefinite kernels and propose an efficient method for finding an approximate eigendecomposition of such kernel matrices. Building on this result, we devise highly scalable methods for learning in reproducing kernel Krein spaces. The devised approaches provide a principled and theoretically well-founded means to tackle large scale learning problems with indefinite kernels. The main motivation for our work comes from problems with structured representations (e.g., graphs, strings, time-series), where it is relatively easy to devise a pairwise (dis)similarity function based on intuition and/or knowledge of domain experts. Such functions are typically not positive definite and it is often well beyond the expertise of practitioners to verify this condition. The effectiveness of the devised approaches is evaluated empirically using indefinite kernels defined on structured and vectorial data representations.",http://proceedings.mlr.press/v97/oglic19a.html,http://proceedings.mlr.press/v97/oglic19a/oglic19a.pdf,ICML
541,2019,Does Data Augmentation Lead to Positive Margin?,"Shashank Rajput,         Zhili Feng,         Zachary Charles,         Po-Ling Loh,         Dimitris Papailiopoulos","Data augmentation (DA) is commonly used during model training, as it significantly improves test error and model robustness. DA artificially expands the training set by applying random noise, rotations, crops, or even adversarial perturbations to the input data. Although DA is widely used, its capacity to provably improve robustness is not fully understood. In this work, we analyze the robustness that DA begets by quantifying the margin that DA enforces on empirical risk minimizers. We first focus on linear separators, and then a class of nonlinear models whose labeling is constant within small convex hulls of data points. We present lower bounds on the number of augmented data points required for non-zero margin, and show that commonly used DA techniques may only introduce significant margin after adding exponentially many points to the data set.",http://proceedings.mlr.press/v97/rajput19a.html,http://proceedings.mlr.press/v97/rajput19a/rajput19a.pdf,ICML
542,2019,Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?,"Samet Oymak,         Mahdi Soltanolkotabi","Many modern learning tasks involve fitting nonlinear models which are trained in an overparameterized regime where the parameters of the model exceed the size of the training dataset. Due to this overparameterization, the training loss may have infinitely many global minima and it is critical to understand the properties of the solutions found by first-order optimization schemes such as (stochastic) gradient descent starting from different initializations. In this paper we demonstrate that when the loss has certain properties over a minimally small neighborhood of the initial point, first order methods such as (stochastic) gradient descent have a few intriguing properties: (1) the iterates converge at a geometric rate to a global optima even when the loss is nonconvex, (2) among all global optima of the loss the iterates converge to one with a near minimal distance to the initial point, (3) the iterates take a near direct route from the initial point to this global optimum. As part of our proof technique, we introduce a new potential function which captures the tradeoff between the loss function and the distance to the initial point as the iterations progress. The utility of our general theory is demonstrated for a variety of problem domains spanning low-rank matrix recovery to shallow neural network training.",http://proceedings.mlr.press/v97/oymak19a.html,http://proceedings.mlr.press/v97/oymak19a/oymak19a.pdf,ICML
543,2019,Manifold Mixup: Better Representations by Interpolating Hidden States,"Vikas Verma,         Alex Lamb,         Christopher Beckham,         Amir Najafi,         Ioannis Mitliagkas,         David Lopez-Paz,         Yoshua Bengio","Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose \manifoldmixup{}, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. \manifoldmixup{} leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with \manifoldmixup{} learn flatter class-representations, that is, with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it empirically on practical situations, and connect it to the previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, \manifoldmixup{} improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.",http://proceedings.mlr.press/v97/verma19a.html,http://proceedings.mlr.press/v97/verma19a/verma19a.pdf,ICML
544,2019,Stay With Me: Lifetime Maximization Through Heteroscedastic Linear Bandits With Reneging,"Ping-Chun Hsieh,         Xi Liu,         Anirban Bhattacharya,         P R Kumar","Sequential decision making for lifetime maximization is a critical problem in many real-world applications, such as medical treatment and portfolio selection. In these applications, a “reneging” phenomenon, where participants may disengage from future interactions after observing an unsatisfiable outcome, is rather prevalent. To address the above issue, this paper proposes a model of heteroscedastic linear bandits with reneging, which allows each participant to have a distinct “satisfaction level,"" with any interaction outcome falling short of that level resulting in that participant reneging. Moreover, it allows the variance of the outcome to be context-dependent. Based on this model, we develop a UCB-type policy, namely HR-UCB, and prove that it achieves O(T(log(T))3−−−−−−−−−√)O(T(log⁡(T))3)\mathcal{O}\big(\sqrt{{T}(\log({T}))^{3}}\big) regret. Finally, we validate the performance of HR-UCB via simulations.",http://proceedings.mlr.press/v97/hsieh19a.html,http://proceedings.mlr.press/v97/hsieh19a/hsieh19a.pdf,ICML
545,2019,On Efficient Optimal Transport: An Analysis of Greedy and Accelerated Mirror Descent Algorithms,"Tianyi Lin,         Nhat Ho,         Michael Jordan","We provide theoretical analyses for two algorithms that solve the regularized optimal transport (OT) problem between two discrete probability measures with at most nnn atoms. We show that a greedy variant of the classical Sinkhorn algorithm, known as the Greenkhorn algorithm, can be improved to \bigOtil(n2/ε2)\bigOtil(n2/ε2)\bigOtil\left(n^2/\varepsilon^2\right), improving on the best known complexity bound of \bigOtil(n2/ε3)\bigOtil(n2/ε3)\bigOtil\left(n^2/\varepsilon^3\right). This matches the best known complexity bound for the Sinkhorn algorithm and helps explain why the Greenkhorn algorithm outperforms the Sinkhorn algorithm in practice. Our proof technique is based on a primal-dual formulation and provide a tight upper bound for the dual solution, leading to a class of adaptive primal-dual accelerated mirror descent (APDAMD) algorithms. We prove that the complexity of these algorithms is \bigOtil(n2γ−−√/ε)\bigOtil(n2γ/ε)\bigOtil\left(n^2\sqrt{\gamma}/\varepsilon\right) in which γ∈(0,n]γ∈(0,n]\gamma \in (0, n] refers to some constants in the Bregman divergence. Experimental results on synthetic and real datasets demonstrate the favorable performance of the Greenkhorn and APDAMD algorithms in practice.",http://proceedings.mlr.press/v97/lin19a.html,http://proceedings.mlr.press/v97/lin19a/lin19a.pdf,ICML
546,2019,On Connected Sublevel Sets in Deep Learning,Quynh Nguyen,This paper shows that every sublevel set of the loss function of a class of deep over-parameterized neural nets with piecewise linear activation functions is connected and unbounded. This implies that the loss has no bad local valleys and all of its global minima are connected within a unique and potentially very large global valley.,http://proceedings.mlr.press/v97/nguyen19a.html,http://proceedings.mlr.press/v97/nguyen19a/nguyen19a.pdf,ICML
547,2019,LatentGNN: Learning Efficient Non-local Relations for Visual Recognition,"Songyang Zhang,         Xuming He,         Shipeng Yan","Capturing long-range dependencies in feature representations is crucial for many visual recognition tasks. Despite recent successes of deep convolutional networks, it remains challenging to model non-local context relations between visual features. A promising strategy is to model the feature context by a fully-connected graph neural network (GNN), which augments traditional convolutional features with an estimated non-local context representation. However, most GNN-based approaches require computing a dense graph affinity matrix and hence have difficulty in scaling up to tackle complex real-world visual problems. In this work, we propose an efficient and yet flexible non-local relation representation based on a novel class of graph neural networks. Our key idea is to introduce a latent space to reduce the complexity of graph, which allows us to use a low-rank representation for the graph affinity matrix and to achieve a linear complexity in computation. Extensive experimental evaluations on three major visual recognition tasks show that our method outperforms the prior works with a large margin while maintaining a low computation cost.",http://proceedings.mlr.press/v97/zhang19f.html,http://proceedings.mlr.press/v97/zhang19f/zhang19f.pdf,ICML
548,2019,Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization,"Than Huy Nguyen,         Umut Simsekli,         Gael Richard","Recent studies on diffusion-based sampling methods have shown that Langevin Monte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and rigorous theoretical guarantees have been proven for both asymptotic and finite-time regimes. Algorithmically, LMC-based algorithms resemble the well-known gradient descent (GD) algorithm, where the GD recursion is perturbed by an additive Gaussian noise whose variance has a particular form. Fractional Langevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the Gaussian noise is replaced by a heavy-tailed αα\alpha-stable noise. As opposed to its Gaussian counterpart, these heavy-tailed perturbations can incur large jumps and it has been empirically demonstrated that the choice of αα\alpha-stable noise can provide several advantages in modern machine learning problems, both in optimization and sampling contexts. However, as opposed to LMC, only asymptotic convergence properties of FLMC have been yet established. In this study, we analyze the non-asymptotic behavior of FLMC for non-convex optimization and prove finite-time bounds for its expected suboptimality. Our results show that the weak-error of FLMC increases faster than LMC, which suggests using smaller step-sizes in FLMC. We finally extend our results to the case where the exact gradients are replaced by stochastic gradients and show that similar results hold in this setting as well.",http://proceedings.mlr.press/v97/nguyen19c.html,http://proceedings.mlr.press/v97/nguyen19c/nguyen19c.pdf,ICML
549,2019,Gauge Equivariant Convolutional Networks and the Icosahedral CNN,"Taco Cohen,         Maurice Weiler,         Berkay Kicanaoglu,         Max Welling","The principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning. We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold, we are able to implement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and practical alternative to Spherical CNNs. Using this method, we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.",http://proceedings.mlr.press/v97/cohen19d.html,http://proceedings.mlr.press/v97/cohen19d/cohen19d.pdf,ICML
550,2019,Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces,"Philipp Becker,         Harit Pandya,         Gregor Gebhardt,         Cheng Zhao,         C. James Taylor,         Gerhard Neumann","In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models, however, such approaches typically rely on approximate inference tech- niques such as variational inference which makes learning more complex and often less scalable due to approximation errors. We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations. Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time step. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter & Schmidhuber, 1997) but uses an explicit representation of uncertainty. As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.",http://proceedings.mlr.press/v97/becker19a.html,http://proceedings.mlr.press/v97/becker19a/becker19a.pdf,ICML
551,2019,Minimal Achievable Sufficient Statistic Learning,"Milan Cvitkovic,         Günther Koliander","We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a machine learning training objective for which the minima are minimal sufficient statistics with respect to a class of functions being optimized over (e.g., deep networks). In deriving MASS Learning, we also introduce Conserved Differential Information (CDI), an information-theoretic quantity that {—} unlike standard mutual information {—} can be usefully applied to deterministically-dependent continuous random variables like the input and output of a deep network. In a series of experiments, we show that deep networks trained with MASS Learning achieve competitive performance on supervised learning, regularization, and uncertainty quantification benchmarks.",http://proceedings.mlr.press/v97/cvitkovic19a.html,http://proceedings.mlr.press/v97/cvitkovic19a/cvitkovic19a.pdf,ICML
552,2019,The Variational Predictive Natural Gradient,"Da Tang,         Rajesh Ranganath","Variational inference transforms posterior inference into parametric optimization thereby enabling the use of latent variable models where otherwise impractical. However, variational inference can be finicky when different variational parameters control variables that are strongly correlated under the model. Traditional natural gradients based on the variational approximation fail to correct for correlations when the approximation is not the true posterior. To address this, we construct a new natural gradient called the Variational Predictive Natural Gradient (VPNG). Unlike traditional natural gradients for variational inference, this natural gradient accounts for the relationship between model parameters and variational parameters. We demonstrate the insight with a simple example as well as the empirical value on a classification task, a deep generative model of images, and probabilistic matrix factorization for recommendation.",http://proceedings.mlr.press/v97/tang19c.html,http://proceedings.mlr.press/v97/tang19c/tang19c.pdf,ICML
553,2019,Breaking Inter-Layer Co-Adaptation by Classifier Anonymization,"Ikuro Sato,         Kohta Ishikawa,         Guoqing Liu,         Masayuki Tanaka","This study addresses an issue of co-adaptation between a feature extractor and a classifier in a neural network. A naive joint optimization of a feature extractor and a classifier often brings situations in which an excessively complex feature distribution adapted to a very specific classifier degrades the test performance. We introduce a method called Feature-extractor Optimization through Classifier Anonymization (FOCA), which is designed to avoid an explicit co-adaptation between a feature extractor and a particular classifier by using many randomly-generated, weak classifiers during optimization. We put forth a mathematical proposition that states the FOCA features form a point-like distribution within the same class in a class-separable fashion under special conditions. Real-data experiments under more general conditions provide supportive evidences.",http://proceedings.mlr.press/v97/sato19a.html,http://proceedings.mlr.press/v97/sato19a/sato19a.pdf,ICML
554,2019,Bayesian Counterfactual Risk Minimization,"Ben London,         Ted Sandler","We present a Bayesian view of counterfactual risk minimization (CRM) for offline learning from logged bandit feedback. Using PAC-Bayesian analysis, we derive a new generalization bound for the truncated inverse propensity score estimator. We apply the bound to a class of Bayesian policies, which motivates a novel, potentially data-dependent, regularization technique for CRM. Experimental results indicate that this technique outperforms standard L2L2L_2 regularization, and that it is competitive with variance regularization while being both simpler to implement and more computationally efficient.",http://proceedings.mlr.press/v97/london19a.html,http://proceedings.mlr.press/v97/london19a/london19a.pdf,ICML
555,2019,Multiplicative Weights Updates as a distributed constrained optimization algorithm: Convergence to second-order stationary points almost always,"Ioannis Panageas,         Georgios Piliouras,         Xiao Wang","Non-concave maximization has been the subject of much recent study in the optimization and machine learning communities, specifically in deep learning. Recent papers ([Ge et al. 2015, Lee et al 2017] and references therein) indicate that first order methods work well and avoid saddles points. Results as in [Lee \etal 2017], however, are limited to the unconstrained case or for cases where the critical points are in the interior of the feasibility set, which fail to capture some of the most interesting applications. In this paper we focus on constrained non-concave maximization. We analyze a variant of a well-established algorithm in machine learning called Multiplicative Weights Update (MWU) for the maximization problem maxx∈DP(x)\max_{\mathbf{x} \in D} P(\mathbf{x}), where PP is non-concave, twice continuously differentiable and DD is a product of simplices. We show that MWU converges almost always for small enough stepsizes to critical points that satisfy the second order KKT conditions, by combining techniques from dynamical systems as well as taking advantage of a recent connection between Baum Eagon inequality and MWU [Palaiopanos et al 2017].",http://proceedings.mlr.press/v97/panageas19a.html,http://proceedings.mlr.press/v97/panageas19a/panageas19a.pdf,ICML
556,2019,Stochastic Optimization for DC Functions and Non-smooth Non-convex Regularizers with Non-asymptotic Convergence,"Yi Xu,         Qi Qi,         Qihang Lin,         Rong Jin,         Tianbao Yang","Difference of convex (DC) functions cover a broad family of non-convex and possibly non-smooth and non-differentiable functions, and have wide applications in machine learning and statistics. Although deterministic algorithms for DC functions have been extensively studied, stochastic optimization that is more suitable for learning with big data remains under-explored. In this paper, we propose new stochastic optimization algorithms and study their first-order convergence theories for solving a broad family of DC functions. We improve the existing algorithms and theories of stochastic optimization for DC functions from both practical and theoretical perspectives. Moreover, we extend the proposed stochastic algorithms for DC functions to solve problems with a general non-convex non-differentiable regularizer, which does not necessarily have a DC decomposition but enjoys an efficient proximal mapping. To the best of our knowledge, this is the first work that gives the first non-asymptotic convergence for solving non-convex optimization whose objective has a general non-convex non-differentiable regularizer.",http://proceedings.mlr.press/v97/xu19c.html,http://proceedings.mlr.press/v97/xu19c/xu19c.pdf,ICML
557,2019,Open Vocabulary Learning on Source Code with a Graph-Structured Cache,"Milan Cvitkovic,         Badal Singh,         Animashree Anandkumar","Machine learning models that take computer program source code as input typically use Natural Language Processing (NLP) techniques. However, a major challenge is that code is written using an open, rapidly changing vocabulary due to, e.g., the coinage of new variable and method names. Reasoning over such a vocabulary is not something for which most NLP methods are designed. We introduce a Graph-Structured Cache to address this problem; this cache contains a node for each new word the model encounters with edges connecting each word to its occurrences in the code. We find that combining this graph-structured cache strategy with recent Graph-Neural-Network-based models for supervised learning on code improves the models’ performance on a code completion task and a variable naming task — with over 100% relative improvement on the latter — at the cost of a moderate increase in computation time.",http://proceedings.mlr.press/v97/cvitkovic19b.html,http://proceedings.mlr.press/v97/cvitkovic19b/cvitkovic19b.pdf,ICML
558,2019,GOODE: A Gaussian Off-The-Shelf Ordinary Differential Equation Solver,"David John,         Vincent Heuveline,         Michael Schober","There are two types of ordinary differential equations (ODEs): initial value problems (IVPs) and boundary value problems (BVPs). While many probabilistic numerical methods for the solution of IVPs have been presented to-date, there exists no efficient probabilistic general-purpose solver for nonlinear BVPs. Our method based on iterated Gaussian process (GP) regression returns a GP posterior over the solution of nonlinear ODEs, which provides a meaningful error estimation via its predictive posterior standard deviation. Our solver is fast (typically of quadratic convergence rate) and the theory of convergence can be transferred from prior non-probabilistic work. Our method performs on par with standard codes for an established benchmark of test problems.",http://proceedings.mlr.press/v97/john19a.html,http://proceedings.mlr.press/v97/john19a/john19a.pdf,ICML
559,2019,What is the Effect of Importance Weighting in Deep Learning?,"Jonathon Byrd,         Zachary Lipton","Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. While the effect of importance weighting is well-characterized for low-capacity misspecified models, little is known about how it impacts over-parameterized, deep neural networks. This work is inspired by recent theoretical results showing that on (linearly) separable data, deep linear networks optimized by SGD learn weight-agnostic solutions, prompting us to ask, for realistic deep networks, for which many practical datasets are separable, what is the effect of importance weighting? We present the surprising finding that while importance weighting impacts models early in training, its effect diminishes over successive epochs. Moreover, while L2 regularization and batch normalization (but not dropout), restore some of the impact of importance weighting, they express the effect via (seemingly) the wrong abstraction: why should practitioners tweak the L2 regularization, and by how much, to produce the correct weighting effect? Our experiments confirm these findings across a range of architectures and datasets.",http://proceedings.mlr.press/v97/byrd19a.html,http://proceedings.mlr.press/v97/byrd19a/byrd19a.pdf,ICML
560,2019,Width Provably Matters in Optimization for Deep Linear Neural Networks,"Simon Du,         Wei Hu","We prove that for an LLL-layer fully-connected linear neural network, if the width of every hidden layer is Ω˜(L⋅r⋅dout⋅κ3)Ω~(L⋅r⋅dout⋅κ3)\widetilde{\Omega}\left(L \cdot r \cdot d_{out} \cdot \kappa^3 \right), where rrr and κκ\kappa are the rank and the condition number of the input data, and doutdoutd_{out} is the output dimension, then gradient descent with Gaussian random initialization converges to a global minimum at a linear rate. The number of iterations to find an ϵϵ\epsilon-suboptimal solution is O(κlog(1ϵ))O(κlog⁡(1ϵ))O(\kappa \log(\frac{1}{\epsilon})). Our polynomial upper bound on the total running time for wide deep linear networks and the exp(Ω(L))exp⁡(Ω(L))\exp\left(\Omega\left(L\right)\right) lower bound for narrow deep linear neural networks [Shamir, 2018] together demonstrate that wide layers are necessary for optimizing deep models.",http://proceedings.mlr.press/v97/du19a.html,http://proceedings.mlr.press/v97/du19a/du19a.pdf,ICML
561,2019,Molecular Hypergraph Grammar with Its Application to Molecular Optimization,Hiroshi Kajino,"Molecular optimization aims to discover novel molecules with desirable properties, and its two fundamental challenges are: (i) it is not trivial to generate valid molecules in a controllable way due to hard chemical constraints such as the valency conditions, and (ii) it is often costly to evaluate a property of a novel molecule, and therefore, the number of property evaluations is limited. These challenges are to some extent alleviated by a combination of a variational autoencoder (VAE) and Bayesian optimization (BO), where VAE converts a molecule into/from its latent continuous vector, and BO optimizes a latent continuous vector (and its corresponding molecule) within a limited number of property evaluations. While the most recent work, for the first time, achieved 100% validity, its architecture is rather complex due to auxiliary neural networks other than VAE, making it difficult to train. This paper presents a molecular hypergraph grammar variational autoencoder (MHG-VAE), which uses a single VAE to achieve 100% validity. Our idea is to develop a graph grammar encoding the hard chemical constraints, called molecular hypergraph grammar (MHG), which guides VAE to always generate valid molecules. We also present an algorithm to construct MHG from a set of molecules.",http://proceedings.mlr.press/v97/kajino19a.html,http://proceedings.mlr.press/v97/kajino19a/kajino19a.pdf,ICML
562,2019,Imputing Missing Events in Continuous-Time Event Streams,"Hongyuan Mei,         Guanghui Qin,         Jason Eisner","Events in the world may be caused by other, unobserved events. We consider sequences of events in continuous time. Given a probability model of complete sequences, we propose particle smoothing—a form of sequential importance sampling—to impute the missing events in an incomplete sequence. We develop a trainable family of proposal distributions based on a type of bidirectional continuous-time LSTM: Bidirectionality lets the proposals condition on future observations, not just on the past as in particle filtering. Our method can sample an ensemble of possible complete sequences (particles), from which we form a single consensus prediction that has low Bayes risk under our chosen loss metric. We experiment in multiple synthetic and real domains, using different missingness mechanisms, and modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events, with particle smoothing consistently improving upon particle filtering.",http://proceedings.mlr.press/v97/mei19a.html,http://proceedings.mlr.press/v97/mei19a/mei19a.pdf,ICML
563,2019,Learning Context-dependent Label Permutations for Multi-label Classification,"Jinseok Nam,         Young-Bum Kim,         Eneldo Loza Mencia,         Sunghyun Park,         Ruhi Sarikaya,         Johannes Fürnkranz","A key problem in multi-label classification is to utilize dependencies among the labels. Chaining classifiers are a simple technique for addressing this problem but current algorithms all assume a fixed, static label ordering. In this work, we propose a multi-label classification approach which allows to choose a dynamic, context-dependent label ordering. Our proposed approach consists of two sub-components: a simple EM-like algorithm which bootstraps the learned model, and a more elaborate approach based on reinforcement learning. Our experiments on three public multi-label classification benchmarks show that our proposed dynamic label ordering approach based on reinforcement learning outperforms recurrent neural networks with fixed label ordering across both bipartition and ranking measures on all the three datasets. As a result, we obtain a powerful sequence prediction-based algorithm for multi-label classification, which is able to efficiently and explicitly exploit label dependencies.",http://proceedings.mlr.press/v97/nam19a.html,http://proceedings.mlr.press/v97/nam19a/nam19a.pdf,ICML
564,2019,A Dynamical Systems Perspective on Nesterov Acceleration,"Michael Muehlebach,         Michael Jordan","We present a dynamical system framework for understanding Nesterov’s accelerated gradient method. In contrast to earlier work, our derivation does not rely on a vanishing step size argument. We show that Nesterov acceleration arises from discretizing an ordinary differential equation with a semi-implicit Euler integration scheme. We analyze both the underlying differential equation as well as the discretization to obtain insights into the phenomenon of acceleration. The analysis suggests that a curvature-dependent damping term lies at the heart of the phenomenon. We further establish connections between the discretized and the continuous-time dynamics.",http://proceedings.mlr.press/v97/muehlebach19a.html,http://proceedings.mlr.press/v97/muehlebach19a/muehlebach19a.pdf,ICML
565,2019,Provably Efficient Maximum Entropy Exploration,"Elad Hazan,         Sham Kakade,         Karan Singh,         Abby Van Soest","Suppose an agent is in a (possibly unknown) Markov Decision Process in the absence of a reward signal, what might we hope that an agent can efficiently learn to do? This work studies a broad class of objectives that are defined solely as functions of the state-visitation frequencies that are induced by how the agent behaves. For example, one natural, intrinsically defined, objective problem is for the agent to learn a policy which induces a distribution over state space that is as uniform as possible, which can be measured in an entropic sense. We provide an efficient algorithm to optimize such such intrinsically defined objectives, when given access to a black box planning oracle (which is robust to function approximation). Furthermore, when restricted to the tabular setting where we have sample based access to the MDP, our proposed algorithm is provably efficient, both in terms of its sample and computational complexities. Key to our algorithmic methodology is utilizing the conditional gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes an approximate MDP solver.",http://proceedings.mlr.press/v97/hazan19a.html,http://proceedings.mlr.press/v97/hazan19a/hazan19a.pdf,ICML
566,2019,Self-Supervised Exploration via Disagreement,"Deepak Pathak,         Dhiraj Gandhi,         Abhinav Gupta","Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent’s policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/",http://proceedings.mlr.press/v97/pathak19a.html,http://proceedings.mlr.press/v97/pathak19a/pathak19a.pdf,ICML
567,2019,Discovering Latent Covariance Structures for Multiple Time Series,"Anh Tong,         Jaesik Choi","Analyzing multivariate time series data is important to predict future events and changes of complex systems in finance, manufacturing, and administrative decisions. The expressiveness power of Gaussian Process (GP) regression methods has been significantly improved by compositional covariance structures. In this paper, we present a new GP model which naturally handles multiple time series by placing an Indian Buffet Process (IBP) prior on the presence of shared kernels. Our selective covariance structure decomposition allows exploiting shared parameters over a set of multiple, selected time series. We also investigate the well-definedness of the models when infinite latent components are introduced. We present a pragmatic search algorithm which explores a larger structure space efficiently. Experiments conducted on five real-world data sets demonstrate that our new model outperforms existing methods in term of structure discoveries and predictive performances.",http://proceedings.mlr.press/v97/tong19a.html,http://proceedings.mlr.press/v97/tong19a/tong19a.pdf,ICML
568,2019,An Optimal Private Stochastic-MAB Algorithm based on Optimal Private Stopping Rule,"Touqir Sajed,         Or Sheffet","We present a provably optimal differentially private algorithm for the stochastic multi-arm bandit problem, as opposed to the private analogue of the UCB-algorithm (Mishra and Thakurta, 2015; Tossou and Dimitrakakis, 2016) which doesn’t meet the recently discovered lower-bound of Ω(Klog(T)ϵ)\Omega \left(\frac{K\log(T)}{\epsilon} \right) (Shariff and Sheffet, 2018). Our construction is based on a different algorithm, Successive Elimination (Even-Dar et al., 2002), that repeatedly pulls all remaining arms until an arm is found to be suboptimal and is then eliminated. In order to devise a private analogue of Successive Elimination we visit the problem of private stopping rule, that takes as input a stream of i.i.d samples from an unknown distribution and returns a multiplicative (1±α)(1 \pm \alpha)-approximation of the distribution’s mean, and prove the optimality of our private stopping rule. We then present the private Successive Elimination algorithm which meets both the non-private lower bound (Lai and Robbins, 1985) and the above-mentioned private lower bound. We also compare empirically the performance of our algorithm with the private UCB algorithm.",http://proceedings.mlr.press/v97/sajed19a.html,http://proceedings.mlr.press/v97/sajed19a/sajed19a.pdf,ICML
569,2019,Obtaining Fairness using Optimal Transport Theory,"Paula Gordaliza,         Eustasio Del Barrio,         Gamboa Fabrice,         Jean-Michel Loubes","In the fair classification setup, we recast the links between fairness and predictability in terms of probability metrics. We analyze repair methods based on mapping conditional distributions to the Wasserstein barycenter. We propose a Random Repair which yields a tradeoff between minimal information loss and a certain amount of fairness.",http://proceedings.mlr.press/v97/gordaliza19a.html,http://proceedings.mlr.press/v97/gordaliza19a/gordaliza19a.pdf,ICML
570,2019,Monge blunts Bayes: Hardness Results for Adversarial Training,"Zac Cranko,         Aditya Menon,         Richard Nock,         Cheng Soon Ong,         Zhan Shi,         Christian Walder","The last few years have seen a staggering number of empirical studies of the robustness of neural networks in a model of adversarial perturbations of their inputs. Most rely on an adversary which carries out local modifications within prescribed balls. None however has so far questioned the broader picture: how to frame a resource-bounded adversary so that it can be severely detrimental to learning, a non-trivial problem which entails at a minimum the choice of loss and classifiers. We suggest a formal answer for losses that satisfy the minimal statistical requirement of being proper. We pin down a simple sufficient property for any given class of adversaries to be detrimental to learning, involving a central measure of “harmfulness” which generalizes the well-known class of integral probability metrics. A key feature of our result is that it holds for all proper losses, and for a popular subset of these, the optimisation of this central measure appears to be independent of the loss. When classifiers are Lipschitz – a now popular approach in adversarial training –, this optimisation resorts to optimal transport to make a low-budget compression of class marginals. Toy experiments reveal a finding recently separately observed: training against a sufficiently budgeted adversary of this kind improves generalization.",http://proceedings.mlr.press/v97/cranko19a.html,http://proceedings.mlr.press/v97/cranko19a/cranko19a.pdf,ICML
571,2019,Learning Neurosymbolic Generative Models via Program Synthesis,"Halley Young,         Osbert Bastani,         Mayur Naik","Generative models have become significantly more powerful in recent years. However, these models continue to have difficulty capturing global structure in data. For example, images of buildings typically contain spatial patterns such as windows repeating at regular intervals, but state-of-the-art models have difficulty generating these patterns. We propose to address this problem by incorporating programs representing global structure into generative models{—}e.g., a 2D for-loop may represent a repeating pattern of windows{—}along with a framework for learning these models by leveraging program synthesis to obtain training data. On both synthetic and real-world data, we demonstrate that our approach substantially outperforms state-of-the-art at both generating and completing images with global structure.",http://proceedings.mlr.press/v97/young19a.html,http://proceedings.mlr.press/v97/young19a/young19a.pdf,ICML
572,2019,Bandit Multiclass Linear Classification: Efficient Algorithms for the Separable Case,"Alina Beygelzimer,         David Pal,         Balazs Szorenyi,         Devanathan Thiruvenkatachari,         Chen-Yu Wei,         Chicheng Zhang","We study the problem of efficient online multiclass linear classification with bandit feedback, where all examples belong to one of KK classes and lie in the dd-dimensional Euclidean space. Previous works have left open the challenge of designing efficient algorithms with finite mistake bounds when the data is linearly separable by a margin γ\gamma. In this work, we take a first step towards this problem. We consider two notions of linear separability: strong and weak. 1. Under the strong linear separability condition, we design an efficient algorithm that achieves a near-optimal mistake bound of O(Kγ2)O\left(\frac{K}{\gamma^2} \right). 2. Under the more challenging weak linear separability condition, we design an efficient algorithm with a mistake bound of 2˜O(min(Klog21γ,√1γlogK))2^{\widetilde{O}(\min(K \log^2 \frac{1}{\gamma}, \sqrt{\frac{1}{\gamma}} \log K))}. Our algorithm is based on kernel Perceptron, which is inspired by the work of Klivans & Servedio (2008) on improperly learning intersection of halfspaces.",http://proceedings.mlr.press/v97/beygelzimer19a.html,http://proceedings.mlr.press/v97/beygelzimer19a/beygelzimer19a.pdf,ICML
573,2019,Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds,"Andrea Zanette,         Emma Brunskill","Strong worst-case performance bounds for episodic reinforcement learning exist but fortunately in practice RL algorithms perform much better than such bounds would predict. Algorithms and theory that provide strong problem-dependent bounds could help illuminate the key features of what makes a RL problem hard and reduce the barrier to using RL algorithms in practice. As a step towards this we derive an algorithm and analysis for finite horizon discrete MDPs with state-of-the-art worst-case regret bounds and substantially tighter bounds if the RL environment has special features but without apriori knowledge of the environment from the algorithm. As a result of our analysis, we also help address an open learning theory question \cite{jiang2018open} about episodic MDPs with a constant upper-bound on the sum of rewards, providing a regret bound function of the number of episodes with no dependence on the horizon.",http://proceedings.mlr.press/v97/zanette19a.html,http://proceedings.mlr.press/v97/zanette19a/zanette19a.pdf,ICML
574,2019,Reinforcement Learning in Configurable Continuous Environments,"Alberto Maria Metelli,         Emanuele Ghelfi,         Marcello Restelli","Configurable Markov Decision Processes (Conf-MDPs) have been recently introduced as an extension of the usual MDP model to account for the possibility of configuring the environment to improve the agent’s performance. Currently, there is still no suitable algorithm to solve the learning problem for real-world Conf-MDPs. In this paper, we fill this gap by proposing a trust-region method, Relative Entropy Model Policy Search (REMPS), able to learn both the policy and the MDP configuration in continuous domains without requiring the knowledge of the true model of the environment. After introducing our approach and providing a finite-sample analysis, we empirically evaluate REMPS on both benchmark and realistic environments by comparing our results with those of the gradient methods.",http://proceedings.mlr.press/v97/metelli19a.html,http://proceedings.mlr.press/v97/metelli19a/metelli19a.pdf,ICML
575,2019,Graph U-Nets,"Hongyang Gao,         Shuiwang Ji","We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.",http://proceedings.mlr.press/v97/gao19a.html,http://proceedings.mlr.press/v97/gao19a/gao19a.pdf,ICML
576,2019,Conditioning by adaptive sampling for robust design,"David Brookes,         Hahnbeom Park,         Jennifer Listgarten","We present a method for design problems wherein the goal is to maximize or specify the value of one or more properties of interest (e.g. maximizing the fluorescence of a protein). We assume access to black box, stochastic “oracle"" predictive functions, each of which maps from design space to a distribution over properties of interest. Because many state-of-the-art predictive models are known to suffer from pathologies, especially for data far from the training distribution, the problem becomes different from directly optimizing the oracles. Herein, we propose a method to solve this problem that uses model-based adaptive sampling to estimate a distribution over the design space, conditioned on the desired properties.",http://proceedings.mlr.press/v97/brookes19a.html,http://proceedings.mlr.press/v97/brookes19a/brookes19a.pdf,ICML
577,2019,Learning to Collaborate in Markov Decision Processes,"Goran Radanovic,         Rati Devidze,         David Parkes,         Adish Singla","We consider a two-agent MDP framework where agents repeatedly solve a task in a collaborative setting. We study the problem of designing a learning algorithm for the first agent (A1) that facilitates a successful collaboration even in cases when the second agent (A2) is adapting its policy in an unknown way. The key challenge in our setting is that the first agent faces non-stationarity in rewards and transitions because of the adaptive behavior of the second agent. We design novel online learning algorithms for agent A1 whose regret decays as O(T1−37⋅α)O(T1−37⋅α)O(T^{1-\frac{3}{7} \cdot \alpha}) with TTT learning episodes provided that the magnitude of agent A2’s policy changes between any two consecutive episodes are upper bounded by O(T−α)O(T−α)O(T^{-\alpha}). Here, the parameter αα\alpha is assumed to be strictly greater than 000, and we show that this assumption is necessary provided that the learning parity with noise problem is computationally hard. We show that sub-linear regret of agent A1 further implies near-optimality of the agents’ joint return for MDPs that manifest the properties of a smooth game.",http://proceedings.mlr.press/v97/radanovic19a.html,http://proceedings.mlr.press/v97/radanovic19a/radanovic19a.pdf,ICML
578,2019,Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables,"Friso Kingma,         Pieter Abbeel,         Jonathan Ho","The bits-back argument suggests that latent variable models can be turned into lossless compression schemes. Translating the bits-back argument into efficient and practical lossless compression schemes for general latent variable models, however, is still an open problem. Bits-Back with Asymmetric Numeral Systems (BB-ANS), recently proposed by Townsend et al,. 2019, makes bits-back coding practically feasible for latent variable models with one latent layer, but it is inefficient for hierarchical latent variable models. In this paper we propose Bit-Swap, a new compression scheme that generalizes BB-ANS and achieves strictly better compression rates for hierarchical latent variable models with Markov chain structure. Through experiments we verify that Bit-Swap results in lossless compression rates that are empirically superior to existing techniques.",http://proceedings.mlr.press/v97/kingma19a.html,http://proceedings.mlr.press/v97/kingma19a/kingma19a.pdf,ICML
579,2019,On the Limitations of Representing Functions on Sets,"Edward Wagstaff,         Fabian Fuchs,         Martin Engelcke,         Ingmar Posner,         Michael A. Osborne","Recent work on the representation of functions on sets has considered the use of summation in a latent space to enforce permutation invariance. In particular, it has been conjectured that the dimension of this latent space may remain fixed as the cardinality of the sets under consideration increases. However, we demonstrate that the analysis leading to this conjecture requires mappings which are highly discontinuous and argue that this is only of limited practical use. Motivated by this observation, we prove that an implementation of this model via continuous mappings (as provided by e.g. neural networks or Gaussian processes) actually imposes a constraint on the dimensionality of the latent space. Practical universal function representation for set inputs can only be achieved with a latent dimension at least the size of the maximum number of input elements.",http://proceedings.mlr.press/v97/wagstaff19a.html,http://proceedings.mlr.press/v97/wagstaff19a/wagstaff19a.pdf,ICML
580,2019,"Phase transition in PCA with missing data: Reduced signal-to-noise ratio, not sample size!","Niels Ipsen,         Lars Kai Hansen","How does missing data affect our ability to learn signal structures? It has been shown that learning signal structure in terms of principal components is dependent on the ratio of sample size and dimensionality and that a critical number of observations is needed before learning starts (Biehl and Mietzner, 1993). Here we generalize this analysis to include missing data. Probabilistic principal component analysis is regularly used for estimating signal structures in datasets with missing data. Our analytic result suggest that the effect of missing data is to effectively reduce signal-to-noise ratio rather than - as generally believed - to reduce sample size. The theory predicts a phase transition in the learning curves and this is indeed found both in simulation data and in real datasets.",http://proceedings.mlr.press/v97/ipsen19a.html,http://proceedings.mlr.press/v97/ipsen19a/ipsen19a.pdf,ICML
581,2019,Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance,"Dasaem Jeong,         Taegyun Kwon,         Yoojin Kim,         Juhan Nam","Music score is often handled as one-dimensional sequential data. Unlike words in a text document, notes in music score can be played simultaneously by the polyphonic nature and each of them has its own duration. In this paper, we represent the unique form of musical score using graph neural network and apply it for rendering expressive piano performance from the music score. Specifically, we design the model using note-level gated graph neural network and measure-level hierarchical attention network with bidirectional long short-term memory with an iterative feedback method. In addition, to model different styles of performance for a given input score, we employ a variational auto-encoder. The result of the listening test shows that our proposed model generated more human-like performances compared to a baseline model and a hierarchical attention network model that handles music score as a word-like sequence.",http://proceedings.mlr.press/v97/jeong19a.html,http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf,ICML
582,2019,Infinite Mixture Prototypes for Few-shot Learning,"Kelsey Allen,         Evan Shelhamer,         Hanul Shin,         Joshua Tenenbaum","We propose infinite mixture prototypes to adaptively represent both simple and complex data distributions for few-shot learning. Infinite mixture prototypes combine deep representation learning with Bayesian nonparametrics, representing each class by a set of clusters, unlike existing prototypical methods that represent each class by a single cluster. By inferring the number of clusters, infinite mixture prototypes interpolate between nearest neighbor and prototypical representations in a learned feature space, which improves accuracy and robustness in the few-shot regime. We show the importance of adaptive capacity for capturing complex data distributions such as super-classes (like alphabets in character recognition), with 10-25% absolute accuracy improvements over prototypical networks, while still maintaining or improving accuracy on standard few-shot learning benchmarks. By clustering labeled and unlabeled data with the same rule, infinite mixture prototypes achieve state-of-the-art semi-supervised accuracy, and can perform purely unsupervised clustering, unlike existing fully- and semi-supervised prototypical methods.",http://proceedings.mlr.press/v97/allen19b.html,http://proceedings.mlr.press/v97/allen19b/allen19b.pdf,ICML
583,2019,Learning interpretable continuous-time models of latent stochastic dynamical systems,"Lea Duncker,         Gergo Bohner,         Julien Boussard,         Maneesh Sahani","We develop an approach to learn an interpretable semi-parametric model of a latent continuous-time stochastic dynamical system, assuming noisy high-dimensional outputs sampled at uneven times. The dynamics are described by a nonlinear stochastic differential equation (SDE) driven by a Wiener process, with a drift evolution function drawn from a Gaussian process (GP) conditioned on a set of learnt fixed points and corresponding local Jacobian matrices. This form yields a flexible nonparametric model of the dynamics, with a representation corresponding directly to the interpretable portraits routinely employed in the study of nonlinear dynamical systems. The learning algorithm combines inference of continuous latent paths underlying observed data with a sparse variational description of the dynamical process. We demonstrate our approach on simulated data from different nonlinear dynamical systems.",http://proceedings.mlr.press/v97/duncker19a.html,http://proceedings.mlr.press/v97/duncker19a/duncker19a.pdf,ICML
584,2019,Rate Distortion For Model Compression:From Theory To Practice,"Weihao Gao,         Yu-Han Liu,         Chong Wang,         Sewoong Oh","The enormous size of modern deep neural net-works makes it challenging to deploy those models in memory and communication limited scenarios. Thus, compressing a trained model without a significant loss in performance has become an increasingly important task. Tremendous advances has been made recently, where the main technical building blocks are pruning, quantization, and low-rank factorization. In this paper, we propose principled approaches to improve upon the common heuristics used in those building blocks, by studying the fundamental limit for model compression via the rate distortion theory. We prove a lower bound for the rate distortion function for model compression and prove its achievability for linear models. Although this achievable compression scheme is intractable in practice, this analysis motivates a novel objective function for model compression, which can be used to improve classes of model compressor such as pruning or quantization. Theoretically, we prove that the proposed scheme is optimal for compressing one-hidden-layer ReLU neural networks. Empirically,we show that the proposed scheme improves upon the baseline in the compression-accuracy tradeoff.",http://proceedings.mlr.press/v97/gao19c.html,http://proceedings.mlr.press/v97/gao19c/gao19c.pdf,ICML
585,2019,Classifying Treatment Responders Under Causal Effect Monotonicity,Nathan Kallus,"In the context of individual-level causal inference, we study the problem of predicting whether someone will respond or not to a treatment based on their features and past examples of features, treatment indicator (e.g., drug/no drug), and a binary outcome (e.g., recovery from disease). As a classification task, the problem is made difficult by not knowing the example outcomes under the opposite treatment indicators. We assume the effect is monotonic, as in advertising’s effect on a purchase or bail-setting’s effect on reappearance in court: either it would have happened regardless of treatment, not happened regardless, or happened only depending on exposure to treatment. Predicting whether the latter is latently the case is our focus. While previous work focuses on conditional average treatment effect estimation, formulating the problem as a classification task allows us to develop new tools more suited to this problem. By leveraging monotonicity, we develop new discriminative and generative algorithms for the responder-classification problem. We explore and discuss connections to corrupted data and policy learning. We provide an empirical study with both synthetic and real datasets to compare these specialized algorithms to standard benchmarks.",http://proceedings.mlr.press/v97/kallus19a.html,http://proceedings.mlr.press/v97/kallus19a/kallus19a.pdf,ICML
586,2019,Sum-of-Squares Polynomial Flow,"Priyank Jaini,         Kira A. Selby,         Yaoliang Yu","Triangular map is a recent construct in probability theory that allows one to transform any source probability density function to any target density function. Based on triangular maps, we propose a general framework for high-dimensional density estimation, by specifying one-dimensional transformations (equivalently conditional densities) and appropriate conditioner networks. This framework (a) reveals the commonalities and differences of existing autoregressive and flow based methods, (b) allows a unified understanding of the limitations and representation power of these recent approaches and, (c) motivates us to uncover a new Sum-of-Squares (SOS) flow that is interpretable, universal, and easy to train. We perform several synthetic experiments on various density geometries to demonstrate the benefits (and short-comings) of such transformations. SOS flows achieve competitive results in simulations and several real-world datasets.",http://proceedings.mlr.press/v97/jaini19a.html,http://proceedings.mlr.press/v97/jaini19a/jaini19a.pdf,ICML
587,2019,Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs,"Yogesh Balaji,         Hamed Hassani,         Rama Chellappa,         Soheil Feizi","Building on the success of deep learning, two modern approaches to learn a probability model from the data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs). VAEs consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function. GANs, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data. The lack of having explicit probability models in GANs prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems. In this work, we resolve this issue by constructing an explicit probability model that can be used to compute sample likelihood statistics in GANs. In particular, we prove that under this probability model, a family of Wasserstein GANs with an entropy regularization can be viewed as a generative model that maximizes a variational lower-bound on average sample log likelihoods, an approach that VAEs are based on. This result makes a principled connection between two modern generative models, namely GANs and VAEs. In addition to the aforementioned theoretical results, we compute likelihood statistics for GANs trained on Gaussian, MNIST, SVHN, CIFAR-10 and LSUN datasets. Our numerical results validate the proposed theory.",http://proceedings.mlr.press/v97/balaji19a.html,http://proceedings.mlr.press/v97/balaji19a/balaji19a.pdf,ICML
588,2019,POLITEX: Regret Bounds for Policy Iteration using Expert Prediction,"Yasin Abbasi-Yadkori,         Peter Bartlett,         Kush Bhatia,         Nevena Lazic,         Csaba Szepesvari,         Gellert Weisz","We present POLITEX (POLicy ITeration with EXpert advice), a variant of policy iteration where each policy is a Boltzmann distribution over the sum of action-value function estimates of the previous policies, and analyze its regret in continuing RL problems. We assume that the value function error after running a policy for ττ\tau time steps scales as ϵ(τ)=ϵ0+O(d/τ−−−√)ϵ(τ)=ϵ0+O(d/τ)\epsilon(\tau) = \epsilon_0 + O(\sqrt{d/\tau}), where ϵ0ϵ0\epsilon_0 is the worst-case approximation error and ddd is the number of features in a compressed representation of the state-action space. We establish that this condition is satisfied by the LSPE algorithm under certain assumptions on the MDP and policies. Under the error assumption, we show that the regret of POLITEX in uniformly mixing MDPs scales as O(d1/2T3/4+ϵ0T)O(d1/2T3/4+ϵ0T)O(d^{1/2}T^{3/4} + \epsilon_0T), where O(⋅)O(⋅)O(\cdot) hides logarithmic terms and problem-dependent constants. Thus, we provide the first regret bound for a fully practical model-free method which only scales in the number of features, and not in the size of the underlying MDP. Experiments on a queuing problem confirm that POLITEX is competitive with some of its alternatives, while preliminary results on Ms Pacman (one of the standard Atari benchmark problems) confirm the viability of POLITEX beyond linear function approximation.",http://proceedings.mlr.press/v97/lazic19a.html,http://proceedings.mlr.press/v97/lazic19a/lazic19a.pdf,ICML
589,2019,Control Regularization for Reduced Variance Reinforcement Learning,"Richard Cheng,         Abhinav Verma,         Gabor Orosz,         Swarat Chaudhuri,         Yisong Yue,         Joel Burdick","Dealing with high variance is a significant challenge in model-free reinforcement learning (RL). Existing methods are unreliable, exhibiting high variance in performance from run to run using different initializations/seeds. Focusing on problems arising in continuous control, we propose a functional regularization approach to augmenting model-free RL. In particular, we regularize the behavior of the deep policy to be similar to a policy prior, i.e., we regularize in function space. We show that functional regularization yields a bias-variance trade-off, and propose an adaptive tuning strategy to optimize this trade-off. When the policy prior has control-theoretic stability guarantees, we further show that this regularization approximately preserves those stability guarantees throughout learning. We validate our approach empirically on a range of settings, and demonstrate significantly reduced variance, guaranteed dynamic stability, and more efficient learning than deep RL alone.",http://proceedings.mlr.press/v97/cheng19a.html,http://proceedings.mlr.press/v97/cheng19a/cheng19a.pdf,ICML
590,2019,Natural Analysts in Adaptive Data Analysis,"Tijana Zrnic,         Moritz Hardt","Adaptive data analysis is frequently criticized for its pessimistic generalization guarantees. The source of these pessimistic bounds is a model that permits arbitrary, possibly adversarial analysts that optimally use information to bias results. While being a central issue in the field, still lacking are notions of natural analysts that allow for more optimistic bounds faithful to the reality that typical analysts aren’t adversarial. In this work, we propose notions of natural analysts that smoothly interpolate between the optimal non-adaptive bounds and the best-known adaptive generalization bounds. To accomplish this, we model the analyst’s knowledge as evolving according to the rules of an unknown dynamical system that takes in revealed information and outputs new statistical queries to the data. This allows us to restrict the analyst through different natural control-theoretic notions. One such notion corresponds to a recency bias, formalizing an inability to arbitrarily use distant information. Another complementary notion formalizes an anchoring bias, a tendency to weight initial information more strongly. Both notions come with quantitative parameters that smoothly interpolate between the non-adaptive case and the fully adaptive case, allowing for a rich spectrum of intermediate analysts that are neither non-adaptive nor adversarial. Natural not only from a cognitive perspective, we show that our notions also capture standard optimization methods, like gradient descent in various settings. This gives a new interpretation to the fact that gradient descent tends to overfit much less than its adaptive nature might suggest.",http://proceedings.mlr.press/v97/zrnic19a.html,http://proceedings.mlr.press/v97/zrnic19a/zrnic19a.pdf,ICML
591,2019,Structured agents for physical construction,"Victor Bapst,         Alvaro Sanchez-Gonzalez,         Carl Doersch,         Kimberly Stachenfeld,         Pushmeet Kohli,         Peter Battaglia,         Jessica Hamrick","Physical construction—the ability to compose objects, subject to physical dynamics, to serve some function—is fundamental to human intelligence. We introduce a suite of challenging physical construction tasks inspired by how children play with blocks, such as matching a target configuration, stacking blocks to connect objects together, and creating shelter-like structures over target objects. We examine how a range of deep reinforcement learning agents fare on these challenges, and introduce several new approaches which provide superior performance. Our results show that agents which use structured representations (e.g., objects and scene graphs) and structured policies (e.g., object-centric actions) outperform those which use less structured representations, and generalize better beyond their training when asked to reason about larger scenes. Model-based agents which use Monte-Carlo Tree Search also outperform strictly model-free agents in our most challenging construction problems. We conclude that approaches which combine structured representations and reasoning with powerful learning are a key path toward agents that possess rich intuitive physics, scene understanding, and planning.",http://proceedings.mlr.press/v97/bapst19a.html,http://proceedings.mlr.press/v97/bapst19a/bapst19a.pdf,ICML
592,2019,Hessian Aided Policy Gradient,"Zebang Shen,         Alejandro Ribeiro,         Hamed Hassani,         Hui Qian,         Chao Mi","Reducing the variance of estimators for policy gradient has long been the focus of reinforcement learning research. 	While classic algorithms like REINFORCE find an ϵϵ\epsilon-approximate first-order stationary point in \OM(1/ϵ4)\OM(1/ϵ4)\OM({1}/{\epsilon^4}) random trajectory simulations, no provable improvement on the complexity has been made so far. 	This paper presents a Hessian aided policy gradient method with the first improved sample complexity of \OM(1/ϵ3)\OM(1/ϵ3)\OM({1}/{\epsilon^3}). 	While our method exploits information from the policy Hessian, it can be implemented in linear time with respect to the parameter dimension and is hence applicable to sophisticated DNN parameterization. 	Simulations on standard tasks validate the efficiency of our method.",http://proceedings.mlr.press/v97/shen19d.html,http://proceedings.mlr.press/v97/shen19d/shen19d.pdf,ICML
593,2019,SGD without Replacement: Sharper Rates for General Smooth Convex Functions,"Dheeraj Nagaraj,         Prateek Jain,         Praneeth Netrapalli","We study stochastic gradient descent without replacement (SGDo) for smooth convex functions. SGDo is widely observed to converge faster than true SGD where each sample is drawn independently with replacement (Bottou,2009) and hence, is more popular in practice. But it’s convergence properties are not well understood as sampling without replacement leads to coupling between iterates and gradients. By using method of exchangeable pairs to bound Wasserstein distance, we provide the first non-asymptotic results for SGDo when applied to general smooth, strongly-convex functions. In particular, we show that SGDo converges at a rate of O(1/K2)O(1/K^2) while SGD is known to converge at O(1/K)O(1/K) rate, where KK denotes the number of passes over data and is required to be large enough. Existing results for SGDo in this setting require additional Hessian Lipschitz assumption (Gurbuzbalaban et al, 2015; HaoChen and Sra 2018). For small KK, we show SGDo can achieve same convergence rate as SGD for general smooth strongly-convex functions. Existing results in this setting require K=1K=1 and hold only for generalized linear models (Shamir,2016). In addition, by careful analysis of the coupling, for both large and small KK, we obtain better dependence on problem dependent parameters like condition number.",http://proceedings.mlr.press/v97/nagaraj19a.html,http://proceedings.mlr.press/v97/nagaraj19a/nagaraj19a.pdf,ICML
594,2019,Model Comparison for Semantic Grouping,"Francisco Vargas,         Kamen Brestnichki,         Nils Hammerla","We introduce a probabilistic framework for quantifying the semantic similarity between two groups of embeddings. We formulate the task of semantic similarity as a model comparison task in which we contrast a generative model which jointly models two sentences versus one that does not. We illustrate how this framework can be used for the Semantic Textual Similarity tasks using clear assumptions about how the embeddings of words are generated. We apply model comparison that utilises information criteria to address some of the shortcomings of Bayesian model comparison, whilst still penalising model complexity. We achieve competitive results by applying the proposed framework with an appropriate choice of likelihood on the STS datasets.",http://proceedings.mlr.press/v97/vargas19a.html,http://proceedings.mlr.press/v97/vargas19a/vargas19a.pdf,ICML
595,2019,Zero-Shot Knowledge Distillation in Deep Networks,"Gaurav Kumar Nayak,         Konda Reddy Mopuri,         Vaisakh Shaj,         Venkatesh Babu Radhakrishnan,         Anirban Chakraborty","Knowledge distillation deals with the problem of training a smaller model (Student) from a high capacity source model (Teacher) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teacher has been trained may not always be feasible if the dataset is very large or it poses privacy or safety concerns (e.g., bio-metric or medical data). Hence, in this paper, we propose a novel data-free method to train the Student from the Teacher. Without even using any meta-data, we synthesize the Data Impressions from the complex Teacher model and utilize these as surrogates for the original training data samples to transfer its learning to Student via knowledge distillation. We, therefore, dub our method “Zero-Shot Knowledge Distillation"" and demonstrate that our framework results in competitive generalization performance as achieved by distillation using the actual training data samples on multiple benchmark datasets.",http://proceedings.mlr.press/v97/nayak19a.html,http://proceedings.mlr.press/v97/nayak19a/nayak19a.pdf,ICML
596,2019,Fair k-Center Clustering for Data Summarization,"Matthäus Kleindessner,         Pranjal Awasthi,         Jamie Morgenstern","In data summarization we want to choose kk prototypes in order to summarize a data set. We study a setting where the data set comprises several demographic groups and we are restricted to choose kik_i prototypes belonging to group ii. A common approach to the problem without the fairness constraint is to optimize a centroid-based clustering objective such as kk-center. A natural extension then is to incorporate the fairness constraint into the clustering problem. Existing algorithms for doing so run in time super-quadratic in the size of the data set, which is in contrast to the standard kk-center problem being approximable in linear time. In this paper, we resolve this gap by providing a simple approximation algorithm for the kk-center problem under the fairness constraint with running time linear in the size of the data set and kk. If the number of demographic groups is small, the approximation guarantee of our algorithm only incurs a constant-factor overhead.",http://proceedings.mlr.press/v97/kleindessner19a.html,http://proceedings.mlr.press/v97/kleindessner19a/kleindessner19a.pdf,ICML
597,2019,Unsupervised Label Noise Modeling and Loss Correction,"Eric Arazo,         Diego Ortego,         Paul Albert,         Noel O’Connor,         Kevin Mcguinness","Despite being robust to small amounts of label noise, convolutional neural networks trained with stochastic gradient methods have been shown to easily fit random labels. When there are a mixture of correct and mislabelled targets, networks tend to fit the former before the latter. This suggests using a suitable two-component mixture model as an unsupervised generative model of sample loss values during training to allow online estimation of the probability that a sample is mislabelled. Specifically, we propose a beta mixture to estimate this probability and correct the loss by relying on the network prediction (the so-called bootstrapping loss). We further adapt mixup augmentation to drive our approach a step further. Experiments on CIFAR-10/100 and TinyImageNet demonstrate a robustness to label noise that substantially outperforms recent state-of-the-art. Source code is available at https://git.io/fjsvE and Appendix at https://arxiv.org/abs/1904.11238.",http://proceedings.mlr.press/v97/arazo19a.html,http://proceedings.mlr.press/v97/arazo19a/arazo19a.pdf,ICML
598,2019,Grid-Wise Control for Multi-Agent Reinforcement Learning in Video Game AI,"Lei Han,         Peng Sun,         Yali Du,         Jiechao Xiong,         Qing Wang,         Xinghai Sun,         Han Liu,         Tong Zhang","We consider the problem of multi-agent reinforcement learning (MARL) in video game AI, where the agents are located in a spatial grid-world environment and the number of agents varies both within and across episodes. The challenge is to flexibly control an arbitrary number of agents while achieving effective collaboration. Existing MARL methods usually suffer from the trade-off between these two considerations. To address the issue, we propose a novel architecture that learns a spatial joint representation of all the agents and outputs grid-wise actions. Each agent will be controlled independently by taking the action from the grid it occupies. By viewing the state information as a grid feature map, we employ a convolutional encoder-decoder as the policy network. This architecture naturally promotes agent communication because of the large receptive field provided by the stacked convolutional layers. Moreover, the spatially shared convolutional parameters enable fast parallel exploration that the experiences discovered by one agent can be immediately transferred to others. The proposed method can be conveniently integrated with general reinforcement learning algorithms, e.g., PPO and Q-learning. We demonstrate the effectiveness of the proposed method in extensive challenging multi-agent tasks in StarCraft II.",http://proceedings.mlr.press/v97/han19a.html,http://proceedings.mlr.press/v97/han19a/han19a.pdf,ICML
599,2019,Matrix-Free Preconditioning in Online Learning,"Ashok Cutkosky,         Tamas Sarlos","We provide an online convex optimization algorithm with regret that interpolates between the regret of an algorithm using an optimal preconditioning matrix and one using a diagonal preconditioning matrix. Our regret bound is never worse than that obtained by diagonal preconditioning, and in certain setting even surpasses that of algorithms with full-matrix preconditioning. Importantly, our algorithm runs in the same time and space complexity as online gradient descent. Along the way we incorporate new techniques that mildly streamline and improve logarithmic factors in prior regret analyses. We conclude by benchmarking our algorithm on synthetic data and deep learning tasks.",http://proceedings.mlr.press/v97/cutkosky19b.html,http://proceedings.mlr.press/v97/cutkosky19b/cutkosky19b.pdf,ICML
600,2019,A Better k-means++ Algorithm via Local Search,"Silvio Lattanzi,         Christian Sohler","In this paper, we develop a new variant of k-means++ seeding that in expectation achieves a constant approximation guarantee. We obtain this result by a simple combination of k-means++ sampling with a local search strategy. We evaluate our algorithm empirically and show that it also improves the quality of a solution in practice.",http://proceedings.mlr.press/v97/lattanzi19a.html,http://proceedings.mlr.press/v97/lattanzi19a/lattanzi19a.pdf,ICML
601,2019,Probabilistic Neural Symbolic Models for Interpretable Visual Question Answering,"Ramakrishna Vedantam,         Karan Desai,         Stefan Lee,         Marcus Rohrbach,         Dhruv Batra,         Devi Parikh","We propose a new class of probabilistic neural-symbolic models, that have symbolic functional programs as a latent, stochastic variable. Instantiated in the context of visual question answering, our probabilistic formulation offers two key conceptual advantages over prior neural-symbolic models for VQA. Firstly, the programs generated by our model are more understandable while requiring less number of teaching examples. Secondly, we show that one can pose counterfactual scenarios to the model, to probe its beliefs on the programs that could lead to a specified answer given an image. Our results on the CLEVR and SHAPES datasets verify our hypotheses, showing that the model gets better program (and answer) prediction accuracy even in the low data regime, and allows one to probe the coherence and consistency of reasoning performed.",http://proceedings.mlr.press/v97/vedantam19a.html,http://proceedings.mlr.press/v97/vedantam19a/vedantam19a.pdf,ICML
602,2019,On the Convergence and Robustness of Adversarial Training,"Yisen Wang,         Xingjun Ma,         James Bailey,         Jinfeng Yi,         Bowen Zhou,         Quanquan Gu","Improving the robustness of deep neural networks (DNNs) to adversarial examples is an important yet challenging problem for secure deep learning. Across existing defense techniques, adversarial training with Projected Gradient Decent (PGD) is amongst the most effective. Adversarial training solves a min-max optimization problem, with the inner maximization generating adversarial examples by maximizing the classification loss, and the outer minimization finding model parameters by minimizing the loss on adversarial examples generated from the inner maximization. A criterion that measures how well the inner maximization is solved is therefore crucial for adversarial training. In this paper, we propose such a criterion, namely First-Order Stationary Condition for constrained optimization (FOSC), to quantitatively evaluate the convergence quality of adversarial examples found in the inner maximization. With FOSC, we find that to ensure better robustness, it is essential to use adversarial examples with better convergence quality at the later stages of training. Yet at the early stages, high convergence quality adversarial examples are not necessary and may even lead to poor robustness. Based on these observations, we propose a dynamic training strategy to gradually increase the convergence quality of the generated adversarial examples, which significantly improves the robustness of adversarial training. Our theoretical and empirical results show the effectiveness of the proposed method.",http://proceedings.mlr.press/v97/wang19i.html,http://proceedings.mlr.press/v97/wang19i/wang19i.pdf,ICML
603,2019,Tight Kernel Query Complexity of Kernel Ridge Regression and Kernel kkk-means Clustering,"Taisuke Yasuda,         David Woodruff,         Manuel Fernandez","Kernel methods generalize machine learning algorithms that only depend on the pairwise inner products of the dataset by replacing inner products with kernel evaluations, a function that passes input points through a nonlinear feature map before taking the inner product in a higher dimensional space. In this work, we present nearly tight lower bounds on the number of kernel evaluations required to approximately solve kernel ridge regression (KRR) and kernel kkk-means clustering (KKMC) on nnn input points. For KRR, our bound for relative error approximation the argmin of the objective function is Ω(ndλeff/ε)Ω(ndλeff/ε)\Omega(nd_{\mathrm{eff}}^\lambda/\varepsilon) where dλeffdλeffd_{\mathrm{eff}}^\lambda is the effective statistical dimension, tight up to a log(dλeff/ε)log(dλeff/ε)\log(d_{\mathrm{eff}}^\lambda/\varepsilon) factor. For KKMC, our bound for finding a kkk-clustering achieving a relative error approximation of the objective function is Ω(nk/ε)Ω(nk/ε)\Omega(nk/\varepsilon), tight up to a log(k/ε)log(k/ε)\log(k/\varepsilon) factor. Our KRR result resolves a variant of an open question of El Alaoui and Mahoney, asking whether the effective statistical dimension is a lower bound on the sampling complexity or not. Furthermore, for the important input distribution case of mixtures of Gaussians, we provide algorithms that bypass the above lower bounds.",http://proceedings.mlr.press/v97/yasuda19a.html,http://proceedings.mlr.press/v97/yasuda19a/yasuda19a.pdf,ICML
604,2019,Hiring Under Uncertainty,"Manish Purohit,         Sreenivas Gollapudi,         Manish Raghavan","In this paper we introduce the hiring under uncertainty problem to model the questions faced by hiring committees in large enterprises and universities alike. Given a set of nnn eligible candidates, the decision maker needs to choose the sequence of candidates to make offers so as to hire the kkk best candidates. However, candidates may choose to reject an offer (for instance, due to a competing offer) and the decision maker has a time limit by which all positions must be filled. Given an estimate of the probabilities of acceptance for each candidate, the hiring under uncertainty problem is to design a strategy of making offers so that the total expected value of all candidates hired by the time limit is maximized. We provide a 2-approximation algorithm for the setting where offers must be made in sequence, an 8-approximation when offers may be made in parallel, and a 10-approximation for the more general stochastic knapsack setting with finite probes.",http://proceedings.mlr.press/v97/purohit19a.html,http://proceedings.mlr.press/v97/purohit19a/purohit19a.pdf,ICML
605,2019,Contextual Memory Trees,"Wen Sun,         Alina Beygelzimer,         Hal Daumé Iii,         John Langford,         Paul Mineiro","We design and study a Contextual Memory Tree (CMT), a learning memory controller that inserts new memories into an experience store of unbounded size. It operates online and is designed to efficiently query for memories from that store, supporting logarithmic time insertion and retrieval operations. Hence CMT can be integrated into existing statistical learning algorithms as an augmented memory unit without substantially increasing training and inference computation. Furthermore CMT operates as a reduction to classification, allowing it to benefit from advances in representation or architecture. We demonstrate the efficacy of CMT by augmenting existing multi-class and multi-label classification algorithms with CMT and observe statistical improvement. We also test CMT learning on several image-captioning tasks to demonstrate that it performs computationally better than a simple nearest neighbors memory system while benefitting from reward learning.",http://proceedings.mlr.press/v97/sun19a.html,http://proceedings.mlr.press/v97/sun19a/sun19a.pdf,ICML
606,2019,MetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement,"Szu-Wei Fu,         Chien-Feng Liao,         Yu Tsao,         Shou-De Lin","Adversarial loss in a conditional generative adversarial network (GAN) is not designed to directly optimize evaluation metrics of a target task, and thus, may not always guide the generator in a GAN to generate data with improved metric scores. To overcome this issue, we propose a novel MetricGAN approach with an aim to optimize the generator with respect to one or multiple evaluation metrics. Moreover, based on MetricGAN, the metric scores of the generated data can also be arbitrarily specified by users. We tested the proposed MetricGAN on a speech enhancement task, which is particularly suitable to verify the proposed approach because there are multiple metrics measuring different aspects of speech signals. Moreover, these metrics are generally complex and could not be fully optimized by Lp or conventional adversarial losses.",http://proceedings.mlr.press/v97/fu19b.html,http://proceedings.mlr.press/v97/fu19b/fu19b.pdf,ICML
607,2019,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,"Mingxing Tan,         Quoc Le","Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.",http://proceedings.mlr.press/v97/tan19a.html,http://proceedings.mlr.press/v97/tan19a/tan19a.pdf,ICML
608,2019,Sorting Out Lipschitz Function Approximation,"Cem Anil,         James Lucas,         Roger Grosse","Training neural networks under a strict Lipschitz constraint is useful for provable adversarial robustness, generalization bounds, interpretable gradients, and Wasserstein distance estimation. By the composition property of Lipschitz functions, it suffices to ensure that each individual affine transformation or nonlinear activation is 1-Lipschitz. The challenge is to do this while maintaining the expressive power. We identify a necessary property for such an architecture: each of the layers must preserve the gradient norm during backpropagation. Based on this, we propose to combine a gradient norm preserving activation function, GroupSort, with norm-constrained weight matrices. We show that norm-constrained GroupSort architectures are universal Lipschitz function approximators. Empirically, we show that norm-constrained GroupSort networks achieve tighter estimates of Wasserstein distance than their ReLU counterparts and can achieve provable adversarial robustness guarantees with little cost to accuracy.",http://proceedings.mlr.press/v97/anil19a.html,http://proceedings.mlr.press/v97/anil19a/anil19a.pdf,ICML
609,2019,Composing Entropic Policies using Divergence Correction,"Jonathan Hunt,         Andre Barreto,         Timothy Lillicrap,         Nicolas Heess","Composing skills mastered in one task to solve novel tasks promises dramatic improvements in the data efficiency of reinforcement learning. Here, we analyze two recent works composing behaviors represented in the form of action-value functions and show that they perform poorly in some situations. As part of this analysis, we extend an important generalization of policy improvement to the maximum entropy framework and introduce an algorithm for the practical implementation of successor features in continuous action spaces. Then we propose a novel approach which addresses the failure cases of prior work and, in principle, recovers the optimal policy during transfer. This method works by explicitly learning the (discounted, future) divergence between base policies. We study this approach in the tabular case and on non-trivial continuous control problems with compositional structure and show that it outperforms or matches existing methods across all tasks considered.",http://proceedings.mlr.press/v97/hunt19a.html,http://proceedings.mlr.press/v97/hunt19a/hunt19a.pdf,ICML
610,2019,Bayesian leave-one-out cross-validation for large data,"Måns Magnusson,         Michael Andersen,         Johan Jonasson,         Aki Vehtari","Model inference, such as model comparison, model checking, and model selection, is an important part of model development. Leave-one-out cross-validation (LOO) is a general approach for assessing the generalizability of a model, but unfortunately, LOO does not scale well to large datasets. We propose a combination of using approximate inference techniques and probability-proportional-to-size-sampling (PPS) for fast LOO model evaluation for large datasets. We provide both theoretical and empirical results showing good properties for large data.",http://proceedings.mlr.press/v97/magnusson19a.html,http://proceedings.mlr.press/v97/magnusson19a/magnusson19a.pdf,ICML
611,2019,Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment,"Yifan Wu,         Ezra Winston,         Divyansh Kaushik,         Zachary Lipton","Domain adaptation addresses the common situation in which the target distribution generating our test data differs from the source distribution generating our training data. While absent assumptions, domain adaptation is impossible, strict conditions, e.g. covariate or label shift, enable principled algorithms. Recently-proposed domain-adversarial approaches consist of aligning source and target encodings, an approach often motivated as minimizing two (of three) terms in a theoretical bound on target error. Unfortunately, this minimization can cause arbitrary increases in the third term, a problem guaranteed to arise under shifting label distributions. We propose asymmetrically-relaxed distribution alignment, a new approach that overcomes some limitations of standard domain-adversarial algorithms. Moreover, we characterize precise assumptions under which our algorithm is theoretically principled and demonstrate empirical benefits on both synthetic and real datasets.",http://proceedings.mlr.press/v97/wu19f.html,http://proceedings.mlr.press/v97/wu19f/wu19f.pdf,ICML
612,2019,Greedy Orthogonal Pivoting Algorithm for Non-Negative Matrix Factorization,"Kai Zhang,         Sheng Zhang,         Jun Liu,         Jun Wang,         Jie Zhang","Non-negative matrix factorization is a powerful tool for learning useful representations in the data and has been widely applied in many problems such as data mining and signal processing. Orthogonal NMF, which can improve the locality of decomposition, has drawn considerable interest in solving clustering problems in recent years. However, imposing simultaneous non-negative and orthogonal structure can be quite difficult, and so existing algorithms can only solve it approximately. To address this challenge, we propose an innovative procedure called Greedy Orthogonal Pivoting Algorithm (GOPA). The GOPA algorithm fully exploits the sparsity of non-negative orthogonal solutions to break the global problem into a series of local optimizations, in which an adaptive subset of coordinates are updated in a greedy, closed-form manner. The biggest advantage of GOPA is that it promotes exact orthogonality and provides solid empirical evidence that stronger orthogonality does contribute favorably to better clustering performance. On the other hand, we further design randomized and parallel version of GOPA, which can further reduce the computational cost and improve accuracy, making it suitable for large data.",http://proceedings.mlr.press/v97/zhang19r.html,http://proceedings.mlr.press/v97/zhang19r/zhang19r.pdf,ICML
613,2019,On Scalable and Efficient Computation of Large Scale Optimal Transport,"Yujia Xie,         Minshuo Chen,         Haoming Jiang,         Tuo Zhao,         Hongyuan Zha","Optimal Transport (OT) naturally arises in many machine learning applications, yet the heavy computational burden limits its wide-spread uses. To address the scalability issue, we propose an implicit generative learning-based framework called SPOT (Scalable Push-forward of Optimal Transport). Specifically, we approximate the optimal transport plan by a pushforward of a reference distribution, and cast the optimal transport problem into a minimax problem. We then can solve OT problems efficiently using primal dual stochastic gradient-type algorithms. We also show that we can recover the density of the optimal transport plan using neural ordinary differential equations. Numerical experiments on both synthetic and real datasets illustrate that SPOT is robust and has favorable convergence behavior. SPOT also allows us to efficiently sample from the optimal transport plan, which benefits downstream applications such as domain adaptation.",http://proceedings.mlr.press/v97/xie19a.html,http://proceedings.mlr.press/v97/xie19a/xie19a.pdf,ICML
614,2019,Adjustment Criteria for Generalizing Experimental Findings,"Juan Correa,         Jin Tian,         Elias Bareinboim","Generalizing causal effects from a controlled experiment to settings beyond the particular study population is arguably one of the central tasks found in empirical circles. While a proper design and careful execution of the experiment would support, under mild conditions, the validity of inferences about the population in which the experiment was conducted, two challenges make the extrapolation step to different populations somewhat involved, namely, transportability and sampling selection bias. The former is concerned with disparities in the distributions and causal mechanisms between the domain (i.e., settings, population, environment) where the experiment is conducted and where the inferences are intended; the latter with distortions in the sample’s proportions due to preferential selection of units into the study. In this paper, we investigate the assumptions and machinery necessary for using covariate adjustment to correct for the biases generated by both of these problems, and generalize experimental data to infer causal effects in a new domain. We derive complete graphical conditions to determine if a set of covariates is admissible for adjustment in this new setting. Building on the graphical characterization, we develop an efficient algorithm that enumerates all possible admissible sets with poly-time delay guarantee; this can be useful for when some variables are preferred over the others due to different costs or amenability to measurement.",http://proceedings.mlr.press/v97/correa19a.html,http://proceedings.mlr.press/v97/correa19a/correa19a.pdf,ICML
615,2019,Escaping Saddle Points with Adaptive Gradient Methods,"Matthew Staib,         Sashank Reddi,         Satyen Kale,         Sanjiv Kumar,         Suvrit Sra","Adaptive methods such as Adam and RMSProp are widely used in deep learning but are not well understood. In this paper, we seek a crisp, clean and precise characterization of their behavior in nonconvex settings. To this end, we first provide a novel view of adaptive methods as preconditioned SGD, where the preconditioner is estimated in an online manner. By studying the preconditioner on its own, we elucidate its purpose: it rescales the stochastic gradient noise to be isotropic near stationary points, which helps escape saddle points. Furthermore, we show that adaptive methods can efficiently estimate the aforementioned preconditioner. By gluing together these two components, we provide the first (to our knowledge) second-order convergence result for any adaptive method. The key insight from our analysis is that, compared to SGD, adaptive methods escape saddle points faster, and can converge faster overall to second-order stationary points.",http://proceedings.mlr.press/v97/staib19a.html,http://proceedings.mlr.press/v97/staib19a/staib19a.pdf,ICML
616,2019,Cognitive model priors for predicting human decisions,"David D. Bourgin,         Joshua C. Peterson,         Daniel Reichman,         Stuart J. Russell,         Thomas L. Griffiths","Human decision-making underlies all economic behavior. For the past four decades, human decision-making under uncertainty has continued to be explained by theoretical models based on prospect theory, a framework that was awarded the Nobel Prize in Economic Sciences. However, theoretical models of this kind have developed slowly, and robust, high-precision predictive models of human decisions remain a challenge. While machine learning is a natural candidate for solving these problems, it is currently unclear to what extent it can improve predictions obtained by current theories. We argue that this is mainly due to data scarcity, since noisy human behavior requires massive sample sizes to be accurately captured by off-the-shelf machine learning methods. To solve this problem, what is needed are machine learning models with appropriate inductive biases for capturing human behavior, and larger datasets. We offer two contributions towards this end: first, we construct “cognitive model priors” by pretraining neural networks with synthetic data generated by cognitive models (i.e., theoretical models developed by cognitive psychologists). We find that fine-tuning these networks on small datasets of real human decisions results in unprecedented state-of-the-art improvements on two benchmark datasets. Second, we present the first large-scale dataset for human decision-making, containing over 240,000 human judgments across over 13,000 decision problems. This dataset reveals the circumstances where cognitive model priors are useful, and provides a new standard for benchmarking prediction of human decisions under uncertainty.",http://proceedings.mlr.press/v97/peterson19a.html,http://proceedings.mlr.press/v97/peterson19a/peterson19a.pdf,ICML
617,2019,Cross-Domain 3D Equivariant Image Embeddings,"Carlos Esteves,         Avneesh Sud,         Zhengyi Luo,         Kostas Daniilidis,         Ameesh Makadia","Spherical convolutional networks have been introduced recently as tools to learn powerful feature representations of 3D shapes. Spherical CNNs are equivariant to 3D rotations making them ideally suited to applications where 3D data may be observed in arbitrary orientations. In this paper we learn 2D image embeddings with a similar equivariant structure: embedding the image of a 3D object should commute with rotations of the object. We introduce a cross-domain embedding from 2D images into a spherical CNN latent space. This embedding encodes images with 3D shape properties and is equivariant to 3D rotations of the observed object. The model is supervised only by target embeddings obtained from a spherical CNN pretrained for 3D shape classification. We show that learning a rich embedding for images with appropriate geometric structure is sufficient for tackling varied applications, such as relative pose estimation and novel view synthesis, without requiring additional task-specific supervision.",http://proceedings.mlr.press/v97/esteves19a.html,http://proceedings.mlr.press/v97/esteves19a/esteves19a.pdf,ICML
618,2019,Differentially Private Empirical Risk Minimization with Non-convex Loss Functions,"Di Wang,         Changyou Chen,         Jinhui Xu","We study the problem of Empirical Risk Minimization (ERM) with (smooth) non-convex loss functions under the differential-privacy (DP) model. Existing approaches for this problem mainly adopt gradient norms to measure the error, which in general cannot guarantee the quality of the solution. To address this issue, we first study the expected excess empirical (or population) risk, which was primarily used as the utility to measure the quality for convex loss functions. Specifically, we show that the excess empirical (or population) risk can be upper bounded by O~(dlog(1/δ)lognϵ2)O~(dlog⁡(1/δ)log⁡nϵ2)\tilde{O}(\frac{d\log (1/\delta)}{\log n\epsilon^2}) in the (ϵ,δ)(ϵ,δ)(\epsilon, \delta)-DP settings, where nnn is the data size and ddd is the dimensionality of the space. The 1logn1log⁡n\frac{1}{\log n} term in the empirical risk bound can be further improved to 1nΩ(1)1nΩ(1)\frac{1}{n^{\Omega(1)}} (when ddd is a constant) by a highly non-trivial analysis on the time-average error. To obtain more efficient solutions, we also consider the connection between achieving differential privacy and finding approximate local minimum. Particularly, we show that when the size nnn is large enough, there are (ϵ,δ)(ϵ,δ)(\epsilon, \delta)-DP algorithms which can find an approximate local minimum of the empirical risk with high probability in both the constrained and non-constrained settings. These results indicate that one can escape saddle points privately.",http://proceedings.mlr.press/v97/wang19c.html,http://proceedings.mlr.press/v97/wang19c/wang19c.pdf,ICML
619,2019,Inferring Heterogeneous Causal Effects in Presence of Spatial Confounding,"Muhammad Osama,         Dave Zachariah,         Thomas B. Schön","We address the problem of inferring the causal effect of an exposure on an outcome across space, using observational data. The data is possibly subject to unmeasured confounding variables which, in a standard approach, must be adjusted for by estimating a nuisance function. Here we develop a method that eliminates the nuisance function, while mitigating the resulting errors-in-variables. The result is a robust and accurate inference method for spatially varying heterogeneous causal effects. The properties of the method are demonstrated on synthetic as well as real data from Germany and the US.",http://proceedings.mlr.press/v97/osama19a.html,http://proceedings.mlr.press/v97/osama19a/osama19a.pdf,ICML
620,2019,Benefits and Pitfalls of the Exponential Mechanism with Applications to Hilbert Spaces and Functional PCA,"Jordan Awan,         Ana Kenney,         Matthew Reimherr,         Aleksandra Slavković","The exponential mechanism is a fundamental tool of Differential Privacy (DP) due to its strong privacy guarantees and flexibility. We study its extension to settings with summaries based on infinite dimensional outputs such as with functional data analysis, shape analysis, and nonparametric statistics. We show that the mechanism must be designed with respect to a specific base measure over the output space, such as a Gaussian process. We provide a positive result that establishes a Central Limit Theorem for the exponential mechanism quite broadly. We also provide a negative result, showing that the magnitude of noise introduced for privacy is asymptotically non-negligible relative to the statistical estimation error. We develop an \ep\ep\ep-DP mechanism for functional principal component analysis, applicable in separable Hilbert spaces, and demonstrate its performance via simulations and applications to two datasets.",http://proceedings.mlr.press/v97/awan19a.html,http://proceedings.mlr.press/v97/awan19a/awan19a.pdf,ICML
621,2019,TarMAC: Targeted Multi-Agent Communication,"Abhishek Das,         Théophile Gervet,         Joshua Romoff,         Dhruv Batra,         Devi Parikh,         Mike Rabbat,         Joelle Pineau","We propose a targeted communication architecture for multi-agent reinforcement learning, where agents learn both what messages to send and whom to address them to while performing cooperative tasks in partially-observable environments. This targeting behavior is learnt solely from downstream task-specific reward without any communication supervision. We additionally augment this with a multi-round communication approach where agents coordinate via multiple rounds of communication before taking actions in the environment. We evaluate our approach on a diverse set of cooperative multi-agent tasks, of varying difficulties, with varying number of agents, in a variety of environments ranging from 2D grid layouts of shapes and simulated traffic junctions to 3D indoor environments, and demonstrate the benefits of targeted and multi-round communication. Moreover, we show that the targeted communication strategies learned by agents are interpretable and intuitive. Finally, we show that our architecture can be easily extended to mixed and competitive environments, leading to improved performance and sample complexity over recent state-of-the-art approaches.",http://proceedings.mlr.press/v97/das19a.html,http://proceedings.mlr.press/v97/das19a/das19a.pdf,ICML
622,2019,ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation,"Yuzhe Yang,         Guo Zhang,         Dina Katabi,         Zhi Xu","Deep neural networks are vulnerable to adversarial attacks. The literature is rich with algorithms that can easily craft successful adversarial examples. In contrast, the performance of defense techniques still lags behind. This paper proposes ME-Net, a defense method that leverages matrix estimation (ME). In ME-Net, images are preprocessed using two steps: first pixels are randomly dropped from the image; then, the image is reconstructed using ME. We show that this process destroys the adversarial structure of the noise, while re-enforcing the global structure in the original image. Since humans typically rely on such global structures in classifying images, the process makes the network mode compatible with human perception. We conduct comprehensive experiments on prevailing benchmarks such as MNIST, CIFAR-10, SVHN, and Tiny-ImageNet. Comparing ME-Net with state-of-the-art defense mechanisms shows that ME-Net consistently outperforms prior techniques, improving robustness against both black-box and white-box attacks.",http://proceedings.mlr.press/v97/yang19e.html,http://proceedings.mlr.press/v97/yang19e/yang19e.pdf,ICML
623,2019,Adaptive Regret of Convex and Smooth Functions,"Lijun Zhang,         Tie-Yan Liu,         Zhi-Hua Zhou","We investigate online convex optimization in changing environments, and choose the adaptive regret as the performance measure. The goal is to achieve a small regret over every interval so that the comparator is allowed to change over time. Different from previous works that only utilize the convexity condition, this paper further exploits smoothness to improve the adaptive regret. To this end, we develop novel adaptive algorithms for convex and smooth functions, and establish problem-dependent regret bounds over any interval. Our regret bounds are comparable to existing results in the worst case, and become much tighter when the comparator has a small loss.",http://proceedings.mlr.press/v97/zhang19j.html,http://proceedings.mlr.press/v97/zhang19j/zhang19j.pdf,ICML
624,2019,Myopic Posterior Sampling for Adaptive Goal Oriented Design of Experiments,"Kirthevasan Kandasamy,         Willie Neiswanger,         Reed Zhang,         Akshay Krishnamurthy,         Jeff Schneider,         Barnabas Poczos","Bayesian methods for adaptive decision-making, such as Bayesian optimisation, active learning, and active search have seen great success in relevant applications. However, real world data collection tasks are more broad and complex, as we may need to achieve a combination of the above goals and/or application specific goals. In such scenarios, specialised methods have limited applicability. In this work, we design a new myopic strategy for a wide class of adaptive design of experiment (DOE) problems, where we wish to collect data in order to fulfil a given goal. Our approach, Myopic Posterior Sampling (MPS), which is inspired by the classical posterior sampling algorithm for multi-armed bandits, enables us to address a broad suite of DOE tasks where a practitioner may incorporate domain expertise about the system and specify her desired goal via a reward function. Empirically, this general-purpose strategy is competitive with more specialised methods in a wide array of synthetic and real world DOE tasks. More importantly, it enables addressing complex DOE goals where no existing method seems applicable. On the theoretical side, we leverage ideas from adaptive submodularity and reinforcement learning to derive conditions under which MPS achieves sublinear regret against natural benchmark policies.",http://proceedings.mlr.press/v97/kandasamy19a.html,http://proceedings.mlr.press/v97/kandasamy19a/kandasamy19a.pdf,ICML
625,2019,SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning,"Marvin Zhang,         Sharad Vikram,         Laura Smith,         Pieter Abbeel,         Matthew Johnson,         Sergey Levine","Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images. In this paper, we present a method for learning representations that are suitable for iterative model-based policy improvement, even when the underlying dynamical system has complex dynamics and image observations, in that these representations are optimized for inferring simple dynamics and cost models given data from the current policy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our approach on a range of robotics tasks, including manipulation with a real-world robotic arm directly from images. We find that our method produces substantially better final performance than other model-based RL methods while being significantly more efficient than model-free RL.",http://proceedings.mlr.press/v97/zhang19m.html,http://proceedings.mlr.press/v97/zhang19m/zhang19m.pdf,ICML
626,2019,A Recurrent Neural Cascade-based Model for Continuous-Time Diffusion,Sylvain Lamprier,"Many works have been proposed in the literature to capture the dynamics of diffusion in networks. While some of them define graphical Markovian models to extract temporal relationships between node infections in networks, others consider diffusion episodes as sequences of infections via recurrent neural models. In this paper we propose a model at the crossroads of these two extremes, which embeds the history of diffusion in infected nodes as hidden continuous states. Depending on the trajectory followed by the content before reaching a given node, the distribution of influence probabilities may vary. However, content trajectories are usually hidden in the data, which induces challenging learning problems. We propose a topological recurrent neural model which exhibits good experimental performances for diffusion modeling and prediction.",http://proceedings.mlr.press/v97/lamprier19a.html,http://proceedings.mlr.press/v97/lamprier19a/lamprier19a.pdf,ICML
627,2019,Regularization in directable environments with application to Tetris,"Jan Malte Lichtenberg,         Özgür Şimşek","Learning from small data sets is difficult in the absence of specific domain knowledge. We present a regularized linear model called STEW that benefits from a generic and prevalent form of prior knowledge: feature directions. STEW shrinks weights toward each other, converging to an equal-weights solution in the limit of infinite regularization. We provide theoretical results on the equal-weights solution that explains how STEW can productively trade-off bias and variance. Across a wide range of learning problems, including Tetris, STEW outperformed existing linear models, including ridge regression, the Lasso, and the non-negative Lasso, when feature directions were known. The model proved to be robust to unreliable (or absent) feature directions, still outperforming alternative models under diverse conditions. Our results in Tetris were obtained by using a novel approach to learning in sequential decision environments based on multinomial logistic regression.",http://proceedings.mlr.press/v97/lichtenberg19a.html,http://proceedings.mlr.press/v97/lichtenberg19a/lichtenberg19a.pdf,ICML
628,2019,Deep Counterfactual Regret Minimization,"Noam Brown,         Adam Lerer,         Sam Gross,         Tuomas Sandholm","Counterfactual Regret Minimization (CFR) is the leading algorithm for solving large imperfect-information games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running CFR. The abstracted game is solved with tabular CFR, and its solution is mapped back to the full game. This process can be problematic because aspects of abstraction are often manual and domain specific, abstraction algorithms may miss important strategic nuances of the game, and there is a chicken-and-egg problem because determining a good abstraction requires knowledge of the equilibrium of the game. This paper introduces Deep Counterfactual Regret Minimization, a form of CFR that obviates the need for abstraction by instead using deep neural networks to approximate the behavior of CFR in the full game. We show that Deep CFR is principled and achieves strong performance in large poker games. This is the first non-tabular variant of CFR to be successful in large games.",http://proceedings.mlr.press/v97/brown19b.html,http://proceedings.mlr.press/v97/brown19b/brown19b.pdf,ICML
629,2019,Fast and Flexible Inference of Joint Distributions from their Marginals,"Charlie Frogner,         Tomaso Poggio","Across the social sciences and elsewhere, practitioners frequently have to reason about relationships between random variables, despite lacking joint observations of the variables. This is sometimes called an ""ecological"" inference; given samples from the marginal distributions of the variables, one attempts to infer their joint distribution. The problem is inherently ill-posed, yet only a few models have been proposed for bringing prior information into the problem, often relying on restrictive or unrealistic assumptions and lacking a unified approach. In this paper, we treat the inference problem generally and propose a unified class of models that encompasses some of those previously proposed while including many new ones. Previous work has relied on either relaxation or approximate inference via MCMC, with the latter known to mix prohibitively slowly for this type of problem. Here we instead give a single exact inference algorithm that works for the entire model class via an efficient fixed point iteration called Dykstra’s method. We investigate empirically both the computational cost of our algorithm and the accuracy of the new models on real datasets, showing favorable performance in both cases and illustrating the impact of increased flexibility in modeling enabled by this work.",http://proceedings.mlr.press/v97/frogner19a.html,http://proceedings.mlr.press/v97/frogner19a/frogner19a.pdf,ICML
630,2019,Toward Understanding the Importance of Noise in Training Neural Networks,"Mo Zhou,         Tianyi Liu,         Yan Li,         Dachao Lin,         Enlu Zhou,         Tuo Zhao","Numerous empirical evidence has corroborated that the noise plays a crucial rule in effective and efficient training of deep neural networks. The theory behind, however, is still largely unknown. This paper studies this fundamental problem through training a simple two-layer convolutional neural network model. Although training such a network requires to solve a non-convex optimization problem with a spurious local optimum and a global optimum, we prove that a perturbed gradient descent algorithm in conjunction with noise annealing is guaranteed to converge to a global optimum in polynomial time with arbitrary initialization. This implies that the noise enables the algorithm to efficiently escape from the spurious local optimum. Numerical experiments are provided to support our theory.",http://proceedings.mlr.press/v97/zhou19d.html,http://proceedings.mlr.press/v97/zhou19d/zhou19d.pdf,ICML
631,2019,"Submodular Maximization beyond Non-negativity: Guarantees, Fast Algorithms, and Applications","Chris Harshaw,         Moran Feldman,         Justin Ward,         Amin Karbasi","It is generally believed that submodular functions–and the more general class of γγ\gamma-weakly submodular functions–may only be optimized under the non-negativity assumption f(S)≥0f(S)≥0f(S) \geq 0. In this paper, we show that once the function is expressed as the difference f=g−cf=g−cf = g - c, where ggg is monotone, non-negative, and γγ\gamma-weakly submodular and ccc is non-negative modular, then strong approximation guarantees may be obtained. We present an algorithm for maximizing g−cg−cg - c under a kkk-cardinality constraint which produces a random feasible set SSS such that E[g(S)−c(S)]≥(1−e−γ−ϵ)g(\opt)−c(\opt)\mathbb{E}[g(S) -c(S)] \geq (1 - e^{-\gamma} - \epsilon) g(\opt) - c(\opt), whose running time is O(nϵlog21ϵ)O (\frac{n}{\epsilon} \log^2 \frac{1}{\epsilon}), independent of kk. We extend these results to the unconstrained setting by describing an algorithm with the same approximation guarantees and faster O(n1ϵlog1ϵ)O(n \frac{1}{\epsilon} \log\frac{1}{\epsilon}) runtime. The main techniques underlying our algorithms are two-fold: the use of a surrogate objective which varies the relative importance between gg and cc throughout the algorithm, and a geometric sweep over possible γ\gamma values. Our algorithmic guarantees are complemented by a hardness result showing that no polynomial-time algorithm which accesses gg through a value oracle can do better. We empirically demonstrate the success of our algorithms by applying them to experimental design on the Boston Housing dataset and directed vertex cover on the Email EU dataset.",http://proceedings.mlr.press/v97/harshaw19a.html,http://proceedings.mlr.press/v97/harshaw19a/harshaw19a.pdf,ICML
632,2019,A Polynomial Time MCMC Method for Sampling from Continuous Determinantal Point Processes,"Alireza Rezaei,         Shayan Oveis Gharan","We study the Gibbs sampling algorithm for discrete and continuous kkk-determinantal point processes. We show that in both cases, the spectral gap of the chain is bounded by a polynomial of kkk and it is independent of the size of the domain. As an immediate corollary, we obtain sublinear time algorithms for sampling from discrete kkk-DPPs given access to polynomially many processors. In the continuous setting, our result leads to the first class of rigorously analyzed efficient algorithms to generate random samples of continuous kkk-DPPs. We achieve this by showing that the Gibbs sampler for a large family of continuous kkk-DPPs can be simulated efficiently when the spectrum is not concentrated on the top kkk eigenvalues.",http://proceedings.mlr.press/v97/rezaei19a.html,http://proceedings.mlr.press/v97/rezaei19a/rezaei19a.pdf,ICML
633,2019,Non-Monotonic Sequential Text Generation,"Sean Welleck,         Kianté Brantley,         Hal Daumé Iii,         Kyunghyun Cho","Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy’s own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order, while achieving competitive performance with conventional left-to-right generation.",http://proceedings.mlr.press/v97/welleck19a.html,http://proceedings.mlr.press/v97/welleck19a/welleck19a.pdf,ICML
634,2019,Efficient Amortised Bayesian Inference for Hierarchical and Nonlinear Dynamical Systems,"Ted Meeds,         Geoffrey Roeder,         Paul Grant,         Andrew Phillips,         Neil Dalchau","We introduce a flexible, scalable Bayesian inference framework for nonlinear dynamical systems characterised by distinct and hierarchical variability at the individual, group, and population levels. Our model class is a generalisation of nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse for many experimental sciences. We cast parameter inference as stochastic optimisation of an end-to-end differentiable, block-conditional variational autoencoder. We specify the dynamics of the data-generating process as an ordinary differential equation (ODE) such that both the ODE and its solver are fully differentiable. This model class is highly flexible: the ODE right-hand sides can be a mixture of user-prescribed or ""white-box"" sub-components and neural network or ""black-box"" sub-components. Using stochastic optimisation, our amortised inference algorithm could seamlessly scale up to massive data collection pipelines (common in labs with robotic automation). Finally, our framework supports interpretability with respect to the underlying dynamics, as well as predictive generalization to unseen combinations of group components (also called “zero-shot"" learning). We empirically validate our method by predicting the dynamic behaviour of bacteria that were genetically engineered to function as biosensors.",http://proceedings.mlr.press/v97/meeds19a.html,http://proceedings.mlr.press/v97/meeds19a/meeds19a.pdf,ICML
635,2019,Zeno: Distributed Stochastic Gradient Descent with Suspicion-based Fault-tolerance,"Cong Xie,         Sanmi Koyejo,         Indranil Gupta","We present Zeno, a technique to make distributed machine learning, particularly Stochastic Gradient Descent (SGD), tolerant to an arbitrary number of faulty workers. Zeno generalizes previous results that assumed a majority of non-faulty nodes; we need assume only one non-faulty worker. Our key idea is to suspect workers that are potentially defective. Since this is likely to lead to false positives, we use a ranking-based preference mechanism. We prove the convergence of SGD for non-convex problems under these scenarios. Experimental results show that Zeno outperforms existing approaches.",http://proceedings.mlr.press/v97/xie19b.html,http://proceedings.mlr.press/v97/xie19b/xie19b.pdf,ICML
636,2019,DP-GP-LVM: A Bayesian Non-Parametric Model for Learning Multivariate Dependency Structures,"Andrew Lawrence,         Carl Henrik Ek,         Neill Campbell",We present a non-parametric Bayesian latent variable model capable of learning dependency structures across dimensions in a multivariate setting. Our approach is based on flexible Gaussian process priors for the generative mappings and interchangeable Dirichlet process priors to learn the structure. The introduction of the Dirichlet process as a specific structural prior allows our model to circumvent issues associated with previous Gaussian process latent variable models. Inference is performed by deriving an efficient variational bound on the marginal log-likelihood of the model. We demonstrate the efficacy of our approach via analysis of discovered structure and superior quantitative performance on missing data imputation.,http://proceedings.mlr.press/v97/lawrence19a.html,http://proceedings.mlr.press/v97/lawrence19a/lawrence19a.pdf,ICML
637,2019,Learning to Infer Program Sketches,"Maxwell Nye,         Luke Hewitt,         Joshua Tenenbaum,         Armando Solar-Lezama","Our goal is to build systems which write code automatically from the kinds of specifications humans can most easily provide, such as examples and natural language instruction. The key idea of this work is that a flexible combination of pattern recognition and explicit reasoning can be used to solve these complex programming problems. We propose a method for dynamically integrating these types of information. Our novel intermediate representation and training algorithm allow a program synthesis system to learn, without direct supervision, when to rely on pattern recognition and when to perform symbolic search. Our model matches the memorization and generalization performance of neural synthesis and symbolic search, respectively, and achieves state-of-the-art performance on a dataset of simple English description-to-code programming problems.",http://proceedings.mlr.press/v97/nye19a.html,http://proceedings.mlr.press/v97/nye19a/nye19a.pdf,ICML
638,2019,Equivariant Transformer Networks,"Kai Sheng Tai,         Peter Bailis,         Gregory Valiant","How can prior knowledge on the transformation invariances of a domain be incorporated into the architecture of a neural network? We propose Equivariant Transformers (ETs), a family of differentiable image-to-image mappings that improve the robustness of models towards pre-defined continuous transformation groups. Through the use of specially-derived canonical coordinate systems, ETs incorporate functions that are equivariant by construction with respect to these transformations. We show empirically that ETs can be flexibly composed to improve model robustness towards more complicated transformation groups in several parameters. On a real-world image classification task, ETs improve the sample efficiency of ResNet classifiers, achieving relative improvements in error rate of up to 15% in the limited data regime while increasing model parameter count by less than 1%.",http://proceedings.mlr.press/v97/tai19a.html,http://proceedings.mlr.press/v97/tai19a/tai19a.pdf,ICML
639,2019,On the Impact of the Activation function on Deep Neural Networks Training,"Soufiane Hayou,         Arnaud Doucet,         Judith Rousseau","The weight initialization and the activation function of deep neural networks have a crucial impact on the performance of the training procedure. An inappropriate selection can lead to the loss of information of the input during forward propagation and the exponential vanishing/exploding of gradients during back-propagation. Understanding the theoretical properties of untrained random networks is key to identifying which deep networks may be trained successfully as recently demonstrated by Samuel et al. (2017) who showed that for deep feedforward neural networks only a specific choice of hyperparameters known as the ‘Edge of Chaos’ can lead to good performance. While the work by Samuel et al. (2017) discuss trainability issues, we focus here on training acceleration and overall performance. We give a comprehensive theoretical analysis of the Edge of Chaos and show that we can indeed tune the initialization parameters and the activation function in order to accelerate the training and improve the performance.",http://proceedings.mlr.press/v97/hayou19a.html,http://proceedings.mlr.press/v97/hayou19a/hayou19a.pdf,ICML
640,2019,Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep Models,"Mor Shpigel Nacson,         Suriya Gunasekar,         Jason Lee,         Nathan Srebro,         Daniel Soudry","With an eye toward understanding complexity control in deep learning, we study how infinitesimal regularization or gradient descent optimization lead to margin maximizing solutions in both homogeneous and non homogeneous models, extending previous work that focused on infinitesimal regularization only in homogeneous models. To this end we study the limit of loss minimization with a diverging norm constraint (the “constrained path”), relate it to the limit of a “margin path” and characterize the resulting solution. For non-homogeneous ensemble models, which output is a sum of homogeneous sub-models, we show that this solution discards the shallowest sub-models if they are unnecessary. For homogeneous models, we show convergence to a “lexicographic max-margin solution”, and provide conditions under which max-margin solutions are also attained as the limit of unconstrained gradient descent.",http://proceedings.mlr.press/v97/nacson19a.html,http://proceedings.mlr.press/v97/nacson19a/nacson19a.pdf,ICML
641,2019,Better generalization with less data using robust gradient descent,"Matthew Holland,         Kazushi Ikeda","For learning tasks where the data (or losses) may be heavy-tailed, algorithms based on empirical risk minimization may require a substantial number of observations in order to perform well off-sample. In pursuit of stronger performance under weaker assumptions, we propose a technique which uses a cheap and robust iterative estimate of the risk gradient, which can be easily fed into any steepest descent procedure. Finite-sample risk bounds are provided under weak moment assumptions on the loss gradient. The algorithm is simple to implement, and empirical tests using simulations and real-world data illustrate that more efficient and reliable learning is possible without prior knowledge of the loss tails.",http://proceedings.mlr.press/v97/holland19a.html,http://proceedings.mlr.press/v97/holland19a/holland19a.pdf,ICML
642,2019,Passed & Spurious: Descent Algorithms and Local Minima in Spiked Matrix-Tensor Models,"Stefano Sarao Mannelli,         Florent Krzakala,         Pierfrancesco Urbani,         Lenka Zdeborova","In this work we analyse quantitatively the interplay between the loss landscape and performance of descent algorithms in a prototypical inference problem, the spiked matrix-tensor model. We study a loss function that is the negative log-likelihood of the model. We analyse the number of local minima at a fixed distance from the signal/spike with the Kac-Rice formula, and locate trivialization of the landscape at large signal-to-noise ratios. We evaluate analytically the performance of a gradient flow algorithm using integro-differential PDEs as developed in physics of disordered systems for the Langevin dynamics. We analyze the performance of an approximate message passing algorithm estimating the maximum likelihood configuration via its state evolution. We conclude by comparing the above results: while we observe a drastic slow down of the gradient flow dynamics even in the region where the landscape is trivial, both the analyzed algorithms are shown to perform well even in the part of the region of parameters where spurious local minima are present.",http://proceedings.mlr.press/v97/mannelli19a.html,http://proceedings.mlr.press/v97/mannelli19a/mannelli19a.pdf,ICML
643,2019,Actor-Attention-Critic for Multi-Agent Reinforcement Learning,"Shariq Iqbal,         Fei Sha","Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.",http://proceedings.mlr.press/v97/iqbal19a.html,http://proceedings.mlr.press/v97/iqbal19a/iqbal19a.pdf,ICML
644,2019,Circuit-GNN: Graph Neural Networks for Distributed Circuit Design,"Guo Zhang,         Hao He,         Dina Katabi","We present Circuit-GNN, a graph neural network (GNN) model for designing distributed circuits. Today, designing distributed circuits is a slow process that can take months from an expert engineer. Our model both automates and speeds up the process. The model learns to simulate the electromagnetic (EM) properties of distributed circuits. Hence, it can be used to replace traditional EM simulators, which typically take tens of minutes for each design iteration. Further, by leveraging neural networks’ differentiability, we can use our model to solve the inverse problem – i.e., given desirable EM specifications, we propagate the gradient to optimize the circuit parameters and topology to satisfy the specifications. We exploit the flexibility of GNN to create one model that works for different circuit topologies. We compare our model with a commercial simulator showing that it reduces simulation time by four orders of magnitude. We also demonstrate the value of our model by using it to design a Terahertz channelizer, a difficult task that requires a specialized expert. The results show that our model produces a channelizer whose performance is as good as a manually optimized design, and can save the expert several weeks of topology and parameter optimization. Most interestingly, our model comes up with new designs that differ from the limited templates commonly used by engineers in the field, hence significantly expanding the design space.",http://proceedings.mlr.press/v97/zhang19e.html,http://proceedings.mlr.press/v97/zhang19e/zhang19e.pdf,ICML
645,2019,Global Convergence of Block Coordinate Descent in Deep Learning,"Jinshan Zeng,         Tim Tsz-Kit Lau,         Shaobo Lin,         Yuan Yao","Deep learning has aroused extensive attention due to its great empirical success. The efficiency of the block coordinate descent (BCD) methods has been recently demonstrated in deep neural network (DNN) training. However, theoretical studies on their convergence properties are limited due to the highly nonconvex nature of DNN training. In this paper, we aim at providing a general methodology for provable convergence guarantees for this type of methods. In particular, for most of the commonly used DNN training models involving both two- and three-splitting schemes, we establish the global convergence to a critical point at a rate of O(1/k){\cal O}(1/k), where kk is the number of iterations. The results extend to general loss functions which have Lipschitz continuous gradients and deep residual networks (ResNets). Our key development adds several new elements to the Kurdyka-Lojasiewicz inequality framework that enables us to carry out the global convergence analysis of BCD in the general scenario of deep learning.",http://proceedings.mlr.press/v97/zeng19a.html,http://proceedings.mlr.press/v97/zeng19a/zeng19a.pdf,ICML
646,2019,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,"Sai Praneeth Karimireddy,         Quentin Rebjock,         Sebastian Stich,         Martin Jaggi","Sign-based algorithms (e.g. signSGD) have been proposed as a biased gradient compression technique to alleviate the communication bottleneck in training large neural networks across multiple workers. We show simple convex counter-examples where signSGD does not converge to the optimum. Further, even when it does converge, signSGD may generalize poorly when compared with SGD. These issues arise because of the biased nature of the sign compression operator. We then show that using error-feedback, i.e. incorporating the error made by the compression operator into the next step, overcomes these issues. We prove that our algorithm (EF-SGD) with arbitrary compression operator achieves the same rate of convergence as SGD without any additional assumptions. Thus EF-SGD achieves gradient compression for free. Our experiments thoroughly substantiate the theory.",http://proceedings.mlr.press/v97/karimireddy19a.html,http://proceedings.mlr.press/v97/karimireddy19a/karimireddy19a.pdf,ICML
647,2019,Power k-Means Clustering,"Jason Xu,         Kenneth Lange","Clustering is a fundamental task in unsupervised machine learning. Lloyd’s 1957 algorithm for k-means clustering remains one of the most widely used due to its speed and simplicity, but the greedy approach is sensitive to initialization and often falls short at a poor solution. This paper explores an alternative to Lloyd’s algorithm that retains its simplicity and mitigates its tendency to get trapped by local minima. Called power k-means, our method embeds the k-means problem in a continuous class of similar, better behaved problems with fewer local minima. Power k-means anneals its way toward the solution of ordinary k-means by way of majorization-minimization (MM), sharing the appealing descent property and low complexity of Lloyd’s algorithm. Further, our method complements widely used seeding strategies, reaping marked improvements when used together as demonstrated on a suite of simulated and real data examples.",http://proceedings.mlr.press/v97/xu19a.html,http://proceedings.mlr.press/v97/xu19a/xu19a.pdf,ICML
648,2019,Adversarial Generation of Time-Frequency Features with application in audio synthesis,"Andrés Marafioti,         Nathanaël Perraudin,         Nicki Holighaus,         Piotr Majdak","Time-frequency (TF) representations provide powerful and intuitive features for the analysis of time series such as audio. But still, generative modeling of audio in the TF domain is a subtle matter. Consequently, neural audio synthesis widely relies on directly modeling the waveform and previous attempts at unconditionally synthesizing audio from neurally generated invertible TF features still struggle to produce audio at satisfying quality. In this article, focusing on the short-time Fourier transform, we discuss the challenges that arise in audio synthesis based on generated invertible TF features and how to overcome them. We demonstrate the potential of deliberate generative TF modeling by training a generative adversarial network (GAN) on short-time Fourier features. We show that by applying our guidelines, our TF-based network was able to outperform a state-of-the-art GAN generating waveforms directly, despite the similar architecture in the two networks.",http://proceedings.mlr.press/v97/marafioti19a.html,http://proceedings.mlr.press/v97/marafioti19a/marafioti19a.pdf,ICML
649,2019,Adaptive Scale-Invariant Online Algorithms for Learning Linear Models,"Michal Kempka,         Wojciech Kotlowski,         Manfred K. Warmuth","We consider online learning with linear models, where the algorithm predicts on sequentially revealed instances (feature vectors), and is compared against the best linear function (comparator) in hindsight. Popular algorithms in this framework, such as Online Gradient Descent (OGD), have parameters (learning rates), which ideally should be tuned based on the scales of the features and the optimal comparator, but these quantities only become available at the end of the learning process. In this paper, we resolve the tuning problem by proposing online algorithms making predictions which are invariant under arbitrary rescaling of the features. The algorithms have no parameters to tune, do not require any prior knowledge on the scale of the instances or the comparator, and achieve regret bounds matching (up to a logarithmic factor) that of OGD with optimally tuned separate learning rates per dimension, while retaining comparable runtime performance.",http://proceedings.mlr.press/v97/kempka19a.html,http://proceedings.mlr.press/v97/kempka19a/kempka19a.pdf,ICML
650,2019,Stochastic Beams and Where To Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement,"Wouter Kool,         Herke Van Hoof,         Max Welling","The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample kk elements without replacement. We show how to implicitly apply this ’Gumbel-Top-kk’ trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in kk and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.",http://proceedings.mlr.press/v97/kool19a.html,http://proceedings.mlr.press/v97/kool19a/kool19a.pdf,ICML
651,2019,On Certifying Non-Uniform Bounds against Adversarial Attacks,"Chen Liu,         Ryota Tomioka,         Volkan Cevher","This work studies the robustness certification problem of neural network models, which aims to find certified adversary-free regions as large as possible around data points. In contrast to the existing approaches that seek regions bounded uniformly along all input features, we consider non-uniform bounds and use it to study the decision boundary of neural network models. We formulate our target as an optimization problem with nonlinear constraints. Then, a framework applicable for general feedforward neural networks is proposed to bound the output logits so that the relaxed problem can be solved by the augmented Lagrangian method. Our experiments show the non-uniform bounds have larger volumes than uniform ones. Compared with normal models, the robust models have even larger non-uniform bounds and better interpretability. Further, the geometric similarity of the non-uniform bounds gives a quantitative, data-agnostic metric of input features’ robustness.",http://proceedings.mlr.press/v97/liu19h.html,http://proceedings.mlr.press/v97/liu19h/liu19h.pdf,ICML
652,2019,Riemannian adaptive stochastic gradient algorithms on matrix manifolds,"Hiroyuki Kasai,         Pratik Jawanpuria,         Bamdev Mishra","Adaptive stochastic gradient algorithms in the Euclidean space have attracted much attention lately. Such explorations on Riemannian manifolds, on the other hand, are relatively new, limited, and challenging. This is because of the intrinsic non-linear structure of the underlying manifold and the absence of a canonical coordinate system. In machine learning applications, however, most manifolds of interest are represented as matrices with notions of row and column subspaces. In addition, the implicit manifold-related constraints may also lie on such subspaces. For example, the Grassmann manifold is the set of column subspaces. To this end, such a rich structure should not be lost by transforming matrices to just a stack of vectors while developing optimization algorithms on manifolds. We propose novel stochastic gradient algorithms for problems on Riemannian matrix manifolds by adapting the row and column subspaces of gradients. Our algorithms are provably convergent and they achieve the convergence rate of order O(log(T)/sqrt(T))O(log(T)/sqrt(T))O(log(T)/sqrt(T)), where TTT is the number of iterations. Our experiments illustrate that the proposed algorithms outperform existing Riemannian adaptive stochastic algorithms.",http://proceedings.mlr.press/v97/kasai19a.html,http://proceedings.mlr.press/v97/kasai19a/kasai19a.pdf,ICML
653,2019,Understanding and correcting pathologies in the training of learned optimizers,"Luke Metz,         Niru Maheswaranathan,         Jeremy Nixon,         Daniel Freeman,         Jascha Sohl-Dickstein","Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process. The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance. This allows us to train neural networks to perform optimization of a specific task faster than tuned first-order methods. Moreover, by training the optimizer against validation loss (as opposed to training loss), we are able to learn optimizers that train networks to generalize better than first order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks faster in wall-clock time compared to tuned first-order methods and with an improvement in test loss.",http://proceedings.mlr.press/v97/metz19a.html,http://proceedings.mlr.press/v97/metz19a/metz19a.pdf,ICML
654,2019,Differentially Private Learning of Geometric Concepts,"Haim Kaplan,         Yishay Mansour,         Yossi Matias,         Uri Stemmer","We present differentially private efficient algorithms for learning union of polygons in the plane (which are not necessarily convex). Our algorithms achieve (α,β)(α,β)(\alpha,\beta)-PAC learning and (ϵ,δ)(ϵ,δ)(\epsilon,\delta)-differential privacy using a sample of size O~(1αϵklogd)O~(1αϵklog⁡d)\tilde{O}\left(\frac{1}{\alpha\epsilon}k\log d\right), where the domain is [d]×[d][d]×[d][d]\times[d] and kkk is the number of edges in the union of polygons.",http://proceedings.mlr.press/v97/kaplan19a.html,http://proceedings.mlr.press/v97/kaplan19a/kaplan19a.pdf,ICML
655,2019,Phaseless PCA: Low-Rank Matrix Recovery from Column-wise Phaseless Measurements,"Seyedehsara Nayer,         Praneeth Narayanamurthy,         Namrata Vaswani","This work proposes the first set of simple, practically useful, and provable algorithms for two inter-related problems. (i) The first is low-rank matrix recovery from magnitude-only (phaseless) linear projections of each of its columns. This finds important applications in phaseless dynamic imaging, e.g., Fourier Ptychographic imaging of live biological specimens. Our guarantee shows that, in the regime of small ranks, the sample complexity required is only a little larger than the order-optimal one, and much smaller than what standard (unstructured) phase retrieval methods need. %Moreover our algorithm is fast and memory-efficient if only the minimum required number of measurements is used (ii) The second problem we study is a dynamic extension of the above: it allows the low-dimensional subspace from which each image/signal (each column of the low-rank matrix) is generated to change with time. We introduce a simple algorithm that is provably correct as long as the subspace changes are piecewise constant.",http://proceedings.mlr.press/v97/nayer19a.html,http://proceedings.mlr.press/v97/nayer19a/nayer19a.pdf,ICML
656,2019,Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design,"Jonathan Ho,         Xi Chen,         Aravind Srinivas,         Yan Duan,         Pieter Abbeel","Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. Despite their computational efficiency, flow-based models generally have much worse density modeling performance compared to state-of-the-art autoregressive models. In this paper, we investigate and improve upon three limiting design choices employed by flow-based models in prior work: the use of uniform noise for dequantization, the use of inexpressive affine flows, and the use of purely convolutional conditioning networks in coupling layers. Based on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant performance gap that has so far existed between autoregressive models and flow-based models.",http://proceedings.mlr.press/v97/ho19a.html,http://proceedings.mlr.press/v97/ho19a/ho19a.pdf,ICML
657,2019,Latent Normalizing Flows for Discrete Sequences,"Zachary Ziegler,         Alexander Rush","Normalizing flows are a powerful class of generative models for continuous random variables, showing both strong model flexibility and the potential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly applying normalizing flows to discrete sequences poses significant additional challenges. We propose a VAE-based generative model which jointly learns a normalizing flow-based distribution in the latent space and a stochastic mapping to an observed discrete space. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To capture this property, we propose several normalizing flow architectures to maximize model flexibility. Experiments consider common discrete sequence tasks of character-level language modeling and polyphonic music generation. Our results indicate that an autoregressive flow-based model can match the performance of a comparable autoregressive baseline, and a non-autoregressive flow-based model can improve generation speed with a penalty to performance.",http://proceedings.mlr.press/v97/ziegler19a.html,http://proceedings.mlr.press/v97/ziegler19a/ziegler19a.pdf,ICML
658,2019,Stochastic Gradient Push for Distributed Deep Learning,"Mahmoud Assran,         Nicolas Loizou,         Nicolas Ballas,         Mike Rabbat","Distributed data-parallel algorithms aim to accelerate the training of deep neural networks by parallelizing the computation of large mini-batch gradient updates across multiple nodes. Approaches that synchronize nodes using exact distributed averaging (e.g., via AllReduce) are sensitive to stragglers and communication delays. The PushSum gossip algorithm is robust to these issues, but only performs approximate distributed averaging. This paper studies Stochastic Gradient Push (SGP), which combines PushSum with stochastic gradient updates. We prove that SGP converges to a stationary point of smooth, non-convex objectives at the same sub-linear rate as SGD, and that all nodes achieve consensus. We empirically validate the performance of SGP on image classification (ResNet-50, ImageNet) and machine translation (Transformer, WMT’16 En-De) workloads.",http://proceedings.mlr.press/v97/assran19a.html,http://proceedings.mlr.press/v97/assran19a/assran19a.pdf,ICML
659,2019,Predictor-Corrector Policy Optimization,"Ching-An Cheng,         Xinyan Yan,         Nathan Ratliff,         Byron Boots","We present a predictor-corrector framework, called PicCoLO, that can transform a first-order model-free reinforcement or imitation learning algorithm into a new hybrid method that leverages predictive models to accelerate policy learning. The new “PicCoLOed” algorithm optimizes a policy by recursively repeating two steps: In the Prediction Step, the learner uses a model to predict the unseen future gradient and then applies the predicted estimate to update the policy; in the Correction Step, the learner runs the updated policy in the environment, receives the true gradient, and then corrects the policy using the gradient error. Unlike previous algorithms, PicCoLO corrects for the mistakes of using imperfect predicted gradients and hence does not suffer from model bias. The development of PicCoLO is made possible by a novel reduction from predictable online learning to adversarial online learning, which provides a systematic way to modify existing first-order algorithms to achieve the optimal regret with respect to predictable information. We show, in both theory and simulation, that the convergence rate of several first-order model-free algorithms can be improved by PicCoLO.",http://proceedings.mlr.press/v97/cheng19b.html,http://proceedings.mlr.press/v97/cheng19b/cheng19b.pdf,ICML
660,2019,A Tree-Based Method for Fast Repeated Sampling of Determinantal Point Processes,"Jennifer Gillenwater,         Alex Kulesza,         Zelda Mariet,         Sergei Vassilvtiskii","It is often desirable in recommender systems and other information retrieval applications to provide diverse results, and determinantal point processes (DPPs) have become a popular way to capture the trade-off between the quality of individual results and the diversity of the overall set. However, sampling from a DPP is inherently expensive: if the underlying collection contains N items, then generating each DPP sample requires time linear in N following a one-time preprocessing phase. Additionally, results often need to be personalized to a user, but standard approaches to personalization invalidate the preprocessing, making personalized samples especially expensive. In this work we address both of these shortcomings. First, we propose a new algorithm for generating DPP samples in time logarithmic in N, following a slightly more expensive preprocessing phase. We then extend the algorithm to support arbitrary query-time feature weights, allowing us to generate samples customized to individual users while still retaining logarithmic runtime; experiments show our approach runs over 300 times faster than traditional DPP sampling on collections of 100,000 items for samples of size 10.",http://proceedings.mlr.press/v97/gillenwater19a.html,http://proceedings.mlr.press/v97/gillenwater19a/gillenwater19a.pdf,ICML
661,2019,Scalable Training of Inference Networks for Gaussian-Process Models,"Jiaxin Shi,         Mohammad Emtiyaz Khan,         Jun Zhu","Inference in Gaussian process (GP) models is computationally challenging for large data, and often difficult to approximate with a small number of inducing points. We explore an alternative approximation that employs stochastic inference networks for a flexible inference. Unfortunately, for such networks, minibatch training is difficult to be able to learn meaningful correlations over function outputs for a large dataset. We propose an algorithm that enables such training by tracking a stochastic, functional mirror-descent algorithm. At each iteration, this only requires considering a finite number of input locations, resulting in a scalable and easy-to-implement algorithm. Empirical results show comparable and, sometimes, superior performance to existing sparse variational GP methods.",http://proceedings.mlr.press/v97/shi19a.html,http://proceedings.mlr.press/v97/shi19a/shi19a.pdf,ICML
662,2019,Incremental Randomized Sketching for Online Kernel Learning,"Xiao Zhang,         Shizhong Liao","Randomized sketching has been used in offline kernel learning, but it cannot be applied directly to online kernel learning due to the lack of incremental maintenances for randomized sketches with regret guarantees. To address these issues, we propose a novel incremental randomized sketching approach for online kernel learning, which has efficient incremental maintenances with theoretical guarantees. We construct two incremental randomized sketches using the sparse transform matrix and the sampling matrix for kernel matrix approximation, update the incremental randomized sketches using rank-111 modifications, and construct an time-varying explicit feature mapping for online kernel learning. We prove that the proposed incremental randomized sketching is statistically unbiased for the matrix product approximation, obtains a 1+ϵ1+ϵ1 + \epsilon relative-error bound for the kernel matrix approximation, enjoys a sublinear regret bound for online kernel learning, and has constant time and space complexities at each round for incremental maintenances. Experimental results demonstrate that the incremental randomized sketching achieves a better learning performance in terms of accuracy and efficiency even in adversarial environments.",http://proceedings.mlr.press/v97/zhang19h.html,http://proceedings.mlr.press/v97/zhang19h/zhang19h.pdf,ICML
663,2019,Distribution calibration for regression,"Hao Song,         Tom Diethe,         Meelis Kull,         Peter Flach","We are concerned with obtaining well-calibrated output distributions from regression models. Such distributions allow us to quantify the uncertainty that the model has regarding the predicted target value. We introduce the novel concept of distribution calibration, and demonstrate its advantages over the existing definition of quantile calibration. We further propose a post-hoc approach to improving the predictions from previously trained regression models, using multi-output Gaussian Processes with a novel Beta link function. The proposed method is experimentally verified on a set of common regression models and shows improvements for both distribution-level and quantile-level calibration.",http://proceedings.mlr.press/v97/song19a.html,http://proceedings.mlr.press/v97/song19a/song19a.pdf,ICML
664,2019,Data Shapley: Equitable Valuation of Data for Machine Learning,"Amirata Ghorbani,         James Zou","As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on nn data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.",http://proceedings.mlr.press/v97/ghorbani19c.html,http://proceedings.mlr.press/v97/ghorbani19c/ghorbani19c.pdf,ICML
665,2019,Temporal Gaussian Mixture Layer for Videos,"Aj Piergiovanni,         Michael Ryoo","We introduce a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer and present how it can be used to efficiently capture longer-term temporal information in continuous activity videos. The TGM layer is a temporal convolutional layer governed by a much smaller set of parameters (e.g., location/variance of Gaussians) that are fully differentiable. We present our fully convolutional video models with multiple TGM layers for activity detection. The extensive experiments on multiple datasets, including Charades and MultiTHUMOS, confirm the effectiveness of TGM layers, significantly outperforming the state-of-the-arts.",http://proceedings.mlr.press/v97/piergiovanni19a.html,http://proceedings.mlr.press/v97/piergiovanni19a/piergiovanni19a.pdf,ICML
666,2019,Online Convex Optimization in Adversarial Markov Decision Processes,"Aviv Rosenberg,         Yishay Mansour","We consider online learning in episodic loop-free Markov decision processes (MDPs), where the loss function can change arbitrarily between episodes, and the transition function is not known to the learner. We show O~(L|X||A|T−−−−√)O~(L|X||A|T)\tilde{O}(L|X|\sqrt{|A|T}) regret bound, where TTT is the number of episodes, XXX is the state space, AAA is the action space, and LLL is the length of each episode. Our online algorithm is implemented using entropic regularization methodology, which allows to extend the original adversarial MDP model to handle convex performance criteria (different ways to aggregate the losses of a single episode) , as well as improve previous regret bounds.",http://proceedings.mlr.press/v97/rosenberg19a.html,http://proceedings.mlr.press/v97/rosenberg19a/rosenberg19a.pdf,ICML
667,2019,A Large-Scale Study on Regularization and Normalization in GANs,"Karol Kurach,         Mario Lučić,         Xiaohua Zhai,         Marcin Michalski,         Sylvain Gelly","Generative adversarial networks (GANs) are a class of deep generative models which aim to learn a target distribution in an unsupervised fashion. While they were successfully applied to many problems, training a GAN is a notoriously challenging task and requires a significant number of hyperparameter tuning, neural architecture engineering, and a non-trivial amount of “tricks"". The success in many practical applications coupled with the lack of a measure to quantify the failure modes of GANs resulted in a plethora of proposed losses, regularization and normalization schemes, as well as neural architectures. In this work we take a sober view of the current state of GANs from a practical perspective. We discuss and evaluate common pitfalls and reproducibility issues, open-source our code on Github, and provide pre-trained models on TensorFlow Hub.",http://proceedings.mlr.press/v97/kurach19a.html,http://proceedings.mlr.press/v97/kurach19a/kurach19a.pdf,ICML
668,2019,Improved Zeroth-Order Variance Reduced Algorithms and Analysis for Nonconvex Optimization,"Kaiyi Ji,         Zhe Wang,         Yi Zhou,         Yingbin Liang","Two types of zeroth-order stochastic algorithms have recently been designed for nonconvex optimization respectively based on the first-order techniques SVRG and SARAH/SPIDER. This paper addresses several important issues that are still open in these methods. First, all existing SVRG-type zeroth-order algorithms suffer from worse function query complexities than either zeroth-order gradient descent (ZO-GD) or stochastic gradient descent (ZO-SGD). In this paper, we propose a new algorithm ZO-SVRG-Coord-Rand and develop a new analysis for an existing ZO-SVRG-Coord algorithm proposed in Liu et al. 2018b, and show that both ZO-SVRG-Coord-Rand and ZO-SVRG-Coord (under our new analysis) outperform other exiting SVRG-type zeroth-order methods as well as ZO-GD and ZO-SGD. Second, the existing SPIDER-type algorithm SPIDER-SZO (Fang et al., 2018) has superior theoretical performance, but suffers from the generation of a large number of Gaussian random variables as well as a ϵ√ϵ\sqrt{\epsilon}-level stepsize in practice. In this paper, we develop a new algorithm ZO-SPIDER-Coord, which is free from Gaussian variable generation and allows a large constant stepsize while maintaining the same convergence rate and query complexity, and we further show that ZO-SPIDER-Coord automatically achieves a linear convergence rate as the iterate enters into a local PL region without restart and algorithmic modification.",http://proceedings.mlr.press/v97/ji19a.html,http://proceedings.mlr.press/v97/ji19a/ji19a.pdf,ICML
669,2019,Bayesian Generative Active Deep Learning,"Toan Tran,         Thanh-Toan Do,         Ian Reid,         Gustavo Carneiro","Deep learning models have demonstrated outstanding performance in several problems, but their training process tends to require immense amounts of computational and human resources for training and labeling, constraining the types of problems that can be tackled. Therefore, the design of effective training methods that require small labeled training sets is an important research direction that will allow a more effective use of resources. Among current approaches designed to address this issue, two are particularly interesting: data augmentation and active learning. Data augmentation achieves this goal by artificially generating new training points, while active learning relies on the selection of the “most informative” subset of unlabeled training samples to be labelled by an oracle. Although successful in practice, data augmentation can waste computational resources because it indiscriminately generates samples that are not guaranteed to be informative, and active learning selects a small subset of informative samples (from a large un-annotated set) that may be insufficient for the training process. In this paper, we propose a Bayesian generative active deep learning approach that combines active learning with data augmentation – we provide theoretical and empirical evidence (MNIST, CIFAR-{10,100}{10,100}\{10,100\}, and SVHN) that our approach has more efficient training and better classification results than data augmentation and active learning.",http://proceedings.mlr.press/v97/tran19a.html,http://proceedings.mlr.press/v97/tran19a/tran19a.pdf,ICML
670,2019,Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces,"Johannes Kirschner,         Mojmir Mutny,         Nicole Hiller,         Rasmus Ischebeck,         Andreas Krause","Bayesian optimization is known to be difficult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space. In order to scale the method and keep its benefits, we propose an algorithm (LineBO) that restricts the problem to a sequence of iteratively chosen one-dimensional sub-problems that can be solved efficiently. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subspace, our method automatically adapts to the effective dimension without changing the algorithm. When combined with the SafeOpt algorithm to solve the sub-problems, we obtain the first safe Bayesian optimization algorithm with theoretical guarantees applicable in high-dimensional settings. We evaluate our method on multiple synthetic benchmarks, where we obtain competitive performance. Further, we deploy our algorithm to optimize the beam intensity of the Swiss Free Electron Laser with up to 40 parameters while satisfying safe operation constraints.",http://proceedings.mlr.press/v97/kirschner19a.html,http://proceedings.mlr.press/v97/kirschner19a/kirschner19a.pdf,ICML
671,2019,Are Generative Classifiers More Robust to Adversarial Attacks?,"Yingzhen Li,         John Bradshaw,         Yash Sharma","There is a rising interest in studying the robustness of deep neural network classifiers against adversaries, with both advanced attack and defence techniques being actively developed. However, most recent work focuses on discriminative classifiers, which only model the conditional distribution of the labels given the inputs. In this paper, we propose and investigate the deep Bayes classifier, which improves classical naive Bayes with conditional deep generative models. We further develop detection methods for adversarial examples, which reject inputs with low likelihood under the generative model. Experimental results suggest that deep Bayes classifiers are more robust than deep discriminative classifiers, and that the proposed detection methods are effective against many recently proposed attacks.",http://proceedings.mlr.press/v97/li19a.html,http://proceedings.mlr.press/v97/li19a/li19a.pdf,ICML
672,2019,Recursive Sketches for Modular Deep Learning,"Badih Ghazi,         Rina Panigrahy,         Joshua Wang","We present a mechanism to compute a sketch (succinct summary) of how a complex modular deep network processes its inputs. The sketch summarizes essential information about the inputs and outputs of the network and can be used to quickly identify key components and summary statistics of the inputs. Furthermore, the sketch is recursive and can be unrolled to identify sub-components of these components and so forth, capturing a potentially complicated DAG structure. These sketches erase gracefully; even if we erase a fraction of the sketch at random, the remainder still retains the “high-weight” information present in the original sketch. The sketches can also be organized in a repository to implicitly form a “knowledge graph”; it is possible to quickly retrieve sketches in the repository that are related to a sketch of interest; arranged in this fashion, the sketches can also be used to learn emerging concepts by looking for new clusters in sketch space. Finally, in the scenario where we want to learn a ground truth deep network, we show that augmenting input/output pairs with these sketches can theoretically make it easier to do so.",http://proceedings.mlr.press/v97/ghazi19a.html,http://proceedings.mlr.press/v97/ghazi19a/ghazi19a.pdf,ICML
673,2019,Nonlinear Stein Variational Gradient Descent for Learning Diversified Mixture Models,"Dilin Wang,         Qiang Liu","Diversification has been shown to be a powerful mechanism for learning robust models in non-convex settings. A notable example is learning mixture models, in which enforcing diversity between the different mixture components allows us to prevent the model collapsing phenomenon and capture more patterns from the observed data. In this work, we present a variational approach for diversity-promoting learning, which leverages the entropy functional as a natural mechanism for enforcing diversity. We develop a simple and efficient functional gradient-based algorithm for optimizing the variational objective function, which provides a significant generalization of Stein variational gradient descent (SVGD). We test our method on various challenging real world problems, including deep embedded clustering and deep anomaly detection. Empirical results show that our method provides an effective mechanism for diversity-promoting learning, achieving substantial improvement over existing methods.",http://proceedings.mlr.press/v97/wang19h.html,http://proceedings.mlr.press/v97/wang19h/wang19h.pdf,ICML
674,2019,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,"Juho Lee,         Yoonho Lee,         Jungtaek Kim,         Adam Kosiorek,         Seungjin Choi,         Yee Whye Teh","Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.",http://proceedings.mlr.press/v97/lee19d.html,http://proceedings.mlr.press/v97/lee19d/lee19d.pdf,ICML
675,2019,An Investigation into Neural Net Optimization via Hessian Eigenvalue Density,"Behrooz Ghorbani,         Shankar Krishnan,         Ying Xiao","To understand the dynamics of training in deep neural networks, we study the evolution of the Hessian eigenvalue density throughout the optimization process. In non-batch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In a batch normalized network, these two effects are almost absent. We give a theoretical rationale to partially explain these phenomena. As part of this work, we adapt advanced tools from numerical linear algebra that allow scalable and accurate estimation of the entire Hessian spectrum of ImageNet-scale neural networks; this technique may be of independent interest in other applications.",http://proceedings.mlr.press/v97/ghorbani19b.html,http://proceedings.mlr.press/v97/ghorbani19b/ghorbani19b.pdf,ICML
676,2019,Variational Annealing of GANs: A Langevin Perspective,"Chenyang Tao,         Shuyang Dai,         Liqun Chen,         Ke Bai,         Junya Chen,         Chang Liu,         Ruiyi Zhang,         Georgiy Bobashev,         Lawrence Carin Duke","The generative adversarial network (GAN) has received considerable attention recently as a model for data synthesis, without an explicit specification of a likelihood function. There has been commensurate interest in leveraging likelihood estimates to improve GAN training. To enrich the understanding of this fast-growing yet almost exclusively heuristic-driven subject, we elucidate the theoretical roots of some of the empirical attempts to stabilize and improve GAN training with the introduction of likelihoods. We highlight new insights from variational theory of diffusion processes to derive a likelihood-based regularizing scheme for GAN training, and present a novel approach to train GANs with an unnormalized distribution instead of empirical samples. To substantiate our claims, we provide experimental evidence on how our theoretically-inspired new algorithms improve upon current practice.",http://proceedings.mlr.press/v97/tao19a.html,http://proceedings.mlr.press/v97/tao19a/tao19a.pdf,ICML
677,2019,Multi-Frequency Phase Synchronization,"Tingran Gao,         Zhizhen Zhao","We propose a novel formulation for phase synchronization—the statistical problem of jointly estimating alignment angles from noisy pairwise comparisons—as a nonconvex optimization problem that enforces consistency among the pairwise comparisons in multiple frequency channels. Inspired by harmonic retrieval in signal processing, we develop a simple yet efficient two-stage algorithm that leverages the multi-frequency information. We demonstrate in theory and practice that the proposed algorithm significantly outperforms state-of-the-art phase synchronization algorithms, at a mild computational costs incurred by using the extra frequency channels. We also extend our algorithmic framework to general synchronization problems over compact Lie groups.",http://proceedings.mlr.press/v97/gao19f.html,http://proceedings.mlr.press/v97/gao19f/gao19f.pdf,ICML
678,2019,CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning,"Cédric Colas,         Pierre Fournier,         Mohamed Chetouani,         Olivier Sigaud,         Pierre-Yves Oudeyer","In open-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove easy and some impossible, agents must actively select which goal to practice at any moment, to maximize their overall mastery on the set of learnable goals. This paper proposes CURIOUS , an algorithm that leverages 1) a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress. Agents focus sequentially on goals of increasing complexity, and focus back on goals that are being forgotten. Experiments conducted in a new modular-goal robotic environment show the resulting developmental self-organization of a learning curriculum, and demonstrate properties of robustness to distracting goals, forgetting and changes in body properties.",http://proceedings.mlr.press/v97/colas19a.html,http://proceedings.mlr.press/v97/colas19a/colas19a.pdf,ICML
679,2019,Concentration Inequalities for Conditional Value at Risk,"Philip Thomas,         Erik Learned-Miller","In this paper we derive new concentration inequalities for the conditional value at risk (CVaR) of a random variable, and compare them to the previous state of the art (Brown, 2007). We show analytically that our lower bound is strictly tighter than Brown’s, and empirically that this difference is significant. While our upper bound may be looser than Brown’s in some cases, we show empirically that in most cases our bound is significantly tighter. After discussing when each upper bound is superior, we conclude with empirical results which suggest that both of our bounds will often be significantly tighter than Brown’s.",http://proceedings.mlr.press/v97/thomas19a.html,http://proceedings.mlr.press/v97/thomas19a/thomas19a.pdf,ICML
680,2019,Distributional Reinforcement Learning for Efficient Exploration,"Borislav Mavrin,         Hengshuai Yao,         Linglong Kong,         Kaiwen Wu,         Yaoliang Yu","In distributional reinforcement learning (RL), the estimated distribution of value functions model both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method achieves 483 % average gain across 49 games in cumulative rewards over QR-DQN. We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves nearoptimal safety rewards twice faster than QRDQN.",http://proceedings.mlr.press/v97/mavrin19a.html,http://proceedings.mlr.press/v97/mavrin19a/mavrin19a.pdf,ICML
681,2019,Boosted Density Estimation Remastered,"Zac Cranko,         Richard Nock","There has recently been a steady increase in the number iterative approaches to density estimation. However, an accompanying burst of formal convergence guarantees has not followed; all results pay the price of heavy assumptions which are often unrealistic or hard to check. The Generative Adversarial Network (GAN) literature — seemingly orthogonal to the aforementioned pursuit — has had the side effect of a renewed interest in variational divergence minimisation (notably fff-GAN). We show how to combine this latter approach and the classical boosting theory in supervised learning to get the first density estimation algorithm that provably achieves geometric convergence under very weak assumptions. We do so by a trick allowing to combine classifiers as the sufficient statistics of an exponential family. Our analysis includes an improved variational characterisation of fff-GAN.",http://proceedings.mlr.press/v97/cranko19b.html,http://proceedings.mlr.press/v97/cranko19b/cranko19b.pdf,ICML
682,2019,Discovering Options for Exploration by Minimizing Cover Time,"Yuu Jinnai,         Jee Won Park,         David Abel,         George Konidaris",One of the main challenges in reinforcement learning is solving tasks with sparse reward. We show that the difficulty of discovering a distant rewarding state in an MDP is bounded by the expected cover time of a random walk over the graph induced by the MDP’s transition dynamics. We therefore propose to accelerate exploration by constructing options that minimize cover time. We introduce a new option discovery algorithm that diminishes the expected cover time by connecting the most distant states in the state-space graph with options. We show empirically that the proposed algorithm improves learning in several domains with sparse rewards.,http://proceedings.mlr.press/v97/jinnai19b.html,http://proceedings.mlr.press/v97/jinnai19b/jinnai19b.pdf,ICML
683,2019,Detecting Overlapping and Correlated Communities without Pure Nodes: Identifiability and Algorithm,"Kejun Huang,         Xiao Fu","Many machine learning problems come in the form of networks with relational data between entities, and one of the key unsupervised learning tasks is to detect communities in such a network. We adopt the mixed-membership stochastic blockmodel as the underlying probabilistic model, and give conditions under which the memberships of a subset of nodes can be uniquely identified. Our method starts by constructing a second-order graph moment, which can be shown to converge to a specific product of the true parameters as the size of the network increases. To correctly recover the true membership parameters, we formulate an optimization problem using insights from convex geometry. We show that if the true memberships satisfy a so-called sufficiently scattered condition, then solving the proposed problem correctly identifies the ground truth. We also propose an efficient algorithm for detecting communities, which is significantly faster than prior work and with better convergence properties. Experiments on synthetic and real data justify the validity of the proposed learning framework for network data.",http://proceedings.mlr.press/v97/huang19c.html,http://proceedings.mlr.press/v97/huang19c/huang19c.pdf,ICML
684,2019,Fair Regression: Quantitative Definitions and Reduction-Based Algorithms,"Alekh Agarwal,         Miroslav Dudik,         Zhiwei Steven Wu","In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness–accuracy frontiers on several standard datasets.",http://proceedings.mlr.press/v97/agarwal19d.html,http://proceedings.mlr.press/v97/agarwal19d/agarwal19d.pdf,ICML
685,2019,"Garbage In, Reward Out: Bootstrapping Exploration in Multi-Armed Bandits","Branislav Kveton,         Csaba Szepesvari,         Sharan Vaswani,         Zheng Wen,         Tor Lattimore,         Mohammad Ghavamzadeh","We propose a bandit algorithm that explores by randomizing its history of rewards. Specifically, it pulls the arm with the highest mean reward in a non-parametric bootstrap sample of its history with pseudo rewards. We design the pseudo rewards such that the bootstrap mean is optimistic with a sufficiently high probability. We call our algorithm Giro, which stands for garbage in, reward out. We analyze Giro in a Bernoulli bandit and derive a O(KΔ−1logn)O(KΔ−1log⁡n)O(K \Delta^{-1} \log n) bound on its nnn-round regret, where ΔΔ\Delta is the difference in the expected rewards of the optimal and the best suboptimal arms, and KKK is the number of arms. The main advantage of our exploration design is that it easily generalizes to structured problems. To show this, we propose contextual Giro with an arbitrary reward generalization model. We evaluate Giro and its contextual variant on multiple synthetic and real-world problems, and observe that it performs well.",http://proceedings.mlr.press/v97/kveton19a.html,http://proceedings.mlr.press/v97/kveton19a/kveton19a.pdf,ICML
686,2019,SAGA with Arbitrary Sampling,"Xun Qian,         Zheng Qu,         Peter Richtárik","We study the problem of minimizing the average of a very large number of smooth functions, which is of key importance in training supervised learning models. One of the most celebrated methods in this context is the SAGA algorithm of Defazio et al. (2014). Despite years of research on the topic, a general-purpose version of SAGA—one that would include arbitrary importance sampling and minibatching schemes—does not exist. We remedy this situation and propose a general and flexible variant of SAGA following the arbitrary sampling paradigm. We perform an iteration complexity analysis of the method, largely possible due to the construction of new stochastic Lyapunov functions. We establish linear convergence rates in the smooth and strongly convex regime, and under certain error bound conditions also in a regime without strong convexity. Our rates match those of the primal-dual method Quartz (Qu et al., 2015) for which an arbitrary sampling analysis is available, which makes a significant step towards closing the gap in our understanding of complexity of primal and dual methods for finite sum problems. Finally, we show through experiments that specific variants of our general SAGA method can perform better in practice than other competing methods.",http://proceedings.mlr.press/v97/qian19a.html,http://proceedings.mlr.press/v97/qian19a/qian19a.pdf,ICML
687,2019,Variational Laplace Autoencoders,"Yookoon Park,         Chris Kim,         Gunhee Kim","Variational autoencoders employ an amortized inference model to approximate the posterior of latent variables. However, such amortized variational inference faces two challenges: (1) the limited posterior expressiveness of fully-factorized Gaussian assumption and (2) the amortization error of the inference model. We present a novel approach that addresses both challenges. First, we focus on ReLU networks with Gaussian output and illustrate their connection to probabilistic PCA. Building on this observation, we derive an iterative algorithm that finds the mode of the posterior and apply fullcovariance Gaussian posterior approximation centered on the mode. Subsequently, we present a general framework named Variational Laplace Autoencoders (VLAEs) for training deep generative models. Based on the Laplace approximation of the latent variable posterior, VLAEs enhance the expressiveness of the posterior while reducing the amortization error. Empirical results on MNIST, Omniglot, Fashion-MNIST, SVHN and CIFAR10 show that the proposed approach significantly outperforms other recent amortized or iterative methods on the ReLU networks.",http://proceedings.mlr.press/v97/park19a.html,http://proceedings.mlr.press/v97/park19a/park19a.pdf,ICML
688,2019,Fairness risk measures,"Robert Williamson,         Aditya Menon","Ensuring that classifiers are non-discriminatory or fair with respect to a sensitive feature (e.g., race or gender) is a topical problem. Progress in this task requires fixing a definition of fairness, and there have been several proposals in this regard over the past few years. Several of these, however, assume either binary sensitive features (thus precluding categorical or real-valued sensitive groups), or result in non-convex objectives (thus adversely affecting the optimisation landscape). In this paper, we propose a new definition of fairness that generalises some existing proposals, while allowing for generic sensitive features and resulting in a convex objective. The key idea is to enforce that the expected losses (or risks) across each subgroup induced by the sensitive feature are commensurate. We show how this relates to the rich literature on risk measures from mathematical finance. As a special case, this leads to a new convex fairness-aware objective based on minimising the conditional value at risk (CVaR).",http://proceedings.mlr.press/v97/williamson19a.html,http://proceedings.mlr.press/v97/williamson19a/williamson19a.pdf,ICML
689,2019,Rademacher Complexity for Adversarially Robust Generalization,"Dong Yin,         Ramchandran Kannan,         Peter Bartlett","Many machine learning models are vulnerable to adversarial attacks; for example, adding adversarial perturbations that are imperceptible to humans can often make machine learning models produce wrong predictions with high confidence; moreover, although we may obtain robust models on the training dataset via adversarial training, in some problems the learned models cannot generalize well to the test data. In this paper, we focus on ℓ∞\ell_\infty attacks, and study the adversarially robust generalization problem through the lens of Rademacher complexity. For binary linear classifiers, we prove tight bounds for the adversarial Rademacher complexity, and show that the adversarial Rademacher complexity is never smaller than its natural counterpart, and it has an unavoidable dimension dependence, unless the weight vector has bounded ℓ1\ell_1 norm, and our results also extend to multi-class linear classifiers; in addition, for (nonlinear) neural networks, we show that the dimension dependence in the adversarial Rademacher complexity also exists. We further consider a surrogate adversarial loss for one-hidden layer ReLU network and prove margin bounds for this setting. Our results indicate that having ℓ1\ell_1 norm constraints on the weight matrices might be a potential way to improve generalization in the adversarial setting. We demonstrate experimental results that validate our theoretical findings.",http://proceedings.mlr.press/v97/yin19b.html,http://proceedings.mlr.press/v97/yin19b/yin19b.pdf,ICML
690,2019,Plug-and-Play Methods Provably Converge with Properly Trained Denoisers,"Ernest Ryu,         Jialin Liu,         Sicheng Wang,         Xiaohan Chen,         Zhangyang Wang,         Wotao Yin","Plug-and-play (PnP) is a non-convex framework that integrates modern denoising priors, such as BM3D or deep learning-based denoisers, into ADMM or other proximal algorithms. An advantage of PnP is that one can use pre-trained denoisers when there is not sufficient data for end-to-end training. Although PnP has been recently studied extensively with great empirical success, theoretical analysis addressing even the most basic question of convergence has been insufficient. In this paper, we theoretically establish convergence of PnP-FBS and PnP-ADMM, without using diminishing stepsizes, under a certain Lipschitz condition on the denoisers. We then propose real spectral normalization, a technique for training deep learning-based denoisers to satisfy the proposed Lipschitz condition. Finally, we present experimental results validating the theory.",http://proceedings.mlr.press/v97/ryu19a.html,http://proceedings.mlr.press/v97/ryu19a/ryu19a.pdf,ICML
691,2019,LR-GLM: High-Dimensional Bayesian Inference Using Low-Rank Data Approximations,"Brian Trippe,         Jonathan Huggins,         Raj Agrawal,         Tamara Broderick","Due to the ease of modern data collection, applied statisticians often have access to a large set of covariates that they wish to relate to some observed outcome. Generalized linear models (GLMs) offer a particularly interpretable framework for such an analysis. In these high-dimensional problems, the number of covariates is often large relative to the number of observations, so we face non-trivial inferential uncertainty; a Bayesian approach allows coherent quantification of this uncertainty. Unfortunately, existing methods for Bayesian inference in GLMs require running times roughly cubic in parameter dimension, and so are limited to settings with at most tens of thousand parameters. We propose to reduce time and memory costs with a low-rank approximation of the data in an approach we call LR-GLM. When used with the Laplace approximation or Markov chain Monte Carlo, LR-GLM provides a full Bayesian posterior approximation and admits running times reduced by a full factor of the parameter dimension. We rigorously establish the quality of our approximation and show how the choice of rank allows a tunable computational–statistical trade-off. Experiments support our theory and demonstrate the efficacy of LR-GLM on real large-scale datasets.",http://proceedings.mlr.press/v97/trippe19a.html,http://proceedings.mlr.press/v97/trippe19a/trippe19a.pdf,ICML
692,2019,Online Algorithms for Rent-Or-Buy with Expert Advice,"Sreenivas Gollapudi,         Debmalya Panigrahi","We study the use of predictions by multiple experts (such as machine learning algorithms) to improve the performance of online algorithms. In particular, we consider the classical rent-or-buy problem (also called ski rental), and obtain algorithms that provably improve their performance over the adversarial scenario by using these predictions. We also prove matching lower bounds to show that our algorithms are the best possible, and perform experiments to empirically validate their performance in practice",http://proceedings.mlr.press/v97/gollapudi19a.html,http://proceedings.mlr.press/v97/gollapudi19a/gollapudi19a.pdf,ICML
693,2019,Breaking the Softmax Bottleneck via Learnable Monotonic Pointwise Non-linearities,"Octavian Ganea,         Sylvain Gelly,         Gary Becigneul,         Aliaksei Severyn","The Softmax function on top of a final linear layer is the de facto method to output probability distributions in neural networks. In many applications such as language models or text generation, this model has to produce distributions over large output vocabularies. Recently, this has been shown to have limited representational capacity due to its connection with the rank bottleneck in matrix factorization. However, little is known about the limitations of Linear-Softmax for quantities of practical interest such as cross entropy or mode estimation, a direction that we explore here. As an efficient and effective solution to alleviate this issue, we propose to learn parametric monotonic functions on top of the logits. We theoretically investigate the rank increasing capabilities of such monotonic functions. Empirically, our method improves in two different quality metrics over the traditional Linear-Softmax layer in synthetic and real language model experiments, adding little time or memory overhead, while being comparable to the more computationally expensive mixture of Softmaxes.",http://proceedings.mlr.press/v97/ganea19a.html,http://proceedings.mlr.press/v97/ganea19a/ganea19a.pdf,ICML
694,2019,Gromov-Wasserstein Learning for Graph Matching and Node Embedding,"Hongteng Xu,         Dixin Luo,         Hongyuan Zha,         Lawrence Carin Duke","A novel Gromov-Wasserstein learning framework is proposed to jointly match (align) graphs and learn embedding vectors for the associated graph nodes. Using Gromov-Wasserstein discrepancy, we measure the dissimilarity between two graphs and find their correspondence, according to the learned optimal transport. The node embeddings associated with the two graphs are learned under the guidance of the optimal transport, the distance of which not only reflects the topological structure of each graph but also yields the correspondence across the graphs. These two learning steps are mutually-beneficial, and are unified here by minimizing the Gromov-Wasserstein discrepancy with structural regularizers. This framework leads to an optimization problem that is solved by a proximal point method. We apply the proposed method to matching problems in real-world networks, and demonstrate its superior performance compared to alternative approaches.",http://proceedings.mlr.press/v97/xu19b.html,http://proceedings.mlr.press/v97/xu19b/xu19b.pdf,ICML
695,2019,Making Deep Q-learning methods robust to time discretization,"Corentin Tallec,         Léonard Blier,         Yann Ollivier","Despite remarkable successes, Deep Reinforce- ment Learning (DRL) is not robust to hyperparam- eterization, implementation details, or small envi- ronment changes (Henderson et al. 2017, Zhang et al. 2018). Overcoming such sensitivity is key to making DRL applicable to real world problems. In this paper, we identify sensitivity to time dis- cretization in near continuous-time environments as a critical factor; this covers, e.g., changing the number of frames per second, or the action frequency of the controller. Empirically, we find that Q-learning-based approaches such as Deep Q- learning (Mnih et al., 2015) and Deep Determinis- tic Policy Gradient (Lillicrap et al., 2015) collapse with small time steps. Formally, we prove that Q-learning does not exist in continuous time. We detail a principled way to build an off-policy RL algorithm that yields similar performances over a wide range of time discretizations, and confirm this robustness empirically.",http://proceedings.mlr.press/v97/tallec19a.html,http://proceedings.mlr.press/v97/tallec19a/tallec19a.pdf,ICML
696,2019,Spectral Clustering of Signed Graphs via Matrix Power Means,"Pedro Mercado,         Francesco Tudisco,         Matthias Hein","Signed graphs encode positive (attractive) and negative (repulsive) relations between nodes. We extend spectral clustering to signed graphs via the one-parameter family of Signed Power Mean Laplacians, defined as the matrix power mean of normalized standard and signless Laplacians of positive and negative edges. We provide a thorough analysis of the proposed approach in the setting of a general Stochastic Block Model that includes models such as the Labeled Stochastic Block Model and the Censored Block Model. We show that in expectation the signed power mean Laplacian captures the ground truth clusters under reasonable settings where state-of-the-art approaches fail. Moreover, we prove that the eigenvalues and eigenvector of the signed power mean Laplacian concentrate around their expectation under reasonable conditions in the general Stochastic Block Model. Extensive experiments on random graphs and real world datasets confirm the theoretically predicted behaviour of the signed power mean Laplacian and show that it compares favourably with state-of-the-art methods.",http://proceedings.mlr.press/v97/mercado19a.html,http://proceedings.mlr.press/v97/mercado19a/mercado19a.pdf,ICML
697,2019,Processing Megapixel Images with Deep Attention-Sampling Models,"Angelos Katharopoulos,         Francois Fleuret","Existing deep architectures cannot operate on very large signals such as megapixel images due to computational and memory constraints. To tackle this limitation, we propose a fully differentiable end-to-end trainable model that samples and processes only a fraction of the full resolution input image. The locations to process are sampled from an attention distribution computed from a low resolution view of the input. We refer to our method as attention sampling and it can process images of several megapixels with a standard single GPU setup. We show that sampling from the attention distribution results in an unbiased estimator of the full model with minimal variance, and we derive an unbiased estimator of the gradient that we use to train our model end-to-end with a normal SGD procedure. This new method is evaluated on three classification tasks, where we show that it allows to reduce computation and memory footprint by an order of magnitude for the same accuracy as classical architectures. We also show the consistency of the sampling that indeed focuses on informative parts of the input images.",http://proceedings.mlr.press/v97/katharopoulos19a.html,http://proceedings.mlr.press/v97/katharopoulos19a/katharopoulos19a.pdf,ICML
698,2019,"Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization","Eldad Meller,         Alexander Finkelstein,         Uri Almog,         Mark Grobman","Quantization of neural networks has become common practice, driven by the need for efficient implementations of deep neural networks on embedded devices. In this paper, we exploit an oft-overlooked degree of freedom in most networks - for a given layer, individual output channels can be scaled by any factor provided that the corresponding weights of the next layer are inversely scaled. Therefore, a given network has many factorizations which change the weights of the network without changing its function. We present a conceptually simple and easy to implement method that uses this property and show that proper factorizations significantly decrease the degradation caused by quantization. We show improvement on a wide variety of networks and achieve state-of-the-art degradation results for MobileNets. While our focus is on quantization, this type of factorization is applicable to other domains such as network-pruning, neural nets regularization and network interpretability.",http://proceedings.mlr.press/v97/meller19a.html,http://proceedings.mlr.press/v97/meller19a/meller19a.pdf,ICML
699,2019,Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment,"Chen Huang,         Shuangfei Zhai,         Walter Talbott,         Miguel Bautista Martin,         Shih-Yu Sun,         Carlos Guestrin,         Josh Susskind","In most machine learning training paradigms a fixed, often handcrafted, loss function is assumed to be a good proxy for an underlying evaluation metric. In this work we assess this assumption by meta-learning an adaptive loss function to directly optimize the evaluation metric. We propose a sample efficient reinforcement learning approach for adapting the loss dynamically during training. We empirically show how this formulation improves performance by simultaneously optimizing the evaluation metric and smoothing the loss landscape. We verify our method in metric learning and classification scenarios, showing considerable improvements over the state-of-the-art on a diverse set of tasks. Importantly, our method is applicable to a wide range of loss functions and evaluation metrics. Furthermore, the learned policies are transferable across tasks and data, demonstrating the versatility of the method.",http://proceedings.mlr.press/v97/huang19f.html,http://proceedings.mlr.press/v97/huang19f/huang19f.pdf,ICML
700,2019,Quantile Stein Variational Gradient Descent for Batch Bayesian Optimization,"Chengyue Gong,         Jian Peng,         Qiang Liu","Batch Bayesian optimization has been shown to be an efficient and successful approach for black-box function optimization, especially when the evaluation of cost function is highly expensive but can be efficiently parallelized. In this paper, we introduce a novel variational framework for batch query optimization, based on the argument that the query batch should be selected to have both high diversity and good worst case performance. This motivates us to introduce a variational objective that combines a quantile-based risk measure (for worst case performance) and entropy regularization (for enforcing diversity). We derive a gradient-based particle-based algorithm for solving our quantile-based variational objective, which generalizes Stein variational gradient descent (SVGD). We evaluate our method on a number of real-world applications and show that it consistently outperforms other recent state-of-the-art batch Bayesian optimization methods. Extensive experimental results indicate that our method achieves better or comparable performance, compared to the existing methods.",http://proceedings.mlr.press/v97/gong19b.html,http://proceedings.mlr.press/v97/gong19b/gong19b.pdf,ICML
701,2019,State-Regularized Recurrent Neural Networks,"Cheng Wang,         Mathias Niepert","Recurrent neural networks are a widely used class of neural architectures with two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We evaluate state-regularized RNNs on (1) regular languages for the purpose of automata extraction; (2) nonregular languages such as balanced parentheses, palindromes, and the copy task where external memory is required; and (3) real-word sequence learning tasks for sentiment analysis, visual object recognition, and language modeling. We show that state-regularization simplifies the extraction of finite state automata from the RNN’s state transition dynamics; forces RNNs to operate more like automata with external memory and less like finite state machines; and makes RNNs more interpretable.",http://proceedings.mlr.press/v97/wang19j.html,http://proceedings.mlr.press/v97/wang19j/wang19j.pdf,ICML
702,2019,Variational Inference for sparse network reconstruction from count data,"Julien Chiquet,         Stephane Robin,         Mahendra Mariadassou","Networks provide a natural yet statistically grounded way to depict and understand how a set of entities interact. However, in many situations interactions are not directly observed and the network needs to be reconstructed based on observations collected for each entity. Our work focuses on the situation where these observations consist of counts. A typical example is the reconstruction of an ecological network based on abundance data. In this setting, the abundance of a set of species is collected in a series of samples and/or environments and we aim at inferring direct interactions between the species. The abundances at hand can be, for example, direct counts of individuals (ecology of macro-organisms) or read counts resulting from metagenomic sequencing (microbial ecology). Whatever the approach chosen to infer such a network, it has to account for the peculiaraties of the data at hand. The first, obvious one, is that the data are counts, i.e. non continuous. Also, the observed counts often vary over many orders of magnitude and are more dispersed than expected under a simple model, such as the Poisson distribution. The observed counts may also result from different sampling efforts in each sample and/or for each entity, which hampers direct comparison. Furthermore, because the network is supposed to reveal only direct interactions, it is highly desirable to account for covariates describing the environment to avoid spurious edges. Many methods of network reconstruction from count data have been proposed. In the context of microbial ecology, most methods (SparCC, REBACCA, SPIEC-EASI, gCODA, BanOCC) rely on a two-step strategy: transform the counts to pseudo Gaussian observations using simple transforms before moving back to the setting of Gaussian Graphical Models, for which state of the art methods exist to infer the network, but only in a Gaussian world. In this work, we consider instead a full-fledged probabilistic model with a latent layer where the counts follow Poisson distributions, conditional to latent (hidden) Gaussian correlated variables. In this model, known as Poisson log-normal (PLN), the dependency structure is completely captured by the latent layer and we model counts, rather than transformations thereof. To our knowledge, the PLN framework is quite new and has only been used by two other recent methods (Mint and plnDAG) to reconstruct networks from count data. In this work, we use the same mathematical framework but adopt a different optimization strategy which alleviates the whole optimization process. We also fully exploit the connection between the PLN framework and generalized linear models to account for the peculiarities of microbiological data sets. The network inference step is done as usual by adding sparsity inducing constraints on the inverse covariance matrix of the latent Gaussian vector to select only the most important interactions between species. Unlike the usual Gaussian setting, the penalized likelihood is generally not tractable in this framework. We resort instead to a variational approximation for parameter inference and solve the corresponding optimization problem by alternating a gradient descent on the variational parameters and a graphical-Lasso step on the covariance matrix. We also select the sparsity parameter using the resampling-based StARS procedure. We show that the sparse PLN approach has better performance than existing methods on simulated datasets and that it extracts relevant signal from microbial ecology datasets. We also show that the inference scales to datasets made up of hundred of species and samples, in line with other methods in the field. In short, our contributions to the field are the following: we extend the use of PLN distributions in network inference by (i) accounting for covariates and offset and thus removing some spurious edges induced by confounding factors, (ii) accounting for different sampling effort to integrate data sets from different sources and thus infer interactions between different types of organisms (e.g. bacteria - fungi), (iii) developing an inference procedure based on the iterative optimization of a well defined objective function. Our objective function is a provable lower bound of the observed likelihood and our procedure accounts for the uncertainty associated with the estimation of the latent variable, unlike the algorithm presented in Mint and plnDAG.",http://proceedings.mlr.press/v97/chiquet19a.html,http://proceedings.mlr.press/v97/chiquet19a/chiquet19a.pdf,ICML
703,2019,Geometric Scattering for Graph Data Analysis,"Feng Gao,         Guy Wolf,         Matthew Hirn","We explore the generalization of scattering transforms from traditional (e.g., image or audio) signals to graph data, analogous to the generalization of ConvNets in geometric deep learning, and the utility of extracted graph features in graph data analysis. In particular, we focus on the capacity of these features to retain informative variability and relations in the data (e.g., between individual graphs, or in aggregate), while relating our construction to previous theoretical results that establish the stability of similar transforms to families of graph deformations. We demonstrate the application of our geometric scattering features in graph classification of social network data, and in data exploration of biochemistry data.",http://proceedings.mlr.press/v97/gao19e.html,http://proceedings.mlr.press/v97/gao19e/gao19e.pdf,ICML
704,2019,Guarantees for Spectral Clustering with Fairness Constraints,"Matthäus Kleindessner,         Samira Samadi,         Pranjal Awasthi,         Jamie Morgenstern","Given the widespread popularity of spectral clustering (SC) for partitioning graph data, we study a version of constrained SC in which we try to incorporate the fairness notion proposed by Chierichetti et al. (2017). According to this notion, a clustering is fair if every demographic group is approximately proportionally represented in each cluster. To this end, we develop variants of both normalized and unnormalized constrained SC and show that they help find fairer clusterings on both synthetic and real data. We also provide a rigorous theoretical analysis of our algorithms on a natural variant of the stochastic block model, where hhh groups have strong inter-group connectivity, but also exhibit a “natural” clustering structure which is fair. We prove that our algorithms can recover this fair clustering with high probability.",http://proceedings.mlr.press/v97/kleindessner19b.html,http://proceedings.mlr.press/v97/kleindessner19b/kleindessner19b.pdf,ICML
705,2019,A Personalized Affective Memory Model for Improving Emotion Recognition,"Pablo Barros,         German Parisi,         Stefan Wermter","Recent models of emotion recognition strongly rely on supervised deep learning solutions for the distinction of general emotion expressions. However, they are not reliable when recognizing online and personalized facial expressions, e.g., for person-specific affective understanding. In this paper, we present a neural model based on a conditional adversarial autoencoder to learn how to represent and edit general emotion expressions. We then propose Grow-When-Required networks as personalized affective memories to learn individualized aspects of emotional expressions. Our model achieves state-of-the-art performance on emotion recognition when evaluated on in-the-wild datasets. Furthermore, our experiments include ablation studies and neural visualizations in order to explain the behavior of our model.",http://proceedings.mlr.press/v97/barros19a.html,http://proceedings.mlr.press/v97/barros19a/barros19a.pdf,ICML
706,2019,Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting,"Xilai Li,         Yingbo Zhou,         Tianfu Wu,         Richard Socher,         Caiming Xiong","Addressing catastrophic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. Despite recent remarkable progress in state-of-the-art deep learning, deep neural networks (DNNs) are still plagued with the catastrophic forgetting problem. This paper presents a conceptually simple yet general and effective framework for handling catastrophic forgetting in continual learning with DNNs. The proposed method consists of two components: a neural structure optimization component and a parameter learning and/or fine-tuning component. By separating the explicit neural structure learning and the parameter estimation, not only is the proposed method capable of evolving neural structures in an intuitively meaningful way, but also shows strong capabilities of alleviating catastrophic forgetting in experiments. Furthermore, the proposed method outperforms all other baselines on the permuted MNIST dataset, the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting.",http://proceedings.mlr.press/v97/li19m.html,http://proceedings.mlr.press/v97/li19m/li19m.pdf,ICML
707,2019,Action Robust Reinforcement Learning and Applications in Continuous Control,"Chen Tessler,         Yonathan Efroni,         Shie Mannor","A policy is said to be robust if it maximizes the reward while considering a bad, or even adversarial, model. In this work we formalize two new criteria of robustness to action uncertainty. Specifically, we consider two scenarios in which the agent attempts to perform an action \action\action\action, and (i) with probability αα\alpha, an alternative adversarial action ¯\action\action¯\bar \action is taken, or (ii) an adversary adds a perturbation to the selected action in the case of continuous action space. We show that our criteria are related to common forms of uncertainty in robotics domains, such as the occurrence of abrupt forces, and suggest algorithms in the tabular case. Building on the suggested algorithms, we generalize our approach to deep reinforcement learning (DRL) and provide extensive experiments in the various MuJoCo domains. Our experiments show that not only does our approach produce robust policies, but it also improves the performance in the absence of perturbations. This generalization indicates that action-robustness can be thought of as implicit regularization in RL problems.",http://proceedings.mlr.press/v97/tessler19a.html,http://proceedings.mlr.press/v97/tessler19a/tessler19a.pdf,ICML
708,2019,Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling,"Shanshan Wu,         Alex Dimakis,         Sujay Sanghavi,         Felix Yu,         Daniel Holtmann-Rice,         Dmitry Storcheus,         Afshin Rostamizadeh,         Sanjiv Kumar","Linear encoding of sparse vectors is widely popular, but is commonly data-independent – missing any possible extra (but a priori unknown) structure beyond sparsity. In this paper we present a new method to learn linear encoders that adapt to data, while still performing well with the widely used ℓ1ℓ1\ell_1 decoder. The convex ℓ1ℓ1\ell_1 decoder prevents gradient propagation as needed in standard gradient-based training. Our method is based on the insight that unrolling the convex decoder into TTT projected subgradient steps can address this issue. Our method can be seen as a data-driven way to learn a compressed sensing measurement matrix. We compare the empirical performance of 10 algorithms over 6 sparse datasets (3 synthetic and 3 real). Our experiments show that there is indeed additional structure beyond sparsity in the real datasets; our method is able to discover it and exploit it to create excellent reconstructions with fewer measurements (by a factor of 1.1-3x) compared to the previous state-of-the-art methods. We illustrate an application of our method in learning label embeddings for extreme multi-label classification, and empirically show that our method is able to match or outperform the precision scores of SLEEC, which is one of the state-of-the-art embedding-based approaches.",http://proceedings.mlr.press/v97/wu19b.html,http://proceedings.mlr.press/v97/wu19b/wu19b.pdf,ICML
709,2019,Beyond the Chinese Restaurant and Pitman-Yor processes: Statistical Models with double power-law behavior,"Fadhel Ayed,         Juho Lee,         Francois Caron","Bayesian nonparametric approaches, in particular the Pitman-Yor process and the associated two-parameter Chinese Restaurant process, have been successfully used in applications where the data exhibit a power-law behavior. Examples include natural language processing, natural images or networks. There is also growing empirical evidence suggesting that some datasets exhibit a two-regime power-law behavior: one regime for small frequencies, and a second regime, with a different exponent, for high frequencies. In this paper, we introduce a class of completely random measures which are doubly regularly-varying. Contrary to the Pitman-Yor process, we show that when completely random measures in this class are normalized to obtain random probability measures and associated random partitions, such partitions exhibit a double power-law behavior. We present two general constructions and discuss in particular two models within this class: the beta prime process (Broderick et al. (2015, 2018) and a novel process called generalized BFRY process. We derive efficient Markov chain Monte Carlo algorithms to estimate the parameters of these models. Finally, we show that the proposed models provide a better fit than the Pitman-Yor process on various datasets.",http://proceedings.mlr.press/v97/ayed19a.html,http://proceedings.mlr.press/v97/ayed19a/ayed19a.pdf,ICML
710,2019,Linear-Complexity Data-Parallel Earth Mover’s Distance Approximations,"Kubilay Atasu,         Thomas Mittelholzer","The Earth Mover’s Distance (EMD) is a state-of-the art metric for comparing discrete probability distributions, but its high distinguishability comes at a high cost in computational complexity. Even though linear-complexity approximation algorithms have been proposed to improve its scalability, these algorithms are either limited to vector spaces with only a few dimensions or they become ineffective when the degree of overlap between the probability distributions is high. We propose novel approximation algorithms that overcome both of these limitations, yet still achieve linear time complexity. All our algorithms are data parallel, and therefore, we can take advantage of massively parallel computing engines, such as Graphics Processing Units (GPUs). On the popular text-based 20 Newsgroups dataset, the new algorithms are four orders of magnitude faster than a multi-threaded CPU implementation of Word Mover’s Distance and match its search accuracy. On MNIST images, the new algorithms are four orders of magnitude faster than Cuturi’s GPU implementation of the Sinkhorn’s algorithm while offering a slightly higher search accuracy.",http://proceedings.mlr.press/v97/atasu19a.html,http://proceedings.mlr.press/v97/atasu19a/atasu19a.pdf,ICML
711,2019,A Quantitative Analysis of the Effect of Batch Normalization on Gradient Descent,"Yongqiang Cai,         Qianxiao Li,         Zuowei Shen","Despite its empirical success and recent theoretical progress, there generally lacks a quantitative analysis of the effect of batch normalization (BN) on the convergence and stability of gradient descent. In this paper, we provide such an analysis on the simple problem of ordinary least squares (OLS), where the precise dynamical properties of gradient descent (GD) is completely known, thus allowing us to isolate and compare the additional effects of BN. More precisely, we show that unlike GD, gradient descent with BN (BNGD) converges for arbitrary learning rates for the weights, and the convergence remains linear under mild conditions. Moreover, we quantify two different sources of acceleration of BNGD over GD – one due to over-parameterization which improves the effective condition number and another due having a large range of learning rates giving rise to fast descent. These phenomena set BNGD apart from GD and could account for much of its robustness properties. These findings are confirmed quantitatively by numerical experiments, which further show that many of the uncovered properties of BNGD in OLS are also observed qualitatively in more complex supervised learning problems.",http://proceedings.mlr.press/v97/cai19a.html,http://proceedings.mlr.press/v97/cai19a/cai19a.pdf,ICML
712,2019,Geometry and Symmetry in Short-and-Sparse Deconvolution,"Han-Wen Kuo,         Yenson Lau,         Yuqian Zhang,         John Wright","We study the Short-and-Sparse (SaS) deconvolution problem of recovering a short signal a0 and a sparse signal x0 from their convolution. We propose a method based on nonconvex optimization, which under certain conditions recovers the target short and sparse signals, up to a signed shift symmetry which is intrinsic to this model. This symmetry plays a central role in shaping the optimization landscape for deconvolution. We give a regional analysis, which characterizes this landscape geometrically, on a union of subspaces. Our geometric characterization holds when the length-p0 short signal a0 has shift coherence {\textmu}, and x0 follows a random sparsity model with sparsity rate θθ\theta ∈∈\in [c1/p0, c2/(p0\sqrt{\mu}+\sqrt{p0})] / (log^2(p0)) . Based on this geometry, we give a provable method that successfully solves SaS deconvolution with high probability.",http://proceedings.mlr.press/v97/kuo19a.html,http://proceedings.mlr.press/v97/kuo19a/kuo19a.pdf,ICML
713,2019,Low Latency Privacy Preserving Inference,"Alon Brutzkus,         Ran Gilad-Bachrach,         Oren Elisha","When applying machine learning to sensitive data, one has to find a balance between accuracy, information security, and computational-complexity. Recent studies combined Homomorphic Encryption with neural networks to make inferences while protecting against information leakage. However, these methods are limited by the width and depth of neural networks that can be used (and hence the accuracy) and exhibit high latency even for relatively simple networks. In this study we provide two solutions that address these limitations. In the first solution, we present more than 10\times improvement in latency and enable inference on wider networks compared to prior attempts with the same level of security. The improved performance is achieved by novel methods to represent the data during the computation. In the second solution, we apply the method of transfer learning to provide private inference services using deep networks with latency of \sim0.16 seconds. We demonstrate the efficacy of our methods on several computer vision tasks.",http://proceedings.mlr.press/v97/brutzkus19a.html,http://proceedings.mlr.press/v97/brutzkus19a/brutzkus19a.pdf,ICML
714,2019,Discovering Conditionally Salient Features with Statistical Guarantees,"Jaime Roquero Gimenez,         James Zou","The goal of feature selection is to identify important features that are relevant to explain a outcome variable. Most of the work in this domain has focused on identifying globally relevant features, which are features that are related to the outcome using evidence across the entire dataset. We study a more fine-grained statistical problem: conditional feature selection, where a feature may be relevant depending on the values of the other features. For example in genetic association studies, variant AAA could be associated with the phenotype in the entire dataset, but conditioned on variant BBB being present it might be independent of the phenotype. In this sense, variant AAA is globally relevant, but conditioned on BBB it is no longer locally relevant in that region of the feature space. We present a generalization of the knockoff procedure that performs conditional feature selection while controlling a generalization of the false discovery rate (FDR) to the conditional setting. By exploiting the feature/response model-free framework of the knockoffs, the quality of the statistical FDR guarantee is not degraded even when we perform conditional feature selections. We implement this method and present an algorithm that automatically partitions the feature space such that it enhances the differences between selected sets in different regions, and validate the statistical theoretical results with experiments.",http://proceedings.mlr.press/v97/gimenez19a.html,http://proceedings.mlr.press/v97/gimenez19a/gimenez19a.pdf,ICML
715,2019,Sparse Multi-Channel Variational Autoencoder for the Joint Analysis of Heterogeneous Data,"Luigi Antelmi,         Nicholas Ayache,         Philippe Robert,         Marco Lorenzi","Interpretable modeling of heterogeneous data channels is essential in medical applications, for example when jointly analyzing clinical scores and medical images. Variational Autoencoders (VAE) are powerful generative models that learn representations of complex data. The flexibility of VAE may come at the expense of lack of interpretability in describing the joint relationship between heterogeneous data. To tackle this problem, in this work we extend the variational framework of VAE to bring parsimony and interpretability when jointly account for latent relationships across multiple channels. In the latent space, this is achieved by constraining the variational distribution of each channel to a common target prior. Parsimonious latent representations are enforced by variational dropout. Experiments on synthetic data show that our model correctly identifies the prescribed latent dimensions and data relationships across multiple testing scenarios. When applied to imaging and clinical data, our method allows to identify the joint effect of age and pathology in describing clinical condition in a large scale clinical cohort.",http://proceedings.mlr.press/v97/antelmi19a.html,http://proceedings.mlr.press/v97/antelmi19a/antelmi19a.pdf,ICML
716,2019,Graph Resistance and Learning from Pairwise Comparisons,"Julien Hendrickx,         Alexander Olshevsky,         Venkatesh Saligrama","We consider the problem of learning the qualities of a collection of items by performing noisy comparisons among them. Following the standard paradigm, we assume there is a fixed “comparison graph” and every neighboring pair of items in this graph is compared k times according to the Bradley-Terry-Luce model (where the probability than an item wins a comparison is proportional the item quality). We are interested in how the relative error in quality estimation scales with the comparison graph in the regime where k is large. We show that, asymptotically, the relevant graph-theoretic quantity is the square root of the resistance of the comparison graph. Specifically, we provide an algorithm with relative error decay that scales with the square root of the graph resistance, and provide a lower bound showing that (up to log factors) a better scaling is impossible. The performance guarantee of our algorithm, both in terms of the graph and the skewness of the item quality distribution, significantly outperforms earlier results.",http://proceedings.mlr.press/v97/hendrickx19a.html,http://proceedings.mlr.press/v97/hendrickx19a/hendrickx19a.pdf,ICML
717,2019,A Deep Reinforcement Learning Perspective on Internet Congestion Control,"Nathan Jay,         Noga Rotman,         Brighten Godfrey,         Michael Schapira,         Aviv Tamar","We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources’ data-transmission rates to efficiently utilize network capacity, and is the subject of extensive attention in light of the advent of Internet services such as live video, virtual reality, Internet-of-Things, and more. We show that casting congestion control as RL enables training deep network policies that capture intricate patterns in data traffic and network conditions, and leverage this to outperform the state-of-the-art. We also highlight significant challenges facing real-world adoption of RL-based congestion control, including fairness, safety, and generalization, which are not trivial to address within conventional RL formalism. To facilitate further research and reproducibility of our results, we present a test suite for RL-guided congestion control based on the OpenAI Gym interface.",http://proceedings.mlr.press/v97/jay19a.html,http://proceedings.mlr.press/v97/jay19a/jay19a.pdf,ICML
718,2019,Greedy Layerwise Learning Can Scale To ImageNet,"Eugene Belilovsky,         Michael Eickenberg,         Edouard Oyallon","Shallow supervised 1-hidden layer neural networks have a number of favorable properties that make them easier to interpret, analyze, and optimize than their deep counterparts, but lack their representational power. Here we use 1-hidden layer learning problems to sequentially build deep networks layer by layer, which can inherit properties from shallow networks. Contrary to previous approaches using shallow networks, we focus on problems where deep learning is reported as critical for success. We thus study CNNs on image classification tasks using the large-scale ImageNet dataset and the CIFAR-10 dataset. Using a simple set of ideas for architecture and training we find that solving sequential 1-hidden-layer auxiliary problems lead to a CNN that exceeds AlexNet performance on ImageNet. Extending this training methodology to construct individual layers by solving 2-and-3-hidden layer auxiliary problems, we obtain an 11-layer network that exceeds several members of the VGG model family on ImageNet, and can train a VGG-11 model to the same accuracy as end-to-end learning. To our knowledge, this is the first competitive alternative to end-to-end training of CNNs that can scale to ImageNet. We illustrate several interesting properties of these models and conduct a range of experiments to study the properties this training induces on the intermediate layers.",http://proceedings.mlr.press/v97/belilovsky19a.html,http://proceedings.mlr.press/v97/belilovsky19a/belilovsky19a.pdf,ICML
719,2019,Disentangling Disentanglement in Variational Autoencoders,"Emile Mathieu,         Tom Rainforth,         N Siddharth,         Yee Whye Teh","We develop a generalisation of disentanglement in variational autoencoders (VAEs)—decomposition of the latent representation—characterising it as the fulfilment of two factors: a) the latent encodings of the data having an appropriate level of overlap, and b) the aggregate encoding of the data conforming to a desired structure, represented through the prior. Decomposition permits disentanglement, i.e. explicit independence between latents, as a special case, but also allows for a much richer class of properties to be imposed on the learnt representation, such as sparsity, clustering, independent subspaces, or even intricate hierarchical dependency relationships. We show that the ββ\beta-VAE varies from the standard VAE predominantly in its control of latent overlap and that for the standard choice of an isotropic Gaussian prior, its objective is invariant to rotations of the latent representation. Viewed from the decomposition perspective, breaking this invariance with simple manipulations of the prior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of prior can assist in producing different decompositions and introduce an alternative training objective that allows the control of both decomposition factors in a principled manner.",http://proceedings.mlr.press/v97/mathieu19a.html,http://proceedings.mlr.press/v97/mathieu19a/mathieu19a.pdf,ICML
720,2019,Metropolis-Hastings Generative Adversarial Networks,"Ryan Turner,         Jane Hung,         Eric Frank,         Yunus Saatchi,         Jason Yosinski","We introduce the Metropolis-Hastings generative adversarial network (MH-GAN), which combines aspects of Markov chain Monte Carlo and GANs. The MH-GAN draws samples from the distribution implicitly defined by a GAN’s discriminator-generator pair, as opposed to standard GANs which draw samples from the distribution defined only by the generator. It uses the discriminator from GAN training to build a wrapper around the generator for improved sampling. With a perfect discriminator, this wrapped generator samples from the true distribution on the data exactly even when the generator is imperfect. We demonstrate the benefits of the improved generator on multiple benchmark datasets, including CIFAR-10 and CelebA, using the DCGAN, WGAN, and progressive GAN.",http://proceedings.mlr.press/v97/turner19a.html,http://proceedings.mlr.press/v97/turner19a/turner19a.pdf,ICML
721,2019,BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning,"Asa Cooper Stickland,         Iain Murray","Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsupervised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a \hbox{single} BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or ‘projected attention layers’, we match the performance of separately fine-tuned models on the GLUE benchmark with ≈≈\approx7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.",http://proceedings.mlr.press/v97/stickland19a.html,http://proceedings.mlr.press/v97/stickland19a/stickland19a.pdf,ICML
722,2019,Learning from a Learner,"Alexis Jacq,         Matthieu Geist,         Ana Paiva,         Olivier Pietquin","In this paper, we propose a novel setting for Inverse Reinforcement Learning (IRL), namely ""Learning from a Learner"" (LfL). As opposed to standard IRL, it does not consist in learning a reward by observing an optimal agent but from observations of another learning (and thus sub-optimal) agent. To do so, we leverage the fact that the observed agent’s policy is assumed to improve over time. The ultimate goal of this approach is to recover the actual environment’s reward and to allow the observer to outperform the learner. To recover that reward in practice, we propose methods based on the entropy-regularized policy iteration framework. We discuss different approaches to learn solely from trajectories in the state-action space. We demonstrate the genericity of our method by observing agents implementing various reinforcement learning algorithms. Finally, we show that, on both discrete and continuous state/action tasks, the observer’s performance (that optimizes the recovered reward) can surpass those of the observed agent.",http://proceedings.mlr.press/v97/jacq19a.html,http://proceedings.mlr.press/v97/jacq19a/jacq19a.pdf,ICML
723,2019,Robust Estimation of Tree Structured Gaussian Graphical Models,"Ashish Katiyar,         Jessica Hoffmann,         Constantine Caramanis","Consider jointly Gaussian random variables whose conditional independence structure is specified by a graphical model. If we observe realizations of the variables, we can compute the covariance matrix, and it is well known that the support of the inverse covariance matrix corresponds to the edges of the graphical model. Instead, suppose we only have noisy observations. If the noise at each node is independent, we can compute the sum of the covariance matrix and an unknown diagonal. The inverse of this sum is (in general) dense. We ask: can the original independence structure be recovered? We address this question for tree structured graphical models. We prove that this problem is unidentifiable, but show that this unidentifiability is limited to a small class of candidate trees. We further present additional constraints under which the problem is identifiable. Finally, we provide an O(n^3) algorithm to find this equivalence class of trees.",http://proceedings.mlr.press/v97/katiyar19a.html,http://proceedings.mlr.press/v97/katiyar19a/katiyar19a.pdf,ICML
724,2019,Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels,"Pengfei Chen,         Ben Ben Liao,         Guangyong Chen,         Shengyu Zhang","Noisy labels are ubiquitous in real-world datasets, which poses a challenge for robustly training deep neural networks (DNNs) as DNNs usually have the high capacity to memorize the noisy labels. In this paper, we find that the test accuracy can be quantitatively characterized in terms of the noise ratio in datasets. In particular, the test accuracy is a quadratic function of the noise ratio in the case of symmetric noise, which explains the experimental findings previously published. Based on our analysis, we apply cross-validation to randomly split noisy datasets, which identifies most samples that have correct labels. Then we adopt the Co-teaching strategy which takes full advantage of the identified samples to train DNNs robustly against noisy labels. Compared with extensive state-of-the-art methods, our strategy consistently improves the generalization performance of DNNs under both synthetic and real-world training noise.",http://proceedings.mlr.press/v97/chen19g.html,http://proceedings.mlr.press/v97/chen19g/chen19g.pdf,ICML
725,2019,Stochastic Iterative Hard Thresholding for Graph-structured Sparsity Optimization,"Baojian Zhou,         Feng Chen,         Yiming Ying","Stochastic optimization algorithms update models with cheap per-iteration costs sequentially, which makes them amenable for large-scale data analysis. Such algorithms have been widely studied for structured sparse models where the sparsity information is very specific, e.g., convex sparsity-inducing norms or ℓ0ℓ0\ell^0-norm. However, these norms cannot be directly applied to the problem of complex (non-convex) graph-structured sparsity models, which have important application in disease outbreak and social networks, etc. In this paper, we propose a stochastic gradient-based method for solving graph-structured sparsity constraint problems, not restricted to the least square loss. We prove that our algorithm enjoys a linear convergence up to a constant error, which is competitive with the counterparts in the batch learning setting. We conduct extensive experiments to show the efficiency and effectiveness of the proposed algorithms.",http://proceedings.mlr.press/v97/zhou19a.html,http://proceedings.mlr.press/v97/zhou19a/zhou19a.pdf,ICML
726,2019,Learning a Prior over Intent via Meta-Inverse Reinforcement Learning,"Kelvin Xu,         Ellis Ratner,         Anca Dragan,         Sergey Levine,         Chelsea Finn","A significant challenge for the practical application of reinforcement learning to real world problems is the need to specify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert demonstrations. While appealing, it can be impractically expensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. opening any type of door). Thus in practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a ""prior"" that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior.",http://proceedings.mlr.press/v97/xu19d.html,http://proceedings.mlr.press/v97/xu19d/xu19d.pdf,ICML
727,2019,Learning with Bad Training Data via Iterative Trimmed Loss Minimization,"Yanyao Shen,         Sujay Sanghavi","In this paper, we study a simple and generic framework to tackle the problem of learning model parameters when a fraction of the training samples are corrupted. Our approach is motivated by a simple observation: in a variety of such settings, the evolution of training accuracy (as a function of training epochs) is different for clean samples and bad samples. We propose to iteratively minimize the trimmed loss, by alternating between (a) selecting samples with lowest current loss, and (b) retraining a model on only these samples. Analytically, we characterize the statistical performance and convergence rate of the algorithm for simple and natural linear and non-linear models. Experimentally, we demonstrate its effectiveness in three settings: (a) deep image classifiers with errors only in labels, (b) generative adversarial networks with bad training images, and (c) deep image classifiers with adversarial (image, label) pairs (i.e., backdoor attacks). For the well-studied setting of random label noise, our algorithm achieves state-of-the-art performance without having access to any a-priori guaranteed clean samples.",http://proceedings.mlr.press/v97/shen19e.html,http://proceedings.mlr.press/v97/shen19e/shen19e.pdf,ICML
728,2019,Efficient optimization of loops and limits with randomized telescoping sums,"Alex Beatson,         Ryan P Adams","We consider optimization problems in which the objective requires an inner loop with many steps or is the limit of a sequence of increasingly costly approximations. Meta-learning, training recurrent neural networks, and optimization of the solutions to differential equations are all examples of optimization problems with this character. In such problems, it can be expensive to compute the objective function value and its gradient, but truncating the loop or using less accurate approximations can induce biases that damage the overall solution. We propose randomized telescope (RT) gradient estimators, which represent the objective as the sum of a telescoping series and sample linear combinations of terms to provide cheap unbiased gradient estimates. We identify conditions under which RT estimators achieve optimization convergence rates independent of the length of the loop or the required accuracy of the approximation. We also derive a method for tuning RT estimators online to maximize a lower bound on the expected decrease in loss per unit of computation. We evaluate our adaptive RT estimators on a range of applications including meta-optimization of learning rates, variational inference of ODE parameters, and training an LSTM to model long sequences.",http://proceedings.mlr.press/v97/beatson19a.html,http://proceedings.mlr.press/v97/beatson19a/beatson19a.pdf,ICML
729,2019,An Investigation of Model-Free Planning,"Arthur Guez,         Mehdi Mirza,         Karol Gregor,         Rishabh Kabra,         Sebastien Racaniere,         Theophane Weber,         David Raposo,         Adam Santoro,         Laurent Orseau,         Tom Eccles,         Greg Wayne,         David Silver,         Timothy Lillicrap","The field of reinforcement learning (RL) is facing increasingly challenging domains with combinatorial complexity. For an RL agent to address these challenges, it is essential that it can plan effectively. Prior work has typically utilized an explicit model of the environment, combined with a specific planning algorithm (such as tree search). More recently, a new family of methods have been proposed that learn how to plan, by providing the structure for planning via an inductive bias in the function approximator (such as a tree structured neural network), trained end-to-end by a model-free RL algorithm. In this paper, we go even further, and demonstrate empirically that an entirely model-free approach, without special structure beyond standard neural network components such as convolutional networks and LSTMs, can learn to exhibit many of the characteristics typically associated with a model-based planner. We measure our agent’s effectiveness at planning in terms of its ability to generalize across a combinatorial and irreversible state space, its data efficiency, and its ability to utilize additional thinking time. We find that our agent has many of the characteristics that one might expect to find in a planning algorithm. Furthermore, it exceeds the state-of-the-art in challenging combinatorial domains such as Sokoban and outperforms other model-free approaches that utilize strong inductive biases toward planning.",http://proceedings.mlr.press/v97/guez19a.html,http://proceedings.mlr.press/v97/guez19a/guez19a.pdf,ICML
730,2019,Online Learning to Rank with Features,"Shuai Li,         Tor Lattimore,         Csaba Szepesvari","We introduce a new model for online ranking in which the click probability factors into an examination and attractiveness function and the attractiveness function is a linear function of a feature vector and an unknown parameter. Only relatively mild assumptions are made on the examination function. A novel algorithm for this setup is analysed, showing that the dependence on the number of items is replaced by a dependence on the dimension, allowing the new algorithm to handle a large number of items. When reduced to the orthogonal case, the regret of the algorithm improves on the state-of-the-art.",http://proceedings.mlr.press/v97/li19f.html,http://proceedings.mlr.press/v97/li19f/li19f.pdf,ICML
731,2019,Complementary-Label Learning for Arbitrary Losses and Models,"Takashi Ishida,         Gang Niu,         Aditya Menon,         Masashi Sugiyama","In contrast to the standard classification paradigm where the true class is given to each training pattern, complementary-label learning only uses training patterns each equipped with a complementary label, which only specifies one of the classes that the pattern does not belong to. The goal of this paper is to derive a novel framework of complementary-label learning with an unbiased estimator of the classification risk, for arbitrary losses and models—all existing methods have failed to achieve this goal. Not only is this beneficial for the learning stage, it also makes model/hyper-parameter selection (through cross-validation) possible without the need of any ordinarily labeled validation data, while using any linear/non-linear models or convex/non-convex loss functions. We further improve the risk estimator by a non-negative correction and gradient ascent trick, and demonstrate its superiority through experiments.",http://proceedings.mlr.press/v97/ishida19a.html,http://proceedings.mlr.press/v97/ishida19a/ishida19a.pdf,ICML
732,2019,Curvature-Exploiting Acceleration of Elastic Net Computations,"Vien Mai,         Mikael Johansson","This paper introduces an efficient second-order method for solving the elastic net problem. Its key innovation is a computationally efficient technique for injecting curvature information in the optimization process which admits a strong theoretical performance guarantee. In particular, we show improved run time over popular first-order methods and quantify the speed-up in terms of statistical measures of the data matrix. The improved time complexity is the result of an extensive exploitation of the problem structure and a careful combination of second-order information, variance reduction techniques, and momentum acceleration. Beside theoretical speed-up, experimental results demonstrate great practical performance benefits of curvature information, especially for ill-conditioned data sets.",http://proceedings.mlr.press/v97/mai19a.html,http://proceedings.mlr.press/v97/mai19a/mai19a.pdf,ICML
733,2019,Certified Adversarial Robustness via Randomized Smoothing,"Jeremy Cohen,         Elan Rosenfeld,         Zico Kolter","We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the L2 norm. While this ""randomized smoothing"" technique has been proposed before in the literature, we are the first to provide a tight analysis, which establishes a close connection between L2 robustness and Gaussian noise. We use the technique to train an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with L2 norm less than 0.5 (=127/255). Smoothing is the only approach to certifiably robust classification which has been shown feasible on full-resolution ImageNet. On smaller-scale datasets where competing approaches to certified L2 robustness are viable, smoothing delivers higher certified accuracies. The empirical success of the approach suggests that provable methods based on randomization at prediction time are a promising direction for future research into adversarially robust classification.",http://proceedings.mlr.press/v97/cohen19c.html,http://proceedings.mlr.press/v97/cohen19c/cohen19c.pdf,ICML
734,2019,Learning Latent Dynamics for Planning from Pixels,"Danijar Hafner,         Timothy Lillicrap,         Ian Fischer,         Ruben Villegas,         David Ha,         Honglak Lee,         James Davidson","Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.",http://proceedings.mlr.press/v97/hafner19a.html,http://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf,ICML
735,2019,LIT: Learned Intermediate Representation Training for Model Compression,"Animesh Koratana,         Daniel Kang,         Peter Bailis,         Matei Zaharia","Researchers have proposed a range of model compression techniques to reduce the computational and memory footprint of deep neural networks (DNNs). In this work, we introduce Learned Intermediate representation Training (LIT), a novel model compression technique that outperforms a range of recent model compression techniques by leveraging the highly repetitive structure of modern DNNs (e.g., ResNet). LIT uses a teacher DNN to train a student DNN of reduced depth by leveraging two key ideas: 1) LIT directly compares intermediate representations of the teacher and student model and 2) LIT uses the intermediate representation from the teacher model’s previous block as input to the current student block during training, improving stability of intermediate representations in the student network. We show that LIT can substantially reduce network size without loss in accuracy on a range of DNN architectures and datasets. For example, LIT can compress ResNet on CIFAR10 by 3.4××\times outperforming network slimming and FitNets. Furthermore, LIT can compress, by depth, ResNeXt 5.5××\times on CIFAR10 (image classification), VDCNN by 1.7××\times on Amazon Reviews (sentiment analysis), and StarGAN by 1.8××\times on CelebA (style transfer, i.e., GANs).",http://proceedings.mlr.press/v97/koratana19a.html,http://proceedings.mlr.press/v97/koratana19a/koratana19a.pdf,ICML
736,2019,Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation,"Sahil Singla,         Eric Wallace,         Shi Feng,         Soheil Feizi","Current saliency map interpretations for neural networks generally rely on two key assumptions. First, they use first-order approximations of the loss function, neglecting higher-order terms such as the loss curvature. Second, they evaluate each feature’s importance in isolation, ignoring feature interdependencies. This work studies the effect of relaxing these two assumptions. First, we characterize a closed-form formula for the input Hessian matrix of a deep ReLU network. Using this formula, we show that, for classification problems with many classes, if a prediction has high probability then including the Hessian term has a small impact on the interpretation. We prove this result by demonstrating that these conditions cause the Hessian matrix to be approximately rank one and its leading eigenvector to be almost parallel to the gradient of the loss. We empirically validate this theory by interpreting ImageNet classifiers. Second, we incorporate feature interdependencies by calculating the importance of group-features using a sparsity regularization term. We use an L0 - L1 relaxation technique along with proximal gradient descent to efficiently compute group-feature importance values. Our empirical results show that our method significantly improves deep learning interpretations.",http://proceedings.mlr.press/v97/singla19a.html,http://proceedings.mlr.press/v97/singla19a/singla19a.pdf,ICML
737,2019,Surrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex Optimization,"Zhenxun Zhuang,         Ashok Cutkosky,         Francesco Orabona","Stochastic Gradient Descent (SGD) has played a central role in machine learning. However, it requires a carefully hand-picked stepsize for fast convergence, which is notoriously tedious and time-consuming to tune. Over the last several years, a plethora of adaptive gradient-based algorithms have emerged to ameliorate this problem. In this paper, we propose new surrogate losses to cast the problem of learning the optimal stepsizes for the stochastic optimization of a non-convex smooth objective function onto an online convex optimization problem. This allows the use of no-regret online algorithms to compute optimal stepsizes on the fly. In turn, this results in a SGD algorithm with self-tuned stepsizes that guarantees convergence rates that are automatically adaptive to the level of noise.",http://proceedings.mlr.press/v97/zhuang19a.html,http://proceedings.mlr.press/v97/zhuang19a/zhuang19a.pdf,ICML
738,2019,Multivariate Submodular Optimization,"Richard Santiago,         F. Bruce Shepherd","Submodular functions have found a wealth of new applications in data science and machine learning models in recent years. This has been coupled with many algorithmic advances in the area of submodular optimization: (SO) min/maxf(S):S∈F\min/\max f(S): S \in \mathcal{F}, where F\mathcal{F} is a given family of feasible sets over a ground set VV and f:2V→Rf:2^V \rightarrow \mathbb{R} is submodular. In this work we focus on a more general class of multivariate submodular optimization (MVSO) problems: min/maxf(S1,S2,…,Sk):S1⊎S2⊎⋯⊎Sk∈F\min/\max f (S_1,S_2,\ldots,S_k): S_1 \uplus S_2 \uplus \cdots \uplus S_k \in \mathcal{F}. Here we use ⊎\uplus to denote union of disjoint sets and hence this model is attractive where resources are being allocated across kk agents, who share a “joint” multivariate nonnegative objective f(S1,S2,…,Sk)f(S_1,S_2,\ldots,S_k) that captures some type of submodularity (i.e. diminishing returns) property. We provide some explicit examples and potential applications for this new framework. For maximization, we show that practical algorithms such as accelerated greedy variants and distributed algorithms achieve good approximation guarantees for very general families (such as matroids and pp-systems). For arbitrary families, we show that monotone (resp. nonmonotone) MVSO admits an α(1−1/e)\alpha (1-1/e) (resp. α⋅0.385\alpha \cdot 0.385) approximation whenever monotone (resp. nonmonotone) SO admits an α\alpha-approximation over the multilinear formulation. This substantially expands the family of tractable models. On the minimization side we give essentially optimal approximations in terms of the curvature of ff.",http://proceedings.mlr.press/v97/santiago19a.html,http://proceedings.mlr.press/v97/santiago19a/santiago19a.pdf,ICML
739,2019,Fast and Stable Maximum Likelihood Estimation for Incomplete Multinomial Models,"Chenyang Zhang,         Guosheng Yin","We propose a fixed-point iteration approach to the maximum likelihood estimation for the incomplete multinomial model, which provides a unified framework for ranking data analysis. Incomplete observations typically fall in a subset of categories, and thus cannot be distinguished as belonging to a unique category. We develop a minorization–maximization (MM) type of algorithm, which requires relatively fewer iterations and shorter time to achieve convergence. Under such a general framework, incomplete multinomial models can be reformulated to include several well-known ranking models as special cases, such as the Bradley–Terry, Plackett–Luce models and their variants. The simple form of iteratively updating equations in our algorithm involves only basic matrix operations, which makes it efficient and easy to implement with large data. Experimental results show that our algorithm runs faster than existing methods on synthetic data and real data.",http://proceedings.mlr.press/v97/zhang19o.html,http://proceedings.mlr.press/v97/zhang19o/zhang19o.pdf,ICML
740,2019,BayesNAS: A Bayesian Approach for Neural Architecture Search,"Hongpeng Zhou,         Minghao Yang,         Jun Wang,         Wei Pan","One-Shot Neural Architecture Search (NAS) is a promising method to significantly reduce search time without any separate training. It can be treated as a Network Compression problem on the architecture parameters from an over-parameterized network. However, there are two issues associated with most one-shot NAS methods. First, dependencies between a node and its predecessors and successors are often disregarded which result in improper treatment over zero operations. Second, architecture parameters pruning based on their magnitude is questionable. In this paper, we employ the classic Bayesian learning approach to alleviate these two issues by modeling architecture parameters using hierarchical automatic relevance determination (HARD) priors. Unlike other NAS methods, we train the over-parameterized network for only one epoch then update the architecture. Impressively, this enabled us to find the architecture in both proxy and proxyless tasks on CIFAR-10 within only 0.2 GPU days using a single GPU. As a byproduct, our approach can be transferred directly to compress convolutional neural networks by enforcing structural sparsity which achieves extremely sparse networks without accuracy deterioration.",http://proceedings.mlr.press/v97/zhou19e.html,http://proceedings.mlr.press/v97/zhou19e/zhou19e.pdf,ICML
741,2019,A Theoretical Analysis of Contrastive Unsupervised Representation Learning,"Nikunj Saunshi,         Orestis Plevrakis,         Sanjeev Arora,         Mikhail Khodak,         Hrishikesh Khandeparkar","Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically “similar"" data points and “negative samples,"" the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.",http://proceedings.mlr.press/v97/saunshi19a.html,http://proceedings.mlr.press/v97/saunshi19a/saunshi19a.pdf,ICML
742,2019,Learning-to-Learn Stochastic Gradient Descent with Biased Regularization,"Giulia Denevi,         Carlo Ciliberto,         Riccardo Grazzi,         Massimiliano Pontil","We study the problem of learning-to-learn: infer- ring a learning algorithm that works well on a family of tasks sampled from an unknown distribution. As class of algorithms we consider Stochastic Gradient Descent (SGD) on the true risk regularized by the square euclidean distance from a bias vector. We present an average excess risk bound for such a learning algorithm that quantifies the potential benefit of using a bias vector with respect to the unbiased case. We then propose a novel meta-algorithm to estimate the bias term online from a sequence of observed tasks. The small memory footprint and low time complexity of our approach makes it appealing in practice while our theoretical analysis provides guarantees on the generalization properties of the meta-algorithm on new tasks. A key feature of our results is that, when the number of tasks grows and their vari- ance is relatively small, our learning-to-learn approach has a significant advantage over learning each task in isolation by standard SGD without a bias term. Numerical experiments demonstrate the effectiveness of our approach in practice.",http://proceedings.mlr.press/v97/denevi19a.html,http://proceedings.mlr.press/v97/denevi19a/denevi19a.pdf,ICML
743,2019,ELF OpenGo: an analysis and open reimplementation of AlphaZero,"Yuandong Tian,         Jerry Ma,         Qucheng Gong,         Shubho Sengupta,         Zhuoyuan Chen,         James Pinkerton,         Larry Zitnick","The AlphaGo, AlphaGo Zero, and AlphaZero series of algorithms are remarkable demonstrations of deep reinforcement learning’s capabilities, achieving superhuman performance in the complex game of Go with progressively increasing autonomy. However, many obstacles remain in the understanding of and usability of these promising approaches by the research community. Toward elucidating unresolved mysteries and facilitating future research, we propose ELF OpenGo, an open-source reimplementation of the AlphaZero algorithm. ELF OpenGo is the first open-source Go AI to convincingly demonstrate superhuman performance with a perfect (20:0) record against global top professionals. We apply ELF OpenGo to conduct extensive ablation studies, and to identify and analyze numerous interesting phenomena in both the model training and in the gameplay inference procedures. Our code, models, selfplay datasets, and auxiliary data are publicly available.",http://proceedings.mlr.press/v97/tian19a.html,http://proceedings.mlr.press/v97/tian19a/tian19a.pdf,ICML
744,2019,Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning,"Dong Yin,         Yudong Chen,         Ramchandran Kannan,         Peter Bartlett","We study robust distributed learning that involves minimizing a non-convex loss function with saddle points. We consider the Byzantine setting where some worker machines have abnormal or even arbitrary and adversarial behavior, and in this setting, the Byzantine machines may create fake local minima near a saddle point that is far away from any true local minimum, even when robust gradient estimators are used. We develop ByzantinePGD, a robust first-order algorithm that can provably escape saddle points and fake local minima, and converge to an approximate true local minimizer with low iteration complexity. As a by-product, we give a simpler algorithm and analysis for escaping saddle points in the usual non-Byzantine setting. We further discuss three robust gradient estimators that can be used in ByzantinePGD, including median, trimmed mean, and iterative filtering. We characterize their performance in concrete statistical settings, and argue for their near-optimality in low and high dimensional regimes.",http://proceedings.mlr.press/v97/yin19a.html,http://proceedings.mlr.press/v97/yin19a/yin19a.pdf,ICML
745,2019,Efficient Full-Matrix Adaptive Regularization,"Naman Agarwal,         Brian Bullins,         Xinyi Chen,         Elad Hazan,         Karan Singh,         Cyril Zhang,         Yi Zhang","Adaptive regularization methods pre-multiply a descent direction by a preconditioning matrix. Due to the large number of parameters of machine learning problems, full-matrix preconditioning methods are prohibitively expensive. We show how to modify full-matrix adaptive regularization in order to make it practical and effective. We also provide a novel theoretical analysis for adaptive regularization in non-convex optimization settings. The core of our algorithm, termed GGT, consists of the efficient computation of the inverse square root of a low-rank matrix. Our preliminary experiments show improved iteration-wise convergence rates across synthetic tasks and standard deep learning benchmarks, and that the more carefully-preconditioned steps sometimes lead to a better solution.",http://proceedings.mlr.press/v97/agarwal19b.html,http://proceedings.mlr.press/v97/agarwal19b/agarwal19b.pdf,ICML
746,2019,Collective Model Fusion for Multiple Black-Box Experts,"Minh Hoang,         Nghia Hoang,         Bryan Kian Hsiang Low,         Carleton Kingsford",Model fusion is a fundamental problem in collec-tive machine learning (ML) where independentexperts with heterogeneous learning architecturesare required to combine expertise to improve pre-dictive performance. This is particularly chal-lenging in information-sensitive domains whereexperts do not have access to each other’s internalarchitecture and local data. This paper presentsthe first collective model fusion framework formultiple experts with heterogeneous black-box ar-chitectures. The proposed method will enable thisby addressing the key issues of how black-boxexperts interact to understand the predictive be-haviors of one another; how these understandingscan be represented and shared efficiently amongthemselves; and how the shared understandingscan be combined to generate high-quality consen-sus prediction. The performance of the resultingframework is analyzed theoretically and demon-strated empirically on several datasets.,http://proceedings.mlr.press/v97/hoang19a.html,http://proceedings.mlr.press/v97/hoang19a/hoang19a.pdf,ICML
747,2019,Calibrated Approximate Bayesian Inference,"Hanwen Xing,         Geoff Nicholls,         Jeong Lee","We give a general purpose computational framework for estimating the bias in coverage resulting from making approximations in Bayesian inference. Coverage is the probability credible sets cover true parameter values. We show how to estimate the actual coverage an approximation scheme achieves when the ideal observation model and the prior can be simulated, but have been replaced, in the Monte Carlo, with approximations as they are intractable. Coverage estimation procedures given in Lee et al. (2018) work well on simple problems, but are biased, and do not scale well, as those authors note. For example, the methods of Lee et al. (2018) fail for calibration of an approximate completely collapsed MCMC algorithm for partition structure in a Dirichlet process for clustering group labels in a hierarchical model. By exploiting the symmetry of the coverage error under permutation of low level group labels and smoothing with Bayesian Additive Regression Trees, we are able to show that the original approximate inference had poor coverage and should not be trusted.",http://proceedings.mlr.press/v97/xing19a.html,http://proceedings.mlr.press/v97/xing19a/xing19a.pdf,ICML
748,2019,Using Pre-Training Can Improve Model Robustness and Uncertainty,"Dan Hendrycks,         Kimin Lee,         Mantas Mazeika","He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on label corruption, class imbalance, adversarial examples, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We show approximately a 10% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.",http://proceedings.mlr.press/v97/hendrycks19a.html,http://proceedings.mlr.press/v97/hendrycks19a/hendrycks19a.pdf,ICML
749,2019,Alternating Minimizations Converge to Second-Order Optimal Solutions,"Qiuwei Li,         Zhihui Zhu,         Gongguo Tang","This work studies the second-order convergence for both standard alternating minimization and proximal alternating minimization. We show that under mild assumptions on the (nonconvex) objective function, both algorithms avoid strict saddles almost surely from random initialization. Together with known first-order convergence results, this implies both algorithms converge to a second-order stationary point. This solves an open problem for the second-order convergence of alternating minimization algorithms that have been widely used in practice to solve large-scale nonconvex problems due to their simple implementation, fast convergence, and superb empirical performance.",http://proceedings.mlr.press/v97/li19n.html,http://proceedings.mlr.press/v97/li19n/li19n.pdf,ICML
750,2019,Stable and Fair Classification,"Lingxiao Huang,         Nisheeth Vishnoi","In a recent study, Friedler et al. pointed out that several fair classification algorithms are not stable with respect to variations in the training set – a crucial consideration in several applications. Motivated by their work, we study the problem of designing classification algorithms that are both fair and stable. We propose an extended framework based on fair classification algorithms that are formulated as optimization problems, by introducing a stability-focused regularization term. Theoretically, we prove an additional stability guarantee, that was lacking in fair classification algorithms, and also provide an accuracy guarantee for our extended framework. Our accuracy guarantee can be used to inform the selection of the regularization parameter in our framework. We assess the benefits of our approach empirically by extending several fair classification algorithms that are shown to achieve the best balance between fairness and accuracy over the \textbf{Adult} dataset. Our empirical results show that our extended framework indeed improves the stability at only a slight sacrifice in accuracy.",http://proceedings.mlr.press/v97/huang19e.html,http://proceedings.mlr.press/v97/huang19e/huang19e.pdf,ICML
751,2019,Memory-Optimal Direct Convolutions for Maximizing Classification Accuracy in Embedded Applications,"Albert Gural,         Boris Murmann","In the age of Internet of Things (IoT), embedded devices ranging from ARM Cortex M0s with hundreds of KB of RAM to Arduinos with 2KB RAM are expected to perform increasingly sophisticated classification tasks, such as voice and gesture recognition, activity tracking, and biometric security. While convolutional neural networks (CNNs), together with spectrogram preprocessing, are a natural solution to many of these classification tasks, storage of the network’s activations often exceeds the hard memory constraints of embedded platforms. This paper presents memory-optimal direct convolutions as a way to push classification accuracy as high as possible given strict hardware memory constraints at the expense of extra compute. We therefore explore the opposite end of the compute-memory trade-off curve from standard approaches that minimize latency. We validate the memory-optimal CNN technique with an Arduino implementation of the 10-class MNIST classification task, fitting the network specification, weights, and activations entirely within 2KB SRAM and achieving a state-of-the-art classification accuracy for small-scale embedded systems of 99.15%.",http://proceedings.mlr.press/v97/gural19a.html,http://proceedings.mlr.press/v97/gural19a/gural19a.pdf,ICML
752,2019,Inference and Sampling of K33K33K_33-free Ising Models,"Valerii Likhosherstov,         Yury Maximov,         Misha Chertkov","We call an Ising model tractable when it is possible to compute its partition function value (statistical inference) in polynomial time. The tractability also implies an ability to sample configurations of this model in polynomial time. The notion of tractability extends the basic case of planar zero-field Ising models. Our starting point is to describe algorithms for the basic case, computing partition function and sampling efficiently. Then, we extend our tractable inference and sampling algorithms to models whose triconnected components are either planar or graphs of O(1)O(1)O(1) size. In particular, it results in a polynomial-time inference and sampling algorithms for K33K33K_{33} (minor)-free topologies of zero-field Ising models—a generalization of planar graphs with a potentially unbounded genus.",http://proceedings.mlr.press/v97/likhosherstov19a.html,http://proceedings.mlr.press/v97/likhosherstov19a/likhosherstov19a.pdf,ICML
753,2019,The Implicit Fairness Criterion of Unconstrained Learning,"Lydia T. Liu,         Max Simchowitz,         Moritz Hardt","We clarify what fairness guarantees we can and cannot expect to follow from unconstrained machine learning. Specifically, we show that in many settings, unconstrained learning on its own implies group calibration, that is, the outcome variable is conditionally independent of group membership given the score. A lower bound confirms the optimality of our upper bound. Moreover, we prove that as the excess risk of the learned score decreases, the more strongly it violates separation and independence, two other standard fairness criteria. Our results challenge the view that group calibration necessitates an active intervention, suggesting that often we ought to think of it as a byproduct of unconstrained machine learning.",http://proceedings.mlr.press/v97/liu19f.html,http://proceedings.mlr.press/v97/liu19f/liu19f.pdf,ICML
754,2019,Asynchronous Batch Bayesian Optimisation with Improved Local Penalisation,"Ahsan Alvi,         Binxin Ru,         Jan-Peter Calliess,         Stephen Roberts,         Michael A. Osborne","Batch Bayesian optimisation (BO) has been successfully applied to hyperparameter tuning using parallel computing, but it is wasteful of resources: workers that complete jobs ahead of others are left idle. We address this problem by developing an approach, Penalising Locally for Asynchronous Bayesian Optimisation on K Workers (PLAyBOOK), for asynchronous parallel BO. We demonstrate empirically the efficacy of PLAyBOOK and its variants on synthetic tasks and a real-world problem. We undertake a comparison between synchronous and asynchronous BO, and show that asynchronous BO often outperforms synchronous batch BO in both wall-clock time and sample efficiency.",http://proceedings.mlr.press/v97/alvi19a.html,http://proceedings.mlr.press/v97/alvi19a/alvi19a.pdf,ICML
755,2019,Towards Understanding Knowledge Distillation,"Mary Phuong,         Christoph Lampert","Knowledge distillation, i.e., one classifier being trained on the outputs of another classifier, is an empirically very successful technique for knowledge transfer between classifiers. It has even been observed that classifiers learn much faster and more reliably if trained with the outputs of another classifier as soft labels, instead of from ground truth data. So far, however, there is no satisfactory theoretical explanation of this phenomenon. In this work, we provide the first insights into the working mechanisms of distillation by studying the special case of linear and deep linear classifiers. Specifically, we prove a generalization bound that establishes fast convergence of the expected risk of a distillation-trained linear classifier. From the bound and its proof we extract three key factors that determine the success of distillation: * data geometry – geometric properties of the data distribution, in particular class separation, has a direct influence on the convergence speed of the risk; * optimization bias – gradient descent optimization finds a very favorable minimum of the distillation objective; and * strong monotonicity – the expected risk of the student classifier always decreases when the size of the training set grows.",http://proceedings.mlr.press/v97/phuong19a.html,http://proceedings.mlr.press/v97/phuong19a/phuong19a.pdf,ICML
756,2019,Optimal Kronecker-Sum Approximation of Real Time Recurrent Learning,"Frederik Benzing,         Marcelo Matheus Gauy,         Asier Mujika,         Anders Martinsson,         Angelika Steger","One of the central goals of Recurrent Neural Networks (RNNs) is to learn long-term dependencies in sequential data. Nevertheless, the most popular training method, Truncated Backpropagation through Time (TBPTT), categorically forbids learning dependencies beyond the truncation horizon. In contrast, the online training algorithm Real Time Recurrent Learning (RTRL) provides untruncated gradients, with the disadvantage of impractically large computational costs. Recently published approaches reduce these costs by providing noisy approximations of RTRL. We present a new approximation algorithm of RTRL, Optimal Kronecker-Sum Approximation (OK). We prove that OK is optimal for a class of approximations of RTRL, which includes all approaches published so far. Additionally, we show that OK has empirically negligible noise: Unlike previous algorithms it matches TBPTT in a real world task (character-level Penn TreeBank) and can exploit online parameter updates to outperform TBPTT in a synthetic string memorization task. Code available at GitHub.",http://proceedings.mlr.press/v97/benzing19a.html,http://proceedings.mlr.press/v97/benzing19a/benzing19a.pdf,ICML
757,2019,Robust Influence Maximization for Hyperparametric Models,"Dimitris Kalimeris,         Gal Kaplun,         Yaron Singer",In this paper we study the problem of robust influence maximization in the independent cascade model under a hyperparametric assumption. In social networks users influence and are influenced by individuals with similar characteristics and as such they are associated with some features. A recent surging research direction in influence maximization focuses on the case where the edge probabilities on the graph are not arbitrary but are generated as a function of the features of the users and a global hyperparameter. We propose a model where the objective is to maximize the worst-case number of influenced users for any possible value of that hyperparameter. We provide theoretical results showing that proper robust solution in our model is NP-hard and an algorithm that achieves improper robust optimization. We make-use of sampling based techniques and of the renowned multiplicative weight updates algorithm. Additionally we validate our method empirically and prove that it outperforms the state-of-the-art robust influence maximization techniques.,http://proceedings.mlr.press/v97/kalimeris19a.html,http://proceedings.mlr.press/v97/kalimeris19a/kalimeris19a.pdf,ICML
758,2019,Rotation Invariant Householder Parameterization for Bayesian PCA,"Rajbir Nirwan,         Nils Bertschinger","We consider probabilistic PCA and related factor models from a Bayesian perspective. These models are in general not identifiable as the likelihood has a rotational symmetry. This gives rise to complicated posterior distributions with continuous subspaces of equal density and thus hinders efficiency of inference as well as interpretation of obtained parameters. In particular, posterior averages over factor loadings become meaningless and only model predictions are unambiguous. Here, we propose a parameterization based on Householder transformations, which remove the rotational symmetry of the posterior. Furthermore, by relying on results from random matrix theory, we establish the parameter distribution which leaves the model unchanged compared to the original rotationally symmetric formulation. In particular, we avoid the need to compute the Jacobian determinant of the parameter transformation. This allows us to efficiently implement probabilistic PCA in a rotation invariant fashion in any state of the art toolbox. Here, we implemented our model in the probabilistic programming language Stan and illustrate it on several examples.",http://proceedings.mlr.press/v97/nirwan19a.html,http://proceedings.mlr.press/v97/nirwan19a/nirwan19a.pdf,ICML
759,2019,Automatic Posterior Transformation for Likelihood-Free Inference,"David Greenberg,         Marcel Nonnenmacher,         Jakob Macke","How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of proposal distributions or require importance weighting that can limit performance in practice. Here we present automatic posterior transformation (APT), a new sequential neural posterior estimation method for simulation-based inference. APT can modify the posterior estimate using arbitrary, dynamically updated proposals, and is compatible with powerful flow-based density estimators. It is more flexible, scalable and efficient than previous simulation-based inference techniques. APT can operate directly on high-dimensional time series and image data, opening up new applications for likelihood-free inference.",http://proceedings.mlr.press/v97/greenberg19a.html,http://proceedings.mlr.press/v97/greenberg19a/greenberg19a.pdf,ICML
760,2019,Neural Collaborative Subspace Clustering,"Tong Zhang,         Pan Ji,         Mehrtash Harandi,         Wenbing Huang,         Hongdong Li","We introduce the Neural Collaborative Subspace Clustering, a neural model that discovers clusters of data points drawn from a union of low-dimensional subspaces. In contrast to previous attempts, our model runs without the aid of spectral clustering. This makes our algorithm one of the kinds that can gracefully scale to large datasets. At its heart, our neural model benefits from a classifier which determines whether a pair of points lies on the same subspace or not. Essential to our model is the construction of two affinity matrices, one from the classifier and the other from a notion of subspace self-expressiveness, to supervise training in a collaborative scheme. We thoroughly assess and contrast the performance of our model against various state-of-the-art clustering algorithms including deep subspace-based ones.",http://proceedings.mlr.press/v97/zhang19g.html,http://proceedings.mlr.press/v97/zhang19g/zhang19g.pdf,ICML
761,2019,Partially Linear Additive Gaussian Graphical Models,"Sinong Geng,         Minhao Yan,         Mladen Kolar,         Sanmi Koyejo","We propose a partially linear additive Gaussian graphical model (PLA-GGM) for the estimation of associations between random variables distorted by observed confounders. Model parameters are estimated using an L1L1L_1-regularized maximal pseudo-profile likelihood estimator (MaPPLE) for which we prove a √nn−−√\sqrt{n}-sparsistency. Importantly, our approach avoids parametric constraints on the effects of confounders on the estimated graphical model structure. Empirically, the PLA-GGM is applied to both synthetic and real-world datasets, demonstrating superior performance compared to competing methods.",http://proceedings.mlr.press/v97/geng19a.html,http://proceedings.mlr.press/v97/geng19a/geng19a.pdf,ICML
762,2019,Efficient Training of BERT by Progressively Stacking,"Linyuan Gong,         Di He,         Zhuohan Li,         Tao Qin,         Liwei Wang,         Tieyan Liu","Unsupervised pre-training is popularly used in natural language processing. By designing proper unsupervised prediction tasks, a deep neural network can be trained and shown to be effective in many downstream tasks. As the data is usually adequate, the model for pre-training is generally huge and contains millions of parameters. Therefore, the training efficiency becomes a critical issue even when using high-performance hardware. In this paper, we explore an efficient training method for the state-of-the-art bidirectional Transformer (BERT) model. By visualizing the self-attention distribution of different layers at different positions in a well-trained BERT model, we find that in most layers, the self-attention distribution will concentrate locally around its position and the start-of-sentence token. Motivating from this, we propose the stacking algorithm to transfer knowledge from a shallow model to a deep model; then we apply stacking progressively to accelerate BERT training. The experimental results showed that the models trained by our training strategy achieve similar performance to models trained from scratch, but our algorithm is much faster.",http://proceedings.mlr.press/v97/gong19a.html,http://proceedings.mlr.press/v97/gong19a/gong19a.pdf,ICML
763,2019,Communication Complexity in Locally Private Distribution Estimation and Heavy Hitters,"Jayadev Acharya,         Ziteng Sun","We consider the problems of distribution estimation, and heavy hitter (frequency) estimation under privacy, and communication constraints. While the constraints have been studied separately, optimal schemes for one are sub-optimal for the other. We propose a sample-optimal \eps\eps\eps-locally differentially private (LDP) scheme for distribution estimation, where each user communicates one bit, and requires no public randomness. We also show that Hadamard Response, a recently proposed scheme for \eps\eps\eps-LDP distribution estimation is also utility-optimal for heavy hitters estimation. Our final result shows that unlike distribution estimation, without public randomness, any utility-optimal heavy hitter estimation algorithm must require Ω(logn)Ω(log⁡n)\Omega(\log n) bits of communication per user.",http://proceedings.mlr.press/v97/acharya19c.html,http://proceedings.mlr.press/v97/acharya19c/acharya19c.pdf,ICML
764,2018,More Robust Doubly Robust Off-policy Evaluation,"Mehrdad Farajtabar,         Yinlam Chow,         Mohammad Ghavamzadeh","We study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In particular, we focus on the doubly robust (DR) estimators that consist of an importance sampling (IS) component and a performance model, and utilize the low (or zero) bias of IS and low variance of the model at the same time. Although the accuracy of the model has a huge impact on the overall performance of DR, most of the work on using the DR estimators in OPE has been focused on improving the IS part, and not much on how to learn the model. In this paper, we propose alternative DR estimators, called more robust doubly robust (MRDR), that learn the model parameter by minimizing the variance of the DR estimator. We first present a formulation for learning the DR model in RL. We then derive formulas for the variance of the DR estimator in both contextual bandits and RL, such that their gradients w.r.t. the model parameters can be estimated from the samples, and propose methods to efficiently minimize the variance. We prove that the MRDR estimators are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR in bandits and RL benchmark problems, and compare its performance with the existing methods.",http://proceedings.mlr.press/v80/farajtabar18a.html,http://proceedings.mlr.press/v80/farajtabar18a/farajtabar18a.pdf,ICML
765,2018,Escaping Saddles with Stochastic Gradients,"Hadi Daneshmand,         Jonas Kohler,         Aurelien Lucchi,         Thomas Hofmann","We analyze the variance of stochastic gradients along negative curvature directions in certain non-convex machine learning models and show that stochastic gradients indeed exhibit a strong component along these directions. Furthermore, we show that - contrary to the case of isotropic noise - this variance is proportional to the magnitude of the corresponding eigenvalues and not decreasing in the dimensionality. Based upon this bservation we propose a new assumption under which we show that the injection of explicit, isotropic noise usually applied to make gradient descent escape saddle points can successfully be replaced by a simple SGD step. Additionally - and under the same condition - we derive the first convergence rate for plain SGD to a second-order stationary point in a number of iterations that is independent of the problem dimension.",http://proceedings.mlr.press/v80/daneshmand18a.html,http://proceedings.mlr.press/v80/daneshmand18a/daneshmand18a.pdf,ICML
766,2018,Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace,"Yoonho Lee,         Seungjin Choi","Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks. While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing. Our primary contribution is the MT-net, which enables the meta-learner to learn on each layer’s activation space a subspace that the task-specific learner performs gradient descent on. Additionally, a task-specific learner of an MT-net performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity. We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner’s adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods. Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks.",http://proceedings.mlr.press/v80/lee18a.html,http://proceedings.mlr.press/v80/lee18a/lee18a.pdf,ICML
767,2018,Orthogonal Machine Learning: Power and Limitations,"Lester Mackey,         Vasilis Syrgkanis,         Ilias Zadik","Double machine learning provides n^{1/2}-consistent estimates of parameters of interest even when high-dimensional or nonparametric nuisance parameters are estimated at an n^{-1/4} rate. The key is to employ Neyman-orthogonal moment equations which are first-order insensitive to perturbations in the nuisance parameters. We show that the n^{-1/4} requirement can be improved to n^{-1/(2k+2)} by employing a k-th order notion of orthogonality that grants robustness to more complex or higher-dimensional nuisance parameters. In the partially linear regression setting popular in causal inference, we show that we can construct second-order orthogonal moments if and only if the treatment residual is not normally distributed. Our proof relies on Stein’s lemma and may be of independent interest. We conclude by demonstrating the robustness benefits of an explicit doubly-orthogonal estimation procedure for treatment effect.",http://proceedings.mlr.press/v80/mackey18a.html,http://proceedings.mlr.press/v80/mackey18a/mackey18a.pdf,ICML
768,2018,A Delay-tolerant Proximal-Gradient Algorithm for Distributed Learning,"Konstantin Mishchenko,         Franck Iutzeler,         Jérôme Malick,         Massih-Reza Amini","Distributed learning aims at computing high-quality models by training over scattered data. This covers a diversity of scenarios, including computer clusters or mobile agents. One of the main challenges is then to deal with heterogeneous machines and unreliable communications. In this setting, we propose and analyze a flexible asynchronous optimization algorithm for solving nonsmooth learning problems. Unlike most existing methods, our algorithm is adjustable to various levels of communication costs, machines computational powers, and data distribution evenness. We prove that the algorithm converges linearly with a fixed learning rate that does not depend on communication delays nor on the number of machines. Although long delays in communication may slow down performance, no delay can break convergence.",http://proceedings.mlr.press/v80/mishchenko18a.html,http://proceedings.mlr.press/v80/mishchenko18a/mishchenko18a.pdf,ICML
769,2018,Massively Parallel Algorithms and Hardness for Single-Linkage Clustering under ℓp\ell_p Distances,"Grigory Yaroslavtsev,         Adithya Vadapalli","We present first massively parallel (MPC) algorithms and hardness of approximation results for computing Single-Linkage Clustering of n input d-dimensional vectors under Hamming, ℓ1,ℓ2\ell_1, \ell_2 and ℓ∞\ell_\infty distances. All our algorithms run in O(log n) rounds of MPC for any fixed d and achieve (1+\epsilon)-approximation for all distances (except Hamming for which we show an exact algorithm). We also show constant-factor inapproximability results for o(\log n)-round algorithms under standard MPC hardness assumptions (for sufficiently large dimension depending on the distance used). Efficiency of implementation of our algorithms in Apache Spark is demonstrated through experiments on the largest available vector datasets from the UCI machine learning repository exhibiting speedups of several orders of magnitude.",http://proceedings.mlr.press/v80/yaroslavtsev18a.html,http://proceedings.mlr.press/v80/yaroslavtsev18a/yaroslavtsev18a.pdf,ICML
770,2018,Hierarchical Clustering with Structural Constraints,"Vaggos Chatziafratis,         Rad Niazadeh,         Moses Charikar","Hierarchical clustering is a popular unsupervised data analysis method. For many real-world applications, we would like to exploit prior information about the data that imposes constraints on the clustering hierarchy, and is not captured by the set of features available to the algorithm. This gives rise to the problem of hierarchical clustering with structural constraints. Structural constraints pose major challenges for bottom-up approaches like average/single linkage and even though they can be naturally incorporated into top-down divisive algorithms, no formal guarantees exist on the quality of their output. In this paper, we provide provable approximation guarantees for two simple top-down algorithms, using a recently introduced optimization viewpoint of hierarchical clustering with pairwise similarity information (Dasgupta, 2016). We show how to find good solutions even in the presence of conflicting prior information, by formulating a constraint-based regularization of the objective. Furthemore, we explore a variation of this objective for dissimilarity information (Cohen-Addad et al., 2018) and improve upon current techniques. Finally, we demonstrate our approach on a real dataset for the taxonomy application.",http://proceedings.mlr.press/v80/chatziafratis18a.html,http://proceedings.mlr.press/v80/chatziafratis18a/chatziafratis18a.pdf,ICML
771,2018,oi-VAE: Output Interpretable VAEs for Nonlinear Group Factor Analysis,"Samuel K. Ainsworth,         Nicholas J. Foti,         Adrian K. C. Lee,         Emily B. Fox","Deep generative models have recently yielded encouraging results in producing subjectively realistic samples of complex data. Far less attention has been paid to making these generative models interpretable. In many scenarios, ranging from scientific applications to finance, the observed variables have a natural grouping. It is often of interest to understand systems of interaction amongst these groups, and latent factor models (LFMs) are an attractive approach. However, traditional LFMs are limited by assuming a linear correlation structure. We present an output interpretable VAE (oi-VAE) for grouped data that models complex, nonlinear latent-to-observed relationships. We combine a structured VAE comprised of group-specific generators with a sparsity-inducing prior. We demonstrate that oi-VAE yields meaningful notions of interpretability in the analysis of motion capture and MEG data. We further show that in these situations, the regularization inherent to oi-VAE can actually lead to improved generalization and learned generative processes.",http://proceedings.mlr.press/v80/ainsworth18a.html,http://proceedings.mlr.press/v80/ainsworth18a/ainsworth18a.pdf,ICML
772,2018,Overcoming Catastrophic Forgetting with Hard Attention to the Task,"Joan Serra,         Didac Suris,         Marius Miron,         Alexandros Karatzoglou","Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks’ information without affecting the current task’s learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.",http://proceedings.mlr.press/v80/serra18a.html,http://proceedings.mlr.press/v80/serra18a/serra18a.pdf,ICML
773,2018,Discrete-Continuous Mixtures in Probabilistic Programming: Generalized Semantics and Inference Algorithms,"Yi Wu,         Siddharth Srivastava,         Nicholas Hay,         Simon Du,         Stuart Russell","Despite the recent successes of probabilistic programming languages (PPLs) in AI applications, PPLs offer only limited support for random variables whose distributions combine discrete and continuous elements. We develop the notion of measure-theoretic Bayesian networks (MTBNs) and use it to provide more general semantics for PPLs with arbitrarily many random variables defined over arbitrary measure spaces. We develop two new general sampling algorithms that are provably correct under the MTBN framework: the lexicographic likelihood weighting (LLW) for general MTBNs and the lexicographic particle filter (LPF), a specialized algorithm for state-space models. We further integrate MTBNs into a widely used PPL system, BLOG, and verify the effectiveness of the new inference algorithms through representative examples.",http://proceedings.mlr.press/v80/wu18f.html,http://proceedings.mlr.press/v80/wu18f/wu18f.pdf,ICML
774,2018,Open Category Detection with PAC Guarantees,"Si Liu,         Risheek Garrepalli,         Thomas Dietterich,         Alan Fern,         Dan Hendrycks","Open category detection is the problem of detecting ""alien"" test instances that belong to categories or classes that were not present in the training data. In many applications, reliably detecting such aliens is central to ensuring the safety and accuracy of test set predictions. Unfortunately, there are no algorithms that provide theoretical guarantees on their ability to detect aliens under general assumptions. Further, while there are algorithms for open category detection, there are few empirical results that directly report alien detection rates. Thus, there are significant theoretical and empirical gaps in our understanding of open category detection. In this paper, we take a step toward addressing this gap by studying a simple, but practically-relevant variant of open category detection. In our setting, we are provided with a ""clean"" training set that contains only the target categories of interest and an unlabeled ""contaminated” training set that contains a fraction alpha of alien examples. Under the assumption that we know an upper bound on alpha we develop an algorithm with PAC-style guarantees on the alien detection rate, while aiming to minimize false alarms. Empirical results on synthetic and standard benchmark datasets demonstrate the regimes in which the algorithm can be effective and provide a baseline for further advancements.",http://proceedings.mlr.press/v80/liu18e.html,http://proceedings.mlr.press/v80/liu18e/liu18e.pdf,ICML
775,2018,Parallel Bayesian Network Structure Learning,"Tian Gao,         Dennis Wei","Recent advances in Bayesian Network (BN) structure learning have focused on local-to-global learning, where the graph structure is learned via one local subgraph at a time. As a natural progression, we investigate parallel learning of BN structures via multiple learning agents simultaneously, where each agent learns one local subgraph at a time. We find that parallel learning can reduce the number of subgraphs requiring structure learning by storing previously queried results and communicating (even partial) results among agents. More specifically, by using novel rules on query subset and superset inference, many subgraph structures can be inferred without learning. We provide a sound and complete parallel structure learning (PSL) algorithm, and demonstrate its improved efficiency over state-of-the-art single-thread learning algorithms.",http://proceedings.mlr.press/v80/gao18b.html,http://proceedings.mlr.press/v80/gao18b/gao18b.pdf,ICML
776,2018,Born Again Neural Networks,"Tommaso Furlanello,         Zachary Lipton,         Michael Tschannen,         Laurent Itti,         Anima Anandkumar","Knowledge Distillation (KD) consists of transferring “knowledge” from one machine learning model (the teacher) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student’s compactness, without sacrificing too much performance. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating the effect of the teacher outputs on both predicted and non-predicted classes.",http://proceedings.mlr.press/v80/furlanello18a.html,http://proceedings.mlr.press/v80/furlanello18a/furlanello18a.pdf,ICML
777,2018,On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups,"Risi Kondor,         Shubhendu Trivedi","Convolutional neural networks have been extremely successful in the image recognition domain because they ensure equivariance with respect to translations. There have been many recent attempts to generalize this framework to other domains, including graphs and data lying on manifolds. In this paper we give a rigorous, theoretical treatment of convolution and equivariance in neural networks with respect to not just translations, but the action of any compact group. Our main result is to prove that (given some natural constraints) convolutional structure is not just a sufficient, but also a necessary condition for equivariance to the action of a compact group. Our exposition makes use of concepts from representation theory and noncommutative harmonic analysis and derives new generalized convolution formulae.",http://proceedings.mlr.press/v80/kondor18a.html,http://proceedings.mlr.press/v80/kondor18a/kondor18a.pdf,ICML
778,2018,"An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning","Dhruv Malik,         Malayandi Palaniappan,         Jaime Fisac,         Dylan Hadfield-Menell,         Stuart Russell,         Anca Dragan","Our goal is for AI systems to correctly identify and act according to their human user’s objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL: the human is a full information agent. This enables us to derive an optimality-preserving modification to the standard Bellman update, which reduces the complexity of the problem by an exponential factor. Additionally, we show that our modified Bellman update allows us to relax CIRL’s assumption of human rationality. We apply this update to a variety of POMDP solvers, including exact methods, point-based methods, and Monte Carlo Tree Search methods. We find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogical (teaching) behavior, while the robot interprets it as such and attains higher value for the human.",http://proceedings.mlr.press/v80/malik18a.html,http://proceedings.mlr.press/v80/malik18a/malik18a.pdf,ICML
779,2018,Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator,"Maryam Fazel,         Rong Ge,         Sham Kakade,         Mehran Mesbahi","Direct policy gradient methods for reinforcement learning and continuous control problems are a popular approach for a variety of reasons: 1) they are easy to implement without explicit knowledge of the underlying model, 2) they are an “end-to-end” approach, directly optimizing the performance metric of interest, 3) they inherently allow for richly parameterized policies. A notable drawback is that even in the most basic continuous control problem (that of linear quadratic regulators), these methods must solve a non-convex optimization problem, where little is understood about their efficiency from both computational and statistical perspectives. In contrast, system identification and model based planning in optimal control theory have a much more solid theoretical footing, where much is known with regards to their computational and statistical properties. This work bridges this gap showing that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities.",http://proceedings.mlr.press/v80/fazel18a.html,http://proceedings.mlr.press/v80/fazel18a/fazel18a.pdf,ICML
780,2018,Variational Bayesian dropout: pitfalls and fixes,"Jiri Hron,         Alex Matthews,         Zoubin Ghahramani","Dropout, a stochastic regularisation technique for training of neural networks, has recently been reinterpreted as a specific type of approximate inference algorithm for Bayesian neural networks. The main contribution of the reinterpretation is in providing a theoretical framework useful for analysing and extending the algorithm. We show that the proposed framework suffers from several issues; from undefined or pathological behaviour of the true posterior related to use of improper priors, to an ill-defined variational objective due to singularity of the approximating distribution relative to the true posterior. Our analysis of the improper log uniform prior used in variational Gaussian dropout suggests the pathologies are generally irredeemable, and that the algorithm still works only because the variational formulation annuls some of the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL) divergence, a new approximate inference objective for approximation of high-dimensional distributions. We show that motivations for variational Bernoulli dropout based on discretisation and noise have QKL as a limit. Properties of QKL are studied both theoretically and on a simple practical example which shows that the QKL-optimal approximation of a full rank Gaussian with a degenerate one naturally leads to the Principal Component Analysis solution.",http://proceedings.mlr.press/v80/hron18a.html,http://proceedings.mlr.press/v80/hron18a/hron18a.pdf,ICML
781,2018,Video Prediction with Appearance and Motion Conditions,"Yunseok Jang,         Gunhee Kim,         Yale Song","Video prediction aims to generate realistic future frames by learning dynamic visual patterns. One fundamental challenge is to deal with future uncertainty: How should a model behave when there are multiple correct, equally probable future? We propose an Appearance-Motion Conditional GAN to address this challenge. We provide appearance and motion information as conditions that specify how the future may look like, reducing the level of uncertainty. Our model consists of a generator, two discriminators taking charge of appearance and motion pathways, and a perceptual ranking module that encourages videos of similar conditions to look similar. To train our model, we develop a novel conditioning scheme that consists of different combinations of appearance and motion conditions. We evaluate our model using facial expression and human action datasets and report favorable results compared to existing methods.",http://proceedings.mlr.press/v80/jang18a.html,http://proceedings.mlr.press/v80/jang18a/jang18a.pdf,ICML
782,2018,Not All Samples Are Created Equal: Deep Learning with Importance Sampling,"Angelos Katharopoulos,         Francois Fleuret","Deep Neural Network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on ""informative"" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%.",http://proceedings.mlr.press/v80/katharopoulos18a.html,http://proceedings.mlr.press/v80/katharopoulos18a/katharopoulos18a.pdf,ICML
783,2018,Adversarial Attack on Graph Structured Data,"Hanjun Dai,         Hui Li,         Tian Tian,         Xin Huang,         Lin Wang,         Jun Zhu,         Le Song","Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool deep learning models by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. We further propose attack methods based on genetic algorithms and gradient descent in the scenario where additional prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.",http://proceedings.mlr.press/v80/dai18b.html,http://proceedings.mlr.press/v80/dai18b/dai18b.pdf,ICML
784,2018,Asynchronous Stochastic Quasi-Newton MCMC for Non-Convex Optimization,"Umut Simsekli,         Cagatay Yildiz,         Than Huy Nguyen,         Taylan Cemgil,         Gael Richard","Recent studies have illustrated that stochastic gradient Markov Chain Monte Carlo techniques have a strong potential in non-convex optimization, where local and global convergence guarantees can be shown under certain conditions. By building up on this recent theory, in this study, we develop an asynchronous-parallel stochastic L-BFGS algorithm for non-convex optimization. The proposed algorithm is suitable for both distributed and shared-memory settings. We provide formal theoretical analysis and show that the proposed method achieves an ergodic convergence rate of O(1/√N){\cal O}(1/\sqrt{N}) (NN being the total number of iterations) and it can achieve a linear speedup under certain conditions. We perform several experiments on both synthetic and real datasets. The results support our theory and show that the proposed algorithm provides a significant speedup over the recently proposed synchronous distributed L-BFGS algorithm.",http://proceedings.mlr.press/v80/simsekli18a.html,http://proceedings.mlr.press/v80/simsekli18a/simsekli18a.pdf,ICML
785,2018,Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices,Zengfeng Huang,"Given a large matrix A∈\realn×dA∈\realn×dA\in\real^{n\times d}, we consider the problem of computing a sketch matrix B∈\realℓ×dB∈\realℓ×dB\in\real^{\ell\times d} which is significantly smaller than but still well approximates AAA. We are interested in minimizing the covariance error \normATA−BTB2.\normATA−BTB2.\norm{A^TA-B^TB}_2.We consider the problems in the streaming model, where the algorithm can only make one pass over the input with limited working space. The popular Frequent Directions algorithm of Liberty (2013) and its variants achieve optimal space-error tradeoff. However, whether the running time can be improved remains an unanswered question.In this paper, we almost settle the time complexity of this problem. In particular, we provide new space-optimal algorithms with faster running times. Moreover, we also show that the running times of our algorithms are near-optimal unless the state-of-the-art running time of matrix multiplication can be improved significantly.",http://proceedings.mlr.press/v80/huang18a.html,http://proceedings.mlr.press/v80/huang18a/huang18a.pdf,ICML
786,2018,Optimizing the Latent Space of Generative Networks,"Piotr Bojanowski,         Armand Joulin,         David Lopez-Pas,         Arthur Szlam","Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most successful applications, GAN models share two common aspects: solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions; and parameterizing the generator and the discriminator as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators using simple reconstruction losses. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors; all of this without the adversarial optimization scheme.",http://proceedings.mlr.press/v80/bojanowski18a.html,http://proceedings.mlr.press/v80/bojanowski18a/bojanowski18a.pdf,ICML
787,2018,An Inference-Based Policy Gradient Method for Learning Options,"Matthew Smith,         Herke Hoof,         Joelle Pineau","In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions. However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem. In this work we develop a novel policy gradient method for the automatic learning of policies with options. This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. The differentiable inference procedure employed yields options that can be easily interpreted. Empirical results confirm these attributes, and indicate that our algorithm has an improved sample efficiency relative to state-of-the-art in learning options end-to-end.",http://proceedings.mlr.press/v80/smith18a.html,http://proceedings.mlr.press/v80/smith18a/smith18a.pdf,ICML
788,2018,A Distributed Second-Order Algorithm You Can Trust,"Celestine Duenner,         Aurelien Lucchi,         Matilde Gargiani,         An Bian,         Thomas Hofmann,         Martin Jaggi","Due to the rapid growth of data and computational resources, distributed optimization has become an active research area in recent years. While first-order methods seem to dominate the field, second-order methods are nevertheless attractive as they potentially require fewer communication rounds to converge. However, there are significant drawbacks that impede their wide adoption, such as the computation and the communication of a large Hessian matrix. In this paper we present a new algorithm for distributed training of generalized linear models that only requires the computation of diagonal blocks of the Hessian matrix on the individual workers. To deal with this approximate information we propose an adaptive approach that - akin to trust-region methods - dynamically adapts the auxiliary model to compensate for modeling errors. We provide theoretical rates of convergence for a wide class of problems including L1L1L_1-regularized objectives. We also demonstrate that our approach achieves state-of-the-art results on multiple large benchmark datasets.",http://proceedings.mlr.press/v80/duenner18a.html,http://proceedings.mlr.press/v80/duenner18a/duenner18a.pdf,ICML
789,2018,MAGAN: Aligning Biological Manifolds,"Matthew Amodio,         Smita Krishnaswamy","It is increasingly common in many types of natural and physical systems (especially biological systems) to have different types of measurements performed on the same underlying system. In such settings, it is important to align the manifolds arising from each measurement in order to integrate such data and gain an improved picture of the system; we tackle this problem using generative adversarial networks (GANs). Recent attempts to use GANs to find correspondences between sets of samples do not explicitly perform proper alignment of manifolds. We present the new Manifold Aligning GAN (MAGAN) that aligns two manifolds such that related points in each measurement space are aligned. We demonstrate applications of MAGAN in single-cell biology in integrating two different measurement types together: cells from the same tissue are measured with both genomic (single-cell RNA-sequencing) and proteomic (mass cytometry) technologies. We show that MAGAN successfully aligns manifolds such that known correlations between measured markers are improved compared to other recently proposed models.",http://proceedings.mlr.press/v80/amodio18a.html,http://proceedings.mlr.press/v80/amodio18a/amodio18a.pdf,ICML
790,2018,Exploring Hidden Dimensions in Accelerating Convolutional Neural Networks,"Zhihao Jia,         Sina Lin,         Charles R. Qi,         Alex Aiken","The past few years have witnessed growth in the computational requirements for training deep convolutional neural networks. Current approaches parallelize training onto multiple devices by applying a single parallelization strategy (e.g., data or model parallelism) to all layers in a network. Although easy to reason about, these approaches result in suboptimal runtime performance in large-scale distributed training, since different layers in a network may prefer different parallelization strategies. In this paper, we propose layer-wise parallelism that allows each layer in a network to use an individual parallelization strategy. We jointly optimize how each layer is parallelized by solving a graph search problem. Our evaluation shows that layer-wise parallelism outperforms state-of-the-art approaches by increasing training throughput, reducing communication costs, achieving better scalability to multiple GPUs, while maintaining original network accuracy.",http://proceedings.mlr.press/v80/jia18a.html,http://proceedings.mlr.press/v80/jia18a/jia18a.pdf,ICML
791,2018,Semi-Supervised Learning via Compact Latent Space Clustering,"Konstantinos Kamnitsas,         Daniel Castro,         Loic Le Folgoc,         Ian Walker,         Ryutaro Tanno,         Daniel Rueckert,         Ben Glocker,         Antonio Criminisi,         Aditya Nori","We present a novel cost function for semi-supervised learning of neural networks that encourages compact clustering of the latent space to facilitate separation. The key idea is to dynamically create a graph over embeddings of labeled and unlabeled samples of a training batch to capture underlying structure in feature space, and use label propagation to estimate its high and low density regions. We then devise a cost function based on Markov chains on the graph that regularizes the latent space to form a single compact cluster per class, while avoiding to disturb existing clusters during optimization. We evaluate our approach on three benchmarks and compare to state-of-the art with promising results. Our approach combines the benefits of graph-based regularization with efficient, inductive inference, does not require modifications to a network architecture, and can thus be easily applied to existing networks to enable an effective use of unlabeled data.",http://proceedings.mlr.press/v80/kamnitsas18a.html,http://proceedings.mlr.press/v80/kamnitsas18a/kamnitsas18a.pdf,ICML
792,2018,State Space Gaussian Processes with Non-Gaussian Likelihood,"Hannes Nickisch,         Arno Solin,         Alexander Grigorevskiy","We provide a comprehensive overview and tooling for GP modelling with non-Gaussian likelihoods using state space methods. The state space formulation allows for solving one-dimensonal GP models in O(n) time and memory complexity. While existing literature has focused on the connection between GP regression and state space methods, the computational primitives allowing for inference using general likelihoods in combination with the Laplace approximation (LA), variational Bayes (VB), and assumed density filtering (ADF) / expectation propagation (EP) schemes has been largely overlooked. We present means of combining the efficient O(n) state space methodology with existing inference methods. We also furher extend existing methods, and provide unifying code implementing all approaches.",http://proceedings.mlr.press/v80/nickisch18a.html,http://proceedings.mlr.press/v80/nickisch18a/nickisch18a.pdf,ICML
793,2018,DiCE: The Infinitely Differentiable Monte Carlo Estimator,"Jakob Foerster,         Gregory Farquhar,         Maruan Al-Shedivat,         Tim Rocktäschel,         Eric Xing,         Shimon Whiteson","The score function estimator is widely used for estimating gradients of stochastic objectives in stochastic computation graphs (SCG), eg., in reinforcement learning and meta-learning. While deriving the first-order gradient estimators by differentiating a surrogate loss (SL) objective is computationally and conceptually simple, using the same approach for higher-order derivatives is more challenging. Firstly, analytically deriving and implementing such estimators is laborious and not compliant with automatic differentiation. Secondly, repeatedly applying SL to construct new objectives for each order derivative involves increasingly cumbersome graph manipulations. Lastly, to match the first-order gradient under differentiation, SL treats part of the cost as a fixed sample, which we show leads to missing and wrong terms for estimators of higher-order derivatives. To address all these shortcomings in a unified way, we introduce DiCE, which provides a single objective that can be differentiated repeatedly, generating correct estimators of derivatives of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing the requisite graph manipulations. We verify the correctness of DiCE both through a proof and numerical evaluation of the DiCE derivative estimates. We also use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available at https://github.com/alshedivat/lola.",http://proceedings.mlr.press/v80/foerster18a.html,http://proceedings.mlr.press/v80/foerster18a/foerster18a.pdf,ICML
794,2018,Learning Hidden Markov Models from Pairwise Co-occurrences with Application to Topic Modeling,"Kejun Huang,         Xiao Fu,         Nicholas Sidiropoulos","We present a new algorithm for identifying the transition and emission probabilities of a hidden Markov model (HMM) from the emitted data. Expectation-maximization becomes computationally prohibitive for long observation records, which are often required for identification. The new algorithm is particularly suitable for cases where the available sample size is large enough to accurately estimate second-order output probabilities, but not higher-order ones. We show that if one is only able to obtain a reliable estimate of the pairwise co-occurrence probabilities of the emissions, it is still possible to uniquely identify the HMM if the emission probability is sufficiently scattered. We apply our method to hidden topic Markov modeling, and demonstrate that we can learn topics with higher quality if documents are modeled as observations of HMMs sharing the same emission (topic) probability, compared to the simple but widely used bag-of-words model.",http://proceedings.mlr.press/v80/huang18c.html,http://proceedings.mlr.press/v80/huang18c/huang18c.pdf,ICML
795,2018,DICOD: Distributed Convolutional Coordinate Descent for Convolutional Sparse Coding,"Thomas Moreau,         Laurent Oudre,         Nicolas Vayatis","In this paper, we introduce DICOD, a convolutional sparse coding algorithm which builds shift invariant representations for long signals. This algorithm is designed to run in a distributed setting, with local message passing, making it communication efficient. It is based on coordinate descent and uses locally greedy updates which accelerate the resolution compared to greedy coordinate selection. We prove the convergence of this algorithm and highlight its computational speed-up which is super-linear in the number of cores used. We also provide empirical evidence for the acceleration properties of our algorithm compared to state-of-the-art methods.",http://proceedings.mlr.press/v80/moreau18a.html,http://proceedings.mlr.press/v80/moreau18a/moreau18a.pdf,ICML
796,2018,Efficient Gradient-Free Variational Inference using Policy Search,"Oleg Arenz,         Gerhard Neumann,         Mingjun Zhong","Inference from complex distributions is a common problem in machine learning needed for many Bayesian methods. We propose an efficient, gradient-free method for learning general GMM approximations of multimodal distributions based on recent insights from stochastic search methods. Our method establishes information-geometric trust regions to ensure efficient exploration of the sampling space and stability of the GMM updates, allowing for efficient estimation of multi-variate Gaussian variational distributions. For GMMs, we apply a variational lower bound to decompose the learning objective into sub-problems given by learning the individual mixture components and the coefficients. The number of mixture components is adapted online in order to allow for arbitrary exact approximations. We demonstrate on several domains that we can learn significantly better approximations than competing variational inference methods and that the quality of samples drawn from our approximations is on par with samples created by state-of-the-art MCMC samplers that require significantly more computational resources.",http://proceedings.mlr.press/v80/arenz18a.html,http://proceedings.mlr.press/v80/arenz18a/arenz18a.pdf,ICML
797,2018,Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam,"Mohammad Khan,         Didrik Nielsen,         Voot Tangkaratt,         Wu Lin,         Yarin Gal,         Akash Srivastava","Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum-likelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.",http://proceedings.mlr.press/v80/khan18a.html,http://proceedings.mlr.press/v80/khan18a/khan18a.pdf,ICML
798,2018,Binary Partitions with Approximate Minimum Impurity,"Eduardo Laber,         Marco Molinaro,         Felipe Mello Pereira","The problem of splitting attributes is one of the main steps in the construction of decision trees. In order to decide the best split, impurity measures such as Entropy and Gini are widely used. In practice, decision-tree inducers use heuristics for finding splits with small impurity when they consider nominal attributes with a large number of distinct values. However, there are no known guarantees for the quality of the splits obtained by these heuristics. To fill this gap, we propose two new splitting procedures that provably achieve near-optimal impurity. We also report experiments that provide evidence that the proposed methods are interesting candidates to be employed in splitting nominal attributes with many values during decision tree/random forest induction.",http://proceedings.mlr.press/v80/laber18a.html,http://proceedings.mlr.press/v80/laber18a/laber18a.pdf,ICML
799,2018,The Edge Density Barrier: Computational-Statistical Tradeoffs in Combinatorial Inference,"Hao Lu,         Yuan Cao,         Zhuoran Yang,         Junwei Lu,         Han Liu,         Zhaoran Wang","We study the hypothesis testing problem of inferring the existence of combinatorial structures in undirected graphical models. Although there exist extensive studies on the information-theoretic limits of this problem, it remains largely unexplored whether such limits can be attained by efficient algorithms. In this paper, we quantify the minimum computational complexity required to attain the information-theoretic limits based on an oracle computational model. We prove that, for testing common combinatorial structures, such as clique, nearest neighbor graph and perfect matching, against an empty graph, or large clique against small clique, the information-theoretic limits are provably unachievable by tractable algorithms in general. More importantly, we define structural quantities called the weak and strong edge densities, which offer deep insight into the existence of such computational-statistical tradeoffs. To the best of our knowledge, our characterization is the first to identify and explain the fundamental tradeoffs between statistics and computation for combinatorial inference problems in undirected graphical models.",http://proceedings.mlr.press/v80/lu18a.html,http://proceedings.mlr.press/v80/lu18a/lu18a.pdf,ICML
800,2018,A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations,"Weili Nie,         Yang Zhang,         Ankit Patel","Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less class-sensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.",http://proceedings.mlr.press/v80/nie18a.html,http://proceedings.mlr.press/v80/nie18a/nie18a.pdf,ICML
801,2018,Importance Weighted Transfer of Samples in Reinforcement Learning,"Andrea Tirinzoni,         Andrea Sessa,         Matteo Pirotta,         Marcello Restelli","We consider the transfer of experience samples (i.e., tuples < s, a, s’, r >) in reinforcement learning (RL), collected from a set of source tasks to improve the learning process in a given target task. Most of the related approaches focus on selecting the most relevant source samples for solving the target task, but then all the transferred samples are used without considering anymore the discrepancies between the task models. In this paper, we propose a model-based technique that automatically estimates the relevance (importance weight) of each source sample for solving the target task. In the proposed approach, all the samples are transferred and used by a batch RL algorithm to solve the target task, but their contribution to the learning process is proportional to their importance weight. By extending the results for importance weighting provided in supervised learning literature, we develop a finite-sample analysis of the proposed batch RL algorithm. Furthermore, we empirically compare the proposed algorithm to state-of-the-art approaches, showing that it achieves better learning performance and is very robust to negative transfer, even when some source tasks are significantly different from the target task.",http://proceedings.mlr.press/v80/tirinzoni18a.html,http://proceedings.mlr.press/v80/tirinzoni18a/tirinzoni18a.pdf,ICML
802,2018,Spotlight: Optimizing Device Placement for Training Deep Neural Networks,"Yuanxiang Gao,         Li Chen,         Baochun Li","Training deep neural networks (DNNs) requires an increasing amount of computation resources, and it becomes typical to use a mixture of GPU and CPU devices. Due to the heterogeneity of these devices, a recent challenge is how each operation in a neural network can be optimally placed on these devices, so that the training process can take the shortest amount of time possible. The current state-of-the-art solution uses reinforcement learning based on the policy gradient method, and it suffers from suboptimal training times. In this paper, we propose Spotlight, a new reinforcement learning algorithm based on proximal policy optimization, designed specifically for finding an optimal device placement for training DNNs. The design of our new algorithm relies upon a new model of the device placement problem: by modeling it as a Markov decision process with multiple stages, we are able to prove that Spotlight achieves a theoretical guarantee on performance improvements. We have implemented Spotlight in the CIFAR-10 benchmark and deployed it on the Google Cloud platform. Extensive experiments have demonstrated that the training time with placements recommended by Spotlight is 60.9% of that recommended by the policy gradient method.",http://proceedings.mlr.press/v80/gao18a.html,http://proceedings.mlr.press/v80/gao18a/gao18a.pdf,ICML
803,2018,Continual Reinforcement Learning with Complex Synapses,"Christos Kaplanis,         Murray Shanahan,         Claudia Clopath","Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna & Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.",http://proceedings.mlr.press/v80/kaplanis18a.html,http://proceedings.mlr.press/v80/kaplanis18a/kaplanis18a.pdf,ICML
804,2018,Decoupling Gradient-Like Learning Rules from Representations,"Philip Thomas,         Christoph Dann,         Emma Brunskill","In machine learning, learning often corresponds to changing the parameters of a parameterized function. A learning rule is an algorithm or mathematical expression that specifies precisely how the parameters should be changed. When creating a machine learning system, we must make two decisions: what representation should be used (i.e., what parameterized function should be used) and what learning rule should be used to search through the resulting set of representable functions. In this paper we focus on gradient-like learning rules, wherein these two decisions are coupled in a subtle (and often unintentional) way. Using most learning rules, these two decisions are coupled in a subtle (and often unintentional) way. That is, using the same learning rule with two different representations that can represent the same sets of functions can result in two different outcomes. After arguing that this coupling is undesirable, particularly when using neural networks, we present a method for partially decoupling these two decisions for a broad class of gradient-like learning rules that span unsupervised learning, reinforcement learning, and supervised learning.",http://proceedings.mlr.press/v80/thomas18a.html,http://proceedings.mlr.press/v80/thomas18a/thomas18a.pdf,ICML
805,2018,Learning to Reweight Examples for Robust Deep Learning,"Mengye Ren,         Wenyuan Zeng,         Bin Yang,         Raquel Urtasun","Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.",http://proceedings.mlr.press/v80/ren18a.html,http://proceedings.mlr.press/v80/ren18a/ren18a.pdf,ICML
806,2018,Minimax Concave Penalized Multi-Armed Bandit Model with High-Dimensional Covariates,"Xue Wang,         Mingcheng Wei,         Tao Yao","In this paper, we propose a Minimax Concave Penalized Multi-Armed Bandit (MCP-Bandit) algorithm for a decision-maker facing high-dimensional data with latent sparse structure in an online learning and decision-making process. We demonstrate that the MCP-Bandit algorithm asymptotically achieves the optimal cumulative regret in sample size T, O(log T), and further attains a tighter bound in both covariates dimension d and the number of significant covariates s, O(s^2 (s + log d). In addition, we develop a linear approximation method, the 2-step Weighted Lasso procedure, to identify the MCP estimator for the MCP-Bandit algorithm under non-i.i.d. samples. Using this procedure, the MCP estimator matches the oracle estimator with high probability. Finally, we present two experiments to benchmark our proposed the MCP-Bandit algorithm to other bandit algorithms. Both experiments demonstrate that the MCP-Bandit algorithm performs favorably over other benchmark algorithms, especially when there is a high level of data sparsity or when the sample size is not too small.",http://proceedings.mlr.press/v80/wang18j.html,http://proceedings.mlr.press/v80/wang18j/wang18j.pdf,ICML
807,2018,Asynchronous Decentralized Parallel Stochastic Gradient Descent,"Xiangru Lian,         Wei Zhang,         Ce Zhang,         Ji Liu","Most commonly used distributed machine learning systems are either synchronous or centralized asynchronous. Synchronous algorithms like AllReduce-SGD perform poorly in a heterogeneous environment, while asynchronous algorithms using a parameter server suffer from 1) communication bottleneck at parameter servers when workers are many, and 2) significantly worse convergence when the traffic to parameter server is congested. Can we design an algorithm that is robust in a heterogeneous environment, while being communication efficient and maintaining the best-possible convergence rate? In this paper, we propose an asynchronous decentralized stochastic gradient decent algorithm (AD-PSGD) satisfying all above expectations. Our theoretical analysis shows AD-PSGD converges at the optimal O(1/K−−√)O(1/\sqrt{K}) rate as SGD and has linear speedup w.r.t. number of workers. Empirically, AD-PSGD outperforms the best of decentralized parallel SGD (D-PSGD), asynchronous parallel SGD (A-PSGD), and standard data parallel SGD (AllReduce-SGD), often by orders of magnitude in a heterogeneous environment. When training ResNet-50 on ImageNet with up to 128 GPUs, AD-PSGD converges (w.r.t epochs) similarly to the AllReduce-SGD, but each epoch can be up to 4-8x faster than its synchronous counterparts in a network-sharing HPC environment. To the best of our knowledge, AD-PSGD is the first asynchronous algorithm that achieves a similar epoch-wise convergence rate as AllReduce-SGD, at an over 100-GPU scale.",http://proceedings.mlr.press/v80/lian18a.html,http://proceedings.mlr.press/v80/lian18a/lian18a.pdf,ICML
808,2018,Towards More Efficient Stochastic Decentralized Learning: Faster Convergence and Sparse Communication,"Zebang Shen,         Aryan Mokhtari,         Tengfei Zhou,         Peilin Zhao,         Hui Qian","Recently, the decentralized optimization problem is attracting growing attention. Most existing methods are deterministic with high per-iteration cost and have a convergence rate quadratically depending on the problem condition number. Besides, the dense communication is necessary to ensure the convergence even if the dataset is sparse. In this paper, we generalize the decentralized optimization problem to a monotone operator root finding problem, and propose a stochastic algorithm named DSBA that (1) converges geometrically with a rate linearly depending on the problem condition number, and (2) can be implemented using sparse communication only. Additionally, DSBA handles important learning problems like AUC-maximization which can not be tackled efficiently in the previous problem setting. Experiments on convex minimization and AUC-maximization validate the efficiency of our method.",http://proceedings.mlr.press/v80/shen18a.html,http://proceedings.mlr.press/v80/shen18a/shen18a.pdf,ICML
809,2018,Fixing a Broken ELBO,"Alexander Alemi,         Ben Poole,         Ian Fischer,         Joshua Dillon,         Rif A. Saurous,         Kevin Murphy","Recent work in unsupervised representation learning has focused on learning deep directed latentvariable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.",http://proceedings.mlr.press/v80/alemi18a.html,http://proceedings.mlr.press/v80/alemi18a/alemi18a.pdf,ICML
810,2018,Neural Relational Inference for Interacting Systems,"Thomas Kipf,         Ethan Fetaya,         Kuan-Chieh Wang,         Max Welling,         Richard Zemel","Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system’s constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data.",http://proceedings.mlr.press/v80/kipf18a.html,http://proceedings.mlr.press/v80/kipf18a/kipf18a.pdf,ICML
811,2018,An Estimation and Analysis Framework for the Rasch Model,"Andrew Lan,         Mung Chiang,         Christoph Studer","The Rasch model is widely used for item response analysis in applications ranging from recommender systems to psychology, education, and finance. While a number of estimators have been proposed for the Rasch model over the last decades, the associated analytical performance guarantees are mostly asymptotic. This paper provides a framework that relies on a novel linear minimum mean-squared error (L-MMSE) estimator which enables an exact, nonasymptotic, and closed-form analysis of the parameter estimation error under the Rasch model. The proposed framework provides guidelines on the number of items and responses required to attain low estimation errors in tests or surveys. We furthermore demonstrate its efficacy on a number of real-world collaborative filtering datasets, which reveals that the proposed L-MMSE estimator performs on par with state-of-the-art nonlinear estimators in terms of predictive performance.",http://proceedings.mlr.press/v80/lan18a.html,http://proceedings.mlr.press/v80/lan18a/lan18a.pdf,ICML
812,2018,Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization,"Louis Filstroff,         Alberto Lumbreras,         Cédric Févotte","We present novel understandings of the Gamma-Poisson (GaP) model, a probabilistic matrix factorization model for count data. We show that GaP can be rewritten free of the score/activation matrix. This gives us new insights about the estimation of the topic/dictionary matrix by maximum marginal likelihood estimation. In particular, this explains the robustness of this estimator to over-specified values of the factorization rank, especially its ability to automatically prune irrelevant dictionary columns, as empirically observed in previous work. The marginalization of the activation matrix leads in turn to a new Monte Carlo Expectation-Maximization algorithm with favorable properties.",http://proceedings.mlr.press/v80/filstroff18a.html,http://proceedings.mlr.press/v80/filstroff18a/filstroff18a.pdf,ICML
813,2018,Accelerating Natural Gradient with Higher-Order Invariance,"Yang Song,         Jiaming Song,         Stefano Ermon","An appealing property of the natural gradient is that it is invariant to arbitrary differentiable reparameterizations of the model. However, this invariance property requires infinitesimal steps and is lost in practical implementations with small but finite step sizes. In this paper, we study invariance properties from a combined perspective of Riemannian geometry and numerical differential equation solving. We define the order of invariance of a numerical method to be its convergence order to an invariant solution. We propose to use higher-order integrators and geodesic corrections to obtain more invariant optimization trajectories. We prove the numerical convergence properties of geodesic corrected updates and show that they can be as computational efficient as plain natural gradient. Experimentally, we demonstrate that invariance leads to faster optimization and our techniques improve on traditional natural gradient in deep neural network training and natural policy gradient for reinforcement learning.",http://proceedings.mlr.press/v80/song18a.html,http://proceedings.mlr.press/v80/song18a/song18a.pdf,ICML
814,2018,Asynchronous Byzantine Machine Learning (the case of SGD),"Georgios Damaskinos,         El Mahdi El Mhamdi,         Rachid Guerraoui,         Rhicheek Patra,         Mahsa Taziki","Asynchronous distributed machine learning solutions have proven very effective so far, but always assuming perfectly functioning workers. In practice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrupt data, or even malicious attacks. We introduce Kardam, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of two complementary components: a filtering and a dampening component. The first is scalar-based and ensures resilience against 1/3 Byzantine workers. Essentially, this filter leverages the Lipschitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attempt to corrupt the progress of SGD. The dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We prove that Kardam guarantees almost sure convergence in the presence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR100 and EMNIST datasets and measure its overhead with respect to non Byzantine-resilient solutions. We empirically show that Kardam does not introduce additional noise to the learning procedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and empirically show to be less than f/n, where f is the number of Byzantine failures tolerated and n the total number of workers. Interestingly, we also empirically observe that the dampening component is interesting in its own right for it enables to build an SGD algorithm that outperforms alternative staleness-aware asynchronous competitors in environments with honest workers.",http://proceedings.mlr.press/v80/damaskinos18a.html,http://proceedings.mlr.press/v80/damaskinos18a/damaskinos18a.pdf,ICML
815,2018,Characterizing Implicit Bias in Terms of Optimization Geometry,"Suriya Gunasekar,         Jason Lee,         Daniel Soudry,         Nathan Srebro","We study the bias of generic optimization methods, including Mirror Descent, Natural Gradient Descent and Steepest Descent with respect to different potentials and norms, when optimizing underdetermined linear models or separable linear classification problems. We ask the question of whether the global minimum (among the many possible global minima) reached by optimization can be characterized in terms of the potential or norm, and indecently of hyper-parameter choices, such as stepsize and momentum.",http://proceedings.mlr.press/v80/gunasekar18a.html,http://proceedings.mlr.press/v80/gunasekar18a/gunasekar18a.pdf,ICML
816,2018,Provable Variable Selection for Streaming Features,"Jing Wang,         Jie Shen,         Ping Li","In large-scale machine learning applications and high-dimensional statistics, it is ubiquitous to address a considerable number of features among which many are redundant. As a remedy, online feature selection has attracted increasing attention in recent years. It sequentially reveals features and evaluates the importance of them. Though online feature selection has proven an elegant methodology, it is usually challenging to carry out a rigorous theoretical characterization. In this work, we propose a provable online feature selection algorithm that utilizes the online leverage score. The selected features are then fed to kkk-means clustering, making the clustering step memory and computationally efficient. We prove that with high probability, performing kkk-means clustering based on the selected feature space does not deviate far from the optimal clustering using the original data. The empirical results on real-world data sets demonstrate the effectiveness of our algorithm.",http://proceedings.mlr.press/v80/wang18g.html,http://proceedings.mlr.press/v80/wang18g/wang18g.pdf,ICML
817,2018,MISSION: Ultra Large-Scale Feature Selection using Count-Sketches,"Amirali Aghazadeh,         Ryan Spring,         Daniel Lejeune,         Gautam Dasarathy,         Anshumali Shrivastava,          baraniuk","Feature selection is an important challenge in machine learning. It plays a crucial role in the explainability of machine-driven decisions that are rapidly permeating throughout modern society. Unfortunately, the explosion in the size and dimensionality of real-world datasets poses a severe challenge to standard feature selection algorithms. Today, it is not uncommon for datasets to have billions of dimensions. At such scale, even storing the feature vector is impossible, causing most existing feature selection methods to fail. Workarounds like feature hashing, a standard approach to large-scale machine learning, helps with the computational feasibility, but at the cost of losing the interpretability of features. In this paper, we present MISSION, a novel framework for ultra large-scale feature selection that performs stochastic gradient descent while maintaining an efficient representation of the features in memory using a Count-Sketch data structure. MISSION retains the simplicity of feature hashing without sacrificing the interpretability of the features while using only O(log^2(p)) working memory. We demonstrate that MISSION accurately and efficiently performs feature selection on real-world, large-scale datasets with billions of dimensions.",http://proceedings.mlr.press/v80/aghazadeh18a.html,http://proceedings.mlr.press/v80/aghazadeh18a/aghazadeh18a.pdf,ICML
818,2018,Decoupled Parallel Backpropagation with Convergence Guarantee,"Zhouyuan Huo,         Bin Gu,          Yang,         Heng Huang","Backpropagation algorithm is indispensable for the training of feedforward neural networks. It requires propagating error gradients sequentially from the output layer all the way back to the input layer. The backward locking in backpropagation algorithm constrains us from updating network layers in parallel and fully leveraging the computing resources. Recently, several algorithms have been proposed for breaking the backward locking. However, their performances degrade seriously when networks are deep. In this paper, we propose decoupled parallel backpropagation algorithm for deep learning optimization with convergence guarantee. Firstly, we decouple the backpropagation algorithm using delayed gradients, and show that the backward locking is removed when we split the networks into multiple modules. Then, we utilize decoupled parallel backpropagation in two stochastic methods and prove that our method guarantees convergence to critical points for the non-convex problem. Finally, we perform experiments for training deep convolutional neural networks on benchmark datasets. The experimental results not only confirm our theoretical analysis, but also demonstrate that the proposed method can achieve significant speedup without loss of accuracy.",http://proceedings.mlr.press/v80/huo18a.html,http://proceedings.mlr.press/v80/huo18a/huo18a.pdf,ICML
819,2018,Deep One-Class Classification,"Lukas Ruff,         Robert Vandermeulen,         Nico Goernitz,         Lucas Deecke,         Shoaib Ahmed Siddiqui,         Alexander Binder,         Emmanuel Müller,         Marius Kloft","Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GTSRB stop signs.",http://proceedings.mlr.press/v80/ruff18a.html,http://proceedings.mlr.press/v80/ruff18a/ruff18a.pdf,ICML
820,2018,Differentially Private Database Release via Kernel Mean Embeddings,"Matej Balog,         Ilya Tolstikhin,         Bernhard Schölkopf","We lay theoretical foundations for new database release mechanisms that allow third-parties to construct consistent estimators of population statistics, while ensuring that the privacy of each individual contributing to the database is protected. The proposed framework rests on two main ideas. First, releasing (an estimate of) the kernel mean embedding of the data generating random variable instead of the database itself still allows third-parties to construct consistent estimators of a wide class of population statistics. Second, the algorithm can satisfy the definition of differential privacy by basing the released kernel mean embedding on entirely synthetic data points, while controlling accuracy through the metric available in a Reproducing Kernel Hilbert Space. We describe two instantiations of the proposed framework, suitable under different scenarios, and prove theoretical results guaranteeing differential privacy of the resulting algorithms and the consistency of estimators constructed from their outputs.",http://proceedings.mlr.press/v80/balog18a.html,http://proceedings.mlr.press/v80/balog18a/balog18a.pdf,ICML
821,2018,Faster Derivative-Free Stochastic Algorithm for Shared Memory Machines,"Bin Gu,         Zhouyuan Huo,         Cheng Deng,         Heng Huang","Asynchronous parallel stochastic gradient optimization has been playing a pivotal role to solve large-scale machine learning problems in big data applications. Zeroth-order (derivative-free) methods estimate the gradient only by two function evaluations, thus have been applied to solve the problems where the explicit gradient calculations are computationally expensive or infeasible. Recently, the first asynchronous parallel stochastic zeroth-order algorithm (AsySZO) was proposed. However, its convergence rate is O(1/SQRT{T}) for the smooth, possibly non-convex learning problems, which is significantly slower than O(1/T) the best convergence rate of (asynchronous) stochastic gradient algorithm. To fill this gap, in this paper, we first point out the fundamental reason leading to the slow convergence rate of AsySZO, and then propose a new asynchronous stochastic zerothorder algorithm (AsySZO+). We provide a faster convergence rate O(1/bT) (b is the mini-batch size) for AsySZO+ by the rigorous theoretical analysis, which is a significant improvement over O(1/SQRT{T}). The experimental results on the application of ensemble learning confirm that our AsySZO+ has a faster convergence rate than the existing (asynchronous) stochastic zeroth-order algorithms.",http://proceedings.mlr.press/v80/gu18a.html,http://proceedings.mlr.press/v80/gu18a/gu18a.pdf,ICML
822,2018,Self-Imitation Learning,"Junhyuk Oh,         Yijie Guo,         Satinder Singh,         Honglak Lee","This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent’s past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.",http://proceedings.mlr.press/v80/oh18b.html,http://proceedings.mlr.press/v80/oh18b/oh18b.pdf,ICML
823,2018,Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,"Mattias Teye,         Hossein Azizpour,         Kevin Smith","We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models. We further demonstrate that this finding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modifications to the network or the training procedure. Our approach is thoroughly validated by measuring the quality of uncertainty in a series of empirical experiments on different tasks. It outperforms baselines with strong statistical significance, and displays competitive performance with recent Bayesian approaches.",http://proceedings.mlr.press/v80/teye18a.html,http://proceedings.mlr.press/v80/teye18a/teye18a.pdf,ICML
824,2018,Let’s be Honest: An Optimal No-Regret Framework for Zero-Sum Games,"Ehsan Asadi Kangarshahi,         Ya-Ping Hsieh,         Mehmet Fatih Sahin,         Volkan Cevher","We revisit the problem of solving two-player zero-sum games in the decentralized setting. We propose a simple algorithmic framework that simultaneously achieves the best rates for honest regret as well as adversarial regret, and in addition resolves the open problem of removing the logarithmic terms in convergence to the value of the game. We achieve this goal in three steps. First, we provide a novel analysis of the optimistic mirror descent (OMD), showing that it can be modified to guarantee fast convergence for both honest regret and value of the game, when the players are playing collaboratively. Second, we propose a new algorithm, dubbed as robust optimistic mirror descent (ROMD), which attains optimal adversarial regret without knowing the time horizon beforehand. Finally, we propose a simple signaling scheme, which enables us to bridge OMD and ROMD to achieve the best of both worlds. Numerical examples are presented to support our theoretical claims and show that our non-adaptive ROMD algorithm can be competitive to OMD with adaptive step-size selection.",http://proceedings.mlr.press/v80/kangarshahi18a.html,http://proceedings.mlr.press/v80/kangarshahi18a/kangarshahi18a.pdf,ICML
825,2018,Semi-Implicit Variational Inference,"Mingzhang Yin,         Mingyuan Zhou","Semi-implicit variational inference (SIVI) is introduced to expand the commonly used analytic variational distribution family, by mixing the variational parameter with a flexible distribution. This mixing distribution can assume any density function, explicit or not, as long as independent random samples can be generated via reparameterization. Not only does SIVI expand the variational family to incorporate highly flexible variational distributions, including implicit ones that have no analytic density functions, but also sandwiches the evidence lower bound (ELBO) between a lower bound and an upper bound, and further derives an asymptotically exact surrogate ELBO that is amenable to optimization via stochastic gradient ascent. With a substantially expanded variational family and a novel optimization algorithm, SIVI is shown to closely match the accuracy of MCMC in inferring the posterior in a variety of Bayesian inference tasks.",http://proceedings.mlr.press/v80/yin18b.html,http://proceedings.mlr.press/v80/yin18b/yin18b.pdf,ICML
826,2018,"CRAFTML, an Efficient Clustering-based Random Forest for Extreme Multi-label Learning","Wissam Siblini,         Pascale Kuntz,         Frank Meyer","Extreme Multi-label Learning (XML) considers large sets of items described by a number of labels that can exceed one million. Tree-based methods, which hierarchically partition the problem into small scale sub-problems, are particularly promising in this context to reduce the learning/prediction complexity and to open the way to parallelization. However, the current best approaches do not exploit tree randomization which has shown its efficiency in random forests and they resort to complex partitioning strategies. To overcome these limits, we here introduce a new random forest based algorithm with a very fast partitioning approach called CRAFTML. Experimental comparisons on nine datasets from the XML literature show that it outperforms the other tree-based approaches. Moreover with a parallelized implementation reduced to five cores, it is competitive with the best state-of-the-art methods which run on one hundred-core machines.",http://proceedings.mlr.press/v80/siblini18a.html,http://proceedings.mlr.press/v80/siblini18a/siblini18a.pdf,ICML
827,2018,Fitting New Speakers Based on a Short Untranscribed Sample,"Eliya Nachmani,         Adam Polyak,         Yaniv Taigman,         Lior Wolf","Learning-based Text To Speech systems have the potential to generalize from one speaker to the next and thus require a relatively short sample of any new voice. However, this promise is currently largely unrealized. We present a method that is designed to capture a new speaker from a short untranscribed audio sample. This is done by employing an additional network that given an audio sample, places the speaker in the embedding space. This network is trained as part of the speech synthesis system using various consistency losses. Our results demonstrate a greatly improved performance on both the dataset speakers, and, more importantly, when fitting new voices, even from very short samples.",http://proceedings.mlr.press/v80/nachmani18a.html,http://proceedings.mlr.press/v80/nachmani18a/nachmani18a.pdf,ICML
828,2018,Stochastic Wasserstein Barycenters,"Sebastian Claici,         Edward Chien,         Justin Solomon","We present a stochastic algorithm to compute the barycenter of a set of probability distributions under the Wasserstein metric from optimal transport. Unlike previous approaches, our method extends to continuous input distributions and allows the support of the barycenter to be adjusted in each iteration. We tackle the problem without regularization, allowing us to recover a sharp output whose support is contained within the support of the true barycenter. We give examples where our algorithm recovers a more meaningful barycenter than previous work. Our method is versatile and can be extended to applications such as generating super samples from a given distribution and recovering blue noise approximations.",http://proceedings.mlr.press/v80/claici18a.html,http://proceedings.mlr.press/v80/claici18a/claici18a.pdf,ICML
829,2018,Spurious Local Minima are Common in Two-Layer ReLU Neural Networks,"Itay Safran,         Ohad Shamir","We consider the optimization problem associated with training simple ReLU neural networks of the form x↦∑ki=1max{0,w⊤ix}\mathbf{x}\mapsto \sum_{i=1}^{k}\max\{0,\mathbf{w}_i^\top \mathbf{x}\} with respect to the squared loss. We provide a computer-assisted proof that even if the input distribution is standard Gaussian, even if the dimension is arbitrarily large, and even if the target values are generated by such a network, with orthonormal parameter vectors, the problem can still have spurious local minima once 6≤k≤206\le k\le 20. By a concentration of measure argument, this implies that in high input dimensions, nearly all target networks of the relevant sizes lead to spurious local minima. Moreover, we conduct experiments which show that the probability of hitting such local minima is quite high, and increasing with the network size. On the positive side, mild over-parameterization appears to drastically reduce such local minima, indicating that an over-parameterization assumption is necessary to get a positive result in this setting.",http://proceedings.mlr.press/v80/safran18a.html,http://proceedings.mlr.press/v80/safran18a/safran18a.pdf,ICML
830,2018,Adaptive Three Operator Splitting,"Fabian Pedregosa,         Gauthier Gidel","We propose and analyze a novel adaptive step size variant of the Davis-Yin three operator splitting, a method that can solve optimization problems composed of a sum of a smooth term for which we have access to its gradient and an arbitrary number of potentially non-smooth terms for which we have access to their proximal operator. The proposed method leverages local information of the objective function, allowing for larger step sizes while preserving the convergence properties of the original method. It only requires two extra function evaluations per iteration and does not depend on any step size hyperparameter besides an initial estimate. We provide a convergence rate analysis of this method, showing sublinear convergence rate for general convex functions and linear convergence under stronger assumptions, matching the best known rates of its non adaptive variant. Finally, an empirical comparison with related methods on 6 different problems illustrates the computational advantage of the adaptive step size strategy.",http://proceedings.mlr.press/v80/pedregosa18a.html,http://proceedings.mlr.press/v80/pedregosa18a/pedregosa18a.pdf,ICML
831,2018,K-means clustering using random matrix sparsification,Kaushik Sinha,"K-means clustering algorithm using Lloyd’s heuristic is one of the most commonly used tools in data mining and machine learning that shows promising performance. However, it suffers from a high computational cost resulting from pairwise Euclidean distance computations between data points and cluster centers in each iteration of Lloyd’s heuristic. Main contributing factor of this computational bottle neck is a matrix-vector multiplication step, where the matrix contains all the data points and the vector is a cluster center. In this paper we show that we can randomly sparsify the original data matrix resulting in a sparse data matrix which can significantly speed up the above mentioned matrix vector multiplication step without significantly affecting cluster quality. In particular, we show that optimal k-means clustering solution of the sparse data matrix, obtained by applying random matrix sparsification, results in an approximately optimal k-means clustering objective of the original data matrix. Our empirical studies on three real world datasets corroborate our theoretical findings and demonstrate that our proposed sparsification method can indeed achieve satisfactory clustering performance.",http://proceedings.mlr.press/v80/sinha18a.html,http://proceedings.mlr.press/v80/sinha18a/sinha18a.pdf,ICML
832,2018,Online Convolutional Sparse Coding with Sample-Dependent Dictionary,"Yaqing Wang,         Quanming Yao,         James Tin-Yau Kwok,         Lionel M. NI","Convolutional sparse coding (CSC) has been popularly used for the learning of shift-invariant dictionaries in image and signal processing. However, existing methods have limited scalability. In this paper, instead of convolving with a dictionary shared by all samples, we propose the use of a sample-dependent dictionary in which each filter is a linear combination of a small set of base filters learned from data. This added flexibility allows a large number of sample-dependent patterns to be captured, which is especially useful in the handling of large or high-dimensional data sets. Computationally, the resultant model can be efficiently learned by online learning. Extensive experimental results on a number of data sets show that the proposed method outperforms existing CSC algorithms with significantly reduced time and space complexities.",http://proceedings.mlr.press/v80/wang18k.html,http://proceedings.mlr.press/v80/wang18k/wang18k.pdf,ICML
833,2018,Learning in Reproducing Kernel Kreı̆n Spaces,"Dino Oglic,         Thomas Gaertner","We formulate a novel regularized risk minimization problem for learning in reproducing kernel Kre{ı̆}n spaces and show that the strong representer theorem applies to it. As a result of the latter, the learning problem can be expressed as the minimization of a quadratic form over a hypersphere of constant radius. We present an algorithm that can find a globally optimal solution to this non-convex optimization problem in time cubic in the number of instances. Moreover, we derive the gradient of the solution with respect to its hyperparameters and, in this way, provide means for efficient hyperparameter tuning. The approach comes with a generalization bound expressed in terms of the Rademacher complexity of the corresponding hypothesis space. The major advantage over standard kernel methods is the ability to learn with various domain specific similarity measures for which positive definiteness does not hold or is difficult to establish. The approach is evaluated empirically using indefinite kernels defined on structured as well as vectorial data. The empirical results demonstrate a superior performance of our approach over the state-of-the-art baselines.",http://proceedings.mlr.press/v80/oglic18a.html,http://proceedings.mlr.press/v80/oglic18a/oglic18a.pdf,ICML
834,2018,Adaptive Exploration-Exploitation Tradeoff for Opportunistic Bandits,"Huasen Wu,         Xueying Guo,         Xin Liu","In this paper, we propose and study opportunistic bandits - a new variant of bandits where the regret of pulling a suboptimal arm varies under different environmental conditions, such as network load or produce price. When the load/price is low, so is the cost/regret of pulling a suboptimal arm (e.g., trying a suboptimal network configuration). Therefore, intuitively, we could explore more when the load/price is low and exploit more when the load/price is high. Inspired by this intuition, we propose an Adaptive Upper-Confidence-Bound (AdaUCB) algorithm to adaptively balance the exploration-exploitation tradeoff for opportunistic bandits. We prove that AdaUCB achieves O(log T) regret with a smaller coefficient than the traditional UCB algorithm. Furthermore, AdaUCB achieves O(1) regret with respect to T if the exploration cost is zero when the load level is below a certain threshold. Last, based on both synthetic data and real-world traces, experimental results show that AdaUCB significantly outperforms other bandit algorithms, such as UCB and TS (Thompson Sampling), under large load/price fluctuations.",http://proceedings.mlr.press/v80/wu18b.html,http://proceedings.mlr.press/v80/wu18b/wu18b.pdf,ICML
835,2018,Continuous-Time Flows for Efficient Inference and Density Estimation,"Changyou Chen,         Chunyuan Li,         Liqun Chen,         Wenlin Wang,         Yunchen Pu,         Lawrence Carin Duke","Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. Algorithms for the two tasks, such as normalizing flows and generative adversarial networks (GANs), are often developed independently. In this paper, we propose the concept of continuous-time flows (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit energy-based distribution with CTFs for density estimation. Both tasks rely on a new technique for distribution matching within amortized learning. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.",http://proceedings.mlr.press/v80/chen18d.html,http://proceedings.mlr.press/v80/chen18d/chen18d.pdf,ICML
836,2018,Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase Procrustes Flow,"Xiao Zhang,         Simon Du,         Quanquan Gu","We revisit the inductive matrix completion problem that aims to recover a rank-rrr matrix with ambient dimension ddd given nnn features as the side prior information. The goal is to make use of the known nnn features to reduce sample and computational complexities. We present and analyze a new gradient-based non-convex optimization algorithm that converges to the true underlying matrix at a linear rate with sample complexity only linearly depending on nnn and logarithmically depending on ddd. To the best of our knowledge, all previous algorithms either have a quadratic dependency on the number of features in sample complexity or a sub-linear computational convergence rate. In addition, we provide experiments on both synthetic and real world data to demonstrate the effectiveness of our proposed algorithm.",http://proceedings.mlr.press/v80/zhang18b.html,http://proceedings.mlr.press/v80/zhang18b/zhang18b.pdf,ICML
837,2018,Testing Sparsity over Known and Unknown Bases,"Siddharth Barman,         Arnab Bhattacharyya,         Suprovat Ghoshal","Sparsity is a basic property of real vectors that is exploited in a wide variety of machine learning applications. In this work, we describe property testing algorithms for sparsity that observe a low-dimensional projec- tion of the input. We consider two settings. In the first setting, we test sparsity with respect to an unknown basis: given input vectors y1,...,yp∈Rdy1,...,yp∈Rdy_1 ,...,y_p \in R^d whose concatenation as columns forms Y∈Rd×pY∈Rd×pY \in R^{d \times p} , does Y=AXY=AXY = AX for matrices A∈Rd×mA∈Rd×mA \in R^{d\times m} and X∈Rm×pX∈Rm×pX \in R^{m \times p} such that each column of XXX is kkk-sparse, or is YYY “far” from having such a decomposition? In the second setting, we test sparsity with respect to a known basis: for a fixed design ma- trix A∈Rd×mA∈Rd×mA \in R^{d \times m} , given input vector y∈Rdy∈Rdy \in R^d , is y=Axy=Axy = Ax for some kkk-sparse vector xxx or is yyy “far” from having such a decomposition? We analyze our algorithms using tools from high-dimensional geometry and probability.",http://proceedings.mlr.press/v80/barman18a.html,http://proceedings.mlr.press/v80/barman18a/barman18a.pdf,ICML
838,2018,Topological mixture estimation,Steve Huntsman,"We introduce topological mixture estimation, a completely nonparametric and computationally efficient solution to the problem of estimating a one-dimensional mixture with generic unimodal components. We repeatedly perturb the unimodal decomposition of Baryshnikov and Ghrist to produce a topologically and information-theoretically optimal unimodal mixture. We also detail a smoothing process that optimally exploits topological persistence of the unimodal category in a natural way when working directly with sample data. Finally, we illustrate these techniques through examples.",http://proceedings.mlr.press/v80/huntsman18a.html,http://proceedings.mlr.press/v80/huntsman18a/huntsman18a.pdf,ICML
839,2018,Composite Functional Gradient Learning of Generative Adversarial Models,"Rie Johnson,         Tong Zhang","This paper first presents a theory for generative adversarial methods that does not rely on the traditional minimax formulation. It shows that with a strong discriminator, a good generator can be learned so that the KL divergence between the distributions of real data and generated data improves after each functional gradient step until it converges to zero. Based on the theory, we propose a new stable generative adversarial method. A theoretical insight into the original GAN from this new viewpoint is also provided. The experiments on image generation show the effectiveness of our new method.",http://proceedings.mlr.press/v80/johnson18a.html,http://proceedings.mlr.press/v80/johnson18a/johnson18a.pdf,ICML
840,2018,Attention-based Deep Multiple Instance Learning,"Maximilian Ilse,         Jakub Tomczak,         Max Welling","Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.",http://proceedings.mlr.press/v80/ilse18a.html,http://proceedings.mlr.press/v80/ilse18a/ilse18a.pdf,ICML
841,2018,Probabilistic Recurrent State-Space Models,"Andreas Doerr,         Christian Daniel,         Martin Schiegg,         Nguyen-Tuong Duy,         Stefan Schaal,         Marc Toussaint,         Trimpe Sebastian","State-space models (SSMs) are a highly expressive model class for learning patterns in time series data and for system identification. Deterministic versions of SSMs (e.g., LSTMs) proved extremely successful in modeling complex time series data. Fully probabilistic SSMs, however, are often found hard to train, even for smaller problems. We propose a novel model formulation and a scalable training algorithm based on doubly stochastic variational inference and Gaussian processes. This combination allows efficient incorporation of latent state temporal correlations, which we found to be key to robust training. The effectiveness of the proposed PR-SSM is evaluated on a set of real-world benchmark datasets in comparison to state-of-the-art probabilistic model learning methods. Scalability and robustness are demonstrated on a high dimensional problem.",http://proceedings.mlr.press/v80/doerr18a.html,http://proceedings.mlr.press/v80/doerr18a/doerr18a.pdf,ICML
842,2018,Rapid Adaptation with Conditionally Shifted Neurons,"Tsendsuren Munkhdalai,         Xingdi Yuan,         Soroush Mehri,         Adam Trischler","We describe a mechanism by which artificial neural networks can learn rapid adaptation - the ability to adapt on the fly, with little data, to new tasks - that we call conditionally shifted neurons. We apply this mechanism in the framework of metalearning, where the aim is to replicate some of the flexibility of human learning in machines. Conditionally shifted neurons modify their activation values with task-specific shifts retrieved from a memory module, which is populated rapidly based on limited task experience. On metalearning benchmarks from the vision and language domains, models augmented with conditionally shifted neurons achieve state-of-the-art results.",http://proceedings.mlr.press/v80/munkhdalai18a.html,http://proceedings.mlr.press/v80/munkhdalai18a/munkhdalai18a.pdf,ICML
843,2018,Essentially No Barriers in Neural Network Energy Landscape,"Felix Draxler,         Kambis Veschgini,         Manfred Salmhofer,         Fred Hamprecht","Training neural networks involves finding minima of a high-dimensional non-convex loss function. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essentially flat in both the training and test landscapes. This implies that minima are perhaps best seen as points on a single connected manifold of low loss, rather than as the bottoms of distinct valleys.",http://proceedings.mlr.press/v80/draxler18a.html,http://proceedings.mlr.press/v80/draxler18a/draxler18a.pdf,ICML
844,2018,Spectrally Approximating Large Graphs with Smaller Graphs,"Andreas Loukas,         Pierre Vandergheynst","How does coarsening affect the spectrum of a general graph? We provide conditions such that the principal eigenvalues and eigenspaces of a coarsened and original graph Laplacian matrices are close. The achieved approximation is shown to depend on standard graph-theoretic properties, such as the degree and eigenvalue distributions, as well as on the ratio between the coarsened and actual graph sizes. Our results carry implications for learning methods that utilize coarsening. For the particular case of spectral clustering, they imply that coarse eigenvectors can be used to derive good quality assignments even without refinement{—}this phenomenon was previously observed, but lacked formal justification.",http://proceedings.mlr.press/v80/loukas18a.html,http://proceedings.mlr.press/v80/loukas18a/loukas18a.pdf,ICML
845,2018,Dimensionality-Driven Learning with Noisy Labels,"Xingjun Ma,         Yisen Wang,         Michael E. Houle,         Shuo Zhou,         Sarah Erfani,         Shutao Xia,         Sudanthi Wijewickrema,         James Bailey","Datasets with significant proportions of noisy (incorrect) class labels present challenges for training accurate Deep Neural Networks (DNNs). We propose a new perspective for understanding DNN generalization for such datasets, by investigating the dimensionality of the deep representation subspace of training samples. We show that from a dimensionality perspective, DNNs exhibit quite distinctive learning styles when trained with clean labels versus when trained with a proportion of noisy labels. Based on this finding, we develop a new dimensionality-driven learning strategy, which monitors the dimensionality of subspaces during training and adapts the loss function accordingly. We empirically demonstrate that our approach is highly tolerant to significant proportions of noisy labels, and can effectively learn low-dimensional local subspaces that capture the data distribution.",http://proceedings.mlr.press/v80/ma18d.html,http://proceedings.mlr.press/v80/ma18d/ma18d.pdf,ICML
846,2018,Mutual Information Neural Estimation,"Mohamed Ishmael Belghazi,         Aristide Baratin,         Sai Rajeshwar,         Sherjil Ozair,         Yoshua Bengio,         Aaron Courville,         Devon Hjelm","We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.",http://proceedings.mlr.press/v80/belghazi18a.html,http://proceedings.mlr.press/v80/belghazi18a/belghazi18a.pdf,ICML
847,2018,Learning to Optimize Combinatorial Functions,"Nir Rosenfeld,         Eric Balkanski,         Amir Globerson,         Yaron Singer","Submodular functions have become a ubiquitous tool in machine learning. They are learnable from data, and can be optimized efficiently and with guarantees. Nonetheless, recent negative results show that optimizing learned surrogates of submodular functions can result in arbitrarily bad approximations of the true optimum. Our goal in this paper is to highlight the source of this hardness, and propose an alternative criterion for optimizing general combinatorial functions from sampled data. We prove a tight equivalence showing that a class of functions is optimizable if and only if it can be learned. We provide efficient and scalable optimization algorithms for several function classes of interest, and demonstrate their utility on the task of optimally choosing trending social media items.",http://proceedings.mlr.press/v80/rosenfeld18a.html,http://proceedings.mlr.press/v80/rosenfeld18a/rosenfeld18a.pdf,ICML
848,2018,Noise2Noise: Learning Image Restoration without Clean Data,"Jaakko Lehtinen,         Jacob Munkberg,         Jon Hasselgren,         Samuli Laine,         Tero Karras,         Miika Aittala,         Timo Aila","We apply basic statistical reasoning to signal reconstruction by machine learning - learning to map corrupted observations to clean signals - with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans - all corrupted by different processes - based on noisy data only.",http://proceedings.mlr.press/v80/lehtinen18a.html,http://proceedings.mlr.press/v80/lehtinen18a/lehtinen18a.pdf,ICML
849,2018,Local Private Hypothesis Testing: Chi-Square Tests,"Marco Gaboardi,         Ryan Rogers","The local model for differential privacy is emerging as the reference model for practical applications of collecting and sharing sensitive information while satisfying strong privacy guarantees. In the local model, there is no trusted entity which is allowed to have each individual’s raw data as is assumed in the traditional curator model. Individuals’ data are usually perturbed before sharing them. We explore the design of private hypothesis tests in the local model, where each data entry is perturbed to ensure the privacy of each participant. Specifically, we analyze locally private chi-square tests for goodness of fit and independence testing.",http://proceedings.mlr.press/v80/gaboardi18a.html,http://proceedings.mlr.press/v80/gaboardi18a/gaboardi18a.pdf,ICML
850,2018,Lipschitz Continuity in Model-based Reinforcement Learning,"Kavosh Asadi,         Dipendra Misra,         Michael Littman",We examine the impact of learning Lipschitz continuous models in the context of model-based reinforcement learning. We provide a novel bound on multi-step prediction error of Lipschitz models where we quantify the error using the Wasserstein metric. We go on to prove an error bound for the value-function estimate arising from Lipschitz models and show that the estimated value function is itself Lipschitz. We conclude with empirical results that show the benefits of controlling the Lipschitz constant of neural-network models.,http://proceedings.mlr.press/v80/asadi18a.html,http://proceedings.mlr.press/v80/asadi18a/asadi18a.pdf,ICML
851,2018,Deep Reinforcement Learning in Continuous Action Spaces: a Case Study in the Game of Simulated Curling,"Kyowoon Lee,         Sol-A Kim,         Jaesik Choi,         Seong-Whan Lee","Many real-world applications of reinforcement learning require an agent to select optimal actions from continuous spaces. Recently, deep neural networks have successfully been applied to games with discrete actions spaces. However, deep neural networks for discrete actions are not suitable for devising strategies for games where a very small change in an action can dramatically affect the outcome. In this paper, we present a new self-play reinforcement learning framework which equips a continuous search algorithm which enables to search in continuous action spaces with a kernel regression method. Without any hand-crafted features, our network is trained by supervised learning followed by self-play reinforcement learning with a high-fidelity simulator for the Olympic sport of curling. The program trained under our framework outperforms existing programs equipped with several hand-crafted features and won an international digital curling competition.",http://proceedings.mlr.press/v80/lee18b.html,http://proceedings.mlr.press/v80/lee18b/lee18b.pdf,ICML
852,2018,Kernelized Synaptic Weight Matrices,"Lorenz Muller,         Julien Martel,         Giacomo Indiveri","In this paper we introduce a novel neural network architecture, in which weight matrices are re-parametrized in terms of low-dimensional vectors, interacting through kernel functions. A layer of our network can be interpreted as introducing a (potentially infinitely wide) linear layer between input and output. We describe the theory underpinning this model and validate it with concrete examples, exploring how it can be used to impose structure on neural networks in diverse applications ranging from data visualization to recommender systems. We achieve state-of-the-art performance in a collaborative filtering task (MovieLens).",http://proceedings.mlr.press/v80/muller18a.html,http://proceedings.mlr.press/v80/muller18a/muller18a.pdf,ICML
853,2018,D2D^2: Decentralized Training over Decentralized Data,"Hanlin Tang,         Xiangru Lian,         Ming Yan,         Ce Zhang,         Ji Liu","While training a machine learning model using multiple workers, each of which collects data from its own data source, it would be useful when the data collected from different workers are unique and different. Ironically, recent analysis of decentralized parallel stochastic gradient descent (D-PSGD) relies on the assumption that the data hosted on different workers are not too different. In this paper, we ask the question: Can we design a decentralized parallel stochastic gradient descent algorithm that is less sensitive to the data variance across workers? In this paper, we present D2^2, a novel decentralized parallel stochastic gradient descent algorithm designed for large data variance \xr{among workers} (imprecisely, “decentralized” data). The core of D2^2 is a variance reduction extension of D-PSGD. It improves the convergence rate from O(σ√nT+(nζ2)13T2/3)O\left({\sigma \over \sqrt{nT}} + {(n\zeta^2)^{\frac{1}{3}} \over T^{2/3}}\right) to O(σ√nT)O\left({\sigma \over \sqrt{nT}}\right) where ζ2\zeta^{2} denotes the variance among data on different workers. As a result, D2^2 is robust to data variance among workers. We empirically evaluated D2^2 on image classification tasks, where each worker has access to only the data of a limited set of labels, and find that D2^2 significantly outperforms D-PSGD.",http://proceedings.mlr.press/v80/tang18a.html,http://proceedings.mlr.press/v80/tang18a/tang18a.pdf,ICML
854,2018,Explicit Inductive Bias for Transfer Learning with Convolutional Networks,"Xuhong LI,         Yves Grandvalet,         Franck Davoine","In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch. When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task. However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task. In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model. We show the benefit of having an explicit inductive bias towards the initial model, and we eventually recommend a simple L2L2L^2 penalty with the pre-trained model being a reference as the baseline of penalty for transfer learning tasks.",http://proceedings.mlr.press/v80/li18a.html,http://proceedings.mlr.press/v80/li18a/li18a.pdf,ICML
855,2018,The Multilinear Structure of ReLU Networks,"Thomas Laurent,         James Brecht","We study the loss surface of neural networks equipped with a hinge loss criterion and ReLU or leaky ReLU nonlinearities. Any such network defines a piecewise multilinear form in parameter space. By appealing to harmonic analysis we show that all local minima of such network are non-differentiable, except for those minima that occur in a region of parameter space where the loss surface is perfectly flat. Non-differentiable minima are therefore not technicalities or pathologies; they are heart of the problem when investigating the loss of ReLU networks. As a consequence, we must employ techniques from nonsmooth analysis to study these loss surfaces. We show how to apply these techniques in some illustrative cases.",http://proceedings.mlr.press/v80/laurent18b.html,http://proceedings.mlr.press/v80/laurent18b/laurent18b.pdf,ICML
856,2018,Learning Binary Latent Variable Models: A Tensor Eigenpair Approach,"Ariel Jaffe,         Roi Weiss,         Boaz Nadler,         Shai Carmi,         Yuval Kluger","Latent variable models with hidden binary units appear in various applications. Learning such models, in particular in the presence of noise, is a challenging computational problem. In this paper we propose a novel spectral approach to this problem, based on the eigenvectors of both the second order moment matrix and third order moment tensor of the observed data. We prove that under mild non-degeneracy conditions, our method consistently estimates the model parameters at the optimal parametric rate. Our tensor-based method generalizes previous orthogonal tensor decomposition approaches, where the hidden units were assumed to be either statistically independent or mutually exclusive. We illustrate the consistency of our method on simulated data and demonstrate its usefulness in learning a common model for population mixtures in genetics.",http://proceedings.mlr.press/v80/jaffe18a.html,http://proceedings.mlr.press/v80/jaffe18a/jaffe18a.pdf,ICML
857,2018,Partial Optimality and Fast Lower Bounds for Weighted Correlation Clustering,"Jan-Hendrik Lange,         Andreas Karrenbauer,         Bjoern Andres","Weighted correlation clustering is hard to solve and hard to approximate for general graphs. Its applications in network analysis and computer vision call for efficient algorithms. To this end, we make three contributions: We establish partial optimality conditions that can be checked efficiently, and doing so recursively solves the problem for series-parallel graphs to optimality, in linear time. We exploit the packing dual of the problem to compute a heuristic, but non-trivial lower bound faster than that of a canonical linear program relaxation. We introduce a re-weighting with the dual solution by which efficient local search algorithms converge to better feasible solutions. The effectiveness of our methods is demonstrated empirically on a number of benchmark instances.",http://proceedings.mlr.press/v80/lange18a.html,http://proceedings.mlr.press/v80/lange18a/lange18a.pdf,ICML
858,2018,Conditional Noise-Contrastive Estimation of Unnormalised Models,"Ciwan Ceylan,         Michael U. Gutmann","Many parametric statistical models are not properly normalised and only specified up to an intractable partition function, which renders parameter estimation difficult. Examples of unnormalised models are Gibbs distributions, Markov random fields, and neural network models in unsupervised deep learning. In previous work, the estimation principle called noise-contrastive estimation (NCE) was introduced where unnormalised models are estimated by learning to distinguish between data and auxiliary noise. An open question is how to best choose the auxiliary noise distribution. We here propose a new method that addresses this issue. The proposed method shares with NCE the idea of formulating density estimation as a supervised learning problem but in contrast to NCE, the proposed method leverages the observed data when generating noise samples. The noise can thus be generated in a semi-automated manner. We first present the underlying theory of the new method, show that score matching emerges as a limiting case, validate the method on continuous and discrete valued synthetic data, and show that we can expect an improved performance compared to NCE when the data lie in a lower-dimensional manifold. Then we demonstrate its applicability in unsupervised deep learning by estimating a four-layer neural image model.",http://proceedings.mlr.press/v80/ceylan18a.html,http://proceedings.mlr.press/v80/ceylan18a/ceylan18a.pdf,ICML
859,2018,Variance Regularized Counterfactual Risk Minimization via Variational Divergence Minimization,"Hang Wu,         May Wang","Off-policy learning, the task of evaluating and improving policies using historic data collected from a logging policy, is important because on-policy evaluation is usually expensive and has adverse impacts. One of the major challenge of off-policy learning is to derive counterfactual estimators that also has low variance and thus low generalization error. In this work, inspired by learning bounds for importance sampling problems, we present a new counterfactual learning principle for off-policy learning with bandit feedbacks. Our method regularizes the generalization error by minimizing the distribution divergence between the logging policy and the new policy, and removes the need for iterating through all training samples to compute sample variance regularization in prior work. With neural network policies, our end-to-end training algorithms using variational divergence minimization showed significant improvement over conventional baseline algorithms and is also consistent with our theoretical results.",http://proceedings.mlr.press/v80/wu18g.html,http://proceedings.mlr.press/v80/wu18g/wu18g.pdf,ICML
860,2018,MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning,"Bo Zhao,         Xinwei Sun,         Yanwei Fu,         Yuan Yao,         Yizhou Wang","It is one typical and general topic of learning a good embedding model to efficiently learn the representation coefficients between two spaces/subspaces. To solve this task, L1L1L_{1} regularization is widely used for the pursuit of feature selection and avoiding overfitting, and yet the sparse estimation of features in L1L1L_{1} regularization may cause the underfitting of training data. L2L2L_{2} regularization is also frequently used, but it is a biased estimator. In this paper, we propose the idea that the features consist of three orthogonal parts, namely sparse strong signals, dense weak signals and random noise, in which both strong and weak signals contribute to the fitting of data. To facilitate such novel decomposition, MSplit LBI is for the first time proposed to realize feature selection and dense estimation simultaneously. We provide theoretical and simulational verification that our method exceeds L1L1L_{1} and L2L2L_{2} regularization, and extensive experimental results show that our method achieves state-of-the-art performance in the few-shot and zero-shot learning.",http://proceedings.mlr.press/v80/zhao18c.html,http://proceedings.mlr.press/v80/zhao18c/zhao18c.pdf,ICML
861,2018,Safe Element Screening for Submodular Function Minimization,"Weizhong Zhang,         Bin Hong,         Lin Ma,         Wei Liu,         Tong Zhang","Submodular functions are discrete analogs of convex functions, which have applications in various fields, including machine learning and computer vision. However, in large-scale applications, solving Submodular Function Minimization (SFM) problems remains challenging. In this paper, we make the first attempt to extend the emerging technique named screening in large-scale sparse learning to SFM for accelerating its optimization process. We first conduct a careful studying of the relationships between SFM and the corresponding convex proximal problems, as well as the accurate primal optimum estimation of the proximal problems. Relying on this study, we subsequently propose a novel safe screening method to quickly identify the elements guaranteed to be included (we refer to them as active) or excluded (inactive) in the final optimal solution of SFM during the optimization process. By removing the inactive elements and fixing the active ones, the problem size can be dramatically reduced, leading to great savings in the computational cost without sacrificing any accuracy. To the best of our knowledge, the proposed method is the first screening method in the fields of SFM and even combinatorial optimization, thus pointing out a new direction for accelerating SFM algorithms. Experiment results on both synthetic and real datasets demonstrate the significant speedups gained by our approach.",http://proceedings.mlr.press/v80/zhang18e.html,http://proceedings.mlr.press/v80/zhang18e/zhang18e.pdf,ICML
862,2018,Tropical Geometry of Deep Neural Networks,"Liwen Zhang,         Gregory Naitzat,         Lek-Heng Lim","We establish, for the first time, explicit connections between feedforward neural networks with ReLU activation and tropical geometry — we show that the family of such neural networks is equivalent to the family of tropical rational maps. Among other things, we deduce that feedforward ReLU neural networks with one hidden layer can be characterized by zonotopes, which serve as building blocks for deeper networks; we relate decision boundaries of such neural networks to tropical hypersurfaces, a major object of study in tropical geometry; and we prove that linear regions of such neural networks correspond to vertices of polytopes associated with tropical rational functions. An insight from our tropical formulation is that a deeper network is exponentially more expressive than a shallow network.",http://proceedings.mlr.press/v80/zhang18i.html,http://proceedings.mlr.press/v80/zhang18i/zhang18i.pdf,ICML
863,2018,Chi-square Generative Adversarial Network,"Chenyang Tao,         Liqun Chen,         Ricardo Henao,         Jianfeng Feng,         Lawrence Carin Duke","To assess the difference between real and synthetic data, Generative Adversarial Networks (GANs) are trained using a distribution discrepancy measure. Three widely employed measures are information-theoretic divergences, integral probability metrics, and Hilbert space discrepancy metrics. We elucidate the theoretical connections between these three popular GAN training criteria and propose a novel procedure, called χ2\chi^2 (Chi-square) GAN, that is conceptually simple, stable at training and resistant to mode collapse. Our procedure naturally generalizes to address the problem of simultaneous matching of multiple distributions. Further, we propose a resampling strategy that significantly improves sample quality, by repurposing the trained critic function via an importance weighting mechanism. Experiments show that the proposed procedure improves stability and convergence, and yields state-of-art results on a wide range of generative modeling tasks.",http://proceedings.mlr.press/v80/tao18b.html,http://proceedings.mlr.press/v80/tao18b/tao18b.pdf,ICML
864,2018,Weakly Submodular Maximization Beyond Cardinality Constraints: Does Randomization Help Greedy?,"Lin Chen,         Moran Feldman,         Amin Karbasi","Submodular functions are a broad class of set functions that naturally arise in many machine learning applications. Due to their combinatorial structures, there has been a myriad of algorithms for maximizing such functions under various constraints. Unfortunately, once a function deviates from submodularity (even slightly), the known algorithms may perform arbitrarily poorly. Amending this issue, by obtaining approximation results for functions obeying properties that generalize submodularity, has been the focus of several recent works. One such class, known as weakly submodular functions, has received a lot of recent attention from the machine learning community due to its strong connections to restricted strong convexity and sparse reconstruction. In this paper, we prove that a randomized version of the greedy algorithm achieves an approximation ratio of (1+1/γ)−2(1+1/γ)−2(1 + 1/\gamma )^{-2} for weakly submodular maximization subject to a general matroid constraint, where γγ\gamma is a parameter measuring the distance from submodularity. To the best of our knowledge, this is the first algorithm with a non-trivial approximation guarantee for this constrained optimization problem. Moreover, our experimental results show that our proposed algorithm performs well in a variety of real-world problems, including regression, video summarization, splice site detection, and black-box interpretation.",http://proceedings.mlr.press/v80/chen18b.html,http://proceedings.mlr.press/v80/chen18b/chen18b.pdf,ICML
865,2018,Learning Independent Causal Mechanisms,"Giambattista Parascandolo,         Niki Kilbertus,         Mateo Rojas-Carulla,         Bernhard Schölkopf","Statistical learning relies upon data sampled from a distribution, and we usually do not care what actually generated it in the first place. From the point of view of causal modeling, the structure of each distribution is induced by physical mechanisms that give rise to dependences between observables. Mechanisms, however, can be meaningful autonomous modules of generative models that make sense beyond a particular entailed data distribution, lending themselves to transfer between problems. We develop an algorithm to recover a set of independent (inverse) mechanisms from a set of transformed data points. The approach is unsupervised and based on a set of experts that compete for data generated by the mechanisms, driving specialization. We analyze the proposed method in a series of experiments on image data. Each expert learns to map a subset of the transformed data back to a reference distribution. The learned mechanisms generalize to novel domains. We discuss implications for transfer learning and links to recent trends in generative modeling.",http://proceedings.mlr.press/v80/parascandolo18a.html,http://proceedings.mlr.press/v80/parascandolo18a/parascandolo18a.pdf,ICML
866,2018,GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms,"Cédric Colas,         Olivier Sigaud,         Pierre-Yves Oudeyer","In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, Quality-Diversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient-descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG . We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger Half-Cheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments.",http://proceedings.mlr.press/v80/colas18a.html,http://proceedings.mlr.press/v80/colas18a/colas18a.pdf,ICML
867,2018,Convolutional Imputation of Matrix Networks,"Qingyun Sun,         Mengyuan Yan,         David Donoho,          boyd","A matrix network is a family of matrices, with their relations modeled as a weighted graph. We consider the task of completing a partially observed matrix network. The observation comes from a novel sampling scheme where a fraction of matrices might be completely unobserved. How can we recover the entire matrix network from incomplete observations? This mathematical problem arises in many applications including medical imaging and social networks. To recover the matrix network, we propose a structural assumption that the matrices are low-rank after the graph Fourier transform on the network. We formulate a convex optimization problem and prove an exact recovery guarantee for the optimization problem. Furthermore, we numerically characterize the exact recovery regime for varying rank and sampling rate and discover a new phase transition phenomenon. Then we give an iterative imputation algorithm to efficiently solve optimization problem and complete large scale matrix networks. We demonstrate the algorithm with a variety of applications such as MRI and Facebook user network.",http://proceedings.mlr.press/v80/sun18d.html,http://proceedings.mlr.press/v80/sun18d/sun18d.pdf,ICML
868,2018,Adversarial Distillation of Bayesian Neural Network Posteriors,"Kuan-Chieh Wang,         Paul Vicol,         James Lucas,         Li Gu,         Roger Grosse,         Richard Zemel","Bayesian neural networks (BNNs) allow us to reason about uncertainty in a principled way. Stochastic Gradient Langevin Dynamics (SGLD) enables efficient BNN learning by drawing samples from the BNN posterior using mini-batches. However, SGLD and its extensions require storage of many copies of the model parameters, a potentially prohibitive cost, especially for large neural networks. We propose a framework, Adversarial Posterior Distillation, to distill the SGLD samples using a Generative Adversarial Network (GAN). At test-time, samples are generated by the GAN. We show that this distillation framework incurs no loss in performance on recent BNN applications including anomaly detection, active learning, and defense against adversarial attacks. By construction, our framework distills not only the Bayesian predictive distribution, but the posterior itself. This allows one to compute quantities such as the approximate model variance, which is useful in downstream tasks. To our knowledge, these are the first results applying MCMC-based BNNs to the aforementioned applications.",http://proceedings.mlr.press/v80/wang18i.html,http://proceedings.mlr.press/v80/wang18i/wang18i.pdf,ICML
869,2018,Contextual Graph Markov Model: A Deep and Generative Approach to Graph Processing,"Davide Bacciu,         Federico Errica,         Alessio Micheli","We introduce the Contextual Graph Markov Model, an approach combining ideas from generative models and neural networks for the processing of graph data. It founds on a constructive methodology to build a deep architecture comprising layers of probabilistic models that learn to encode the structured information in an incremental fashion. Context is diffused in an efficient and scalable way across the graph vertexes and edges. The resulting graph encoding is used in combination with discriminative models to address structure classification benchmarks.",http://proceedings.mlr.press/v80/bacciu18a.html,http://proceedings.mlr.press/v80/bacciu18a/bacciu18a.pdf,ICML
870,2018,Recurrent Predictive State Policy Networks,"Ahmed Hefny,         Zita Marinho,         Wen Sun,         Siddhartha Srinivasa,         Geoffrey Gordon","We introduce Recurrent Predictive State Policy(RPSP) networks, a recurrent architecture that brings insights from predictive state representations to reinforcement learning in partially ob-servable environments. Predictive state policy networks consist of a recursive filter, which keeps track of a belief about the state of the environment, and a reactive policy that directly maps beliefs to actions, to maximize the cumulative reward. The recursive filter leverages predictive state representations (PSRs) (Rosencrantz & Gordon, 2004; Sun et al., 2016) by modeling predictive state{—}a prediction of the distribution of future observations conditioned on history and future actions.This representation gives rise to a rich class of statistically consistent algorithms (Hefny et al.,2017) to initialize the recursive filter. Predictive stats serves as an equivalent representation of a belief state. Therefore, the policy component of the RPSP-network can be purely reactive, simplifying training while still allowing optimal behavior. Moreover, we use the PSR interpretation during training as well, by incorporating prediction error in the loss function. The entire network (recursive filter and reactive policy) is still differentiable and can be trained using gradient-based methods. We optimize our policy using a combination of policy gradient based on rewards (Williams, 1992)and gradient descent based on prediction error.We show the efficacy of RPSP-networks on a set of robotic control tasks from OpenAI Gym. We empirically show that RPSP-networks perform well compared with memory-preserving networks such as GRUs, as well as finite memory models, being the overall best performing method.",http://proceedings.mlr.press/v80/hefny18a.html,http://proceedings.mlr.press/v80/hefny18a/hefny18a.pdf,ICML
871,2018,Smoothed Action Value Functions for Learning Gaussian Policies,"Ofir Nachum,         Mohammad Norouzi,         George Tucker,         Dale Schuurmans","State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value. We show that such smoothed Q-values still satisfy a Bellman equation, making them learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned smoothed Q-value approximator. The approach is additionally amenable to proximal optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training leads to significantly improved results on standard continuous control benchmarks.",http://proceedings.mlr.press/v80/nachum18a.html,http://proceedings.mlr.press/v80/nachum18a/nachum18a.pdf,ICML
872,2018,Improving Optimization for Models With Continuous Symmetry Breaking,"Robert Bamler,         Stephan Mandt","Many loss functions in representation learning are invariant under a continuous symmetry transformation. For example, the loss function of word embeddings (Mikolov et al., 2013) remains unchanged if we simultaneously rotate all word and context embedding vectors. We show that representation learning models for time series possess an approximate continuous symmetry that leads to slow convergence of gradient descent. We propose a new optimization algorithm that speeds up convergence using ideas from gauge theory in physics. Our algorithm leads to orders of magnitude faster convergence and to more interpretable representations, as we show for dynamic extensions of matrix factorization and word embedding models. We further present an example application of our proposed algorithm that translates modern words into their historic equivalents.",http://proceedings.mlr.press/v80/bamler18a.html,http://proceedings.mlr.press/v80/bamler18a/bamler18a.pdf,ICML
873,2018,Accurate Inference for Adaptive Linear Models,"Yash Deshpande,         Lester Mackey,         Vasilis Syrgkanis,         Matt Taddy","Estimators computed from adaptively collected data do not behave like their non-adaptive brethren.Rather, the sequential dependence of the collection policy can lead to severe distributional biases that persist even in the infinite data limit. We develop a general method – WW\mathbf{W}-decorrelation – for transforming the bias of adaptive linear regression estimators into variance. The method uses only coarse-grained information about the data collection policy and does not need access to propensity scores or exact knowledge of the policy.We bound the finite-sample bias and variance of the WW\mathbf{W}-estimator and develop asymptotically correct confidence intervals based on a novel martingale central limit theorem. We then demonstrate the empirical benefits of the generic WW\mathbf{W}-decorrelation procedure in two different adaptive data settings: the multi-armed bandit and the autoregressive time series.",http://proceedings.mlr.press/v80/deshpande18a.html,http://proceedings.mlr.press/v80/deshpande18a/deshpande18a.pdf,ICML
874,2018,Efficient First-Order Algorithms for Adaptive Signal Denoising,"Dmitrii Ostrovskii,         Zaid Harchaoui","We consider the problem of discrete-time signal denoising, focusing on a specific family of non-linear convolution-type estimators. Each such estimator is associated with a time-invariant filter which is obtained adaptively, by solving a certain convex optimization problem. Adaptive convolution-type estimators were demonstrated to have favorable statistical properties, see (Juditsky & Nemirovski, 2009; 2010; Harchaoui et al., 2015b; Ostrovsky et al., 2016). Our first contribution is an efficient implementation of these estimators via the known first-order proximal algorithms. Our second contribution is a computational complexity analysis of the proposed procedures, which takes into account their statistical nature and the related notion of statistical accuracy. The proposed procedures and their analysis are illustrated on a simulated data benchmark.",http://proceedings.mlr.press/v80/ostrovskii18a.html,http://proceedings.mlr.press/v80/ostrovskii18a/ostrovskii18a.pdf,ICML
875,2018,A Unified Framework for Structured Low-rank Matrix Learning,"Pratik Jawanpuria,         Bamdev Mishra","We consider the problem of learning a low-rank matrix, constrained to lie in a linear subspace, and introduce a novel factorization for modeling such matrices. A salient feature of the proposed factorization scheme is it decouples the low-rank and the structural constraints onto separate factors. We formulate the optimization problem on the Riemannian spectrahedron manifold, where the Riemannian framework allows to develop computationally efficient conjugate gradient and trust-region algorithms. Experiments on problems such as standard/robust/non-negative matrix completion, Hankel matrix learning and multi-task learning demonstrate the efficacy of our approach.",http://proceedings.mlr.press/v80/jawanpuria18a.html,http://proceedings.mlr.press/v80/jawanpuria18a/jawanpuria18a.pdf,ICML
876,2018,Programmatically Interpretable Reinforcement Learning,"Abhinav Verma,         Vijayaraghavan Murali,         Rishabh Singh,         Pushmeet Kohli,         Swarat Chaudhuri","We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural “oracle”. We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.",http://proceedings.mlr.press/v80/verma18a.html,http://proceedings.mlr.press/v80/verma18a/verma18a.pdf,ICML
877,2018,Linear Spectral Estimators and an Application to Phase Retrieval,"Ramina Ghods,         Andrew Lan,         Tom Goldstein,         Christoph Studer","Phase retrieval refers to the problem of recovering real- or complex-valued vectors from magnitude measurements. The best-known algorithms for this problem are iterative in nature and rely on so-called spectral initializers that provide accurate initialization vectors. We propose a novel class of estimators suitable for general nonlinear measurement systems, called linear spectral estimators (LSPEs), which can be used to compute accurate initialization vectors for phase retrieval problems. The proposed LSPEs not only provide accurate initialization vectors for noisy phase retrieval systems with structured or random measurement matrices, but also enable the derivation of sharp and nonasymptotic mean-squared error bounds. We demonstrate the efficacy of LSPEs on synthetic and real-world phase retrieval problems, and we show that our estimators significantly outperform existing methods for structured measurement systems that arise in practice.",http://proceedings.mlr.press/v80/ghods18a.html,http://proceedings.mlr.press/v80/ghods18a/ghods18a.pdf,ICML
878,2018,On the Power of Over-parametrization in Neural Networks with Quadratic Activation,"Simon Du,         Jason Lee","We provide new theoretical insights on why over-parametrization is effective in learning neural networks. For a kkk hidden node shallow network with quadratic activation and nnn training data points, we show as long as k≥2n−−√k≥2n k \ge \sqrt{2n}, over-parametrization enables local search algorithms to find a globally optimal solution for general smooth and convex loss functions. Further, despite that the number of parameters may exceed the sample size, using theory of Rademacher complexity, we show with weight decay, the solution also generalizes well if the data is sampled from a regular distribution such as Gaussian. To prove when k≥2n−−√k≥2nk\ge \sqrt{2n}, the loss function has benign landscape properties, we adopt an idea from smoothed analysis, which may have other applications in studying loss surfaces of neural networks.",http://proceedings.mlr.press/v80/du18a.html,http://proceedings.mlr.press/v80/du18a/du18a.pdf,ICML
879,2018,"Yes, but Did It Work?: Evaluating Variational Inference","Yuling Yao,         Aki Vehtari,         Daniel Simpson,         Andrew Gelman","While it’s always possible to compute a variational approximation to a posterior distribution, it can be difficult to discover problems with this approximation. We propose two diagnostic algorithms to alleviate this problem. The Pareto-smoothed importance sampling (PSIS) diagnostic gives a goodness of fit measurement for joint distributions, while simultaneously improving the error in the estimate. The variational simulation-based calibration (VSBC) assesses the average performance of point estimates.",http://proceedings.mlr.press/v80/yao18a.html,http://proceedings.mlr.press/v80/yao18a/yao18a.pdf,ICML
880,2018,Comparison-Based Random Forests,"Siavash Haghiri,         Damien Garreau,         Ulrike Luxburg","Assume we are given a set of items from a general metric space, but we neither have access to the representation of the data nor to the distances between data points. Instead, suppose that we can actively choose a triplet of items (A, B, C) and ask an oracle whether item A is closer to item B or to item C. In this paper, we propose a novel random forest algorithm for regression and classification that relies only on such triplet comparisons. In the theory part of this paper, we establish sufficient conditions for the consistency of such a forest. In a set of comprehensive experiments, we then demonstrate that the proposed random forest is efficient both for classification and regression. In particular, it is even competitive with other methods that have direct access to the metric representation of the data.",http://proceedings.mlr.press/v80/haghiri18a.html,http://proceedings.mlr.press/v80/haghiri18a/haghiri18a.pdf,ICML
881,2018,Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning,"Thomas Dietterich,         George Trimponias,         Zhitang Chen",Exogenous state variables and rewards can slow down reinforcement learning by injecting uncontrolled variation into the reward signal. We formalize exogenous state variables and rewards and identify conditions under which an MDP with exogenous state can be decomposed into an exogenous Markov Reward Process involving only the exogenous state+reward and an endogenous Markov Decision Process defined with respect to only the endogenous rewards. We also derive a variance-covariance condition under which Monte Carlo policy evaluation on the endogenous MDP is accelerated compared to using the full MDP. Similar speedups are likely to carry over to all RL algorithms. We develop two algorithms for discovering the exogenous variables and test them on several MDPs. Results show that the algorithms are practical and can significantly speed up reinforcement learning.,http://proceedings.mlr.press/v80/dietterich18a.html,http://proceedings.mlr.press/v80/dietterich18a/dietterich18a.pdf,ICML
882,2018,A Progressive Batching L-BFGS Method for Machine Learning,"Raghu Bollapragada,         Jorge Nocedal,         Dheevatsa Mudigere,         Hao-Jun Shi,         Ping Tak Peter Tang","The standard L-BFGS method relies on gradient approximations that are not dominated by noise, so that search directions are descent directions, the line search is reliable, and quasi-Newton updating yields useful quadratic models of the objective function. All of this appears to call for a full batch approach, but since small batch sizes give rise to faster algorithms with better generalization properties, L-BFGS is currently not considered an algorithm of choice for large-scale machine learning applications. One need not, however, choose between the two extremes represented by the full batch or highly stochastic regimes, and may instead follow a progressive batching approach in which the sample size increases during the course of the optimization. In this paper, we present a new version of the L-BFGS algorithm that combines three basic components - progressive batching, a stochastic line search, and stable quasi-Newton updating - and that performs well on training logistic regression and deep neural networks. We provide supporting convergence theory for the method.",http://proceedings.mlr.press/v80/bollapragada18a.html,http://proceedings.mlr.press/v80/bollapragada18a/bollapragada18a.pdf,ICML
883,2018,SADAGRAD: Strongly Adaptive Stochastic Gradient Methods,"Zaiyi Chen,         Yi Xu,         Enhong Chen,         Tianbao Yang","Although the convergence rates of existing variants of ADAGRAD have a better dependence on the number of iterations under the strong convexity condition, their iteration complexities have a explicitly linear dependence on the dimensionality of the problem. To alleviate this bad dependence, we propose a simple yet novel variant of ADAGRAD for stochastic (weakly) strongly convex optimization. Different from existing variants, the proposed variant (referred to as SADAGRAD) uses an adaptive restarting scheme in which (i) ADAGRAD serves as a sub-routine and is restarted periodically; (ii) the number of iterations for restarting ADAGRAD depends on the history of learning that incorporates knowledge of the geometry of the data. In addition to the adaptive proximal functions and adaptive number of iterations for restarting, we also develop a variant that is adaptive to the (implicit) strong convexity from the data, which together makes the proposed algorithm strongly adaptive. In terms of iteration complexity, in the worst case SADAGRAD has an O(1/\epsilon) for finding an \epsilon-optimal solution similar to other variants. However, it could enjoy faster convergence and much better dependence on the problem’s dimensionality when stochastic gradients are sparse. Extensive experiments on large-scale data sets demonstrate the efficiency of the proposed algorithms in comparison with several variants of ADAGRAD and stochastic gradient method.",http://proceedings.mlr.press/v80/chen18m.html,http://proceedings.mlr.press/v80/chen18m/chen18m.pdf,ICML
884,2018,Accelerated Spectral Ranking,"Arpit Agarwal,         Prathamesh Patil,         Shivani Agarwal","The problem of rank aggregation from pairwise and multiway comparisons has a wide range of implications, ranging from recommendation systems to sports rankings to social choice. Some of the most popular algorithms for this problem come from the class of spectral ranking algorithms; these include the rank centrality (RC) algorithm for pairwise comparisons, which returns consistent estimates under the Bradley-Terry-Luce (BTL) model for pairwise comparisons (Negahban et al., 2017), and its generalization, the Luce spectral ranking (LSR) algorithm, which returns consistent estimates under the more general multinomial logit (MNL) model for multiway comparisons (Maystre & Grossglauser, 2015). In this paper, we design a provably faster spectral ranking algorithm, which we call accelerated spectral ranking (ASR), that is also consistent under the MNL/BTL models. Our accelerated algorithm is achieved by designing a random walk that has a faster mixing time than the random walks associated with previous algorithms. In addition to a faster algorithm, our results yield improved sample complexity bounds for recovery of the MNL/BTL parameters: to the best of our knowledge, we give the first general sample complexity bounds for recovering the parameters of the MNL model from multiway comparisons under any (connected) comparison graph (and improve significantly over previous bounds for the BTL model for pairwise comparisons). We also give a message-passing interpretation of our algorithm, which suggests a decentralized distributed implementation. Our experiments on several real-world and synthetic datasets confirm that our new ASR algorithm is indeed orders of magnitude faster than existing algorithms.",http://proceedings.mlr.press/v80/agarwal18b.html,http://proceedings.mlr.press/v80/agarwal18b/agarwal18b.pdf,ICML
885,2018,Dynamic Regret of Strongly Adaptive Methods,"Lijun Zhang,         Tianbao Yang,          jin,         Zhi-Hua Zhou","To cope with changing environments, recent developments in online learning have introduced the concepts of adaptive regret and dynamic regret independently. In this paper, we illustrate an intrinsic connection between these two concepts by showing that the dynamic regret can be expressed in terms of the adaptive regret and the functional variation. This observation implies that strongly adaptive algorithms can be directly leveraged to minimize the dynamic regret. As a result, we present a series of strongly adaptive algorithms that have small dynamic regrets for convex functions, exponentially concave functions, and strongly convex functions, respectively. To the best of our knowledge, this is the first time that exponential concavity is utilized to upper bound the dynamic regret. Moreover, all of those adaptive algorithms do not need any prior knowledge of the functional variation, which is a significant advantage over previous specialized methods for minimizing dynamic regret.",http://proceedings.mlr.press/v80/zhang18o.html,http://proceedings.mlr.press/v80/zhang18o/zhang18o.pdf,ICML
886,2018,Active Testing: An Efficient and Robust Framework for Estimating Accuracy,"Phuc Nguyen,         Deva Ramanan,         Charless Fowlkes","Much recent work on large-scale visual recogni- tion aims to scale up learning to massive, noisily- annotated datasets. We address the problem of scaling-up the evaluation of such models to large- scale datasets with noisy labels. Current protocols for doing so require a human user to either vet (re-annotate) a small fraction of the testset and ignore the rest, or else correct errors in annotation as they are found through manual inspection of results. In this work, we re-formulate the problem as one of active testing, and examine strategies for efficiently querying a user so as to obtain an accurate performance estimate with minimal vet- ting. We demonstrate the effectiveness of our proposed active testing framework on estimating two performance metrics, Precision@K and mean Average Precisions, for two popular Computer Vi- sion tasks, multilabel classification and instance segmentation, respectively. We further show that our approach is able to siginificantly save human annotation effort and more robust than alterna- tive evaluation protocols.",http://proceedings.mlr.press/v80/nguyen18d.html,http://proceedings.mlr.press/v80/nguyen18d/nguyen18d.pdf,ICML
887,2018,Hierarchical Multi-Label Classification Networks,"Jonatas Wehrmann,         Ricardo Cerri,         Rodrigo Barros","One of the most challenging machine learning problems is a particular case of data classification in which classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time. This task is known as hierarchical multi-label classification (HMC), with applications in text classification, image annotation, and in bioinformatics problems such as protein function prediction. In this paper, we propose novel neural network architectures for HMC called HMCN, capable of simultaneously optimizing local and global loss functions for discovering local hierarchical class-relationships and global information from the entire class hierarchy while penalizing hierarchical violations. We evaluate its performance in 21 datasets from four distinct domains, and we compare it against the current HMC state-of-the-art approaches. Results show that HMCN substantially outperforms all baselines with statistical significance, arising as the novel state-of-the-art for HMC.",http://proceedings.mlr.press/v80/wehrmann18a.html,http://proceedings.mlr.press/v80/wehrmann18a/wehrmann18a.pdf,ICML
888,2018,The Generalization Error of Dictionary Learning with Moreau Envelopes,Alexandros Georgogiannis,"This is a theoretical study on the sample complexity of dictionary learning with a general type of reconstruction loss. The goal is to estimate a m×dm \times d matrix DD of unit-norm columns when the only available information is a set of training samples. Points xx in Rm\mathbb{R}^m are subsequently approximated by the linear combination DaDa after solving the problem mina∈RdΦ(x−Da)+g(a)\min_{a \in \mathbb{R}^d}  \Phi(x - Da) + g(a); function g:Rd→[0,+∞)g:\mathbb{R}^d \to [0,+\infty) is either an indicator function or a sparsity promoting regularizer. Here is considered the case where Φ(x)=infz∈Rm||x−z||22+h(||z||2) \Phi(x) = \inf_{z \in \mathbb{R}^m} { ||x-z||_2^2 + h(||z||_2)} and hh is an even and univariate function on the real line. Connections are drawn between Φ\Phi and the Moreau envelope of hh. A new sample complexity result concerning the kk-sparse dictionary problem removes the spurious condition on the coherence of DD appearing in previous works. Finally, comments are made on the approximation error of certain families of losses. The derived generalization bounds are of order O(√logn/n)\mathcal{O}(\sqrt{\log n /n}) and valid without any further restrictions on the set of dictionaries with unit-norm columns.",http://proceedings.mlr.press/v80/georgogiannis18a.html,http://proceedings.mlr.press/v80/georgogiannis18a/georgogiannis18a.pdf,ICML
889,2018,Stochastic PCA with ℓ2ℓ2\ell_2 and ℓ1ℓ1\ell_1 Regularization,"Poorya Mianjy,         Raman Arora","We revisit convex relaxation based methods for stochastic optimization of principal component analysis (PCA). While methods that directly solve the nonconvex problem have been shown to be optimal in terms of statistical and computational efficiency, the methods based on convex relaxation have been shown to enjoy comparable, or even superior, empirical performance – this motivates the need for a deeper formal understanding of the latter. Therefore, in this paper, we study variants of stochastic gradient descent for a convex relaxation of PCA with (a) ℓ2ℓ2\ell_2, (b) ℓ1ℓ1\ell_1, and (c) elastic net (ℓ1+ℓ2)ℓ1+ℓ2)\ell_1+\ell_2) regularization in the hope that these variants yield (a) better iteration complexity, (b) better control on the rank of the intermediate iterates, and (c) both, respectively. We show, theoretically and empirically, that compared to previous work on convex relaxation based methods, the proposed variants yield faster convergence and improve overall runtime to achieve a certain user-specified ϵϵ\epsilon-suboptimality on the PCA objective. Furthermore, the proposed methods are shown to converge both in terms of the PCA objective as well as the distance between subspaces. However, there still remains a gap in computational requirements for the proposed methods when compared with existing nonconvex approaches.",http://proceedings.mlr.press/v80/mianjy18a.html,http://proceedings.mlr.press/v80/mianjy18a/mianjy18a.pdf,ICML
890,2018,QuantTree: Histograms for Change Detection in Multivariate Data Streams,"Giacomo Boracchi,         Diego Carrera,         Cristiano Cervellera,         Danilo Macciò","We address the problem of detecting distribution changes in multivariate data streams by means of histograms. Histograms are very general and flexible models, which have been relatively ignored in the change-detection literature as they often require a number of bins that grows unfeasibly with the data dimension. We present QuantTree, a recursive binary splitting scheme that adaptively defines the histogram bins to ease the detection of any distribution change. Our design scheme implies that i) we can easily control the overall number of bins and ii) the bin probabilities do not depend on the distribution of stationary data. This latter is a very relevant aspect in change detection, since thresholds of tests statistics based on these histograms (e.g., the Pearson statistic or the total variation) can be numerically computed from univariate and synthetically generated data, yet guaranteeing a controlled false positive rate. Our experiments show that the proposed histograms are very effective in detecting changes in high dimensional data streams, and that the resulting thresholds can effectively control the false positive rate, even when the number of training samples is relatively small.",http://proceedings.mlr.press/v80/boracchi18a.html,http://proceedings.mlr.press/v80/boracchi18a/boracchi18a.pdf,ICML
891,2018,Towards Black-box Iterative Machine Teaching,"Weiyang Liu,         Bo Dai,         Xingguo Li,         Zhen Liu,         James Rehg,         Le Song","In this paper, we make an important step towards the black-box machine teaching by considering the cross-space machine teaching, where the teacher and the learner use different feature representations and the teacher can not fully observe the learner’s model. In such scenario, we study how the teacher is still able to teach the learner to achieve faster convergence rate than the traditional passive learning. We propose an active teacher model that can actively query the learner (i.e., make the learner take exams) for estimating the learner’s status and provably guide the learner to achieve faster convergence. The sample complexities for both teaching and query are provided. In the experiments, we compare the proposed active teacher with the omniscient teacher and verify the effectiveness of the active teacher model.",http://proceedings.mlr.press/v80/liu18b.html,http://proceedings.mlr.press/v80/liu18b/liu18b.pdf,ICML
892,2018,Learning unknown ODE models with Gaussian processes,"Markus Heinonen,         Cagatay Yildiz,         Henrik Mannerström,         Jukka Intosalmi,         Harri Lähdesmäki","In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics. In these settings, parametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge. We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism. We demonstrate the model’s capabilities to infer dynamics from sparse data and to simulate the system forward into future.",http://proceedings.mlr.press/v80/heinonen18a.html,http://proceedings.mlr.press/v80/heinonen18a/heinonen18a.pdf,ICML
893,2018,Universal Planning Networks: Learning Generalizable Representations for Visuomotor Control,"Aravind Srinivas,         Allan Jabri,         Pieter Abbeel,         Sergey Levine,         Chelsea Finn","A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities. Visit https://sites.google. com/view/upn-public/home for video highlights.",http://proceedings.mlr.press/v80/srinivas18b.html,http://proceedings.mlr.press/v80/srinivas18b/srinivas18b.pdf,ICML
894,2018,Detecting non-causal artifacts in multivariate linear regression models,"Dominik Janzing,         Bernhard Schölkopf","We consider linear models where d potential causes X_1,...,X_d are correlated with one target quantity Y and propose a method to infer whether the association is causal or whether it is an artifact caused by overfitting or hidden common causes. We employ the idea that in the former case the vector of regression coefficients has ‘generic’ orientation relative to the covariance matrix Sigma_{XX} of X. Using an ICA based model for confounding, we show that both confounding and overfitting yield regression vectors that concentrate mainly in the space of low eigenvalues of Sigma_{XX}.",http://proceedings.mlr.press/v80/janzing18a.html,http://proceedings.mlr.press/v80/janzing18a/janzing18a.pdf,ICML
895,2018,Pathwise Derivatives Beyond the Reparameterization Trick,"Martin Jankowiak,         Fritz Obermeyer","We observe that gradients computed via the reparameterization trick are in direct correspondence with solutions of the transport equation in the formalism of optimal transport. We use this perspective to compute (approximate) pathwise gradients for probability distributions not directly amenable to the reparameterization trick: Gamma, Beta, and Dirichlet. We further observe that when the reparameterization trick is applied to the Cholesky-factorized multivariate Normal distribution, the resulting gradients are suboptimal in the sense of optimal transport. We derive the optimal gradients and show that they have reduced variance in a Gaussian Process regression task. We demonstrate with a variety of synthetic experiments and stochastic variational inference tasks that our pathwise gradients are competitive with other methods.",http://proceedings.mlr.press/v80/jankowiak18a.html,http://proceedings.mlr.press/v80/jankowiak18a/jankowiak18a.pdf,ICML
896,2018,Visualizing and Understanding Atari Agents,"Samuel Greydanus,         Anurag Koul,         Jonathan Dodge,         Alan Fern","While deep reinforcement learning (deep RL) agents are effective at maximizing rewards, it is often unclear what strategies they use to do so. In this paper, we take a step toward explaining deep RL agents through a case study using Atari 2600 environments. In particular, we focus on using saliency maps to understand how an agent learns and executes a policy. We introduce a method for generating useful saliency maps and use it to show 1) what strong agents attend to, 2) whether agents are making decisions for the right or wrong reasons, and 3) how agents evolve during learning. We also test our method on non-expert human subjects and find that it improves their ability to reason about these agents. Overall, our results show that saliency information can provide significant insight into an RL agent’s decisions and learning behavior.",http://proceedings.mlr.press/v80/greydanus18a.html,http://proceedings.mlr.press/v80/greydanus18a/greydanus18a.pdf,ICML
897,2018,Hierarchical Imitation and Reinforcement Learning,"Hoang Le,         Nan Jiang,         Alekh Agarwal,         Miroslav Dudik,         Yisong Yue,         Hal Daumé","We study how to effectively leverage expert feedback to learn sequential decision-making policies. We focus on problems with sparse rewards and long time horizons, which typically pose significant challenges in reinforcement learning. We propose an algorithmic framework, called hierarchical guidance, that leverages the hierarchical structure of the underlying problem to integrate different modes of expert interaction. Our framework can incorporate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels, leading to dramatic reductions in both expert effort and cost of exploration. Using long-horizon benchmarks, including Montezuma’s Revenge, we demonstrate that our approach can learn significantly faster than hierarchical RL, and be significantly more label-efficient than standard IL. We also theoretically analyze labeling cost for certain instantiations of our framework.",http://proceedings.mlr.press/v80/le18a.html,http://proceedings.mlr.press/v80/le18a/le18a.pdf,ICML
898,2018,Constant-Time Predictive Distributions for Gaussian Processes,"Geoff Pleiss,         Jacob Gardner,         Kilian Weinberger,         Andrew Gordon Wilson","One of the most compelling features of Gaussian process (GP) regression is its ability to provide well-calibrated posterior distributions. Recent advances in inducing point methods have sped up GP marginal likelihood and posterior mean computations, leaving posterior covariance estimation and sampling as the remaining computational bottlenecks. In this paper we address these shortcomings by using the Lanczos algorithm to rapidly approximate the predictive covariance matrix. Our approach, which we refer to as LOVE (LanczOs Variance Estimates), substantially improves time and space complexity. In our experiments, LOVE computes covariances up to 2,000 times faster and draws samples 18,000 times faster than existing methods, all without sacrificing accuracy.",http://proceedings.mlr.press/v80/pleiss18a.html,http://proceedings.mlr.press/v80/pleiss18a/pleiss18a.pdf,ICML
899,2018,RadialGAN: Leveraging multiple datasets to improve target-specific predictive models using Generative Adversarial Networks,"Jinsung Yoon,         James Jordon,         Mihaela Schaar","Training complex machine learning models for prediction often requires a large amount of data that is not always readily available. Leveraging these external datasets from related but different sources is therefore an important task if good predictive models are to be built for deployment in settings where data can be rare. In this paper we propose a novel approach to the problem in which we use multiple GAN architectures to learn to translate from one dataset to another, thereby allowing us to effectively enlarge the target dataset, and therefore learn better predictive models than if we simply used the target dataset. We show the utility of such an approach, demonstrating that our method improves the prediction performance on the target domain over using just the target dataset and also show that our framework outperforms several other benchmarks on a collection of real-world medical datasets.",http://proceedings.mlr.press/v80/yoon18b.html,http://proceedings.mlr.press/v80/yoon18b/yoon18b.pdf,ICML
900,2018,Large-Scale Sparse Inverse Covariance Estimation via Thresholding and Max-Det Matrix Completion,"Richard Zhang,         Salar Fattahi,         Somayeh Sojoudi","The sparse inverse covariance estimation problem is commonly solved using an ℓ1ℓ1\ell_{1}-regularized Gaussian maximum likelihood estimator known as “graphical lasso”, but its computational cost becomes prohibitive for large data sets. A recently line of results showed{–}under mild assumptions{–}that the graphical lasso estimator can be retrieved by soft-thresholding the sample covariance matrix and solving a maximum determinant matrix completion (MDMC) problem. This paper proves an extension of this result, and describes a Newton-CG algorithm to efficiently solve the MDMC problem. Assuming that the thresholded sample covariance matrix is sparse with a sparse Cholesky factorization, we prove that the algorithm converges to an ϵϵ\epsilon-accurate solution in O(nlog(1/ϵ))O(nlog⁡(1/ϵ))O(n\log(1/\epsilon)) time and O(n)O(n)O(n) memory. The algorithm is highly efficient in practice: we solve the associated MDMC problems with as many as 200,000 variables to 7-9 digits of accuracy in less than an hour on a standard laptop computer running MATLAB.",http://proceedings.mlr.press/v80/zhang18c.html,http://proceedings.mlr.press/v80/zhang18c/zhang18c.pdf,ICML
901,2018,Measuring abstract reasoning in neural networks,"David Barrett,         Felix Hill,         Adam Santoro,         Ari Morcos,         Timothy Lillicrap","Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation ’regimes’ in which the training data and test questions differ in clearly-defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model’s ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.",http://proceedings.mlr.press/v80/barrett18a.html,http://proceedings.mlr.press/v80/barrett18a/barrett18a.pdf,ICML
902,2018,SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation,"Bo Dai,         Albert Shaw,         Lihong Li,         Lin Xiao,         Niao He,         Zhen Liu,         Jianshu Chen,         Le Song","When function approximation is used, solving the Bellman optimality equation with stability guarantees has remained a major open problem in reinforcement learning for decades. The fundamental difficulty is that the Bellman operator may become an expansion in general, resulting in oscillating and even divergent behavior of popular algorithms like Q-learning. In this paper, we revisit the Bellman equation, and reformulate it into a novel primal-dual optimization problem using Nesterov’s smoothing technique and the Legendre-Fenchel transformation. We then develop a new algorithm, called Smoothed Bellman Error Embedding, to solve this optimization problem where any differentiable function class may be used. We provide what we believe to be the first convergence guarantee for general nonlinear function approximation, and analyze the algorithm’s sample complexity. Empirically, our algorithm compares favorably to state-of-the-art baselines in several benchmark control problems.",http://proceedings.mlr.press/v80/dai18c.html,http://proceedings.mlr.press/v80/dai18c/dai18c.pdf,ICML
903,2018,Gradient Descent Learns One-hidden-layer CNN: Don’t be Afraid of Spurious Local Minima,"Simon Du,         Jason Lee,         Yuandong Tian,         Aarti Singh,         Barnabas Poczos","We consider the problem of learning an one-hidden-layer neural network with non-overlapping convolutional layer and ReLU activation function, i.e., f(Z;w,a)=∑jajσ(w⊤Zj)f(Z;w,a)=∑jajσ(w⊤Zj)f(Z; w, a) = \sum_j a_j\sigma(w^\top Z_j), in which both the convolutional weights www and the output weights aaa are parameters to be learned. We prove that with Gaussian input ZZ\mathbf{Z} there is a spurious local minimizer. Surprisingly, in the presence of the spurious local minimizer, starting from randomly initialized weights, gradient descent with weight normalization can still be proven to recover the true parameters with constant probability (which can be boosted to probability 111 with multiple restarts). We also show that with constant probability, the same procedure could also converge to the spurious local minimum, showing that the local minimum plays a non-trivial role in the dynamics of gradient descent. Furthermore, a quantitative analysis shows that the gradient descent dynamics has two phases: it starts off slow, but converges much faster after several iterations.",http://proceedings.mlr.press/v80/du18b.html,http://proceedings.mlr.press/v80/du18b/du18b.pdf,ICML
904,2018,Variational Inference and Model Selection with Generalized Evidence Bounds,"Liqun Chen,         Chenyang Tao,         Ruiyi Zhang,         Ricardo Henao,         Lawrence Carin Duke","Recent advances on the scalability and flexibility of variational inference have made it successful at unravelling hidden patterns in complex data. In this work we propose a new variational bound formulation, yielding an estimator that extends beyond the conventional variational bound. It naturally subsumes the importance-weighted and Renyi bounds as special cases, and it is provably sharper than these counterparts. We also present an improved estimator for variational learning, and advocate a novel high signal-to-variance ratio update rule for the variational parameters. We discuss model-selection issues associated with existing evidence-lower-bound-based variational inference procedures, and show how to leverage the flexibility of our new formulation to address them. Empirical evidence is provided to validate our claims.",http://proceedings.mlr.press/v80/chen18k.html,http://proceedings.mlr.press/v80/chen18k/chen18k.pdf,ICML
905,2018,Deep Predictive Coding Network for Object Recognition,"Haiguang Wen,         Kuan Han,         Junxing Shi,         Yizhen Zhang,         Eugenio Culurciello,         Zhongming Liu","Based on the predictive coding theory in neuro- science, we designed a bi-directional and recur- rent neural net, namely deep predictive coding networks (PCN), that has feedforward, feedback, and recurrent connections. Feedback connections from a higher layer carry the prediction of its lower-layer representation; feedforward connec- tions carry the prediction errors to its higher-layer. Given image input, PCN runs recursive cycles of bottom-up and top-down computation to update its internal representations and reduce the differ- ence between bottom-up input and top-down pre- diction at every layer. After multiple cycles of recursive updating, the representation is used for image classification. With benchmark datasets (CIFAR-10/100, SVHN, and MNIST), PCN was found to always outperform its feedforward-only counterpart: a model without any mechanism for recurrent dynamics, and its performance tended to improve given more cycles of computation over time. In short, PCN reuses a single architecture to recursively run bottom-up and top-down pro- cesses to refine its representation towards more accurate and definitive object recognition.",http://proceedings.mlr.press/v80/wen18a.html,http://proceedings.mlr.press/v80/wen18a/wen18a.pdf,ICML
906,2018,Randomized Block Cubic Newton Method,"Nikita Doikov,         Peter Richtarik,         University Edinburgh","We study the problem of minimizing the sum of three convex functions: a differentiable, twice-differentiable and a non-smooth term in a high dimensional setting. To this effect we propose and analyze a randomized block cubic Newton (RBCN) method, which in each iteration builds a model of the objective function formed as the sum of the natural models of its three components: a linear model with a quadratic regularizer for the differentiable term, a quadratic model with a cubic regularizer for the twice differentiable term, and perfect (proximal) model for the nonsmooth term. Our method in each iteration minimizes the model over a random subset of blocks of the search variable. RBCN is the first algorithm with these properties, generalizing several existing methods, matching the best known bounds in all special cases. We establish O(1/ϵ)O(1/ϵ){\cal O}(1/\epsilon), O(1/ϵ√)O(1/ϵ){\cal O}(1/\sqrt{\epsilon}) and O(log(1/ϵ))O(log⁡(1/ϵ)){\cal O}(\log (1/\epsilon)) rates under different assumptions on the component functions. Lastly, we show numerically that our method outperforms the state-of-the-art on a variety of machine learning problems, including cubically regularized least-squares, logistic regression with constraints, and Poisson regression.",http://proceedings.mlr.press/v80/doikov18a.html,http://proceedings.mlr.press/v80/doikov18a/doikov18a.pdf,ICML
907,2018,Improving the Privacy and Accuracy of ADMM-Based Distributed Algorithms,"Xueru Zhang,         Mohammad Mahdi Khalili,         Mingyan Liu","Alternating direction method of multiplier (ADMM) is a popular method used to design distributed versions of a machine learning algorithm, whereby local computations are performed on local data with the output exchanged among neighbors in an iterative fashion. During this iterative process the leakage of data privacy arises. A differentially private ADMM was proposed in prior work (Zhang & Zhu, 2017) where only the privacy loss of a single node during one iteration was bounded, a method that makes it difficult to balance the tradeoff between the utility attained through distributed computation and privacy guarantees when considering the total privacy loss of all nodes over the entire iterative process. We propose a perturbation method for ADMM where the perturbed term is correlated with the penalty parameters; this is shown to improve the utility and privacy simultaneously. The method is based on a modified ADMM where each node independently determines its own penalty parameter in every iteration and decouples it from the dual updating step size. The condition for convergence of the modified ADMM and the lower bound on the convergence rate are also derived.",http://proceedings.mlr.press/v80/zhang18f.html,http://proceedings.mlr.press/v80/zhang18f/zhang18f.pdf,ICML
908,2018,Representation Tradeoffs for Hyperbolic Embeddings,"Frederic Sala,         Chris De Sa,         Albert Gu,         Christopher Re","Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures. We give a combinatorial construction that embeds trees into hyperbolic space with arbitrarily low distortion without optimization. On WordNet, this algorithm obtains a mean-average-precision of 0.989 with only two dimensions, outperforming existing work by 0.11 points. We provide bounds characterizing the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that enables us to reduce dimensionality. Finally, we extract lessons from the algorithms and theory above to design a scalable PyTorch-based implementation that can handle incomplete information.",http://proceedings.mlr.press/v80/sala18a.html,http://proceedings.mlr.press/v80/sala18a/sala18a.pdf,ICML
909,2018,Towards Fast Computation of Certified Robustness for ReLU Networks,"Lily Weng,         Huan Zhang,         Hongge Chen,         Zhao Song,         Cho-Jui Hsieh,         Luca Daniel,         Duane Boning,         Inderjit Dhillon","Verifying the robustness property of a general Rectified Linear Unit (ReLU) network is an NP-complete problem. Although finding the exact minimum adversarial distortion is hard, giving a certified lower bound of the minimum distortion is possible. Current available methods of computing such a bound are either time-consuming or deliver low quality bounds that are too loose to be useful. In this paper, we exploit the special structure of ReLU networks and provide two computationally efficient algorithms (Fast-Lin, Fast-Lip) that are able to certify non-trivial lower bounds of minimum adversarial distortions. Experiments show that (1) our methods deliver bounds close to (the gap is 2-3X) exact minimum distortions found by Reluplex in small networks while our algorithms are more than 10,000 times faster; (2) our methods deliver similar quality of bounds (the gap is within 35% and usually around 10%; sometimes our bounds are even better) for larger networks compared to the methods based on solving linear programming problems but our algorithms are 33-14,000 times faster; (3) our method is capable of solving large MNIST and CIFAR networks up to 7 layers with more than 10,000 neurons within tens of seconds on a single CPU core. In addition, we show that there is no polynomial time algorithm that can approximately find the minimum ℓ1ℓ1\ell_1 adversarial distortion of a ReLU network with a 0.99lnn0.99ln⁡n0.99\ln n approximation ratio unless NP=P, where nnn is the number of neurons in the network.",http://proceedings.mlr.press/v80/weng18a.html,http://proceedings.mlr.press/v80/weng18a/weng18a.pdf,ICML
910,2018,Approximation Guarantees for Adaptive Sampling,"Eric Balkanski,         Yaron Singer","In this paper we analyze an adaptive sampling approach for submodular maximization. Adaptive sampling is a technique that has recently been shown to achieve a constant factor approximation guarantee for submodular maximization under a cardinality constraint with exponentially fewer adaptive rounds than any previously studied constant factor approximation algorithm for this problem. Adaptivity quantifies the number of sequential rounds that an algorithm makes when function evaluations can be executed in parallel and is the parallel running time of an algorithm, up to low order terms. Adaptive sampling achieves its exponential speedup at the expense of approximation. In theory, it is guaranteed to produce a solution that is a 1/3 approximation to the optimum. Nevertheless, experiments show that adaptive sampling techniques achieve far better values in practice. In this paper we provide theoretical justification for this phenomenon. In particular, we show that under very mild conditions of curvature of a function, adaptive sampling techniques achieve an approximation arbitrarily close to 1/2 while maintaining their low adaptivity. Furthermore, we show that the approximation ratio approaches 1 in direct relationship to a homogeneity property of the submodular function. In addition, we conduct experiments on real data sets in which the curvature and homogeneity properties can be easily manipulated and demonstrate the relationship between approximation and curvature, as well as the effectiveness of adaptive sampling in practice.",http://proceedings.mlr.press/v80/balkanski18a.html,http://proceedings.mlr.press/v80/balkanski18a/balkanski18a.pdf,ICML
911,2018,Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval and Matrix Completion,"Cong Ma,         Kaizheng Wang,         Yuejie Chi,         Yuxin Chen","Recent years have seen a flurry of activities in designing provably efficient nonconvex optimization procedures for solving statistical estimation problems. For various problems like phase retrieval or low-rank matrix completion, state-of-the-art nonconvex procedures require proper regularization (e.g. trimming, regularized cost, projection) in order to guarantee fast convergence. When it comes to vanilla procedures such as gradient descent, however, prior theory either recommends highly conservative learning rates to avoid overshooting, or completely lacks performance guarantees. This paper uncovers a striking phenomenon in several nonconvex problems: even in the absence of explicit regularization, gradient descent follows a trajectory staying within a basin that enjoys nice geometry, consisting of points incoherent with the sampling mechanism. This “implicit regularization” feature allows gradient descent to proceed in a far more aggressive fashion without overshooting, which in turn results in substantial computational savings. Focusing on two statistical estimation problems, i.e. solving random quadratic systems of equations and low-rank matrix completion, we establish that gradient descent achieves near-optimal statistical and computational guarantees without explicit regularization. As a byproduct, for noisy matrix completion, we demonstrate that gradient descent enables optimal control of both entrywise and spectral-norm errors.",http://proceedings.mlr.press/v80/ma18c.html,http://proceedings.mlr.press/v80/ma18c/ma18c.pdf,ICML
912,2018,Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced Aggregation of Sparsely Interacting Workers,"Yao Ma,         Alexander Olshevsky,         Csaba Szepesvari,         Venkatesh Saligrama","We consider worker skill estimation for the single coin Dawid-Skene crowdsourcing model. In practice skill-estimation is challenging because worker assignments are sparse and irregular due to the arbitrary, and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlation-matrix completion problem, where the observed components correspond to observed label correlation between workers. We show that the correlation matrix can be successfully recovered and skills identifiable if and only if the sampling matrix (observed components) is irreducible and aperiodic. We then propose an efficient gradient descent scheme and show that skill estimates converges to the desired global optima for such sampling matrices. Our proof is original and the results are surprising in light of the fact that even the weighted rank-one matrix factorization problem is NP hard in general. Next we derive sample complexity bounds for the noisy case in terms of spectral properties of the signless Laplacian of the sampling matrix. Our proposed scheme achieves state-of-art performance on a number of real-world datasets.",http://proceedings.mlr.press/v80/ma18b.html,http://proceedings.mlr.press/v80/ma18b/ma18b.pdf,ICML
913,2018,Mixed batches and symmetric discriminators for GAN training,"Thomas LUCAS,         Corentin Tallec,         Yann Ollivier,         Jakob Verbeek","Generative adversarial networks (GANs) are pow- erful generative models based on providing feed- back to a generative network via a discriminator network. However, the discriminator usually as- sesses individual samples. This prevents the dis- criminator from accessing global distributional statistics of generated samples, and often leads to mode dropping: the generator models only part of the target distribution. We propose to feed the discriminator with mixed batches of true and fake samples, and train it to predict the ratio of true samples in the batch. The latter score does not depend on the order of samples in a batch. Rather than learning this invariance, we introduce a generic permutation-invariant discriminator ar- chitecture. This architecture is provably a uni- versal approximator of all symmetric functions. Experimentally, our approach reduces mode col- lapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.",http://proceedings.mlr.press/v80/lucas18a.html,http://proceedings.mlr.press/v80/lucas18a/lucas18a.pdf,ICML
914,2018,Understanding and Simplifying One-Shot Architecture Search,"Gabriel Bender,         Pieter-Jan Kindermans,         Barret Zoph,         Vijay Vasudevan,         Quoc Le","There is growing interest in automating neural network architecture design. Existing architecture search methods can be computationally expensive, requiring thousands of different architectures to be trained from scratch. Recent work has explored weight sharing across models to amortize the cost of training. Although previous methods reduced the cost of architecture search by orders of magnitude, they remain complex, requiring hypernetworks or reinforcement learning controllers. We aim to understand weight sharing for one-shot architecture search. With careful experimental analysis, we show that it is possible to efficiently identify promising architectures from a complex search space without either hypernetworks or RL.",http://proceedings.mlr.press/v80/bender18a.html,http://proceedings.mlr.press/v80/bender18a/bender18a.pdf,ICML
915,2018,Geometry Score: A Method For Comparing Generative Adversarial Networks,"Valentin Khrulkov,         Ivan Oseledets","One of the biggest challenges in the research of generative adversarial networks (GANs) is assessing the quality of generated samples and detecting various levels of mode collapse. In this work, we construct a novel measure of performance of a GAN by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and quantitative means for evaluation. Our algorithm can be applied to datasets of an arbitrary nature and is not limited to visual data. We test the obtained metric on various real-life models and datasets and demonstrate that our method provides new insights into properties of GANs.",http://proceedings.mlr.press/v80/khrulkov18a.html,http://proceedings.mlr.press/v80/khrulkov18a/khrulkov18a.pdf,ICML
916,2018,Inference Suboptimality in Variational Autoencoders,"Chris Cremer,         Xuechen Li,         David Duvenaud","Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.",http://proceedings.mlr.press/v80/cremer18a.html,http://proceedings.mlr.press/v80/cremer18a/cremer18a.pdf,ICML
917,2018,Learning a Mixture of Two Multinomial Logits,"Flavio Chierichetti,         Ravi Kumar,         Andrew Tomkins","The classical Multinomial Logit (MNL) is a behavioral model for user choice. In this model, a user is offered a slate of choices (a subset of a finite universe of nnn items), and selects exactly one item from the slate, each with probability proportional to its (positive) weight. Given a set of observed slates and choices, the likelihood-maximizing item weights are easy to learn at scale, and easy to interpret. However, the model fails to represent common real-world behavior. As a result, researchers in user choice often turn to mixtures of MNLs, which are known to approximate a large class of models of rational user behavior. Unfortunately, the only known algorithms for this problem have been heuristic in nature. In this paper we give the first polynomial-time algorithms for exact learning of uniform mixtures of two MNLs. Interestingly, the parameters of the model can be learned for any nnn by sampling the behavior of random users only on slates of sizes 2 and 3; in contrast, we show that slates of size 2 are insufficient by themselves.",http://proceedings.mlr.press/v80/chierichetti18a.html,http://proceedings.mlr.press/v80/chierichetti18a/chierichetti18a.pdf,ICML
918,2018,Spline Filters For End-to-End Deep Learning,"Randall Balestriero,         Romain Cosentino,         Herve Glotin,         Richard Baraniuk","We propose to tackle the problem of end-to-end learning for raw waveform signals by introducing learnable continuous time-frequency atoms. The derivation of these filters is achieved by defining a functional space with a given smoothness order and boundary conditions. From this space, we derive the parametric analytical filters. Their differentiability property allows gradient-based optimization. As such, one can utilize any Deep Neural Network (DNN) with these filters. This enables us to tackle in a front-end fashion a large scale bird detection task based on the freefield1010 dataset known to contain key challenges, such as the dimensionality of the inputs data (>100,000>100,000) and the presence of additional noises: multiple sources and soundscapes.",http://proceedings.mlr.press/v80/balestriero18a.html,http://proceedings.mlr.press/v80/balestriero18a/balestriero18a.pdf,ICML
919,2018,Computational Optimal Transport: Complexity by Accelerated Gradient Descent Is Better Than by Sinkhorn’s Algorithm,"Pavel Dvurechensky,         Alexander Gasnikov,         Alexey Kroshnin","We analyze two algorithms for approximating the general optimal transport (OT) distance between two discrete distributions of size nnn, up to accuracy εε\varepsilon. For the first algorithm, which is based on the celebrated Sinkhorn’s algorithm, we prove the complexity bound ˜O(n2ε2)O˜(n2ε2)\widetilde{O}\left(\frac{n^2}{\varepsilon^2}\right) arithmetic operations (˜OO˜\widetilde{O} hides polylogarithmic factors (lnn)c(lnn)c(\ln n)^c, c>0c>0c>0). For the second one, which is based on our novel Adaptive Primal-Dual Accelerated Gradient Descent (APDAGD) algorithm, we prove the complexity bound ˜O(min{n9/4ε,n2ε2})O˜(min{n9/4ε,n2ε2})\widetilde{O}\left(\min\left\{\frac{n^{9/4}}{\varepsilon}, \frac{n^{2}}{\varepsilon^2} \right\}\right) arithmetic operations. Both bounds have better dependence on εε\varepsilon than the state-of-the-art result given by ˜O(n2ε3)O˜(n2ε3)\widetilde{O}\left(\frac{n^2}{\varepsilon^3}\right). Our second algorithm not only has better dependence on εε\varepsilon in the complexity bound, but also is not specific to entropic regularization and can solve the OT problem with different regularizers.",http://proceedings.mlr.press/v80/dvurechensky18a.html,http://proceedings.mlr.press/v80/dvurechensky18a/dvurechensky18a.pdf,ICML
920,2018,Learning Implicit Generative Models with the Method of Learned Moments,"Suman Ravuri,         Shakir Mohamed,         Mihaela Rosca,         Oriol Vinyals","We propose a method of moments (MoM) algorithm for training large-scale implicit generative models. Moment estimation in this setting encounters two problems: it is often difficult to define the millions of moments needed to learn the model parameters, and it is hard to determine which properties are useful when specifying moments. To address the first issue, we introduce a moment network, and define the moments as the network’s hidden units and the gradient of the network’s output with respect to its parameters. To tackle the second problem, we use asymptotic theory to highlight desiderata for moments – namely they should minimize the asymptotic variance of estimated model parameters – and introduce an objective to learn better moments. The sequence of objectives created by this Method of Learned Moments (MoLM) can train high-quality neural image samplers. On CIFAR-10, we demonstrate that MoLM-trained generators achieve significantly higher Inception Scores and lower Frechet Inception Distances than those trained with gradient penalty-regularized and spectrally-normalized adversarial objectives. These generators also achieve nearly perfect Multi-Scale Structural Similarity Scores on CelebA, and can create high-quality samples of 128x128 images.",http://proceedings.mlr.press/v80/ravuri18a.html,http://proceedings.mlr.press/v80/ravuri18a/ravuri18a.pdf,ICML
921,2018,Learning Registered Point Processes from Idiosyncratic Observations,"Hongteng Xu,         Lawrence Carin,         Hongyuan Zha","A parametric point process model is developed, with modeling based on the assumption that sequential observations often share latent phenomena, while also possessing idiosyncratic effects. An alternating optimization method is proposed to learn a “registered” point process that accounts for shared structure, as well as “warping” functions that characterize idiosyncratic aspects of each observed sequence. Under reasonable constraints, in each iteration we update the sample-specific warping functions by solving a set of constrained nonlinear programming problems in parallel, and update the model by maximum likelihood estimation. The justifiability, complexity and robustness of the proposed method are investigated in detail, and the influence of sequence stitching on the learning results is examined empirically. Experiments on both synthetic and real-world data demonstrate that the method yields explainable point process models, achieving encouraging results compared to state-of-the-art methods.",http://proceedings.mlr.press/v80/xu18b.html,http://proceedings.mlr.press/v80/xu18b/xu18b.pdf,ICML
922,2018,The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning,"Siyuan Ma,         Raef Bassily,         Mikhail Belkin","In this paper we aim to formally explain the phenomenon of fast convergence of Stochastic Gradient Descent (SGD) observed in modern machine learning. The key observation is that most modern learning architectures are over-parametrized and are trained to interpolate the data by driving the empirical loss (classification and regression) close to zero. While it is still unclear why these interpolated solutions perform well on test data, we show that these regimes allow for fast convergence of SGD, comparable in number of iterations to full gradient descent. For convex loss functions we obtain an exponential convergence bound for mini-batch SGD parallel to that for full gradient descent. We show that there is a critical batch size m∗m∗m^* such that: (a) SGD iteration with mini-batch size m≤m∗m≤m∗m\leq m^* is nearly equivalent to mmm iterations of mini-batch size 111 (linear scaling regime). (b) SGD iteration with mini-batch m>m∗m>m∗m> m^* is nearly equivalent to a full gradient descent iteration (saturation regime). Moreover, for the quadratic loss, we derive explicit expressions for the optimal mini-batch and step size and explicitly characterize the two regimes above. The critical mini-batch size can be viewed as the limit for effective mini-batch parallelization. It is also nearly independent of the data size, implying O(n)O(n)O(n) acceleration over GD per unit of computation. We give experimental evidence on real data which closely follows our theoretical analyses. Finally, we show how our results fit in the recent developments in training deep neural networks and discuss connections to adaptive rates for SGD and variance reduction.",http://proceedings.mlr.press/v80/ma18a.html,http://proceedings.mlr.press/v80/ma18a/ma18a.pdf,ICML
923,2018,Dynamic Evaluation of Neural Sequence Models,"Ben Krause,         Emmanuel Kahembwe,         Iain Murray,         Steve Renals","We explore dynamic evaluation, where sequence models are adapted to the recent sequence history using gradient descent, assigning higher probabilities to re-occurring sequential patterns. We develop a dynamic evaluation approach that outperforms existing adaptation approaches in our comparisons. We apply dynamic evaluation to outperform all previous word-level perplexities on the Penn Treebank and WikiText-2 datasets (achieving 51.1 and 44.3 respectively) and all previous character-level cross-entropies on the text8 and Hutter Prize datasets (achieving 1.19 bits/char and 1.08 bits/char respectively).",http://proceedings.mlr.press/v80/krause18a.html,http://proceedings.mlr.press/v80/krause18a/krause18a.pdf,ICML
924,2018,Temporal Poisson Square Root Graphical Models,"Sinong Geng,         Zhaobin Kuang,         Peggy Peissig,         David Page","We propose temporal Poisson square root graphical models (TPSQRs), a generalization of Poisson square root graphical models (PSQRs) specifically designed for modeling longitudinal event data. By estimating the temporal relationships for all possible pairs of event types, TPSQRs can offer a holistic perspective about whether the occurrences of any given event type could excite or inhibit any other type. A TPSQR is learned by estimating a collection of interrelated PSQRs that share the same template parameterization. These PSQRs are estimated jointly in a pseudo-likelihood fashion, where Poisson pseudo-likelihood is used to approximate the original more computationally intensive pseudo-likelihood problem stemming from PSQRs. Theoretically, we demonstrate that under mild assumptions, the Poisson pseudolikelihood approximation is sparsistent for recovering the underlying PSQR. Empirically, we learn TPSQRs from a real-world large-scale electronic health record (EHR) with millions of drug prescription and condition diagnosis events, for adverse drug reaction (ADR) detection. Experimental results demonstrate that the learned TPSQRs can recover ADR signals from the EHR effectively and efficiently.",http://proceedings.mlr.press/v80/geng18a.html,http://proceedings.mlr.press/v80/geng18a/geng18a.pdf,ICML
925,2018,Progress & Compress: A scalable framework for continual learning,"Jonathan Schwarz,         Wojciech Czarnecki,         Jelena Luketina,         Agnieszka Grabska-Barwinska,         Yee Whye Teh,         Razvan Pascanu,         Raia Hadsell","We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress & compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.",http://proceedings.mlr.press/v80/schwarz18a.html,http://proceedings.mlr.press/v80/schwarz18a/schwarz18a.pdf,ICML
926,2018,Scalable Bilinear Pi Learning Using State and Action Features,"Yichen Chen,         Lihong Li,         Mengdi Wang","Approximate linear programming (ALP) represents one of the major algorithmic families to solve large-scale Markov decision processes (MDP). In this work, we study a primal-dual formulation of the ALP, and develop a scalable, model-free algorithm called bilinear ππ\pi learning for reinforcement learning when a sampling oracle is provided. This algorithm enjoys a number of advantages. First, it adopts linear and bilinear models to represent the high-dimensional value function and state-action distributions, respectively, using given state and action features. Its run-time complexity depends on the number of features, not the size of the underlying MDPs. Second, it operates in a fully online fashion without having to store any sample, thus having minimal memory footprint. Third, we prove that it is sample-efficient, solving for the optimal policy to high precision with a sample complexity linear in the dimension of the parameter space.",http://proceedings.mlr.press/v80/chen18e.html,http://proceedings.mlr.press/v80/chen18e/chen18e.pdf,ICML
927,2018,Kernel Recursive ABC: Point Estimation with Intractable Likelihood,"Takafumi Kajihara,         Motonobu Kanagawa,         Keisuke Yamazaki,         Kenji Fukumizu","We propose a novel approach to parameter estimation for simulator-based statistical models with intractable likelihood. Our proposed method involves recursive application of kernel ABC and kernel herding to the same observed data. We provide a theoretical explanation regarding why the approach works, showing (for the population setting) that, under a certain assumption, point estimates obtained with this method converge to the true parameter, as recursion proceeds. We have conducted a variety of numerical experiments, including parameter estimation for a real-world pedestrian flow simulator, and show that in most cases our method outperforms existing approaches.",http://proceedings.mlr.press/v80/kajihara18a.html,http://proceedings.mlr.press/v80/kajihara18a/kajihara18a.pdf,ICML
928,2018,SGD and Hogwild! Convergence Without the Bounded Gradients Assumption,"Lam Nguyen,         PHUONG HA NGUYEN,         Marten Dijk,         Peter Richtarik,         Katya Scheinberg,         Martin Takac","Stochastic gradient descent (SGD) is the optimization algorithm of choice in many machine learning applications such as regularized empirical risk minimization and training deep neural networks. The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is always violated for cases where the objective function is strongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. Here we show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime, which results in more relaxed conditions than those in (Bottou et al.,2016). We then move on the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime, obtaining the first convergence results for this method in the case of diminished learning rate.",http://proceedings.mlr.press/v80/nguyen18c.html,http://proceedings.mlr.press/v80/nguyen18c/nguyen18c.pdf,ICML
929,2018,Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples,"Gail Weiss,         Yoav Goldberg,         Eran Yahav","We present a novel algorithm that uses exact learning and abstraction to extract a deterministic finite automaton describing the state dynamics of a given trained RNN. We do this using Angluin’s \lstar algorithm as a learner and the trained RNN as an oracle. Our technique efficiently extracts accurate automata from trained RNNs, even when the state vectors are large and require fine differentiation.",http://proceedings.mlr.press/v80/weiss18a.html,http://proceedings.mlr.press/v80/weiss18a/weiss18a.pdf,ICML
930,2018,On Acceleration with Noise-Corrupted Gradients,"Michael Cohen,         Jelena Diakonikolas,         Lorenzo Orecchia","Accelerated algorithms have broad applications in large-scale optimization, due to their generality and fast convergence. However, their stability in the practical setting of noise-corrupted gradient oracles is not well-understood. This paper provides two main technical contributions: (i) a new accelerated method AGDP that generalizes Nesterov’s AGD and improves on the recent method AXGD (Diakonikolas & Orecchia, 2018), and (ii) a theoretical study of accelerated algorithms under noisy and inexact gradient oracles, which is supported by numerical experiments. This study leverages the simplicity of AGDP and its analysis to clarify the interaction between noise and acceleration and to suggest modifications to the algorithm that reduce the mean and variance of the error incurred due to the gradient noise.",http://proceedings.mlr.press/v80/cohen18a.html,http://proceedings.mlr.press/v80/cohen18a/cohen18a.pdf,ICML
931,2018,Budgeted Experiment Design for Causal Structure Learning,"AmirEmad Ghassami,         Saber Salehkaleybar,         Negar Kiyavash,         Elias Bareinboim","We study the problem of causal structure learning when the experimenter is limited to perform at most kk non-adaptive experiments of size 11. We formulate the problem of finding the best intervention target set as an optimization problem, which aims to maximize the average number of edges whose directions are resolved. We prove that the corresponding objective function is submodular and a greedy algorithm suffices to achieve (1−1e)(1-\frac{1}{e})-approximation of the optimal value. We further present an accelerated variant of the greedy algorithm, which can lead to orders of magnitude performance speedup. We validate our proposed approach on synthetic and real graphs. The results show that compared to the purely observational setting, our algorithm orients the majority of the edges through a considerably small number of interventions.",http://proceedings.mlr.press/v80/ghassami18a.html,http://proceedings.mlr.press/v80/ghassami18a/ghassami18a.pdf,ICML
932,2018,Machine Theory of Mind,"Neil Rabinowitz,         Frank Perbet,         Francis Song,         Chiyuan Zhang,         S. M. Ali Eslami,         Matthew Botvinick","Theory of mind (ToM) broadly refers to humans’ ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network {–} a ToMnet {–} which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents’ future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents’ characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the ""Sally-Anne"" test of recognising that others can hold false beliefs about the world.",http://proceedings.mlr.press/v80/rabinowitz18a.html,http://proceedings.mlr.press/v80/rabinowitz18a/rabinowitz18a.pdf,ICML
933,2018,Parallel and Streaming Algorithms for K-Core Decomposition,"Hossein Esfandiari,         Silvio Lattanzi,         Vahab Mirrokni","The k-core decomposition is a fundamental primitive in many machine learning and data mining applications. We present the first distributed and the first streaming algorithms to compute and maintain an approximate k-core decomposition with provable guarantees. Our algorithms achieve rigorous bounds on space complexity while bounding the number of passes or number of rounds of computation. We do so by presenting a new powerful sketching technique for k-core decomposition, and then by showing it can be computed efficiently in both streaming and MapReduce models. Finally, we confirm the effectiveness of our sketching technique empirically on a number of publicly available graphs.",http://proceedings.mlr.press/v80/esfandiari18a.html,http://proceedings.mlr.press/v80/esfandiari18a/esfandiari18a.pdf,ICML
934,2018,MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels,"Lu Jiang,         Zhengyuan Zhou,         Thomas Leung,         Li-Jia Li,         Li Fei-Fei","Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels.",http://proceedings.mlr.press/v80/jiang18c.html,http://proceedings.mlr.press/v80/jiang18c/jiang18c.pdf,ICML
935,2018,Constraining the Dynamics of Deep Probabilistic Models,"Marco Lorenzi,         Maurizio Filippone","We introduce a novel generative formulation of deep probabilistic models implementing ""soft"" constraints on their function dynamics. In particular, we develop a flexible methodological framework where the modeled functions and derivatives of a given order are subject to inequality or equality constraints. We then characterize the posterior distribution over model and constraint parameters through stochastic variational inference. As a result, the proposed approach allows for accurate and scalable uncertainty quantification on the predictions and on all parameters. We demonstrate the application of equality constraints in the challenging problem of parameter inference in ordinary differential equation models, while we showcase the application of inequality constraints on the problem of monotonic regression of count data. The proposed approach is extensively tested in several experimental settings, leading to highly competitive results in challenging modeling applications, while offering high expressiveness, flexibility and scalability.",http://proceedings.mlr.press/v80/lorenzi18a.html,http://proceedings.mlr.press/v80/lorenzi18a/lorenzi18a.pdf,ICML
936,2018,Parallel WaveNet: Fast High-Fidelity Speech Synthesis,"Aaron Oord,         Yazhe Li,         Igor Babuschkin,         Karen Simonyan,         Oriol Vinyals,         Koray Kavukcuoglu,         George Driessche,         Edward Lockhart,         Luis Cobo,         Florian Stimberg,         Norman Casagrande,         Dominik Grewe,         Seb Noury,         Sander Dieleman,         Erich Elsen,         Nal Kalchbrenner,         Heiga Zen,         Alex Graves,         Helen King,         Tom Walters,         Dan Belov,         Demis Hassabis","The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today’s massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, a 1000x speed up relative to the original WaveNet, and capable of serving multiple English and Japanese voices in a production setting.",http://proceedings.mlr.press/v80/oord18a.html,http://proceedings.mlr.press/v80/oord18a/oord18a.pdf,ICML
937,2018,GAIN: Missing Data Imputation using Generative Adversarial Nets,"Jinsung Yoon,         James Jordon,         Mihaela Schaar","We propose a novel method for imputing missing data by adapting the well-known Generative Adversarial Nets (GAN) framework. Accordingly, we call our method Generative Adversarial Imputation Nets (GAIN). The generator (G) observes some components of a real data vector, imputes the missing components conditioned on what is actually observed, and outputs a completed vector. The discriminator (D) then takes a completed vector and attempts to determine which components were actually observed and which were imputed. To ensure that D forces G to learn the desired distribution, we provide D with some additional information in the form of a hint vector. The hint reveals to D partial information about the missingness of the original sample, which is used by D to focus its attention on the imputation quality of particular components. This hint ensures that G does in fact learn to generate according to the true data distribution. We tested our method on various datasets and found that GAIN significantly outperforms state-of-the-art imputation methods.",http://proceedings.mlr.press/v80/yoon18a.html,http://proceedings.mlr.press/v80/yoon18a/yoon18a.pdf,ICML
938,2018,Blind Justice: Fairness with Encrypted Sensitive Attributes,"Niki Kilbertus,         Adria Gascon,         Matt Kusner,         Michael Veale,         Krishna Gummadi,         Adrian Weller","Recent work has explored how to train machine learning models which do not discriminate against any subgroup of the population as determined by sensitive attributes such as gender or race. To avoid disparate treatment, sensitive attributes should not be considered. On the other hand, in order to avoid disparate impact, sensitive attributes must be examined, e.g., in order to learn a fair model, or to check if a given model is fair. We introduce methods from secure multi-party computation which allow us to avoid both. By encrypting sensitive attributes, we show how an outcome-based fair model may be learned, checked, or have its outputs verified and held to account, without users revealing their sensitive attributes.",http://proceedings.mlr.press/v80/kilbertus18a.html,http://proceedings.mlr.press/v80/kilbertus18a/kilbertus18a.pdf,ICML
939,2018,Adversarial Time-to-Event Modeling,"Paidamoyo Chapfuwa,         Chenyang Tao,         Chunyuan Li,         Courtney Page,         Benjamin Goldstein,         Lawrence Carin Duke,         Ricardo Henao","Modern health data science applications leverage abundant molecular and electronic health data, providing opportunities for machine learning to build statistical models to support clinical practice. Time-to-event analysis, also called survival analysis, stands as one of the most representative examples of such statistical models. We present a deep-network-based approach that leverages adversarial learning to address a key challenge in modern time-to-event modeling: nonparametric estimation of event-time distributions. We also introduce a principled cost function to exploit information from censored events (events that occur subsequent to the observation window). Unlike most time-to-event models, we focus on the estimation of time-to-event distributions, rather than time ordering. We validate our model on both benchmark and real datasets, demonstrating that the proposed formulation yields significant performance gains relative to a parametric alternative, which we also propose.",http://proceedings.mlr.press/v80/chapfuwa18a.html,http://proceedings.mlr.press/v80/chapfuwa18a/chapfuwa18a.pdf,ICML
940,2018,Composite Marginal Likelihood Methods for Random Utility Models,"Zhibing Zhao,         Lirong Xia","We propose a novel and flexible rank-breaking-then-composite-marginal-likelihood (RBCML) framework for learning random utility models (RUMs), which include the Plackett-Luce model. We characterize conditions for the objective function of RBCML to be strictly log-concave by proving that strict log-concavity is preserved under convolution and marginalization. We characterize necessary and sufficient conditions for RBCML to satisfy consistency and asymptotic normality. Experiments on synthetic data show that RBCML for Gaussian RUMs achieves better statistical efficiency and computation efficiency than the state-of-the-art algorithm and our RBCML for the Plackett-Luce model provides flexible tradeoffs between running time and statistical efficiency.",http://proceedings.mlr.press/v80/zhao18d.html,http://proceedings.mlr.press/v80/zhao18d/zhao18d.pdf,ICML
941,2018,An Alternative View: When Does SGD Escape Local Minima?,"Bobby Kleinberg,         Yuanzhi Li,         Yang Yuan","Stochastic gradient descent (SGD) is widely used in machine learning. Although being commonly viewed as a fast but not accurate version of gradient descent (GD), it always finds better solutions than GD for modern neural networks. In order to understand this phenomenon, we take an alternative view that SGD is working on the convolved (thus smoothed) version of the loss function. We show that, even if the function ff has many bad local minima or saddle points, as long as for every point xx, the weighted average of the gradients of its neighborhoods is one point convex with respect to the desired solution x∗x^*, SGD will get close to, and then stay around x∗x^* with constant probability. Our result identifies a set of functions that SGD provably works, which is much larger than the set of convex functions. Empirically, we observe that the loss surface of neural networks enjoys nice one point convexity properties locally, therefore our theorem helps explain why SGD works so well for neural networks.",http://proceedings.mlr.press/v80/kleinberg18a.html,http://proceedings.mlr.press/v80/kleinberg18a/kleinberg18a.pdf,ICML
942,2018,Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization properties of Entropy-SGD and data-dependent priors,"Gintare Karolina Dziugaite,         Daniel Roy","We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we rely on a result showing that data-dependent priors obtained by stochastic gradient Langevin dynamics (SGLD) yield valid PAC-Bayes bounds provided the target distribution of SGLD is eps-differentially private. We observe that test error on MNIST and CIFAR10 falls within the (empirically nonvacuous) risk bounds computed under the assumption that SGLD reaches stationarity. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.",http://proceedings.mlr.press/v80/dziugaite18a.html,http://proceedings.mlr.press/v80/dziugaite18a/dziugaite18a.pdf,ICML
943,2018,An Algorithmic Framework of Variable Metric Over-Relaxed Hybrid Proximal Extra-Gradient Method,"Li Shen,         Peng Sun,         Yitong Wang,         Wei Liu,         Tong Zhang","We propose a novel algorithmic framework of Variable Metric Over-Relaxed Hybrid Proximal Extra-gradient (VMOR-HPE) method with a global convergence guarantee for the maximal monotone operator inclusion problem. Its iteration complexities and local linear convergence rate are provided, which theoretically demonstrate that a large over-relaxed step-size contributes to accelerating the proposed VMOR-HPE as a byproduct. Specifically, we find that a large class of primal and primal-dual operator splitting algorithms are all special cases of VMOR-HPE. Hence, the proposed framework offers a new insight into these operator splitting algorithms. In addition, we apply VMOR-HPE to the Karush-Kuhn-Tucker (KKT) generalized equation of linear equality constrained multi-block composite convex optimization, yielding a new algorithm, namely nonsymmetric Proximal Alternating Direction Method of Multipliers with a preconditioned Extra-gradient step in which the preconditioned metric is generated by a blockwise Barzilai-Borwein line search technique (PADMM-EBB). We also establish iteration complexities of PADMM-EBB in terms of the KKT residual. Finally, we apply PADMM-EBB to handle the nonnegative dual graph regularized low-rank representation problem. Promising results on synthetic and real datasets corroborate the efficacy of PADMM-EBB.",http://proceedings.mlr.press/v80/shen18b.html,http://proceedings.mlr.press/v80/shen18b/shen18b.pdf,ICML
944,2018,A Spline Theory of Deep Learning,"Randall Balestriero,          baraniuk","We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion. As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.",http://proceedings.mlr.press/v80/balestriero18b.html,http://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf,ICML
945,2018,Adversarial Learning with Local Coordinate Coding,"Jiezhang Cao,         Yong Guo,         Qingyao Wu,         Chunhua Shen,         Junzhou Huang,         Mingkui Tan","Generative adversarial networks (GANs) aim to generate realistic data from some prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data, which, however, is hard to be used for sampling in GANs. In this paper, rather than sampling from the pre-defined prior distribution, we propose a Local Coordinate Coding (LCC) based sampling method to improve GANs. We derive a generalization bound for LCC based GANs and prove that a small dimensional input is sufficient to achieve good generalization. Extensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method.",http://proceedings.mlr.press/v80/cao18a.html,http://proceedings.mlr.press/v80/cao18a/cao18a.pdf,ICML
946,2018,signSGD: Compressed Optimisation for Non-Convex Problems,"Jeremy Bernstein,         Yu-Xiang Wang,         Kamyar Azizzadenesheli,         Animashree Anandkumar","Training large neural networks requires distributing learning across multiple workers, where the cost of communicating gradients can be a significant bottleneck. signSGD alleviates this problem by transmitting just the sign of each minibatch stochastic gradient. We prove that it can get the best of both worlds: compressed gradients and SGD-level convergence rate. The relative ℓ1/ℓ2ℓ1/ℓ2\ell_1/\ell_2 geometry of gradients, noise and curvature informs whether signSGD or SGD is theoretically better suited to a particular problem. On the practical side we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models. We extend our theory to the distributed setting, where the parameter server uses majority vote to aggregate gradient signs from each worker enabling 1-bit compression of worker-server communication in both directions. Using a theorem by Gauss we prove that majority vote can achieve the same reduction in variance as full precision distributed SGD. Thus, there is great promise for sign-based optimisation schemes to achieve fast communication and fast convergence. Code to reproduce experiments is to be found at https://github.com/jxbz/signSGD.",http://proceedings.mlr.press/v80/bernstein18a.html,http://proceedings.mlr.press/v80/bernstein18a/bernstein18a.pdf,ICML
947,2018,Learning Memory Access Patterns,"Milad Hashemi,         Kevin Swersky,         Jamie Smith,         Grant Ayers,         Heiner Litz,         Jichuan Chang,         Christos Kozyrakis,         Parthasarathy Ranganathan","The explosion in workload complexity and the recent slow-down in Moore’s law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations; augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.",http://proceedings.mlr.press/v80/hashemi18a.html,http://proceedings.mlr.press/v80/hashemi18a/hashemi18a.pdf,ICML
948,2018,Locally Private Hypothesis Testing,Or Sheffet,"We initiate the study of differentially private hypothesis testing in the local-model, under both the standard (symmetric) randomized-response mechanism (Warner 1965, Kasiviswanathan et al, 2008) and the newer (non-symmetric) mechanisms (Bassily & Smith, 2015, Bassily et al, 2017). First, we study the general framework of mapping each user’s type into a signal and show that the problem of finding the maximum-likelihood distribution over the signals is feasible. Then we discuss the randomized-response mechanism and show that, in essence, it maps the null- and alternative-hypotheses onto new sets, an affine translation of the original sets. We then give sample complexity bounds for identity and independence testing under randomized-response. We then move to the newer non-symmetric mechanisms and show that there too the problem of finding the maximum-likelihood distribution is feasible. Under the mechanism of Bassily et al we give identity and independence testers with better sample complexity than the testers in the symmetric case, and we also propose a χ2\chi^2-based identity tester which we investigate empirically.",http://proceedings.mlr.press/v80/sheffet18a.html,http://proceedings.mlr.press/v80/sheffet18a/sheffet18a.pdf,ICML
949,2018,Orthogonal Recurrent Neural Networks with Scaled Cayley Transform,"Kyle Helfrich,         Devin Willmott,         Qiang Ye","Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients. Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Memory networks (LSTMs). We propose a simpler and novel update scheme to maintain orthogonal recurrent weight matrices without using complex valued matrices. This is done by parametrizing with a skew-symmetric matrix using the Cayley transform; such a parametrization is unable to represent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones. The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs.",http://proceedings.mlr.press/v80/helfrich18a.html,http://proceedings.mlr.press/v80/helfrich18a/helfrich18a.pdf,ICML
950,2018,Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks,"Brenden Lake,         Marco Baroni","Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb ""dax,"" he or she can immediately understand the meaning of ""dax twice"" or ""sing and dax."" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply ""mix-and-match"" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the ""dax"" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.",http://proceedings.mlr.press/v80/lake18a.html,http://proceedings.mlr.press/v80/lake18a/lake18a.pdf,ICML
951,2018,Delayed Impact of Fair Machine Learning,"Lydia T. Liu,         Sarah Dean,         Esther Rolf,         Max Simchowitz,         Moritz Hardt","Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.",http://proceedings.mlr.press/v80/liu18c.html,http://proceedings.mlr.press/v80/liu18c/liu18c.pdf,ICML
952,2018,Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training,"Xi Wu,         Uyeong Jang,         Jiefeng Chen,         Lingjiao Chen,         Somesh Jha","In this paper we study leveraging confidence information induced by adversarial training to reinforce adversarial robustness of a given adversarially trained model. A natural measure of confidence is ‖F(x)‖∞\|F(x)\|_\infty (i.e. how confident FF is about its prediction?). We start by analyzing an adversarial training formulation proposed by Madry et al.. We demonstrate that, under a variety of instantiations, an only somewhat good solution to their objective induces confidence to be a discriminator, which can distinguish between right and wrong model predictions in a neighborhood of a point sampled from the underlying distribution. Based on this, we propose Highly Confident Near Neighbor (HCNN) a framework that combines confidence information and nearest neighbor search, to reinforce adversarial robustness of a base model. We give algorithms in this framework and perform a detailed empirical study. We report encouraging experimental results that support our analysis, and also discuss problems we observed with existing adversarial training.",http://proceedings.mlr.press/v80/wu18e.html,http://proceedings.mlr.press/v80/wu18e/wu18e.pdf,ICML
953,2018,Inter and Intra Topic Structure Learning with Word Embeddings,"He Zhao,         Lan Du,         Wray Buntine,         Mingyuan Zhou","One important task of topic modeling for text analysis is interpretability. By discovering structured topics one is able to yield improved interpretability as well as modeling accuracy. In this paper, we propose a novel topic model with a deep structure that explores both inter-topic and intra-topic structures informed by word embeddings. Specifically, our model discovers inter topic structures in the form of topic hierarchies and discovers intra topic structures in the form of sub-topics, each of which is informed by word embeddings and captures a fine-grained thematic aspect of a normal topic. Extensive experiments demonstrate that our model achieves the state-of-the-art performance in terms of perplexity, document classification, and topic quality. Moreover, with topic hierarchies and sub-topics, the topics discovered in our model are more interpretable, providing an illuminating means to understand text data.",http://proceedings.mlr.press/v80/zhao18a.html,http://proceedings.mlr.press/v80/zhao18a/zhao18a.pdf,ICML
954,2018,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,"Judy Hoffman,         Eric Tzeng,         Taesung Park,         Jun-Yan Zhu,         Phillip Isola,         Kate Saenko,         Alexei Efros,         Trevor Darrell","Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space methods are difficult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment. Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a specific discriminatively trained task and avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.",http://proceedings.mlr.press/v80/hoffman18a.html,http://proceedings.mlr.press/v80/hoffman18a/hoffman18a.pdf,ICML
955,2018,"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks","Lechao Xiao,         Yasaman Bahri,         Jascha Sohl-Dickstein,         Samuel Schoenholz,         Jeffrey Pennington","In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures.",http://proceedings.mlr.press/v80/xiao18a.html,http://proceedings.mlr.press/v80/xiao18a/xiao18a.pdf,ICML
956,2018,Improved large-scale graph learning through ridge spectral sparsification,"Daniele Calandriello,         Alessandro Lazaric,         Ioannis Koutis,         Michal Valko","The representation and learning benefits of methods based on graph Laplacians, such as Laplacian smoothing or harmonic function solution for semi-supervised learning (SSL), are empirically and theoretically well supported. Nonetheless, the exact versions of these methods scale poorly with the number of nodes nnn of the graph. In this paper, we combine a spectral sparsification routine with Laplacian learning. Given a graph GGG as input, our algorithm computes a sparsifier in a distributed way in O(nlog3(n))O(nlog3⁡(n))O(n\log^3(n)) time, O(mlog3(n))O(mlog3⁡(n))O(m\log^3(n)) work and O(nlog(n))O(nlog⁡(n))O(n\log(n)) memory, using only log(n)log⁡(n)\log(n) rounds of communication. Furthermore, motivated by the regularization often employed in learning algorithms, we show that constructing sparsifiers that preserve the spectrum of the Laplacian only up to the regularization level may drastically reduce the size of the final graph. By constructing a spectrally-similar graph, we are able to bound the error induced by the sparsification for a variety of downstream tasks (e.g., SSL). We empirically validate the theoretical guarantees on Amazon co-purchase graph and compare to the state-of-the-art heuristics.",http://proceedings.mlr.press/v80/calandriello18a.html,http://proceedings.mlr.press/v80/calandriello18a/calandriello18a.pdf,ICML
957,2018,Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry,"Maximillian Nickel,         Douwe Kiela","We are concerned with the discovery of hierarchical relationships from large-scale unstructured similarity scores. For this purpose, we study different models of hyperbolic space and find that learning embeddings in the Lorentz model is substantially more efficient than in the Poincar{é}-ball model. We show that the proposed approach allows us to learn high-quality embeddings of large taxonomies which yield improvements over Poincar{é} embeddings, especially in low dimensions. Lastly, we apply our model to discover hierarchies in two real-world datasets: we show that an embedding in hyperbolic space can reveal important aspects of a company’s organizational structure as well as reveal historical relationships between language families.",http://proceedings.mlr.press/v80/nickel18a.html,http://proceedings.mlr.press/v80/nickel18a/nickel18a.pdf,ICML
958,2018,BOCK : Bayesian Optimization with Cylindrical Kernels,"ChangYong Oh,         Efstratios Gavves,         Max Welling","A major challenge in Bayesian Optimization is the boundary issue where an algorithm spends too many evaluations near the boundary of its search space. In this paper, we propose BOCK, Bayesian Optimization with Cylindrical Kernels, whose basic idea is to transform the ball geometry of the search space using a cylindrical transformation. Because of the transformed geometry, the Gaussian Process-based surrogate model spends less budget searching near the boundary, while concentrating its efforts relatively more near the center of the search region, where we expect the solution to be located. We evaluate BOCK extensively, showing that it is not only more accurate and efficient, but it also scales successfully to problems with a dimensionality as high as 500. We show that the better accuracy and scalability of BOCK even allows optimizing modestly sized neural network layers, as well as neural network hyperparameters.",http://proceedings.mlr.press/v80/oh18a.html,http://proceedings.mlr.press/v80/oh18a/oh18a.pdf,ICML
959,2018,Finding Influential Training Samples for Gradient Boosted Decision Trees,"Boris Sharchilev,         Yury Ustinovskiy,         Pavel Serdyukov,         Maarten Rijke","We address the problem of finding influential training samples for a particular case of tree ensemble-based models, e.g., Random Forest (RF) or Gradient Boosted Decision Trees (GBDT). A natural way of formalizing this problem is studying how the model’s predictions change upon leave-one-out retraining, leaving out each individual training sample. Recent work has shown that, for parametric models, this analysis can be conducted in a computationally efficient way. We propose several ways of extending this framework to non-parametric GBDT ensembles under the assumption that tree structures remain fixed. Furthermore, we introduce a general scheme of obtaining further approximations to our method that balance the trade-off between performance and computational complexity. We evaluate our approaches on various experimental setups and use-case scenarios and demonstrate both the quality of our approach to finding influential training samples in comparison to the baselines and its computational efficiency.",http://proceedings.mlr.press/v80/sharchilev18a.html,http://proceedings.mlr.press/v80/sharchilev18a/sharchilev18a.pdf,ICML
960,2018,Solving Partial Assignment Problems using Random Clique Complexes,"Charu Sharma,         Deepak Nathani,         Manohar Kaul","We present an alternate formulation of the partial assignment problem as matching random clique complexes, that are higher-order analogues of random graphs, designed to provide a set of invariants that better detect higher-order structure. The proposed method creates random clique adjacency matrices for each k-skeleton of the random clique complexes and matches them, taking into account each point as the affine combination of its geometric neighborhood. We justify our solution theoretically, by analyzing the runtime and storage complexity of our algorithm along with the asymptotic behavior of the quadratic assignment problem (QAP) that is associated with the underlying random clique adjacency matrices. Experiments on both synthetic and real-world datasets, containing severe occlusions and distortions, provide insight into the accuracy, efficiency, and robustness of our approach. We outperform diverse matching algorithms by a significant margin.",http://proceedings.mlr.press/v80/sharma18a.html,http://proceedings.mlr.press/v80/sharma18a/sharma18a.pdf,ICML
961,2018,Mean Field Multi-Agent Reinforcement Learning,"Yaodong Yang,         Rui Luo,         Minne Li,         Ming Zhou,         Weinan Zhang,         Jun Wang","Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent’s optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field approaches. In addition, we report the first result to solve the Ising model via model-free reinforcement learning methods.",http://proceedings.mlr.press/v80/yang18d.html,http://proceedings.mlr.press/v80/yang18d/yang18d.pdf,ICML
962,2018,Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV),"Been Kim,         Martin Wattenberg,         Justin Gilmer,         Carrie Cai,         James Wexler,         Fernanda Viegas,         Rory sayres","The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.",http://proceedings.mlr.press/v80/kim18d.html,http://proceedings.mlr.press/v80/kim18d/kim18d.pdf,ICML
963,2018,Approximate Leave-One-Out for Fast Parameter Tuning in High Dimensions,"Shuaiwen Wang,         Wenda Zhou,         Haihao Lu,         Arian Maleki,         Vahab Mirrokni","We study the parameter tuning problem for the penalized regression model. Finding the optimal choice of the regularization parameter is a challenging problem in high-dimensional regimes where both the number of observations n and the number of parameters p are large. We propose two frameworks to obtain a computationally efficient approximation ALO of the leave-one-out cross validation (LOOCV) risk for nonsmooth losses and regularizers. Our two frameworks are based on the primal and dual formulations of the penalized regression model. We prove the equivalence of the two approaches under smoothness conditions. This equivalence enables us to justify the accuracy of both methods under such conditions. We use our approaches to obtain a risk estimate for several standard problems, including generalized LASSO, nuclear norm regularization and support vector machines. We experimentally demonstrate the effectiveness of our results for non-differentiable cases.",http://proceedings.mlr.press/v80/wang18m.html,http://proceedings.mlr.press/v80/wang18m/wang18m.pdf,ICML
964,2018,Efficient Neural Audio Synthesis,"Nal Kalchbrenner,         Erich Elsen,         Karen Simonyan,         Seb Noury,         Norman Casagrande,         Edward Lockhart,         Florian Stimberg,         Aaron Oord,         Sander Dieleman,         Koray Kavukcuoglu","Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating desired samples. Efficient sampling for this class of models at the cost of little to no loss in quality has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24 kHz 16-bit audio 4 times faster than real time on a GPU. Secondly, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds past sparsity levels of more than 96%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile phone CPU in real time. Finally, we describe a new dependency scheme for sampling that lets us trade a constant number of non-local, distant dependencies for the ability to generate samples in batches. The Batch WaveRNN produces 8 samples per step without loss of quality and offers orthogonal ways of further increasing sampling efficiency.",http://proceedings.mlr.press/v80/kalchbrenner18a.html,http://proceedings.mlr.press/v80/kalchbrenner18a/kalchbrenner18a.pdf,ICML
965,2018,Improved Training of Generative Adversarial Networks Using Representative Features,"Duhyeon Bang,         Hyunjung Shim","Despite the success of generative adversarial networks (GANs) for image generation, the trade-off between visual quality and image diversity remains a significant issue. This paper achieves both aims simultaneously by improving the stability of training GANs. The key idea of the proposed approach is to implicitly regularize the discriminator using representative features. Focusing on the fact that standard GAN minimizes reverse Kullback-Leibler (KL) divergence, we transfer the representative feature, which is extracted from the data distribution using a pre-trained autoencoder (AE), to the discriminator of standard GANs. Because the AE learns to minimize forward KL divergence, our GAN training with representative features is influenced by both reverse and forward KL divergence. Consequently, the proposed approach is verified to improve visual quality and diversity of state of the art GANs using extensive evaluations.",http://proceedings.mlr.press/v80/bang18a.html,http://proceedings.mlr.press/v80/bang18a/bang18a.pdf,ICML
966,2018,Nonparametric variable importance using an augmented neural network with multi-task learning,"Jean Feng,         Brian Williamson,         Noah Simon,         Marco Carone","In predictive modeling applications, it is often of interest to determine the relative contribution of subsets of features in explaining the variability of an outcome. It is useful to consider this variable importance as a function of the unknown, underlying data-generating mechanism rather than the specific predictive algorithm used to fit the data. In this paper, we connect these ideas in nonparametric variable importance to machine learning, and provide a method for efficient estimation of variable importance when building a predictive model using a neural network. We show how a single augmented neural network with multi-task learning simultaneously estimates the importance of many feature subsets, improving on previous procedures for estimating importance. We demonstrate on simulated data that our method is both accurate and computationally efficient, and apply our method to both a study of heart disease and for predicting mortality in ICU patients.",http://proceedings.mlr.press/v80/feng18a.html,http://proceedings.mlr.press/v80/feng18a/feng18a.pdf,ICML
967,2018,Learning Diffusion using Hyperparameters,"Dimitris Kalimeris,         Yaron Singer,         Karthik Subbian,         Udi Weinsberg","In this paper we advocate for a hyperparametric approach to learn diffusion in the independent cascade (IC) model. The sample complexity of this model is a function of the number of edges in the network and consequently learning becomes infeasible when the network is large. We study a natural restriction of the hypothesis class using additional information available in order to dramatically reduce the sample complexity of the learning process. In particular we assume that diffusion probabilities can be described as a function of a global hyperparameter and features of the individuals in the network. One of the main challenges with this approach is that training a model reduces to optimizing a non-convex objective. Despite this obstacle, we can shrink the best-known sample complexity bound for learning IC by a factor of |E|/d where |E| is the number of edges in the graph and d is the dimension of the hyperparameter. We show that under mild assumptions about the distribution generating the samples one can provably train a model with low generalization error. Finally, we use large-scale diffusion data from Facebook to show that a hyperparametric model using approximately 20 features per node achieves remarkably high accuracy.",http://proceedings.mlr.press/v80/kalimeris18a.html,http://proceedings.mlr.press/v80/kalimeris18a/kalimeris18a.pdf,ICML
968,2018,Semi-Supervised Learning on Data Streams via Temporal Label Propagation,"Tal Wagner,         Sudipto Guha,         Shiva Kasiviswanathan,         Nina Mishra","We consider the problem of labeling points on a fast-moving data stream when only a small number of labeled examples are available. In our setting, incoming points must be processed efficiently and the stream is too large to store in its entirety. We present a semi-supervised learning algorithm for this task. The algorithm maintains a small synopsis of the stream which can be quickly updated as new points arrive, and labels every incoming point by provably learning from the full history of the stream. Experiments on real datasets validate that the algorithm can quickly and accurately classify points on a stream with a small quantity of labeled examples.",http://proceedings.mlr.press/v80/wagner18a.html,http://proceedings.mlr.press/v80/wagner18a/wagner18a.pdf,ICML
969,2018,Adaptive Sampled Softmax with Kernel Based Sampling,"Guy Blanc,         Steffen Rendle","Softmax is the most commonly used output function for multiclass problems and is widely used in areas such as vision, natural language processing, and recommendation. A softmax model has linear costs in the number of classes which makes it too expensive for many real-world problems. A common approach to speed up training involves sampling only some of the classes at each training step. It is known that this method is biased and that the bias increases the more the sampling distribution deviates from the output distribution. Nevertheless, almost all recent work uses simple sampling distributions that require a large sample size to mitigate the bias. In this work, we propose a new class of kernel based sampling methods and develop an efficient sampling algorithm. Kernel based sampling adapts to the model as it is trained, thus resulting in low bias. It can also be easily applied to many models because it relies only on the model’s last hidden layer. We empirically study the trade-off of bias, sampling distribution and sample size and show that kernel based sampling results in low bias with few samples.",http://proceedings.mlr.press/v80/blanc18a.html,http://proceedings.mlr.press/v80/blanc18a/blanc18a.pdf,ICML
970,2018,Level-Set Methods for Finite-Sum Constrained Convex Optimization,"Qihang Lin,         Runchao Ma,         Tianbao Yang","We consider the constrained optimization where the objective function and the constraints are defined as summation of finitely many loss functions. This model has applications in machine learning such as Neyman-Pearson classification. We consider two level-set methods to solve this class of problems, an existing inexact Newton method and a new feasible level-set method. To update the level parameter towards the optimality, both methods require an oracle that generates upper and lower bounds as well as an affine-minorant of the level function. To construct the desired oracle, we reformulate the level function as the value of a saddle-point problem using the conjugate and perspective of the loss functions. Then a stochastic variance-reduced gradient method with a special Bregman divergence is proposed as the oracle for solving that saddle-point problem. The special divergence ensures the proximal mapping in each iteration can be solved in a closed form. The total complexity of both level-set methods using the proposed oracle are analyzed.",http://proceedings.mlr.press/v80/lin18c.html,http://proceedings.mlr.press/v80/lin18c/lin18c.pdf,ICML
971,2018,Celer: a Fast Solver for the Lasso with Dual Extrapolation,"Mathurin MASSIAS,         Alexandre Gramfort,         Joseph Salmon","Convex sparsity-inducing regularizations are ubiquitous in high-dimensional machine learning, but solving the resulting optimization problems can be slow. To accelerate solvers, state-of-the-art approaches consist in reducing the size of the optimization problem at hand. In the context of regression, this can be achieved either by discarding irrelevant features (screening techniques) or by prioritizing features likely to be included in the support of the solution (working set techniques). Duality comes into play at several steps in these techniques. Here, we propose an extrapolation technique starting from a sequence of iterates in the dual that leads to the construction of improved dual points. This enables a tighter control of optimality as used in stopping criterion, as well as better screening performance of Gap Safe rules. Finally, we propose a working set strategy based on an aggressive use of Gap Safe screening rules. Thanks to our new dual point construction, we show significant computational speedups on multiple real-world problems.",http://proceedings.mlr.press/v80/massias18a.html,http://proceedings.mlr.press/v80/massias18a/massias18a.pdf,ICML
972,2018,Convergence guarantees for a class of non-convex and non-smooth optimization problems,"Koulik Khamaru,         Martin Wainwright","Non-convex optimization problems arise frequently in machine learning, including feature selection, structured matrix learning, mixture modeling, and neural network training. We consider the problem of finding critical points of a broad class of non-convex problems with non-smooth components. We analyze the behavior of two gradient-based methods—namely a sub-gradient method, and a proximal method. Our main results are to establish rates of convergence for general problems, and also exhibit faster rates for sub-analytic functions. As an application of our theory, we obtain a simplification of the popular CCCP algorithm, which retains all the desirable convergence properties of the original method, along with a significantly lower cost per iteration. We illustrate our methods and theory via application to the problems of best subset selection, robust estimation, and shape from shading reconstruction.",http://proceedings.mlr.press/v80/khamaru18a.html,http://proceedings.mlr.press/v80/khamaru18a/khamaru18a.pdf,ICML
973,2018,LeapsAndBounds: A Method for Approximately Optimal Algorithm Configuration,"Gellert Weisz,         Andras Gyorgy,         Csaba Szepesvari","We consider the problem of configuring general-purpose solvers to run efficiently on problem instances drawn from an unknown distribution. The goal of the configurator is to find a configuration that runs fast on average on most instances, and do so with the least amount of total work. It can run a chosen solver on a random instance until the solver finishes or a timeout is reached. We propose LeapsAndBounds, an algorithm that tests configurations on randomly selected problem instances for longer and longer time. We prove that the capped expected runtime of the configuration returned by LeapsAndBounds is close to the optimal expected runtime, while our algorithm’s running time is near-optimal. Our results show that LeapsAndBounds is more efficient than the recent algorithm of Kleinberg et al. (2017), which, to our knowledge, is the only other algorithm configuration method with non-trivial theoretical guarantees. Experimental results on configuring a public SAT solver on a new benchmark dataset also stand witness to the superiority of our method.",http://proceedings.mlr.press/v80/weisz18a.html,http://proceedings.mlr.press/v80/weisz18a/weisz18a.pdf,ICML
974,2018,Analyzing the Robustness of Nearest Neighbors to Adversarial Examples,"Yizhen Wang,         Somesh Jha,         Kamalika Chaudhuri","Motivated by safety-critical applications, test-time attacks on classifiers via adversarial examples has recently received a great deal of attention. However, there is a general lack of understanding on why adversarial examples arise; whether they originate due to inherent properties of data or due to lack of training samples remains ill-understood. In this work, we introduce a theoretical framework analogous to bias-variance theory for understanding these effects. We use our framework to analyze the robustness of a canonical non-parametric classifier {–} the k-nearest neighbors. Our analysis shows that its robustness properties depend critically on the value of k {–} the classifier may be inherently non-robust for small k, but its robustness approaches that of the Bayes Optimal classifier for fast-growing k. We propose a novel modified 1-nearest neighbor classifier, and guarantee its robustness in the large sample limit. Our experiments suggest that this classifier may have good robustness properties even for reasonable data set sizes.",http://proceedings.mlr.press/v80/wang18c.html,http://proceedings.mlr.press/v80/wang18c/wang18c.pdf,ICML
975,2018,On the Implicit Bias of Dropout,"Poorya Mianjy,         Raman Arora,         Rene Vidal","Algorithmic approaches endow deep learning systems with implicit bias that helps them generalize even in over-parametrized settings. In this paper, we focus on understanding such a bias induced in learning through dropout, a popular technique to avoid overfitting in deep learning. For single hidden-layer linear neural networks, we show that dropout tends to make the norm of incoming/outgoing weight vectors of all the hidden nodes equal. In addition, we provide a complete characterization of the optimization landscape induced by dropout.",http://proceedings.mlr.press/v80/mianjy18b.html,http://proceedings.mlr.press/v80/mianjy18b/mianjy18b.pdf,ICML
976,2018,Classification from Pairwise Similarity and Unlabeled Data,"Han Bao,         Gang Niu,         Masashi Sugiyama","Supervised learning needs a huge amount of labeled data, which can be a big bottleneck under the situation where there is a privacy concern or labeling cost is high. To overcome this problem, we propose a new weakly-supervised learning setting where only similar (S) data pairs (two examples belong to the same class) and unlabeled (U) data points are needed instead of fully labeled data, which is called SU classification. We show that an unbiased estimator of the classification risk can be obtained only from SU data, and the estimation error of its empirical risk minimizer achieves the optimal parametric convergence rate. Finally, we demonstrate the effectiveness of the proposed method through experiments.",http://proceedings.mlr.press/v80/bao18a.html,http://proceedings.mlr.press/v80/bao18a/bao18a.pdf,ICML
977,2018,Structured Evolution with Compact Architectures for Scalable Policy Optimization,"Krzysztof Choromanski,         Mark Rowland,         Vikas Sindhwani,         Richard Turner,         Adrian Weller","We present a new method of blackbox optimization via gradient approximation with the use of structured random orthogonal matrices, providing more accurate estimators than baselines and with provable theoretical guarantees. We show that this algorithm can be successfully applied to learn better quality compact policies than those using standard gradient estimation techniques. The compact policies we learn have several advantages over unstructured ones, including faster training algorithms and faster inference. These benefits are important when the policy is deployed on real hardware with limited resources. Further, compact policies provide more scalable architectures for derivative-free optimization (DFO) in high-dimensional spaces. We show that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm introduced by Salimans et al. (2017). We do not need heuristics such as fitness shaping to learn good quality policies, resulting in a simple and theoretically motivated training mechanism.",http://proceedings.mlr.press/v80/choromanski18a.html,http://proceedings.mlr.press/v80/choromanski18a/choromanski18a.pdf,ICML
978,2018,"Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis","Yuxuan Wang,         Daisy Stanton,         Yu Zhang,         RJ-Skerry Ryan,         Eric Battenberg,         Joel Shor,         Ying Xiao,         Ye Jia,         Fei Ren,         Rif A. Saurous","In this work, we propose “global style tokens” (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable “labels” they generate can be used to control synthesis in novel ways, such as varying speed and speaking style – independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.",http://proceedings.mlr.press/v80/wang18h.html,http://proceedings.mlr.press/v80/wang18h/wang18h.pdf,ICML
979,2018,PixelSNAIL: An Improved Autoregressive Generative Model,"XI Chen,         Nikhil Mishra,         Mostafa Rohaninejad,         Pieter Abbeel","Autoregressive generative models achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this paper, we describe the resulting model and present state-of-the-art log-likelihood results on heavily benchmarked datasets: CIFAR-10, 32×3232×3232 \times 32 ImageNet and 64×6464×6464 \times 64 ImageNet. Our implementation will be made available at \url{https://github.com/neocxi/pixelsnail-public}.",http://proceedings.mlr.press/v80/chen18h.html,http://proceedings.mlr.press/v80/chen18h/chen18h.pdf,ICML
980,2018,Sound Abstraction and Decomposition of Probabilistic Programs,"Steven Holtzen,         Guy Broeck,         Todd Millstein","Probabilistic programming languages are a flexible tool for specifying statistical models, but this flexibility comes at the cost of efficient analysis. It is currently difficult to compactly represent the subtle independence properties of a probabilistic program, and exploit independence properties to decompose inference. Classical graphical model abstractions do capture some properties of the underlying distribution, enabling inference algorithms to operate at the level of the graph topology. However, we observe that graph-based abstractions are often too coarse to capture interesting properties of programs. We propose a form of sound abstraction for probabilistic programs wherein the abstractions are themselves simplified programs. We provide a theoretical foundation for these abstractions, as well as an algorithm to generate them. Experimentally, we also illustrate the practical benefits of our framework as a tool to decompose probabilistic program inference.",http://proceedings.mlr.press/v80/holtzen18a.html,http://proceedings.mlr.press/v80/holtzen18a/holtzen18a.pdf,ICML
981,2018,Data-Dependent Stability of Stochastic Gradient Descent,"Ilja Kuzborskij,         Christoph Lampert","We establish a data-dependent notion of algorithmic stability for Stochastic Gradient Descent (SGD), and employ it to develop novel generalization bounds. This is in contrast to previous distribution-free algorithmic stability results for SGD which depend on the worst-case constants. By virtue of the data-dependent argument, our bounds provide new insights into learning with SGD on convex and non-convex problems. In the convex case, we show that the bound on the generalization error depends on the risk at the initialization point. In the non-convex case, we prove that the expected curvature of the objective function around the initialization point has crucial influence on the generalization error. In both cases, our results suggest a simple data-driven strategy to stabilize SGD by pre-screening its initialization. As a corollary, our results allow us to show optimistic generalization bounds that exhibit fast convergence rates for SGD subject to a vanishing empirical risk and low noise of stochastic gradient.",http://proceedings.mlr.press/v80/kuzborskij18a.html,http://proceedings.mlr.press/v80/kuzborskij18a/kuzborskij18a.pdf,ICML
982,2018,Continuous and Discrete-time Accelerated Stochastic Mirror Descent for Strongly Convex Functions,"Pan Xu,         Tianhao Wang,         Quanquan Gu","We provide a second-order stochastic differential equation (SDE), which characterizes the continuous-time dynamics of accelerated stochastic mirror descent (ASMD) for strongly convex functions. This SDE plays a central role in designing new discrete-time ASMD algorithms via numerical discretization, and providing neat analyses of their convergence rates based on Lyapunov functions. Our results suggest that the only existing ASMD algorithm, namely, AC-SA proposed in Ghadimi & Lan (2012) is one instance of its kind, and we can actually derive new instances of ASMD with fewer tuning parameters. This sheds light on revisiting accelerated stochastic optimization through the lens of SDEs, which can lead to a better understanding of acceleration in stochastic optimization, as well as new simpler algorithms. Numerical experiments on both synthetic and real data support our theory.",http://proceedings.mlr.press/v80/xu18g.html,http://proceedings.mlr.press/v80/xu18g/xu18g.pdf,ICML
983,2018,Nearly Optimal Robust Subspace Tracking,"Praneeth Narayanamurthy,         Namrata Vaswani","Robust subspace tracking (RST) can be simply understood as a dynamic (time-varying) extension of robust PCA. More precisely, it is the problem of tracking data lying in a fixed or slowly-changing low-dimensional subspace while being robust to sparse outliers. This work develops a recursive projected compressive sensing algorithm called “Nearly Optimal RST (NORST)”, and obtains one of the first guarantees for it. We show that NORST provably solves RST under weakened standard RPCA assumptions, slow subspace change, and a lower bound on (most) outlier magnitudes. Our guarantee shows that (i) NORST is online (after initialization) and enjoys near-optimal values of tracking delay, lower bound on required delay between subspace change times, and of memory complexity; and (ii) it has a significantly improved worst-case outlier tolerance compared with all previous robust PCA or RST methods without requiring any model on how the outlier support is generated.",http://proceedings.mlr.press/v80/narayanamurthy18a.html,http://proceedings.mlr.press/v80/narayanamurthy18a/narayanamurthy18a.pdf,ICML
984,2018,WSNet: Compact and Efficient Networks Through Weight Sampling,"Xiaojie Jin,         Yingzhen Yang,         Ning Xu,         Jianchao Yang,         Nebojsa Jojic,         Jiashi Feng,         Shuicheng Yan","We present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via ad hoc processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces parameter sharing throughout the learning process. We demonstrate that such a novel weight sampling approach (and induced WSNet) promotes both weights and computation sharing favorably. By employing this method, we can more efficiently learn much smaller networks with competitive performance compared to baseline networks with equal numbers of convolution filters. Specifically, we consider learning compact and efficient 1D convolutional neural networks for audio classification. Extensive experiments on multiple audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are up to 180x smaller and theoretically up to 16x faster than the well-established baselines, without noticeable performance drop.",http://proceedings.mlr.press/v80/jin18d.html,http://proceedings.mlr.press/v80/jin18d/jin18d.pdf,ICML
985,2018,Knowledge Transfer with Jacobian Matching,"Suraj Srinivas,         Francois Fleuret","Classical distillation methods transfer representations from a “teacher” neural network to a “student” network by matching their output activations. Recent methods also match the Jacobians, or the gradient of output activations with the input. However, this involves making some ad hoc decisions, in particular, the choice of the loss function. In this paper, we first establish an equivalence between Jacobian matching and distillation with input noise, from which we derive appropriate loss functions for Jacobian matching. We then rely on this analysis to apply Jacobian matching to transfer learning by establishing equivalence of a recent transfer learning procedure to distillation. We then show experimentally on standard image datasets that Jacobian-based penalties improve distillation, robustness to noisy inputs, and transfer learning.",http://proceedings.mlr.press/v80/srinivas18a.html,http://proceedings.mlr.press/v80/srinivas18a/srinivas18a.pdf,ICML
986,2018,Large-Scale Cox Process Inference using Variational Fourier Features,"ST John,         James Hensman","Gaussian process modulated Poisson processes provide a flexible framework for modeling spatiotemporal point patterns. So far this had been restricted to one dimension, binning to a pre-determined grid, or small data sets of up to a few thousand data points. Here we introduce Cox process inference based on Fourier features. This sparse representation induces global rather than local constraints on the function space and is computationally efficient. This allows us to formulate a grid-free approximation that scales well with the number of data points and the size of the domain. We demonstrate that this allows MCMC approximations to the non-Gaussian posterior. In practice, we find that Fourier features have more consistent optimization behavior than previous approaches. Our approximate Bayesian method can fit over 100 000 events with complex spatiotemporal patterns in three dimensions on a single GPU.",http://proceedings.mlr.press/v80/john18a.html,http://proceedings.mlr.press/v80/john18a/john18a.pdf,ICML
987,2018,Learning to search with MCTSnets,"Arthur Guez,         Theophane Weber,         Ioannis Antonoglou,         Karen Simonyan,         Oriol Vinyals,         Daan Wierstra,         Remi Munos,         David Silver","Planning problems are among the most important and well-studied problems in artificial intelligence. They are most typically solved by tree search algorithms that simulate ahead into the future, evaluate future states, and back-up those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely used. A typical implementation of MCTS uses cleverly designed rules, optimised to the particular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations. In this paper we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incorporates simulation-based search inside a neural network, by expanding, evaluating and backing-up a vector embedding. The parameters of the network are trained end-to-end using gradient-based optimisation. When applied to small searches in the well-known planning problem Sokoban, the learned search algorithm significantly outperformed MCTS baselines.",http://proceedings.mlr.press/v80/guez18a.html,http://proceedings.mlr.press/v80/guez18a/guez18a.pdf,ICML
988,2018,Synthesizing Programs for Images using Reinforced Adversarial Learning,"Yaroslav Ganin,         Tejas Kulkarni,         Igor Babuschkin,         S. M. Ali Eslami,         Oriol Vinyals","Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator’s output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, Omniglot, CelebA) and synthetic 3D datasets. A video of the agent can be found at https://youtu.be/iSyvwAwa7vk.",http://proceedings.mlr.press/v80/ganin18a.html,http://proceedings.mlr.press/v80/ganin18a/ganin18a.pdf,ICML
989,2018,On Matching Pursuit and Coordinate Descent,"Francesco Locatello,         Anant Raj,         Sai Praneeth Karimireddy,         Gunnar Raetsch,         Bernhard Schölkopf,         Sebastian Stich,         Martin Jaggi","Two popular examples of first-order optimization methods over linear spaces are coordinate descent and matching pursuit algorithms, with their randomized variants. While the former targets the optimization by moving along coordinates, the latter considers a generalized notion of directions. Exploiting the connection between the two algorithms, we present a unified analysis of both, providing affine invariant sublinear O(1/t)O(1/t) rates on smooth objectives and linear convergence on strongly convex objectives. As a byproduct of our affine invariant analysis of matching pursuit, our rates for steepest coordinate descent are the tightest known. Furthermore, we show the first accelerated convergence rate O(1/t2)O(1/t^2) for matching pursuit and steepest coordinate descent on convex objectives.",http://proceedings.mlr.press/v80/locatello18a.html,http://proceedings.mlr.press/v80/locatello18a/locatello18a.pdf,ICML
990,2018,ADMM and Accelerated ADMM as Continuous Dynamical Systems,"Guilherme Franca,         Daniel Robinson,         Rene Vidal","Recently, there has been an increasing interest in using tools from dynamical systems to analyze the behavior of simple optimization algorithms such as gradient descent and accelerated variants. This paper strengthens such connections by deriving the differential equations that model the continuous limit of the sequence of iterates generated by the alternating direction method of multipliers, as well as an accelerated variant. We employ the direct method of Lyapunov to analyze the stability of critical points of the dynamical systems and to obtain associated convergence rates.",http://proceedings.mlr.press/v80/franca18a.html,http://proceedings.mlr.press/v80/franca18a/franca18a.pdf,ICML
991,2018,DCFNet: Deep Neural Network with Decomposed Convolutional Filters,"Qiang Qiu,         Xiuyuan Cheng,          Calderbank,         Guillermo Sapiro","Filters in a Convolutional Neural Network (CNN) contain model parameters learned from enormous amounts of data. In this paper, we suggest to decompose convolutional filters in CNN as a truncated expansion with pre-fixed bases, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data. Such a structure not only reduces the number of trainable parameters and computation, but also imposes filter regularity by bases truncation. Through extensive experiments, we consistently observe that DCFNet maintains accuracy for image classification tasks with a significant reduction of model parameters, particularly with Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we analyze the representation stability of DCFNet with respect to input variations, and prove representation stability under generic assumptions on the expansion coefficients. The analysis is consistent with the empirical observations.",http://proceedings.mlr.press/v80/qiu18a.html,http://proceedings.mlr.press/v80/qiu18a/qiu18a.pdf,ICML
992,2018,Model-Level Dual Learning,"Yingce Xia,         Xu Tan,         Fei Tian,         Tao Qin,         Nenghai Yu,         Tie-Yan Liu","Many artificial intelligence tasks appear in dual forms like English↔↔\leftrightarrowFrench translation and speech↔↔\leftrightarrowtext transformation. Existing dual learning schemes, which are proposed to solve a pair of such dual tasks, explore how to leverage such dualities from data level. In this work, we propose a new learning framework, model-level dual learning, which takes duality of tasks into consideration while designing the architectures for the primal/dual models, and ties the model parameters that playing similar roles in the two tasks. We study both symmetric and asymmetric model-level dual learning. Our algorithms achieve significant improvements on neural machine translation and sentiment analysis.",http://proceedings.mlr.press/v80/xia18a.html,http://proceedings.mlr.press/v80/xia18a/xia18a.pdf,ICML
993,2018,Stein Variational Gradient Descent Without Gradient,"Jun Han,         Qiang Liu","Stein variational gradient decent (SVGD) has been shown to be a powerful approximate inference algorithm for complex distributions. However, the standard SVGD requires calculating the gradient of the target density and cannot be applied when the gradient is unavailable. In this work, we develop a gradient-free variant of SVGD (GF-SVGD), which replaces the true gradient with a surrogate gradient, and corrects the introduced bias by re-weighting the gradients in a proper form. We show that our GF-SVGD can be viewed as the standard SVGD with a special choice of kernel, and hence directly inherits all the theoretical properties of SVGD. We shed insights on the empirical choice of the surrogate gradient and further, propose an annealed GF-SVGD that consistently outperforms a number of recent advanced gradient-free MCMC methods in our empirical studies.",http://proceedings.mlr.press/v80/han18b.html,http://proceedings.mlr.press/v80/han18b/han18b.pdf,ICML
994,2018,Stochastic Video Generation with a Learned Prior,"Emily Denton,         Rob Fergus","Generating video frames that accurately predict future world states is challenging. Existing approaches either fail to capture the full distribution of outcomes, or yield blurry generations, or both. In this paper we introduce a video generation model with a learned prior over stochastic latent variables at each time step. Video frames are generated by drawing samples from this prior and combining them with a deterministic estimate of the future frame. The approach is simple and easily trained end-to-end on a variety of datasets. Sample generations are both varied and sharp, even many frames into the future, and compare favorably to those from existing approaches.",http://proceedings.mlr.press/v80/denton18a.html,http://proceedings.mlr.press/v80/denton18a/denton18a.pdf,ICML
995,2018,Differentiable Abstract Interpretation for Provably Robust Neural Networks,"Matthew Mirman,         Timon Gehr,         Martin Vechev",We introduce a scalable method for training robust neural networks based on abstract interpretation. We present several abstract transformers which balance efficiency with precision and show these can be used to train large neural networks that are certifiably robust to adversarial perturbations.,http://proceedings.mlr.press/v80/mirman18b.html,http://proceedings.mlr.press/v80/mirman18b/mirman18b.pdf,ICML
996,2018,A Boo(n) for Evaluating Architecture Performance,"Ondrej Bajgar,         Rudolf Kadlec,         Jan Kleindienst","We point out important problems with the common practice of using the best single model performance for comparing deep learning architectures, and we propose a method that corrects these flaws. Each time a model is trained, one gets a different result due to random factors in the training process, which include random parameter initialization and random data shuffling. Reporting the best single model performance does not appropriately address this stochasticity. We propose a normalized expected best-out-of-nnn performance (BoonBoon\text{Boo}_n) as a way to correct these problems.",http://proceedings.mlr.press/v80/bajgar18a.html,http://proceedings.mlr.press/v80/bajgar18a/bajgar18a.pdf,ICML
997,2018,Prediction Rule Reshaping,"Matt Bonakdarpour,         Sabyasachi Chatterjee,         Rina Foygel Barber,         John Lafferty","Two methods are proposed for high-dimensional shape-constrained regression and classification. These methods reshape pre-trained prediction rules to satisfy shape constraints like monotonicity and convexity. The first method can be applied to any pre-trained prediction rule, while the second method deals specifically with random forests. In both cases, efficient algorithms are developed for computing the estimators, and experiments are performed to demonstrate their performance on four datasets. We find that reshaping methods enforce shape constraints without compromising predictive accuracy.",http://proceedings.mlr.press/v80/bonakdarpour18a.html,http://proceedings.mlr.press/v80/bonakdarpour18a/bonakdarpour18a.pdf,ICML
998,2018,Learning by Playing Solving Sparse Reward Tasks from Scratch,"Martin Riedmiller,         Roland Hafner,         Thomas Lampe,         Michael Neunert,         Jonas Degrave,         Tom Wiele,         Vlad Mnih,         Nicolas Heess,         Jost Tobias Springenberg","We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.",http://proceedings.mlr.press/v80/riedmiller18a.html,http://proceedings.mlr.press/v80/riedmiller18a/riedmiller18a.pdf,ICML
999,2018,Feedback-Based Tree Search for Reinforcement Learning,"Daniel Jiang,         Emmanuel Ekwedike,         Han Liu","Inspired by recent successes of Monte-Carlo tree search (MCTS) in a number of artificial intelligence (AI) application domains, we propose a reinforcement learning (RL) technique that iteratively applies MCTS on batches of small, finite-horizon versions of the original infinite-horizon Markov decision process. The terminal condition of the finite-horizon problems, or the leaf-node evaluator of the decision tree generated by MCTS, is specified using a combination of an estimated value function and an estimated policy function. The recommendations generated by the MCTS procedure are then provided as feedback in order to refine, through classification and regression, the leaf-node evaluator for the next iteration. We provide the first sample complexity bounds for a tree search-based RL algorithm. In addition, we show that a deep neural network implementation of the technique can create a competitive AI agent for the popular multi-player online battle arena (MOBA) game King of Glory.",http://proceedings.mlr.press/v80/jiang18a.html,http://proceedings.mlr.press/v80/jiang18a/jiang18a.pdf,ICML
1000,2018,Gradient Primal-Dual Algorithm Converges to Second-Order Stationary Solution for Nonconvex Distributed Optimization Over Networks,"Mingyi Hong,         Meisam Razaviyayn,         Jason Lee","In this work, we study two first-order primal-dual based algorithms, the Gradient Primal-Dual Algorithm (GPDA) and the Gradient Alternating Direction Method of Multipliers (GADMM), for solving a class of linearly constrained non-convex optimization problems. We show that with random initialization of the primal and dual variables, both algorithms are able to compute second-order stationary solutions (ss2) with probability one. This is the first result showing that primal-dual algorithm is capable of finding ss2 when only using first-order information; it also extends the existing results for first-order, but {primal-only} algorithms. An important implication of our result is that it also gives rise to the first global convergence result to the ss2, for two classes of unconstrained distributed non-convex learning problems over multi-agent networks.",http://proceedings.mlr.press/v80/hong18a.html,http://proceedings.mlr.press/v80/hong18a/hong18a.pdf,ICML
1001,2018,Nonparametric Regression with Comparisons: Escaping the Curse of Dimensionality with Ordinal Information,"Yichong Xu,         Hariank Muthakana,         Sivaraman Balakrishnan,         Aarti Singh,         Artur Dubrawski","In supervised learning, we leverage a labeled dataset to design methods for function estimation. In many practical situations, we are able to obtain alternative feedback, possibly at a low cost. A broad goal is to understand the usefulness of, and to design algorithms to exploit, this alternative feedback. We focus on a semi-supervised setting where we obtain additional ordinal (or comparison) information for potentially unlabeled samples. We consider ordinal feedback of varying qualities where we have either a perfect ordering of the samples, a noisy ordering of the samples or noisy pairwise comparisons between the samples. We provide a precise quantification of the usefulness of these types of ordinal feedback in non-parametric regression, showing that in many cases it is possible to accurately estimate an underlying function with a very small labeled set, effectively escaping the curse of dimensionality. We develop an algorithm called Ranking-Regression (RR) and analyze its accuracy as a function of size of the labeled and unlabeled datasets and various noise parameters. We also present lower bounds, that establish fundamental limits for the task and show that RR is optimal in a variety of settings. Finally, we present experiments that show the efficacy of RR and investigate its robustness to various sources of noise and model-misspecification.",http://proceedings.mlr.press/v80/xu18e.html,http://proceedings.mlr.press/v80/xu18e/xu18e.pdf,ICML
1002,2018,"The Limits of Maxing, Ranking, and Preference Learning","Moein Falahatgar,         Ayush Jain,         Alon Orlitsky,         Venkatadheeraj Pichapati,         Vaishakh Ravindrakumar","We present a comprehensive understanding of three important problems in PAC preference learning: maximum selection (maxing), ranking, and estimating all pairwise preference probabilities, in the adaptive setting. With just Weak Stochastic Transitivity, we show that maxing requires Ω(n2)\Omega(n^2) comparisons and with slightly more restrictive Medium Stochastic Transitivity, we present a linear complexity maxing algorithm. With Strong Stochastic Transitivity and Stochastic Triangle Inequality, we derive a ranking algorithm with optimal O(nlogn)\mathcal{O}(n\log n) complexity and an optimal algorithm that estimates all pairwise preference probabilities.",http://proceedings.mlr.press/v80/falahatgar18a.html,http://proceedings.mlr.press/v80/falahatgar18a/falahatgar18a.pdf,ICML
1003,2018,Structured Control Nets for Deep Reinforcement Learning,"Mario Srouji,         Jian Zhang,         Ruslan Salakhutdinov","In recent years, Deep Reinforcement Learning has made impressive advances in solving several important benchmark problems for sequential decision making. Many control applications use a generic multilayer perceptron (MLP) for non-vision parts of the policy network. In this work, we propose a new neural network architecture for the policy network representation that is simple yet effective. The proposed Structured Control Net (SCN) splits the generic MLP into two separate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlinear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hypothesize that this will bring together the benefits of both linear and nonlinear policies: improve training sample efficiency, final episodic reward, and generalization of learned policy, while requiring a smaller network and being generally applicable to different training methods. We validated our hypothesis with competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom urban driving environment, with various ablation and generalization tests, trained with multiple black-box and policy gradient training methods. The proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture. As a case study, we demonstrate much improved performance for locomotion tasks by emulating the biological central pattern generators (CPGs) as the nonlinear part of the architecture.",http://proceedings.mlr.press/v80/srouji18a.html,http://proceedings.mlr.press/v80/srouji18a/srouji18a.pdf,ICML
1004,2018,Estimation of Markov Chain via Rank-Constrained Likelihood,"Xudong Li,         Mengdi Wang,         Anru Zhang",This paper studies the estimation of low-rank Markov chains from empirical trajectories. We propose a non-convex estimator based on rank-constrained likelihood maximization. Statistical upper bounds are provided for the Kullback-Leiber divergence and the ℓ2ℓ2\ell_2 risk between the estimator and the true transition matrix. The estimator reveals a compressed state space of the Markov chain. We also develop a novel DC (difference of convex function) programming algorithm to tackle the rank-constrained non-smooth optimization problem. Convergence results are established. Experiments show that the proposed estimator achieves better empirical performance than other popular approaches.,http://proceedings.mlr.press/v80/li18g.html,http://proceedings.mlr.press/v80/li18g/li18g.pdf,ICML
1005,2018,Fast Approximate Spectral Clustering for Dynamic Networks,"Lionel Martin,         Andreas Loukas,         Pierre Vandergheynst","Spectral clustering is a widely studied problem, yet its complexity is prohibitive for dynamic graphs of even modest size. We claim that it is possible to reuse information of past cluster assignments to expedite computation. Our approach builds on a recent idea of sidestepping the main bottleneck of spectral clustering, i.e., computing the graph eigenvectors, by a polynomial-based randomized sketching technique. We show that the proposed algorithm achieves clustering assignments with quality approximating that of spectral clustering and that it can yield significant complexity benefits when the graph dynamics are appropriately bounded. In our experiments, our method clusters 30k node graphs 3.9××\times faster in average and deviates from the correct assignment by less than 0.1%.",http://proceedings.mlr.press/v80/martin18a.html,http://proceedings.mlr.press/v80/martin18a/martin18a.pdf,ICML
1006,2018,Stein Variational Message Passing for Continuous Graphical Models,"Dilin Wang,         Zhe Zeng,         Qiang Liu","We propose a novel distributed inference algorithm for continuous graphical models, by extending Stein variational gradient descent (SVGD) to leverage the Markov dependency structure of the distribution of interest. Our approach combines SVGD with a set of structured local kernel functions defined on the Markov blanket of each node, which alleviates the curse of high dimensionality and simultaneously yields a distributed algorithm for decentralized inference tasks. We justify our method with theoretical analysis and show that the use of local kernels can be viewed as a new type of localized approximation that matches the target distribution on the conditional distributions of each node over its Markov blanket. Our empirical results show that our method outperforms a variety of baselines including standard MCMC and particle message passing methods.",http://proceedings.mlr.press/v80/wang18l.html,http://proceedings.mlr.press/v80/wang18l/wang18l.pdf,ICML
1007,2018,Lightweight Stochastic Optimization for Minimizing Finite Sums with Infinite Data,"Shuai Zheng,         James Tin-Yau Kwok","Variance reduction has been commonly used in stochastic optimization. It relies crucially on the assumption that the data set is finite. However, when the data are imputed with random noise as in data augmentation, the perturbed data set becomes essentially infinite. Recently, the stochastic MISO (S-MISO) algorithm is introduced to address this expected risk minimization problem. Though it converges faster than SGD, a significant amount of memory is required. In this paper, we propose two SGD-like algorithms for expected risk minimization with random perturbation, namely, stochastic sample average gradient (SSAG) and stochastic SAGA (S-SAGA). The memory cost of SSAG does not depend on the sample size, while that of S-SAGA is the same as those of variance reduction methods on unperturbed data. Theoretical analysis and experimental results on logistic regression and AUC maximization show that SSAG has faster convergence rate than SGD with comparable space requirement while S-SAGA outperforms S-MISO in terms of both iteration complexity and storage.",http://proceedings.mlr.press/v80/zheng18a.html,http://proceedings.mlr.press/v80/zheng18a/zheng18a.pdf,ICML
1008,2018,Variational Network Inference: Strong and Stable with Concrete Support,"Amir Dezfouli,         Edwin Bonilla,         Richard Nock","Traditional methods for the discovery of latent network structures are limited in two ways: they either assume that all the signal comes from the network (i.e. there is no source of signal outside the network) or they place constraints on the network parameters to ensure model or algorithmic stability. We address these limitations by proposing a model that incorporates a Gaussian process prior on a network-independent component and formally proving that we get algorithmic stability for free while providing a novel perspective on model stability as well as robustness results and precise intervals for key inference parameters. We show that, on three applications, our approach outperforms previous methods consistently.",http://proceedings.mlr.press/v80/dezfouli18a.html,http://proceedings.mlr.press/v80/dezfouli18a/dezfouli18a.pdf,ICML
1009,2018,Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions,"Quynh Nguyen,         Mahesh Chandra Mukkamala,         Matthias Hein","In the recent literature the important role of depth in deep learning has been emphasized. In this paper we argue that sufficient width of a feedforward network is equally important by answering the simple question under which conditions the decision regions of a neural network are connected. It turns out that for a class of activation functions including leaky ReLU, neural networks having a pyramidal structure, that is no layer has more hidden units than the input dimension, produce necessarily connected decision regions. This implies that a sufficiently wide hidden layer is necessary to guarantee that the network can produce disconnected decision regions. We discuss the implications of this result for the construction of neural networks, in particular the relation to the problem of adversarial manipulation of classifiers.",http://proceedings.mlr.press/v80/nguyen18b.html,http://proceedings.mlr.press/v80/nguyen18b/nguyen18b.pdf,ICML
1010,2018,Coordinated Exploration in Concurrent Reinforcement Learning,"Maria Dimakopoulou,         Benjamin Van Roy","We consider a team of reinforcement learning agents that concurrently learn to operate in a common environment. We identify three properties - adaptivity, commitment, and diversity - which are necessary for efficient coordinated exploration and demonstrate that straightforward extensions to single-agent optimistic and posterior sampling approaches fail to satisfy them. As an alternative, we propose seed sampling, which extends posterior sampling in a manner that meets these requirements. Simulation results investigate how per-agent regret decreases as the number of agents grows, establishing substantial advantages of seed sampling over alternative exploration schemes.",http://proceedings.mlr.press/v80/dimakopoulou18a.html,http://proceedings.mlr.press/v80/dimakopoulou18a/dimakopoulou18a.pdf,ICML
1011,2018,Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning,"Rodrigo Toro Icarte,         Toryn Klassen,         Richard Valenzano,         Sheila McIlraith","In this paper we propose Reward Machines {—} a type of finite state machine that supports the specification of reward functions while exposing reward function structure to the learner and supporting decomposition. We then present Q-Learning for Reward Machines (QRM), an algorithm which appropriately decomposes the reward machine and uses off-policy q-learning to simultaneously learn subpolicies for the different components. QRM is guaranteed to converge to an optimal policy in the tabular case, in contrast to Hierarchical Reinforcement Learning methods which might converge to suboptimal policies. We demonstrate this behavior experimentally in two discrete domains. We also show how function approximation methods like neural networks can be incorporated into QRM, and that doing so can find better policies more quickly than hierarchical methods in a domain with a continuous state space.",http://proceedings.mlr.press/v80/icarte18a.html,http://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf,ICML
1012,2018,Addressing Function Approximation Error in Actor-Critic Methods,"Scott Fujimoto,         Herke Hoof,         David Meger","In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.",http://proceedings.mlr.press/v80/fujimoto18a.html,http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf,ICML
1013,2018,End-to-End Learning for the Deep Multivariate Probit Model,"Di Chen,         Yexiang Xue,         Carla Gomes","The multivariate probit model (MVP) is a popular classic model for studying binary responses of multiple entities. Nevertheless, the computational challenge of learning the MVP model, given that its likelihood involves integrating over a multidimensional constrained space of latent variables, significantly limits its application in practice. We propose a flexible deep generalization of the classic MVP, the Deep Multivariate Probit Model (DMVP), which is an end-to-end learning scheme that uses an efficient parallel sampling process of the multivariate probit model to exploit GPU-boosted deep neural networks. We present both theoretical and empirical analysis of the convergence behavior of DMVP’s sampling process with respect to the resolution of the correlation structure. We provide convergence guarantees for DMVP and our empirical analysis demonstrates the advantages of DMVP’s sampling compared with standard MCMC-based methods. We also show that when applied to multi-entity modelling problems, which are natural DMVP applications, DMVP trains faster than classical MVP, by at least an order of magnitude, captures rich correlations among entities, and further improves the joint likelihood of entities compared with several competitive models.",http://proceedings.mlr.press/v80/chen18o.html,http://proceedings.mlr.press/v80/chen18o/chen18o.pdf,ICML
1014,2018,Improving Regression Performance with Distributional Losses,"Ehsan Imani,         Martha White","There is growing evidence that converting targets to soft targets in supervised learning can provide considerable gains in performance. Much of this work has considered classification, converting hard zero-one values to soft labels—such as by adding label noise, incorporating label ambiguity or using distillation. In parallel, there is some evidence from a regression setting in reinforcement learning that learning distributions can improve performance. In this work, we investigate the reasons for this improvement, in a regression setting. We introduce a novel distributional regression loss, and similarly find it significantly improves prediction accuracy. We investigate several common hypotheses, around reducing overfitting and improved representations. We instead find evidence for an alternative hypothesis: this loss is easier to optimize, with better behaved gradients, resulting in improved generalization. We provide theoretical support for this alternative hypothesis, by characterizing the norm of the gradients of this loss.",http://proceedings.mlr.press/v80/imani18a.html,http://proceedings.mlr.press/v80/imani18a/imani18a.pdf,ICML
1015,2018,Adversarial Risk and the Dangers of Evaluating Against Weak Attacks,"Jonathan Uesato,         Brendan O’Donoghue,         Pushmeet Kohli,         Aaron Oord","This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.",http://proceedings.mlr.press/v80/uesato18a.html,http://proceedings.mlr.press/v80/uesato18a/uesato18a.pdf,ICML
1016,2018,Learning Policy Representations in Multiagent Systems,"Aditya Grover,         Maruan Al-Shedivat,         Jayesh Gupta,         Yuri Burda,         Harrison Edwards","Modeling agent behavior is central to understanding the emergence of complex phenomena in multiagent systems. Prior work in agent modeling has largely been task-specific and driven by hand-engineering domain-specific prior knowledge. We propose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a representation learning problem. Consequently, we construct a novel objective inspired by imitation learning and agent identification and design an algorithm for unsupervised learning of representations of agent policies. We demonstrate empirically the utility of the proposed framework in (i) a challenging high-dimensional competitive environment for continuous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning.",http://proceedings.mlr.press/v80/grover18a.html,http://proceedings.mlr.press/v80/grover18a/grover18a.pdf,ICML
1017,2018,Thompson Sampling for Combinatorial Semi-Bandits,"Siwei Wang,         Wei Chen","We study the application of the Thompson sampling (TS) methodology to the stochastic combinatorial multi-armed bandit (CMAB) framework. We analyze the standard TS algorithm for the general CMAB, and obtain the first distribution-dependent regret bound of O(mlogT/Δmin)O(mlog⁡T/Δmin)O(m\log T / \Delta_{\min})  for TS under general CMAB, where mmm is the number of arms, TTT is the time horizon, and ΔminΔmin\Delta_{\min} is the minimum gap between the expected reward of the optimal solution and any non-optimal solution. We also show that one cannot use an approximate oracle in TS algorithm for even MAB problems. Then we expand the analysis to matroid bandit, a special case of CMAB and for which we could remove the independence assumption across arms and achieve a better regret bound. Finally, we use some experiments to show the comparison of regrets of CUCB and CTS algorithms.",http://proceedings.mlr.press/v80/wang18a.html,http://proceedings.mlr.press/v80/wang18a/wang18a.pdf,ICML
1018,2018,Bayesian Optimization of Combinatorial Structures,"Ricardo Baptista,         Matthias Poloczek","The optimization of expensive-to-evaluate black-box functions over combinatorial structures is an ubiquitous task in machine learning, engineering and the natural sciences. The combinatorial explosion of the search space and costly evaluations pose challenges for current techniques in discrete optimization and machine learning, and critically require new algorithmic ideas. This article proposes, to the best of our knowledge, the first algorithm to overcome these challenges, based on an adaptive, scalable model that identifies useful combinatorial structure even when data is scarce. Our acquisition function pioneers the use of semidefinite programming to achieve efficiency and scalability. Experimental evaluations demonstrate that this algorithm consistently outperforms other methods from combinatorial and Bayesian optimization.",http://proceedings.mlr.press/v80/baptista18a.html,http://proceedings.mlr.press/v80/baptista18a/baptista18a.pdf,ICML
1019,2018,"An Iterative, Sketching-based Framework for Ridge Regression","Agniva Chowdhury,         Jiasen Yang,         Petros Drineas","Ridge regression is a variant of regularized least squares regression that is particularly suitable in settings where the number of predictor variables greatly exceeds the number of observations. We present a simple, iterative, sketching-based algorithm for ridge regression that guarantees high-quality approximations to the optimal solution vector. Our analysis builds upon two simple structural results that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized linear algebra. An important contribution of our work is the analysis of the behavior of subsampled ridge regression problems when the ridge leverage scores are used: we prove that accurate approximations can be achieved by a sample whose size depends on the degrees of freedom of the ridge-regression problem rather than the dimensions of the design matrix. Our experimental evaluations verify our theoretical results on both real and synthetic data.",http://proceedings.mlr.press/v80/chowdhury18a.html,http://proceedings.mlr.press/v80/chowdhury18a/chowdhury18a.pdf,ICML
1020,2018,"Fast Maximization of Non-Submodular, Monotonic Functions on the Integer Lattice","Alan Kuhnle,         J. David Smith,         Victoria Crawford,         My Thai","The optimization of submodular functions on the integer lattice has received much attention recently, but the objective functions of many applications are non-submodular. We provide two approximation algorithms for maximizing a non-submodular function on the integer lattice subject to a cardinality constraint; these are the first algorithms for this purpose that have polynomial query complexity. We propose a general framework for influence maximization on the integer lattice that generalizes prior works on this topic, and we demonstrate the efficiency of our algorithms in this context.",http://proceedings.mlr.press/v80/kuhnle18a.html,http://proceedings.mlr.press/v80/kuhnle18a/kuhnle18a.pdf,ICML
1021,2018,Active Learning with Logged Data,"Songbai Yan,         Kamalika Chaudhuri,         Tara Javidi","We consider active learning with logged data, where labeled examples are drawn conditioned on a predetermined logging policy, and the goal is to learn a classifier on the entire population, not just conditioned on the logging policy. Prior work addresses this problem either when only logged data is available, or purely in a controlled random experimentation setting where the logged data is ignored. In this work, we combine both approaches to provide an algorithm that uses logged data to bootstrap and inform experimentation, thus achieving the best of both worlds. Our work is inspired by a connection between controlled random experimentation and active learning, and modifies existing disagreement-based active learning algorithms to exploit logged data.",http://proceedings.mlr.press/v80/yan18a.html,http://proceedings.mlr.press/v80/yan18a/yan18a.pdf,ICML
1022,2018,Bounding and Counting Linear Regions of Deep Neural Networks,"Thiago Serra,         Christian Tjandraatmadja,         Srikumar Ramalingam","We investigate the complexity of deep neural networks (DNN) that represent piecewise linear (PWL) functions. In particular, we study the number of linear regions, i.e. pieces, that a PWL function represented by a DNN can attain, both theoretically and empirically. We present (i) tighter upper and lower bounds for the maximum number of linear regions on rectifier networks, which are exact for inputs of dimension one; (ii) a first upper bound for multi-layer maxout networks; and (iii) a first method to perform exact enumeration or counting of the number of regions by modeling the DNN with a mixed-integer linear formulation. These bounds come from leveraging the dimension of the space defining each linear region. The results also indicate that a deep rectifier network can only have more linear regions than every shallow counterpart with same number of neurons if that number exceeds the dimension of the input.",http://proceedings.mlr.press/v80/serra18b.html,http://proceedings.mlr.press/v80/serra18b/serra18b.pdf,ICML
1023,2018,A Conditional Gradient Framework for Composite Convex Minimization with Applications to Semidefinite Programming,"Alp Yurtsever,         Olivier Fercoq,         Francesco Locatello,         Volkan Cevher","We propose a conditional gradient framework for a composite convex minimization template with broad applications. Our approach combines smoothing and homotopy techniques under the CGM framework, and provably achieves the optimal convergence rate. We demonstrate that the same rate holds if the linear subproblems are solved approximately with additive or multiplicative error. In contrast with the relevant work, we are able to characterize the convergence when the non-smooth term is an indicator function. Specific applications of our framework include the non-smooth minimization, semidefinite programming, and minimization with linear inclusion constraints over a compact domain. Numerical evidence demonstrates the benefits of our framework.",http://proceedings.mlr.press/v80/yurtsever18a.html,http://proceedings.mlr.press/v80/yurtsever18a/yurtsever18a.pdf,ICML
1024,2018,Stochastic Proximal Algorithms for AUC Maximization,"Michael Natole,         Yiming Ying,         Siwei Lyu","Stochastic optimization algorithms such as SGDs update the model sequentially with cheap per-iteration costs, making them amenable for large-scale data analysis. However, most of the existing studies focus on the classification accuracy which can not be directly applied to the important problems of maximizing the Area under the ROC curve (AUC) in imbalanced classification and bipartite ranking. In this paper, we develop a novel stochastic proximal algorithm for AUC maximization which is referred to as SPAM. Compared with the previous literature, our algorithm SPAM applies to a non-smooth penalty function, and achieves a convergence rate of O(log t/t) for strongly convex functions while both space and per-iteration costs are of one datum.",http://proceedings.mlr.press/v80/natole18a.html,http://proceedings.mlr.press/v80/natole18a/natole18a.pdf,ICML
1025,2018,Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates,"Dong Yin,         Yudong Chen,         Ramchandran Kannan,         Peter Bartlett","In this paper, we develop distributed optimization algorithms that are provably robust against Byzantine failures—arbitrary and potentially adversarial behavior, in distributed computing systems, with a focus on achieving optimal statistical performance. A main result of this work is a sharp analysis of two robust distributed gradient descent algorithms based on median and trimmed mean operations, respectively. We prove statistical error rates for all of strongly convex, non-strongly convex, and smooth non-convex population loss functions. In particular, these algorithms are shown to achieve order-optimal statistical error rates for strongly convex losses. To achieve better communication efficiency, we further propose a median-based distributed algorithm that is provably robust, and uses only one communication round. For strongly convex quadratic loss, we show that this algorithm achieves the same optimal error rate as the robust distributed gradient descent algorithms.",http://proceedings.mlr.press/v80/yin18a.html,http://proceedings.mlr.press/v80/yin18a/yin18a.pdf,ICML
1026,2018,Fast Decoding in Sequence Models Using Discrete Latent Variables,"Lukasz Kaiser,         Samy Bengio,         Aurko Roy,         Ashish Vaswani,         Niki Parmar,         Jakob Uszkoreit,         Noam Shazeer","Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and Transformer are the state-of-the-art on many tasks. However, they lack parallelism and are thus slow for long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallel during training, but still lack parallelism during decoding. We present a method to extend sequence models using discrete latent variables that makes decoding much more parallel. The main idea behind this approach is to first autoencode the target sequence into a shorter discrete latent sequence, which is generated autoregressively, and finally decode the full sequence from this shorter latent sequence in a parallel manner. To this end, we introduce a new method for constructing discrete latent variables and compare it with previously introduced methods. Finally, we verify that our model works on the task of neural machine translation, where our models are an order of magnitude faster than comparable autoregressive models and, while lower in BLEU than purely autoregressive models, better than previously proposed non-autogregressive translation.",http://proceedings.mlr.press/v80/kaiser18a.html,http://proceedings.mlr.press/v80/kaiser18a/kaiser18a.pdf,ICML
1027,2018,Learning with Abandonment,"Sven Schmit,         Ramesh Johari","Consider a platform that wants to learn a personalized policy for each user, but the platform faces the risk of a user abandoning the platform if they are dissatisfied with the actions of the platform. For example, a platform is interested in personalizing the number of newsletters it sends, but faces the risk that the user unsubscribes forever. We propose a general thresholded learning model for scenarios like this, and discuss the structure of optimal policies. We describe salient features of optimal personalization algorithms and how feedback the platform receives impacts the results. Furthermore, we investigate how the platform can efficiently learn the heterogeneity across users by interacting with a population and provide performance guarantees.",http://proceedings.mlr.press/v80/schmit18a.html,http://proceedings.mlr.press/v80/schmit18a/schmit18a.pdf,ICML
1028,2018,The Weighted Kendall and High-order Kernels for Permutations,"Yunlong Jiao,         Jean-Philippe Vert","We propose new positive definite kernels for permutations. First we introduce a weighted version of the Kendall kernel, which allows to weight unequally the contributions of different item pairs in the permutations depending on their ranks. Like the Kendall kernel, we show that the weighted version is invariant to relabeling of items and can be computed efficiently in O(n ln(n)) operations, where n is the number of items in the permutation. Second, we propose a supervised approach to learn the weights by jointly optimizing them with the function estimated by a kernel machine. Third, while the Kendall kernel considers pairwise comparison between items, we extend it by considering higher-order comparisons among tuples of items and show that the supervised approach of learning the weights can be systematically generalized to higher-order permutation kernels.",http://proceedings.mlr.press/v80/jiao18a.html,http://proceedings.mlr.press/v80/jiao18a/jiao18a.pdf,ICML
1029,2018,Focused Hierarchical RNNs for Conditional Sequence Processing,"Nan Rosemary Ke,         Konrad Żołna,         Alessandro Sordoni,         Zhouhan Lin,         Adam Trischler,         Yoshua Bengio,         Joelle Pineau,         Laurent Charlin,         Christopher Pal","Recurrent Neural Networks (RNNs) with attention mechanisms have obtained state-of-the-art results for many sequence processing tasks. Most of these models use a simple form of encoder with attention that looks over the entire sequence and assigns a weight to each token independently. We present a mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed. We formulate this using a multi-layer conditional hierarchical sequence encoder that reads in one token at a time and makes a discrete decision on whether the token is relevant to the context or question being asked. The discrete gating mechanism takes in the context embedding and the current hidden state as inputs and controls information flow into the layer above. We train it using policy gradient methods. We evaluate this method on several types of tasks with different attributes. First, we evaluate the method on synthetic tasks which allow us to evaluate the model for its generalization ability and probe the behavior of the gates in more controlled settings. We then evaluate this approach on large scale Question Answering tasks including the challenging MS MARCO and SearchQA tasks. Our models shows consistent improvements for both tasks over prior work and our baselines. It has also shown to generalize significantly better on synthetic tasks as compared to the baselines.",http://proceedings.mlr.press/v80/ke18a.html,http://proceedings.mlr.press/v80/ke18a/ke18a.pdf,ICML
1030,2018,The Uncertainty Bellman Equation and Exploration,"Brendan O’Donoghue,         Ian Osband,         Remi Munos,         Vlad Mnih","We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory benefit of a policy beyond individual time-steps. We prove that the unique fixed point of the UBE yields an upper bound on the variance of the posterior distribution of the Q-values induced by any policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for ϵϵ\epsilon-greedy improves DQN performance on 51 out of 57 games in the Atari suite.",http://proceedings.mlr.press/v80/odonoghue18a.html,http://proceedings.mlr.press/v80/odonoghue18a/odonoghue18a.pdf,ICML
1031,2018,Nonoverlap-Promoting Variable Selection,"Pengtao Xie,         Hongbao Zhang,         Yichen Zhu,         Eric Xing","Variable selection is a classic problem in machine learning (ML), widely used to find important explanatory factors, and improve generalization performance and interpretability of ML models. In this paper, we consider variable selection for models where multiple responses are to be predicted based on the same set of covariates. Since each response is relevant to a unique subset of covariates, we desire the selected variables for different responses have small overlap. We propose a regularizer that simultaneously encourage orthogonality and sparsity, which jointly brings in an effect of reducing overlap. We apply this regularizer to four model instances and develop efficient algorithms to solve the regularized problems. We provide a formal analysis on why the proposed regularizer can reduce generalization error. Experiments on both simulation studies and real-world datasets demonstrate the effectiveness of the proposed regularizer in selecting less-overlapped variables and improving generalization performance.",http://proceedings.mlr.press/v80/xie18b.html,http://proceedings.mlr.press/v80/xie18b/xie18b.pdf,ICML
1032,2018,Black Box FDR,"Wesley Tansey,         Yixin Wang,         David Blei,         Raul Rabadan","Analyzing large-scale, multi-experiment studies requires scientists to test each experimental outcome for statistical significance and then assess the results as a whole. We present Black Box FDR (BB-FDR), an empirical-Bayes method for analyzing multi-experiment studies when many covariates are gathered per experiment. BB-FDR learns a series of black box predictive models to boost power and control the false discovery rate (FDR) at two stages of study analysis. In Stage 1, it uses a deep neural network prior to report which experiments yielded significant outcomes. In Stage 2, a separate black box model of each covariate is used to select features that have significant predictive power across all experiments. In benchmarks, BB-FDR outperforms competing state-of-the-art methods in both stages of analysis. We apply BB-FDR to two real studies on cancer drug efficacy. For both studies, BB-FDR increases the proportion of significant outcomes discovered and selects variables that reveal key genomic drivers of drug sensitivity and resistance in cancer.",http://proceedings.mlr.press/v80/tansey18a.html,http://proceedings.mlr.press/v80/tansey18a/tansey18a.pdf,ICML
1033,2018,Local Density Estimation in High Dimensions,"Xian Wu,         Moses Charikar,         Vishnu Natchu","An important question that arises in the study of high dimensional vector representations learned from data is: given a set D of vectors and a query q, estimate the number of points within a specified distance threshold of q. Our algorithm uses locality sensitive hashing to preprocess the data to accurately and efficiently estimate the answers to such questions via an unbiased estimator that uses importance sampling. A key innovation is the ability to maintain a small number of hash tables via preprocessing data structures and algorithms that sample from multiple buckets in each hash table. We give bounds on the space requirements and query complexity of our scheme, and demonstrate the effectiveness of our algorithm by experiments on a standard word embedding dataset.",http://proceedings.mlr.press/v80/wu18a.html,http://proceedings.mlr.press/v80/wu18a/wu18a.pdf,ICML
1034,2018,Path-Level Network Transformation for Efficient Architecture Search,"Han Cai,         Jiacheng Yang,         Weinan Zhang,         Song Han,         Yong Yu","We introduce a new function-preserving transformation for efficient neural architecture search. This network transformation allows reusing previously trained networks and existing successful architectures that improves sample efficiency. We aim to address the limitation of current network transformation operations that can only perform layer-level architecture modifications, such as adding (pruning) filters or inserting (removing) a layer, which fails to change the topology of connection paths. Our proposed path-level transformation operations enable the meta-controller to modify the path topology of the given network while keeping the merits of reusing weights, and thus allow efficiently designing effective structures with complex path topologies like Inception models. We further propose a bidirectional tree-structured reinforcement learning meta-controller to explore a simple yet highly expressive tree-structured architecture space that can be viewed as a generalization of multi-branch architectures. We experimented on the image classification datasets with limited computational resources (about 200 GPU-hours), where we observed improved parameter efficiency and better test results (97.70% test accuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet in the mobile setting), demonstrating the effectiveness and transferability of our designed architectures.",http://proceedings.mlr.press/v80/cai18a.html,http://proceedings.mlr.press/v80/cai18a/cai18a.pdf,ICML
1035,2018,Signal and Noise Statistics Oblivious Orthogonal Matching Pursuit,"Sreejith Kallummil,         Sheetal Kalyani","Orthogonal matching pursuit (OMP) is a widely used algorithm for recovering sparse high dimensional vectors in linear regression models. The optimal performance of OMP requires a priori knowledge of either the sparsity of regression vector or noise statistics. Both these statistics are rarely known a priori and are very difficult to estimate. In this paper, we present a novel technique called residual ratio thresholding (RRT) to operate OMP without any a priori knowledge of sparsity and noise statistics and establish finite sample and large sample support recovery guarantees for the same. Both analytical results and numerical simulations in real and synthetic data sets indicate that RRT has a performance comparable to OMP with a priori knowledge of sparsity and noise statistics.",http://proceedings.mlr.press/v80/kallummil18a.html,http://proceedings.mlr.press/v80/kallummil18a/kallummil18a.pdf,ICML
1036,2018,Subspace Embedding and Linear Regression with Orlicz Norm,"Alexandr Andoni,         Chengyu Lin,         Ying Sheng,         Peilin Zhong,         Ruiqi Zhong","We consider a generalization of the classic linear regression problem to the case when the loss is an Orlicz norm. An Orlicz norm is parameterized by a non-negative convex function G: R_+ - > R_+ with G(0) = 0: the Orlicz norm of a n-dimensional vector x is defined as |x|_G = inf{ alpha > 0 | sum_{i = 1}^n G( |x_i| / alpha ) < = 1 }. We consider the cases where the function G grows subquadratically. Our main result is based on a new oblivious embedding which embeds the column space of a given nxd matrix A with Orlicz norm into a lower dimensional space with L2 norm. Specifically, we show how to efficiently find an mxn embedding matrix S (m < n), such that for every d-dimensional vector x, we have Omega(1/(d log n)) |Ax|_G < = |SAx|_2 < = O(d^2 log n) |Ax|_G. By applying this subspace embedding technique, we show an approximation algorithm for the regression problem min_x |Ax-b|_G, up to a O( d log^2 n ) factor. As a further application of our techniques, we show how to also use them to improve on the algorithm for the Lp low rank matrix approximation problem for 1 < = p < 2.",http://proceedings.mlr.press/v80/andoni18a.html,http://proceedings.mlr.press/v80/andoni18a/andoni18a.pdf,ICML
1037,2018,The Well-Tempered Lasso,"Yuanzhi Li,         Yoram Singer","We study the complexity of the entire regularization path for least squares regression with 1-norm penalty, known as the Lasso. Every regression parameter in the Lasso changes linearly as a function of the regularization value. The number of changes is regarded as the Lasso’s complexity. Experimental results using exact path following exhibit polynomial complexity of the Lasso in the problem size. Alas, the path complexity of the Lasso on artificially designed regression problems is exponential We use smoothed analysis as a mechanism for bridging the gap between worst case settings and the de facto low complexity. Our analysis assumes that the observed data has a tiny amount of intrinsic noise. We then prove that the Lasso’s complexity is polynomial in the problem size.",http://proceedings.mlr.press/v80/li18f.html,http://proceedings.mlr.press/v80/li18f/li18f.pdf,ICML
1038,2018,Quickshift++: Provably Good Initializations for Sample-Based Mean Shift,"Heinrich Jiang,         Jennifer Jang,         Samory Kpotufe","We provide initial seedings to the Quick Shift clustering algorithm, which approximate the locally high-density regions of the data. Such seedings act as more stable and expressive cluster-cores than the singleton modes found by Quick Shift. We establish statistical consistency guarantees for this modification. We then show strong clustering performance on real datasets as well as promising applications to image segmentation.",http://proceedings.mlr.press/v80/jiang18b.html,http://proceedings.mlr.press/v80/jiang18b/jiang18b.pdf,ICML
1039,2018,Structured Variationally Auto-encoded Optimization,"Xiaoyu Lu,         Javier Gonzalez,         Zhenwen Dai,         Neil Lawrence","We tackle the problem of optimizing a black-box objective function defined over a highly-structured input space. This problem is ubiquitous in science and engineering. In machine learning, inferring the structure of a neural network or the Automatic Statistician (AS), where the optimal kernel combination for a Gaussian process is selected, are two important examples. We use the \as as a case study to describe our approach, that can be easily generalized to other domains. We propose an Structure Generating Variational Auto-encoder (SG-VAE) to embed the original space of kernel combinations into some low-dimensional continuous manifold where Bayesian optimization (BO) ideas are used. This is possible when structural knowledge of the problem is available, which can be given via a simulator or any other form of generating potentially good solutions. The right exploration-exploitation balance is imposed by propagating into the search the uncertainty of the latent space of the SG-VAE, that is computed using variational inference. The key aspect of our approach is that the SG-VAE can be used to bias the search towards relevant regions, making it suitable for transfer learning tasks. Several experiments in various application domains are used to illustrate the utility and generality of the approach described in this work.",http://proceedings.mlr.press/v80/lu18c.html,http://proceedings.mlr.press/v80/lu18c/lu18c.pdf,ICML
1040,2018,First Order Generative Adversarial Networks,"Calvin Seward,         Thomas Unterthiner,         Urs Bergmann,         Nikolay Jetchev,         Sepp Hochreiter","GANs excel at learning high dimensional distributions, but they can update generator parameters in directions that do not correspond to the steepest descent direction of the objective. Prominent examples of problematic update directions include those used in both Goodfellow’s original GAN and the WGAN-GP. To formally describe an optimal update direction, we introduce a theoretical framework which allows the derivation of requirements on both the divergence and corresponding method for determining an update direction, with these requirements guaranteeing unbiased mini-batch updates in the direction of steepest descent. We propose a novel divergence which approximates the Wasserstein distance while regularizing the critic’s first order information. Together with an accompanying update direction, this divergence fulfills the requirements for unbiased steepest descent updates. We verify our method, the First Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a new state of the art on the One Billion Word language generation task.",http://proceedings.mlr.press/v80/seward18a.html,http://proceedings.mlr.press/v80/seward18a/seward18a.pdf,ICML
1041,2018,Analyzing Uncertainty in Neural Machine Translation,"Myle Ott,         Michael Auli,         David Grangier,         Marc’Aurelio Ranzato","Machine translation is a popular test bed for research in neural sequence-to-sequence models but despite much recent research, there is still a lack of understanding of these models. Practitioners report performance degradation with large beams, the under-estimation of rare words and a lack of diversity in the final translations. Our study relates some of these issues to the inherent uncertainty of the task, due to the existence of multiple valid translations for a single source sentence, and to the extrinsic uncertainty caused by noisy training data. We propose tools and metrics to assess how uncertainty in the data is captured by the model distribution and how it affects search strategies that generate translations. Our results show that search works remarkably well but that the models tend to spread too much probability mass over the hypothesis space. Next, we propose tools to assess model calibration and show how to easily fix some shortcomings of current models. We release both code and multiple human reference translations for two popular benchmarks.",http://proceedings.mlr.press/v80/ott18a.html,http://proceedings.mlr.press/v80/ott18a/ott18a.pdf,ICML
1042,2018,On the Theory of Variance Reduction for Stochastic Gradient Monte Carlo,"Niladri Chatterji,         Nicolas Flammarion,         Yian Ma,         Peter Bartlett,         Michael Jordan","We provide convergence guarantees in Wasserstein distance for a variety of variance-reduction methods: SAGA Langevin diffusion, SVRG Langevin diffusion and control-variate underdamped Langevin diffusion. We analyze these methods under a uniform set of assumptions on the log-posterior distribution, assuming it to be smooth, strongly convex and Hessian Lipschitz. This is achieved by a new proof technique combining ideas from finite-sum optimization and the analysis of sampling methods. Our sharp theoretical bounds allow us to identify regimes of interest where each method performs better than the others. Our theory is verified with experiments on real-world and synthetic datasets.",http://proceedings.mlr.press/v80/chatterji18a.html,http://proceedings.mlr.press/v80/chatterji18a/chatterji18a.pdf,ICML
1043,2018,Canonical Tensor Decomposition for Knowledge Base Completion,"Timothee Lacroix,         Nicolas Usunier,         Guillaume Obozinski","The problem of Knowledge Base Completion can be framed as a 3rd-order binary tensor completion problem. In this light, the Canonical Tensor Decomposition (CP) seems like a natural solution; however, current implementations of CP on standard Knowledge Base Completion benchmarks are lagging behind their competitors. In this work, we attempt to understand the limits of CP for knowledge base completion. First, we motivate and test a novel regularizer, based on tensor nuclear p-norms. Then, we present a reformulation of the problem that makes it invariant to arbitrary choices in the inclusion of predicates or their reciprocals in the dataset. These two methods combined allow us to beat the current state of the art on several datasets with a CP decomposition, and obtain even better results using the more advanced ComplEx model.",http://proceedings.mlr.press/v80/lacroix18a.html,http://proceedings.mlr.press/v80/lacroix18a/lacroix18a.pdf,ICML
1044,2018,Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent,"Trevor Campbell,         Tamara Broderick","Coherent uncertainty quantification is a key strength of Bayesian methods. But modern algorithms for approximate Bayesian posterior inference often sacrifice accurate posterior uncertainty estimation in the pursuit of scalability. This work shows that previous Bayesian coreset construction algorithms—which build a small, weighted subset of the data that approximates the full dataset—are no exception. We demonstrate that these algorithms scale the coreset log-likelihood suboptimally, resulting in underestimated posterior uncertainty. To address this shortcoming, we develop greedy iterative geodesic ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales the coreset log-likelihood optimally. GIGA provides geometric decay in posterior approximation error as a function of coreset size, and maintains the fast running time of its predecessors. The paper concludes with validation of GIGA on both synthetic and real datasets, demonstrating that it reduces posterior approximation error by orders of magnitude compared with previous coreset constructions.",http://proceedings.mlr.press/v80/campbell18a.html,http://proceedings.mlr.press/v80/campbell18a/campbell18a.pdf,ICML
1045,2018,Representation Learning on Graphs with Jumping Knowledge Networks,"Keyulu Xu,         Chengtao Li,         Yonglong Tian,         Tomohiro Sonobe,         Ken-ichi Kawarabayashi,         Stefanie Jegelka","Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of ""neighboring"" nodes that a node’s representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture – jumping knowledge (JK) networks – that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models’ performance.",http://proceedings.mlr.press/v80/xu18c.html,http://proceedings.mlr.press/v80/xu18c/xu18c.pdf,ICML
1046,2018,Error Estimation for Randomized Least-Squares Algorithms via the Bootstrap,"Miles Lopes,         Shusen Wang,         Michael Mahoney","Over the course of the past decade, a variety of randomized algorithms have been proposed for computing approximate least-squares (LS) solutions in large-scale settings. A longstanding practical issue is that, for any given input, the user rarely knows the actual error of an approximate solution (relative to the exact solution). Likewise, it is difficult for the user to know precisely how much computation is needed to achieve the desired error tolerance. Consequently, the user often appeals to worst-case error bounds that tend to offer only qualitative guidance. As a more practical alternative, we propose a bootstrap method to compute a posteriori error estimates for randomized LS algorithms. These estimates permit the user to numerically assess the error of a given solution, and to predict how much work is needed to improve a ""preliminary"" solution. In addition, we provide theoretical consistency results for the method, which are the first such results in this context (to the best of our knowledge). From a practical standpoint, the method also has considerable flexibility, insofar as it can be applied to several popular sketching algorithms, as well as a variety of error metrics. Moreover, the extra step of error estimation does not add much cost to an underlying sketching algorithm. Finally, we demonstrate the effectiveness of the method with empirical results.",http://proceedings.mlr.press/v80/lopes18a.html,http://proceedings.mlr.press/v80/lopes18a/lopes18a.pdf,ICML
1047,2018,Autoregressive Quantile Networks for Generative Modeling,"Georg Ostrovski,         Will Dabney,         Remi Munos","We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception scores, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.",http://proceedings.mlr.press/v80/ostrovski18a.html,http://proceedings.mlr.press/v80/ostrovski18a/ostrovski18a.pdf,ICML
1048,2018,Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing—and Back,"Elliot Meyerson,         Risto Miikkulainen","Deep multitask learning boosts performance by sharing learned structure across related tasks. This paper adapts ideas from deep multitask learning to the setting where only a single task is available. The method is formalized as pseudo-task augmentation, in which models are trained with multiple decoders for each task. Pseudo-tasks simulate the effect of training towards closely-related tasks drawn from the same universe. In a suite of experiments, pseudo-task augmentation is shown to improve performance on single-task learning problems. When combined with multitask learning, further improvements are achieved, including state-of-the-art performance on the CelebA dataset, showing that pseudo-task augmentation and multitask learning have complementary value. All in all, pseudo-task augmentation is a broadly applicable and efficient way to boost performance in deep learning systems.",http://proceedings.mlr.press/v80/meyerson18a.html,http://proceedings.mlr.press/v80/meyerson18a/meyerson18a.pdf,ICML
1049,2018,Deep Bayesian Nonparametric Tracking,"Aonan Zhang,         John Paisley","Time-series data often exhibit irregular behavior, making them hard to analyze and explain with a simple dynamic model. For example, information in social networks may show change-point-like bursts that then diffuse with smooth dynamics. Powerful models such as deep neural networks learn smooth functions from data, but are not as well-suited (in off-the-shelf form) for discovering and explaining sparse, discrete and bursty dynamic patterns. Bayesian models can do this well by encoding the appropriate probabilistic assumptions in the model prior. We propose an integration of Bayesian nonparametric methods within deep neural networks for modeling irregular patterns in time-series data. We use a Bayesian nonparametrics to model change-point behavior in time, and a deep neural network to model nonlinear latent space dynamics. We compare with a non-deep linear version of the model also proposed here. Empirical evaluations demonstrates improved performance and interpretable results when tracking stock prices and Twitter trends.",http://proceedings.mlr.press/v80/zhang18j.html,http://proceedings.mlr.press/v80/zhang18j/zhang18j.pdf,ICML
1050,2018,Leveraging Well-Conditioned Bases: Streaming and Distributed Summaries in Minkowski ppp-Norms,"Charlie Dickens,         Graham Cormode,         David Woodruff","Work on approximate linear algebra has led to efficient distributed and streaming algorithms for problems such as approximate matrix multiplication, low rank approximation, and regression, primarily for the Euclidean norm ℓ2ℓ2\ell_2. We study other ℓpℓp\ell_p norms, which are more robust for p<2p<2p < 2, and can be used to find outliers for p>2p>2p > 2. Unlike previous algorithms for such norms, we give algorithms that are (1) deterministic, (2) work simultaneously for every p≥1p≥1p \geq 1, including p=∞p=∞p = \infty, and (3) can be implemented in both distributed and streaming environments. We study ℓpℓp\ell_p-regression, entrywise ℓpℓp\ell_p-low rank approximation, and versions of approximate matrix multiplication.",http://proceedings.mlr.press/v80/dickens18a.html,http://proceedings.mlr.press/v80/dickens18a/dickens18a.pdf,ICML
1051,2018,Causal Bandits with Propagating Inference,"Akihiro Yabe,         Daisuke Hatano,         Hanna Sumita,         Shinji Ito,         Naonori Kakimura,         Takuro Fukunaga,         Ken-ichi Kawarabayashi","Bandit is a framework for designing sequential experiments, where a learner selects an arm A∈AA \in \mathcal{A} and obtains an observation corresponding to AA in each experiment. Theoretically, the tight regret lower-bound for the general bandit is polynomial with respect to the number of arms |A||\mathcal{A}|, and thus, to overcome this bound, the bandit problem with side-information is often considered. Recently, a bandit framework over a causal graph was introduced, where the structure of the causal graph is available as side-information and the arms are identified with interventions on the causal graph. Existing algorithms for causal bandit overcame the Ω(√|A|/T)\Omega(\sqrt{|\mathcal{A}|/T}) simple-regret lower-bound; however, their algorithms work only when the interventions A\mathcal{A} are localized around a single node (i.e., an intervention propagates only to its neighbors). We then propose a novel causal bandit algorithm for an arbitrary set of interventions, which can propagate throughout the causal graph. We also show that it achieves O(√γ∗log(|A|T)/T)O(\sqrt{ \gamma^*\log(|\mathcal{A}|T) / T}) regret bound, where γ∗\gamma^* is determined by using a causal graph structure. In particular, if the maximum in-degree of the causal graph is a constant, then γ∗=O(N2)\gamma^* = O(N^2), where NN is the number of nodes.",http://proceedings.mlr.press/v80/yabe18a.html,http://proceedings.mlr.press/v80/yabe18a/yabe18a.pdf,ICML
1052,2018,Reviving and Improving Recurrent Back-Propagation,"Renjie Liao,         Yuwen Xiong,         Ethan Fetaya,         Lisa Zhang,         KiJung Yoon,         Xaq Pitkow,         Raquel Urtasun,         Richard Zemel","In this paper, we revisit the recurrent back-propagation (RBP) algorithm, discuss the conditions under which it applies as well as how to satisfy them in deep neural networks. We show that RBP can be unstable and propose two variants based on conjugate gradient on the normal equations (CG-RBP) and Neumann series (Neumann-RBP). We further investigate the relationship between Neumann-RBP and back propagation through time (BPTT) and its truncated version (TBPTT). Our Neumann-RBP has the same time complexity as TBPTT but only requires constant memory, whereas TBPTT’s memory cost scales linearly with the number of truncation steps. We examine all RBP variants along with BPTT and TBPTT in three different application domains: associative memory with continuous Hopfield networks, document classification in citation networks using graph neural networks and hyperparameter optimization for fully connected networks. All experiments demonstrate that RBPs, especially the Neumann-RBP variant, are efficient and effective for optimizing convergent recurrent neural networks.",http://proceedings.mlr.press/v80/liao18c.html,http://proceedings.mlr.press/v80/liao18c/liao18c.pdf,ICML
1053,2018,Iterative Amortized Inference,"Joe Marino,         Yisong Yue,         Stephan Mandt","Inference models are a key component in scaling variational inference to deep latent variable models, most notably as encoder networks in variational auto-encoders (VAEs). By replacing conventional optimization-based inference with a learned model, inference is amortized over data examples and therefore more computationally efficient. However, standard inference models are restricted to direct mappings from data to approximate posterior estimates. The failure of these models to reach fully optimized approximate posterior estimates results in an amortization gap. We aim toward closing this gap by proposing iterative inference models, which learn to perform inference optimization through repeatedly encoding gradients. Our approach generalizes standard inference models in VAEs and provides insight into several empirical findings, including top-down inference techniques. We demonstrate the inference optimization capabilities of iterative inference models and show that they outperform standard inference models on several benchmark data sets of images and text.",http://proceedings.mlr.press/v80/marino18a.html,http://proceedings.mlr.press/v80/marino18a/marino18a.pdf,ICML
1054,2018,Graphical Nonconvex Optimization via an Adaptive Convex Relaxation,"Qiang Sun,         Kean Ming Tan,         Han Liu,         Tong Zhang","We consider the problem of learning high-dimensional Gaussian graphical models. The graphical lasso is one of the most popular methods for estimating Gaussian graphical models. However, it does not achieve the oracle rate of convergence. In this paper, we propose the graphical nonconvex optimization for optimal estimation in Gaussian graphical models, which is then approximated by a sequence of convex programs. Our proposal is computationally tractable and produces an estimator that achieves the oracle rate of convergence. The statistical error introduced by the sequential approximation using a sequence of convex programs is clearly demonstrated via a contraction property. The proposed methodology is then extended to modeling semiparametric graphical models. We show via numerical studies that the proposed estimator outperforms other popular methods for estimating Gaussian graphical models.",http://proceedings.mlr.press/v80/sun18c.html,http://proceedings.mlr.press/v80/sun18c/sun18c.pdf,ICML
1055,2018,Optimal Tuning for Divide-and-conquer Kernel Ridge Regression with Massive Data,"Ganggang Xu,         Zuofeng Shang,         Guang Cheng","Divide-and-conquer is a powerful approach for large and massive data analysis. In the nonparameteric regression setting, although various theoretical frameworks have been established to achieve optimality in estimation or hypothesis testing, how to choose the tuning parameter in a practically effective way is still an open problem. In this paper, we propose a data-driven procedure based on divide-and-conquer for selecting the tuning parameters in kernel ridge regression by modifying the popular Generalized Cross-validation (GCV, Wahba, 1990). While the proposed criterion is computationally scalable for massive data sets, it is also shown under mild conditions to be asymptotically optimal in the sense that minimizing the proposed distributed-GCV (dGCV) criterion is equivalent to minimizing the true global conditional empirical loss of the averaged function estimator, extending the existing optimality results of GCV to the divide-and-conquer framework.",http://proceedings.mlr.press/v80/xu18f.html,http://proceedings.mlr.press/v80/xu18f/xu18f.pdf,ICML
1056,2018,Extreme Learning to Rank via Low Rank Assumption,"Minhao Cheng,         Ian Davidson,         Cho-Jui Hsieh","We consider the setting where we wish to perform ranking for hundreds of thousands of users which is common in recommender systems and web search ranking. Learning a single ranking function is unlikely to capture the variability across all users while learning a ranking function for each person is time-consuming and requires large amounts of data from each user. To address this situation, we propose a Factorization RankSVM algorithm which learns a series of k basic ranking functions and then constructs for each user a local ranking function that is a combination of them. We develop a fast algorithm to reduce the time complexity of gradient descent solver by exploiting the low-rank structure, and the resulting algorithm is much faster than existing methods. Furthermore, we prove that the generalization error of the proposed method can be significantly better than training individual RankSVMs. Finally, we present some interesting patterns in the principal ranking functions learned by our algorithms.",http://proceedings.mlr.press/v80/cheng18a.html,http://proceedings.mlr.press/v80/cheng18a/cheng18a.pdf,ICML
1057,2018,Stronger Generalization Bounds for Deep Nets via a Compression Approach,"Sanjeev Arora,         Rong Ge,         Behnam Neyshabur,         Yi Zhang","Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The current paper shows generalization bounds that are orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net — a compression that is explicit and efficient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justification for widespread empirical success in compressing deep nets. Analysis of correctness of our compression relies upon some newly identified noise stability properties of trained deep nets, which are also experimentally verified. The study of these properties and resulting generalization bounds are also extended to convolutional nets, which had eluded earlier attempts on proving generalization.",http://proceedings.mlr.press/v80/arora18b.html,http://proceedings.mlr.press/v80/arora18b/arora18b.pdf,ICML
1058,2018,Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization,"Jiong Zhang,         Qi Lei,         Inderjit Dhillon","Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed Spectral-RNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially in capturing long range dependencies, as shown on the synthetic addition and copy tasks, as well as on MNIST and Penn Tree Bank data sets.",http://proceedings.mlr.press/v80/zhang18g.html,http://proceedings.mlr.press/v80/zhang18g/zhang18g.pdf,ICML
1059,2018,K-Beam Minimax: Efficient Optimization for Deep Adversarial Learning,"Jihun Hamm,         Yung-Kyun Noh","Minimax optimization plays a key role in adversarial training of machine learning algorithms, such as learning generative models, domain adaptation, privacy preservation, and robust learning. In this paper, we demonstrate the failure of alternating gradient descent in minimax optimization problems due to the discontinuity of solutions of the inner maximization. To address this, we propose a new ϵϵ\epsilon-subgradient descent algorithm that addresses this problem by simultaneously tracking KKK candidate solutions. Practically, the algorithm can find solutions that previous saddle-point algorithms cannot find, with only a sublinear increase of complexity in KKK. We analyze the conditions under which the algorithm converges to the true solution in detail. A significant improvement in stability and convergence speed of the algorithm is observed in simple representative problems, GAN training, and domain-adaptation problems.",http://proceedings.mlr.press/v80/hamm18a.html,http://proceedings.mlr.press/v80/hamm18a/hamm18a.pdf,ICML
1060,2018,Junction Tree Variational Autoencoder for Molecular Graph Generation,"Wengong Jin,         Regina Barzilay,         Tommi Jaakkola","We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.",http://proceedings.mlr.press/v80/jin18a.html,http://proceedings.mlr.press/v80/jin18a/jin18a.pdf,ICML
1061,2018,Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems,"Eugenio Bargiacchi,         Timothy Verstraeten,         Diederik Roijers,         Ann Nowé,         Hado Hasselt","Learning to coordinate between multiple agents is an important problem in many reinforcement learning problems. Key to learning to coordinate is exploiting loose couplings, i.e., conditional independences between agents. In this paper we study learning in repeated fully cooperative games, multi-agent multi-armed bandits (MAMABs), in which the expected rewards can be expressed as a coordination graph. We propose multi-agent upper confidence exploration (MAUCE), a new algorithm for MAMABs that exploits loose couplings, which enables us to prove a regret bound that is logarithmic in the number of arm pulls and only linear in the number of agents. We empirically compare MAUCE to sparse cooperative Q-learning, and a state-of-the-art combinatorial bandit approach, and show that it performs much better on a variety of settings, including learning control policies for wind farms.",http://proceedings.mlr.press/v80/bargiacchi18a.html,http://proceedings.mlr.press/v80/bargiacchi18a/bargiacchi18a.pdf,ICML
1062,2018,Learning to Act in Decentralized Partially Observable MDPs,"Jilles Dibangoye,         Olivier Buffet","We address a long-standing open problem of reinforcement learning in decentralized partially observable Markov decision processes. Previous attempts focussed on different forms of generalized policy iteration, which at best led to local optima. In this paper, we restrict attention to plans, which are simpler to store and update than policies. We derive, under certain conditions, the first near-optimal cooperative multi-agent reinforcement learning algorithm. To achieve significant scalability gains, we replace the greedy maximization by mixed-integer linear programming. Experiments show our approach can learn to act near-optimally in many finite domains from the literature.",http://proceedings.mlr.press/v80/dibangoye18a.html,http://proceedings.mlr.press/v80/dibangoye18a/dibangoye18a.pdf,ICML
1063,2018,Structured Output Learning with Abstention: Application to Accurate Opinion Prediction,"Alexandre Garcia,         Chloé Clavel,         Slim Essid,         Florence d’Alche-Buc","Motivated by Supervised Opinion Analysis, we propose a novel framework devoted to Structured Output Learning with Abstention (SOLA). The structure prediction model is able to abstain from predicting some labels in the structured output at a cost chosen by the user in a flexible way. For that purpose, we decompose the problem into the learning of a pair of predictors, one devoted to structured abstention and the other, to structured output prediction. To compare fully labeled training data with predictions potentially containing abstentions, we define a wide class of asymmetric abstention-aware losses. Learning is achieved by surrogate regression in an appropriate feature space while prediction with abstention is performed by solving a new pre-image problem. Thus, SOLA extends recent ideas about Structured Output Prediction via surrogate problems and calibration theory and enjoys statistical guarantees on the resulting excess risk. Instantiated on a hierarchical abstention-aware loss, SOLA is shown to be relevant for fine-grained opinion mining and gives state-of-the-art results on this task. Moreover, the abstention-aware representations can be used to competitively predict user-review ratings based on a sentence-level opinion predictor.",http://proceedings.mlr.press/v80/garcia18a.html,http://proceedings.mlr.press/v80/garcia18a/garcia18a.pdf,ICML
1064,2018,Max-Mahalanobis Linear Discriminant Analysis Networks,"Tianyu Pang,         Chao Du,         Jun Zhu","A deep neural network (DNN) consists of a nonlinear transformation from an input to a feature representation, followed by a common softmax linear classifier. Though many efforts have been devoted to designing a proper architecture for nonlinear transformation, little investigation has been done on the classifier part. In this paper, we show that a properly designed classifier can improve robustness to adversarial attacks and lead to better prediction results. Specifically, we define a Max-Mahalanobis distribution (MMD) and theoretically show that if the input distributes as a MMD, the linear discriminant analysis (LDA) classifier will have the best robustness to adversarial examples. We further propose a novel Max-Mahalanobis linear discriminant analysis (MM-LDA) network, which explicitly maps a complicated data distribution in the input space to a MMD in the latent feature space and then applies LDA to make predictions. Our results demonstrate that the MM-LDA networks are significantly more robust to adversarial attacks, and have better performance in class-biased classification.",http://proceedings.mlr.press/v80/pang18a.html,http://proceedings.mlr.press/v80/pang18a/pang18a.pdf,ICML
1065,2018,Fairness Without Demographics in Repeated Loss Minimization,"Tatsunori Hashimoto,         Megha Srivastava,         Hongseok Namkoong,         Percy Liang","Machine learning models (e.g., speech recognizers) trained on average loss suffer from representation disparity—minority groups (e.g., non-native speakers) carry less weight in the training objective, and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even turn initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.",http://proceedings.mlr.press/v80/hashimoto18a.html,http://proceedings.mlr.press/v80/hashimoto18a/hashimoto18a.pdf,ICML
1066,2018,Bucket Renormalization for Approximate Inference,"Sungsoo Ahn,         Michael Chertkov,         Adrian Weller,         Jinwoo Shin","Probabilistic graphical models are a key tool in machine learning applications. Computing the partition function, i.e., normalizing constant, is a fundamental task of statistical inference but is generally computationally intractable, leading to extensive study of approximation methods. Iterative variational methods are a popular and successful family of approaches. However, even state of the art variational methods can return poor results or fail to converge on difficult instances. In this paper, we instead consider computing the partition function via sequential summation over variables. We develop robust approximate algorithms by combining ideas from mini-bucket elimination with tensor network and renormalization group methods from statistical physics. The resulting “convergence-free” methods show good empirical performance on both synthetic and real-world benchmark models, even for difficult instances.",http://proceedings.mlr.press/v80/ahn18a.html,http://proceedings.mlr.press/v80/ahn18a/ahn18a.pdf,ICML
1067,2018,Decentralized Submodular Maximization: Bridging Discrete and Continuous Settings,"Aryan Mokhtari,         Hamed Hassani,         Amin Karbasi","In this paper, we showcase the interplay between discrete and continuous optimization in network-structured settings. We propose the first fully decentralized optimization method for a wide class of non-convex objective functions that possess a diminishing returns property. More specifically, given an arbitrary connected network and a global continuous submodular function, formed by a sum of local functions, we develop Decentralized Continuous Greedy (DCG), a message passing algorithm that converges to the tight (1−1/e)(1-1/e) approximation factor of the optimum global solution using only local computation and communication. We also provide strong convergence bounds as a function of network size and spectral characteristics of the underlying topology. Interestingly, DCG readily provides a simple recipe for decentralized discrete submodular maximization through the means of continuous relaxations. Formally, we demonstrate that by lifting the local discrete functions to continuous domains and using DCG as an interface we can develop a consensus algorithm that also achieves the tight (1−1/e)(1-1/e) approximation guarantee of the global discrete solution once a proper rounding scheme is applied.",http://proceedings.mlr.press/v80/mokhtari18a.html,http://proceedings.mlr.press/v80/mokhtari18a/mokhtari18a.pdf,ICML
1068,2018,Fast Gradient-Based Methods with Exponential Rate: A Hybrid Control Framework,"Arman Sharifi Kolarijani,         Peyman Mohajerin Esfahani,         Tamas Keviczky","Ordinary differential equations, and in general a dynamical system viewpoint, have seen a resurgence of interest in developing fast optimization methods, mainly thanks to the availability of well-established analysis tools. In this study, we pursue a similar objective and propose a class of hybrid control systems that adopts a 2nd-order differential equation as its continuous flow. A distinctive feature of the proposed differential equation in comparison with the existing literature is a state-dependent, time-invariant damping term that acts as a feedback control input. Given a user-defined scalar αα\alpha, it is shown that the proposed control input steers the state trajectories to the global optimizer of a desired objective function with a guaranteed rate of convergence O(e−αt)O(e−αt)\mathcal{O}(e^{-\alpha t}). Our framework requires that the objective function satisfies the so called Polyak–{Ł}ojasiewicz inequality. Furthermore, a discretization method is introduced such that the resulting discrete dynamical system possesses an exponential rate of convergence.",http://proceedings.mlr.press/v80/kolarijani18a.html,http://proceedings.mlr.press/v80/kolarijani18a/kolarijani18a.pdf,ICML
1069,2018,Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse Annotations,"Ashwin Kalyan,         Stefan Lee,         Anitha Kannan,         Dhruv Batra","Many structured prediction problems (particularly in vision and language domains) are ambiguous, with multiple outputs being ‘correct’ for an input {–} e.g. there are many ways of describing an image, multiple ways of translating a sentence; however, exhaustively annotating the applicability of all possible outputs is intractable due to exponentially large output spaces (e.g. all English sentences). In practice, these problems are cast as multi-class prediction, with the likelihood of only a sparse set of annotations being maximized {–} unfortunately penalizing for placing beliefs on plausible but unannotated outputs. We make and test the following hypothesis {–} for a given input, the annotations of its neighbors may serve as an additional supervisory signal. Specifically, we propose an objective that transfers supervision from neighboring examples. We first study the properties of our developed method in a controlled toy setup before reporting results on multi-label classification and two image-grounded sequence modeling tasks {–} captioning and question generation. We evaluate using standard task-specific metrics and measures of output diversity, finding consistent improvements over standard maximum likelihood training and other baselines.",http://proceedings.mlr.press/v80/kalyan18a.html,http://proceedings.mlr.press/v80/kalyan18a/kalyan18a.pdf,ICML
1070,2018,Disentangling by Factorising,"Hyunjik Kim,         Andriy Mnih","We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.",http://proceedings.mlr.press/v80/kim18b.html,http://proceedings.mlr.press/v80/kim18b/kim18b.pdf,ICML
1071,2018,Non-linear motor control by local learning in spiking neural networks,"Aditya Gilra,         Wulfram Gerstner","Learning weights in a spiking neural network with hidden neurons, using local, stable and online rules, to control non-linear body dynamics is an open problem. Here, we employ a supervised scheme, Feedback-based Online Local Learning Of Weights (FOLLOW), to train a heterogeneous network of spiking neurons with hidden layers, to control a two-link arm so as to reproduce a desired state trajectory. We show that the network learns an inverse model of the non-linear dynamics, i.e. it infers from state trajectory as input to the network, the continuous-time command that produced the trajectory. Connection weights are adjusted via a local plasticity rule that involves pre-synaptic firing and post-synaptic feedback of the error in the inferred command. We propose a network architecture, termed differential feedforward, and show that it gives a lower test error than other feedforward and recurrent architectures. We demonstrate the performance of the inverse model to control a two-link arm along a desired trajectory.",http://proceedings.mlr.press/v80/gilra18a.html,http://proceedings.mlr.press/v80/gilra18a/gilra18a.pdf,ICML
1072,2018,"Matrix Norms in Data Streams: Faster, Multi-Pass and Row-Order","Vladimir Braverman,         Stephen Chestnut,         Robert Krauthgamer,         Yi Li,         David Woodruff,         Lin Yang","A central problem in mining massive data streams is characterizing which functions of an underlying frequency vector can be approximated efficiently. Given the prevalence of large scale linear algebra problems in machine learning, recently there has been considerable effort in extending this data stream problem to that of estimating functions of a matrix. This setting generalizes classical problems to the analogous ones for matrices. For example, instead of estimating frequent-item counts, we now wish to estimate “frequent-direction” counts. A related example is to estimate norms, which now correspond to estimating a vector norm on the singular values of the matrix. Despite recent efforts, the current understanding for such matrix problems is considerably weaker than that for vector problems. We study a number of aspects of estimating matrix norms in a stream that have not previously been considered: (1) multi-pass algorithms, (2) algorithms that see the underlying matrix one row at a time, and (3) time-efficient algorithms. Our multi-pass and row-order algorithms use less memory than what is provably required in the single-pass and entrywise-update models, and thus give separations between these models (in terms of memory). Moreover, all of our algorithms are considerably faster than previous ones. We also prove a number of lower bounds, and obtain for instance, a near-complete characterization of the memory required of row-order algorithms for estimating Schatten ppp-norms of sparse matrices. We complement our results with numerical experiments.",http://proceedings.mlr.press/v80/braverman18a.html,http://proceedings.mlr.press/v80/braverman18a/braverman18a.pdf,ICML
1073,2018,"Been There, Done That: Meta-Learning with Episodic Recall","Samuel Ritter,         Jane Wang,         Zeb Kurth-Nelson,         Siddhant Jayakumar,         Charles Blundell,         Razvan Pascanu,         Matthew Botvinick","Meta-learning agents excel at rapidly learning new tasks from open-ended task distributions; yet, they forget what they learn about each task as soon as the next begins. When tasks reoccur {–} as they do in natural environments {–} meta-learning agents must explore again instead of immediately exploiting previously discovered solutions. We propose a formalism for generating open-ended yet repetitious environments, then develop a meta-learning architecture for solving these environments. This architecture melds the standard LSTM working memory with a differentiable neural episodic memory. We explore the capabilities of agents with this episodic LSTM in five meta-learning environments with reoccurring tasks, ranging from bandits to navigation and stochastic sequential decision problems.",http://proceedings.mlr.press/v80/ritter18a.html,http://proceedings.mlr.press/v80/ritter18a/ritter18a.pdf,ICML
1074,2018,Noisy Natural Gradient as Variational Inference,"Guodong Zhang,         Shengyang Sun,         David Duvenaud,         Roger Grosse","Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g. fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo’s predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.",http://proceedings.mlr.press/v80/zhang18l.html,http://proceedings.mlr.press/v80/zhang18l/zhang18l.pdf,ICML
1075,2018,Composable Planning with Attributes,"Amy Zhang,         Sainbayar Sukhbaatar,         Adam Lerer,         Arthur Szlam,         Rob Fergus","The tasks that an agent will need to solve often are not known during training. However, if the agent knows which properties of the environment are important then, after learning how its actions affect those properties, it may be able to use this knowledge to solve complex tasks without training specifically for them. Towards this end, we consider a setup in which an environment is augmented with a set of user defined attributes that parameterize the features of interest. We propose a method that learns a policy for transitioning between “nearby” sets of attributes, and maintains a graph of possible transitions. Given a task at test time that can be expressed in terms of a target set of attributes, and a current state, our model infers the attributes of the current state and searches over paths through attribute space to get a high level plan, and then uses its low level policy to execute the plan. We show in 3D block stacking, grid-world games, and StarCraft that our model is able to generalize to longer, more complex tasks at test time by composing simpler learned policies.",http://proceedings.mlr.press/v80/zhang18k.html,http://proceedings.mlr.press/v80/zhang18k/zhang18k.pdf,ICML
1076,2018,"Proportional Allocation: Simple, Distributed, and Diverse Matching with High Entropy","Shipra Agrawal,         Morteza Zadimoghaddam,         Vahab Mirrokni","Inspired by many applications of bipartite matching in online advertising and machine learning, we study a simple and natural iterative proportional allocation algorithm: Maintain a priority score \prioritya\prioritya\priority_a for each node a∈\mathdsAa∈\mathdsAa\in \mathds{A} on one side of the bipartition, initialized as \prioritya=1\prioritya=1\priority_a=1. Iteratively allocate the nodes i∈\impressionsi∈\impressionsi\in \impressions on the other side to eligible nodes in \mathdsA\mathdsA\mathds{A} in proportion of their priority scores. After each round, for each node a∈\mathdsAa∈\mathdsAa\in \mathds{A}, decrease or increase the score \prioritya\prioritya\priority_a based on whether it is over- or under- allocated. Our first result is that this simple, distributed algorithm converges to a (1−ϵ)(1−ϵ)(1-\epsilon)-approximate fractional bbb-matching solution in O(lognϵ2)O(log⁡nϵ2)O({\log n\over \epsilon^2} ) rounds. We also extend the proportional allocation algorithm and convergence results to the maximum weighted matching problem, and show that the algorithm can be naturally tuned to produce maximum matching with high entropy. High entropy, in turn, implies additional desirable properties of this matching, e.g., it satisfies certain diversity and fairness (aka anonymity) properties that are desirable in a variety of applications in online advertising and machine learning.",http://proceedings.mlr.press/v80/agrawal18b.html,http://proceedings.mlr.press/v80/agrawal18b/agrawal18b.pdf,ICML
1077,2018,Robust and Scalable Models of Microbiome Dynamics,"Travis Gibson,         Georg Gerber","Microbes are everywhere, including in and on our bodies, and have been shown to play key roles in a variety of prevalent human diseases. Consequently, there has been intense interest in the design of bacteriotherapies or ""bugs as drugs,"" which are communities of bacteria administered to patients for specific therapeutic applications. Central to the design of such therapeutics is an understanding of the causal microbial interaction network and the population dynamics of the organisms. In this work we present a Bayesian nonparametric model and associated efficient inference algorithm that addresses the key conceptual and practical challenges of learning microbial dynamics from time series microbe abundance data. These challenges include high-dimensional (300+ strains of bacteria in the gut) but temporally sparse and non-uniformly sampled data; high measurement noise; and, nonlinear and physically non-negative dynamics. Our contributions include a new type of dynamical systems model for microbial dynamics based on what we term interaction modules, or learned clusters of latent variables with redundant interaction structure (reducing the expected number of interaction coefficients from O(n^2) to O((log n)^2)); a fully Bayesian formulation of the stochastic dynamical systems model that propagates measurement and latent state uncertainty throughout the model; and introduction of a temporally varying auxiliary variable technique to enable efficient inference by relaxing the hard non-negativity constraint on states. We apply our method to simulated and real data, and demonstrate the utility of our technique for system identification from limited data and gaining new biological insights into bacteriotherapy design.",http://proceedings.mlr.press/v80/gibson18a.html,http://proceedings.mlr.press/v80/gibson18a/gibson18a.pdf,ICML
1078,2018,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,"Anish Athalye,         Nicholas Carlini,         David Wagner","We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.",http://proceedings.mlr.press/v80/athalye18a.html,http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf,ICML
1079,2018,prDeep: Robust Phase Retrieval with a Flexible Deep Network,"Christopher Metzler,         Phillip Schniter,         Ashok Veeraraghavan,          baraniuk","Phase retrieval algorithms have become an important component in many modern computational imaging systems. For instance, in the context of ptychography and speckle correlation imaging, they enable imaging past the diffraction limit and through scattering media, respectively. Unfortunately, traditional phase retrieval algorithms struggle in the presence of noise. Progress has been made recently on developing more robust algorithms using signal priors, but at the expense of limiting the range of supported measurement models (e.g., to Gaussian or coded diffraction patterns). In this work we leverage the regularization-by-denoising framework and a convolutional neural network denoiser to create prDeep, a new phase retrieval algorithm that is both robust and broadly applicable. We test and validate prDeep in simulation to demonstrate that it is robust to noise and can handle a variety of system models.",http://proceedings.mlr.press/v80/metzler18a.html,http://proceedings.mlr.press/v80/metzler18a/metzler18a.pdf,ICML
1080,2018,ContextNet: Deep learning for Star Galaxy Classification,"Noble Kennamer,         David Kirkby,         Alexander Ihler,         Francisco Javier Sanchez-Lopez","We present a framework to compose artificial neural networks in cases where the data cannot be treated as independent events. Our particular motivation is star galaxy classification for ground based optical surveys. Due to a turbulent atmosphere and imperfect instruments, a single image of an astronomical object is not enough to definitively classify it as a star or galaxy. Instead the context of the surrounding objects imaged at the same time need to be considered in order to make an optimal classification. The model we present is divided into three distinct ANNs: one designed to capture local features about each object, the second to compare these features across all objects in an image, and the third to make a final prediction for each object based on the local and compared features. By exploiting the ability to replicate the weights of an ANN, the model can handle an arbitrary and variable number of individual objects embedded in a larger exposure. We train and test our model on simulations of a large up and coming ground based survey, the Large Synoptic Survey Telescope (LSST). We compare to the state of the art approach, showing improved overall performance as well as better performance for a specific class of objects that is important for the LSST.",http://proceedings.mlr.press/v80/kennamer18a.html,http://proceedings.mlr.press/v80/kennamer18a/kennamer18a.pdf,ICML
1081,2018,Learning Steady-States of Iterative Algorithms over Graphs,"Hanjun Dai,         Zornitsa Kozareva,         Bo Dai,         Alex Smola,         Le Song","Many graph analytics problems can be solved via iterative algorithms where the solutions are often characterized by a set of steady-state conditions. Different algorithms respect to different set of fixed point constraints, so instead of using these traditional algorithms, can we learn an algorithm which can obtain the same steady-state solutions automatically from examples, in an effective and scalable way? How to represent the meta learner for such algorithm and how to carry out the learning? In this paper, we propose an embedding representation for iterative algorithms over graphs, and design a learning method which alternates between updating the embeddings and projecting them onto the steady-state constraints. We demonstrate the effectiveness of our framework using a few commonly used graph algorithms, and show that in some cases, the learned algorithm can handle graphs with more than 100,000,000 nodes in a single machine.",http://proceedings.mlr.press/v80/dai18a.html,http://proceedings.mlr.press/v80/dai18a/dai18a.pdf,ICML
1082,2018,Hierarchical Text Generation and Planning for Strategic Dialogue,"Denis Yarats,         Mike Lewis","End-to-end models for goal-orientated dialogue are challenging to train, because linguistic and strategic aspects are entangled in latent state vectors. We introduce an approach to learning representations of messages in dialogues by maximizing the likelihood of subsequent sentences and actions, which decouples the semantics of the dialogue utterance from its linguistic realization. We then use these latent sentence representations for hierarchical language generation, planning and reinforcement learning. Experiments show that our approach increases the end-task reward achieved by the model, improves the effectiveness of long-term planning using rollouts, and allows self-play reinforcement learning to improve decision making without diverging from human language. Our hierarchical latent-variable model outperforms previous work both linguistically and strategically.",http://proceedings.mlr.press/v80/yarats18a.html,http://proceedings.mlr.press/v80/yarats18a/yarats18a.pdf,ICML
1083,2018,Geodesic Convolutional Shape Optimization,"Pierre Baque,         Edoardo Remelli,         Francois Fleuret,         Pascal Fua","Aerodynamic shape optimization has many industrial applications. Existing methods, however, are so computationally demanding that typical engineering practices are to either simply try a limited number of hand-designed shapes or restrict oneself to shapes that can be parameterized using only few degrees of freedom. In this work, we introduce a new way to optimize complex shapes fast and accurately. To this end, we train Geodesic Convolutional Neural Networks to emulate a fluidynamics simulator. The key to making this approach practical is remeshing the original shape using a poly-cube map, which makes it possible to perform the computations on GPUs instead of CPUs. The neural net is then used to formulate an objective function that is differentiable with respect to the shape parameters, which can then be optimized using a gradient-based technique. This outperforms state-of-the-art methods by 5 to 20% for standard problems and, even more importantly, our approach applies to cases that previous methods cannot handle.",http://proceedings.mlr.press/v80/baque18a.html,http://proceedings.mlr.press/v80/baque18a/baque18a.pdf,ICML
1084,2018,Fourier Policy Gradients,"Matthew Fellows,         Kamil Ciosek,         Shimon Whiteson","We propose a new way of deriving policy gradient updates for reinforcement learning. Our technique, based on Fourier analysis, recasts integrals that arise with expected policy gradients as convolutions and turns them into multiplications. The obtained analytical solutions allow us to capture the low variance benefits of EPG in a broad range of settings. For the critic, we treat trigonometric and radial basis functions, two function families with the universal approximation property. The choice of policy can be almost arbitrary, including mixtures or hybrid continuous-discrete probability distributions. Moreover, we derive a general family of sample-based estimators for stochastic policy gradients, which unifies existing results on sample-based approximation. We believe that this technique has the potential to shape the next generation of policy gradient approaches, powered by analytical results.",http://proceedings.mlr.press/v80/fellows18a.html,http://proceedings.mlr.press/v80/fellows18a/fellows18a.pdf,ICML
1085,2018,High Performance Zero-Memory Overhead Direct Convolutions,"Jiyuan Zhang,         Franz Franchetti,         Tze Meng Low","The computation of convolution layers in deep neural networks typically rely on high performance routines that trade space for time by using additional memory (either for packing purposes or required as part of the algorithm) to improve performance. The problems with such an approach are two-fold. First, these routines incur additional memory overhead which reduces the overall size of the network that can fit on embedded devices with limited memory capacity. Second, these high performance routines were not optimized for performing convolution, which means that the performance obtained is usually less than conventionally expected. In this paper, we demonstrate that direct convolution, when implemented correctly, eliminates all memory overhead, and yields performance that is between 10% to 400% times better than existing high performance implementations of convolution layers on conventional and embedded CPU architectures. We also show that a high performance direct convolution exhibits better scaling performance, i.e. suffers less performance drop, when increasing the number of threads.",http://proceedings.mlr.press/v80/zhang18d.html,http://proceedings.mlr.press/v80/zhang18d/zhang18d.pdf,ICML
1086,2018,Learning to Speed Up Structured Output Prediction,"Xingyuan Pan,         Vivek Srikumar","Predicting structured outputs can be computationally onerous due to the combinatorially large output spaces. In this paper, we focus on reducing the prediction time of a trained black-box structured classifier without losing accuracy. To do so, we train a speedup classifier that learns to mimic a black-box classifier under the learning-to-search approach. As the structured classifier predicts more examples, the speedup classifier will operate as a learned heuristic to guide search to favorable regions of the output space. We present a mistake bound for the speedup classifier and identify inference situations where it can independently make correct judgments without input features. We evaluate our method on the task of entity and relation extraction and show that the speedup classifier outperforms even greedy search in terms of speed without loss of accuracy.",http://proceedings.mlr.press/v80/pan18b.html,http://proceedings.mlr.press/v80/pan18b/pan18b.pdf,ICML
1087,2018,Bayesian Model Selection for Change Point Detection and Clustering,"Othmane Mazhar,         Cristian Rojas,         Carlo Fischione,          Mohammad Reza Hesamzadeh","We address a generalization of change point detection with the purpose of detecting the change locations and the levels of clusters of a piecewise constant signal. Our approach is to model it as a nonparametric penalized least square model selection on a family of models indexed over the collection of partitions of the design points and propose a computationally efficient algorithm to approximately solve it. Statistically, minimizing such a penalized criterion yields an approximation to the maximum a-posteriori probability (MAP) estimator. The criterion is then analyzed and an oracle inequality is derived using a Gaussian concentration inequality. The oracle inequality is used to derive on one hand conditions for consistency and on the other hand an adaptive upper bound on the expected square risk of the estimator, which statistically motivates our approximation. Finally, we apply our algorithm to simulated data to experimentally validate the statistical guarantees and illustrate its behavior.",http://proceedings.mlr.press/v80/mazhar18a.html,http://proceedings.mlr.press/v80/mazhar18a/mazhar18a.pdf,ICML
1088,2018,Clustering Semi-Random Mixtures of Gaussians,"Aravindan Vijayaraghavan,         Pranjal Awasthi","Gaussian mixture models (GMM) are the most widely used statistical model for the k-means clustering problem and form a popular framework for clustering in machine learning and data analysis. In this paper, we propose a natural robust model for k-means clustering that generalizes the Gaussian mixture model, and that we believe will be useful in identifying robust algorithms. Our first contribution is a polynomial time algorithm that provably recovers the ground-truth up to small classification error w.h.p., assuming certain separation between the components. Perhaps surprisingly, the algorithm we analyze is the popular Lloyd’s algorithm for k-means clustering that is the method-of-choice in practice. Our second result complements the upper bound by giving a nearly matching lower bound on the number of misclassified points incurred by any k-means clustering algorithm on the semi-random model.",http://proceedings.mlr.press/v80/vijayaraghavan18a.html,http://proceedings.mlr.press/v80/vijayaraghavan18a/vijayaraghavan18a.pdf,ICML
1089,2018,Candidates vs. Noises Estimation for Large Multi-Class Classification Problem,"Lei Han,         Yiheng Huang,         Tong Zhang","This paper proposes a method for multi-class classification problems, where the number of classes K is large. The method, referred to as Candidates vs. Noises Estimation (CANE), selects a small subset of candidate classes and samples the remaining classes. We show that CANE is always consistent and computationally efficient. Moreover, the resulting estimator has low statistical variance approaching that of the maximum likelihood estimator, when the observed label belongs to the selected candidates with high probability. In practice, we use a tree structure with leaves as classes to promote fast beam search for candidate selection. We further apply the CANE method to estimate word probabilities in learning large neural language models. Extensive experimental results show that CANE achieves better prediction accuracy over the Noise-Contrastive Estimation (NCE), its variants and a number of the state-of-the-art tree classifiers, while it gains significant speedup compared to standard O(K) methods.",http://proceedings.mlr.press/v80/han18a.html,http://proceedings.mlr.press/v80/han18a/han18a.pdf,ICML
1090,2018,Policy Optimization as Wasserstein Gradient Flows,"Ruiyi Zhang,         Changyou Chen,         Chunyuan Li,         Lawrence Carin","Policy optimization is a core component of reinforcement learning (RL), and most existing RL methods directly optimize parameters of a policy based on maximizing the expected total reward, or its surrogate. Though often achieving encouraging empirical success, its correspondence to policy-distribution optimization has been unclear mathematically. We place policy optimization into the space of probability measures, and interpret it as Wasserstein gradient flows. On the probability-measure space, under specified circumstances, policy optimization becomes convex in terms of distribution optimization. To make optimization feasible, we develop efficient algorithms by numerically solving the corresponding discrete gradient flows. Our technique is applicable to several RL settings, and is related to many state-of-the-art policy-optimization algorithms. Specifically, we define gradient flows on both the parameter-distribution space and policy-distribution space, leading to what we term indirect-policy and direct-policy learning frameworks, respectively. Extensive experiments verify the effectiveness of our framework, often obtaining better performance compared to related algorithms.",http://proceedings.mlr.press/v80/zhang18a.html,http://proceedings.mlr.press/v80/zhang18a/zhang18a.pdf,ICML
1091,2018,On the Limitations of First-Order Approximation in GAN Dynamics,"Jerry Li,         Aleksander Madry,         John Peebles,         Ludwig Schmidt","While Generative Adversarial Networks (GANs) have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, both in theory and in practice. To address this issue, we study GAN dynamics in a simple yet rich parametric model that exhibits several of the common problematic convergence behaviors such as vanishing gradients, mode collapse, and diverging or oscillatory behavior. In spite of the non-convex nature of our model, we are able to perform a rigorous theoretical analysis of its convergence behavior. Our analysis reveals an interesting dichotomy: a GAN with an optimal discriminator provably converges, while first order approximations of the discriminator steps lead to unstable GAN dynamics and mode collapse. Our result suggests that using first order discriminator steps (the de-facto standard in most existing GAN setups) might be one of the factors that makes GAN training challenging in practice.",http://proceedings.mlr.press/v80/li18d.html,http://proceedings.mlr.press/v80/li18d/li18d.pdf,ICML
1092,2018,Alternating Randomized Block Coordinate Descent,"Jelena Diakonikolas,         Lorenzo Orecchia","Block-coordinate descent algorithms and alternating minimization methods are fundamental optimization algorithms and an important primitive in large-scale optimization and machine learning. While various block-coordinate-descent-type methods have been studied extensively, only alternating minimization – which applies to the setting of only two blocks – is known to have convergence time that scales independently of the least smooth block. A natural question is then: is the setting of two blocks special? We show that the answer is “no” as long as the least smooth block can be optimized exactly – an assumption that is also needed in the setting of alternating minimization. We do so by introducing a novel algorithm AR-BCD, whose convergence time scales independently of the least smooth (possibly non-smooth) block. The basic algorithm generalizes both alternating minimization and randomized block coordinate (gradient) descent, and we also provide its accelerated version – AAR-BCD.",http://proceedings.mlr.press/v80/diakonikolas18a.html,http://proceedings.mlr.press/v80/diakonikolas18a/diakonikolas18a.pdf,ICML
1093,2018,WHInter: A Working set algorithm for High-dimensional sparse second order Interaction models,"Marine Le Morvan,         Jean-Philippe Vert","Learning sparse linear models with two-way interactions is desirable in many application domains such as genomics. ℓ1ℓ1\ell_1-regularised linear models are popular to estimate sparse models, yet standard implementations fail to address specifically the quadratic explosion of candidate two-way interactions in high dimensions, and typically do not scale to genetic data with hundreds of thousands of features. Here we present WHInter, a working set algorithm to solve large ℓ1ℓ1\ell_1-regularised problems with two-way interactions for binary design matrices. The novelty of WHInter stems from a new bound to efficiently identify working sets while avoiding to scan all features, and on fast computations inspired from solutions to the maximum inner product search problem. We apply WHInter to simulated and real genetic data and show that it is more scalable and two orders of magnitude faster than the state of the art.",http://proceedings.mlr.press/v80/morvan18a.html,http://proceedings.mlr.press/v80/morvan18a/morvan18a.pdf,ICML
1094,2018,"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients","Lukas Balles,         Philipp Hennig","The ADAM optimizer is exceedingly popular in the deep learning community. Often it works very well, sometimes it doesn’t. Why? We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of stochastic gradients, whereas the update magnitude is determined by an estimate of their relative variance. We disentangle these two aspects and analyze them in isolation, gaining insight into the mechanisms underlying ADAM. This analysis also extends recent results on adverse effects of ADAM on generalization, isolating the sign aspect as the problematic one. Transferring the variance adaptation to SGD gives rise to a novel method, completing the practitioner’s toolbox for problems where ADAM fails.",http://proceedings.mlr.press/v80/balles18a.html,http://proceedings.mlr.press/v80/balles18a/balles18a.pdf,ICML
1095,2018,Beyond the One-Step Greedy Approach in Reinforcement Learning,"Yonathan Efroni,         Gal Dalal,         Bruno Scherrer,         Shie Mannor","The famous Policy Iteration algorithm alternates between policy improvement and policy evaluation. Implementations of this algorithm with several variants of the latter evaluation stage, e.g, n-step and trace-based returns, have been analyzed in previous works. However, the case of multiple-step lookahead policy improvement, despite the recent increase in empirical evidence of its strength, has to our knowledge not been carefully analyzed yet. In this work, we introduce the first such analysis. Namely, we formulate variants of multiple-step policy improvement, derive new algorithms using these definitions and prove their convergence. Moreover, we show that recent prominent Reinforcement Learning algorithms are, in fact, instances of our framework. We thus shed light on their empirical success and give a recipe for deriving new algorithms for future study.",http://proceedings.mlr.press/v80/efroni18a.html,http://proceedings.mlr.press/v80/efroni18a/efroni18a.pdf,ICML
1096,2018,A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in Learning Multiple Related Sparse Gaussian Graphical Models,"Beilun Wang,         Arshdeep Sekhon,         Yanjun Qi","We consider the problem of including additional knowledge in estimating sparse Gaussian graphical models (sGGMs) from aggregated samples, arising often in bioinformatics and neuroimaging applications. Previous joint sGGM estimators either fail to use existing knowledge or cannot scale-up to many tasks (large KK) under a high-dimensional (large pp) situation. In this paper, we propose a novel \underline{J}oint \underline{E}lementary \underline{E}stimator incorporating additional \underline{K}nowledge (JEEK) to infer multiple related sparse Gaussian Graphical models from large-scale heterogeneous data. Using domain knowledge as weights, we design a novel hybrid norm as the minimization objective to enforce the superposition of two weighted sparsity constraints, one on the shared interactions and the other on the task-specific structural patterns. This enables JEEK to elegantly consider various forms of existing knowledge based on the domain at hand and avoid the need to design knowledge-specific optimization. JEEK is solved through a fast and entry-wise parallelizable solution that largely improves the computational efficiency of the state-of-the-art O(p5K4)O(p^5K^4) to O(p2K4)O(p^2K^4). We conduct a rigorous statistical analysis showing that JEEK achieves the same convergence rate O(log(Kp)/ntot)O(\log(Kp)/n_{tot}) as the state-of-the-art estimators that are much harder to compute. Empirically, on multiple synthetic datasets and one real-world data from neuroscience, JEEP outperforms the speed of the state-of-arts significantly while achieving the same level of prediction accuracy.",http://proceedings.mlr.press/v80/wang18f.html,http://proceedings.mlr.press/v80/wang18f/wang18f.pdf,ICML
1097,2018,Problem Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure in MDPs,"Andrea Zanette,         Emma Brunskill","In order to make good decision under uncertainty an agent must learn from observations. To do so, two of the most common frameworks are Contextual Bandits and Markov Decision Processes (MDPs). In this paper, we study whether there exist algorithms for the more general framework (MDP) which automatically provide the best performance bounds for the specific problem at hand without user intervention and without modifying the algorithm. In particular, it is found that a very minor variant of a recently proposed reinforcement learning algorithm for MDPs already matches the best possible regret bound O~(SAT−−−−√)O~(SAT)\tilde O (\sqrt{SAT}) in the dominant term if deployed on a tabular Contextual Bandit problem despite the agent being agnostic to such setting.",http://proceedings.mlr.press/v80/zanette18a.html,http://proceedings.mlr.press/v80/zanette18a/zanette18a.pdf,ICML
1098,2018,Low-Rank Riemannian Optimization on Positive Semidefinite Stochastic Matrices with Applications to Graph Clustering,"Ahmed Douik,         Babak Hassibi","This paper develops a Riemannian optimization framework for solving optimization problems on the set of symmetric positive semidefinite stochastic matrices. The paper first reformulates the problem by factorizing the optimization variable as X=YYTX=YYT\mathbf{X}=\mathbf{Y}\mathbf{Y}^T and deriving conditions on ppp, i.e., the number of columns of YY\mathbf{Y}, under which the factorization yields a satisfactory solution. The reparameterization of the problem allows its formulation as an optimization over either an embedded or quotient Riemannian manifold whose geometries are investigated. In particular, the paper explicitly derives the tangent space, Riemannian gradients and retraction operator that allow the design of efficient optimization methods on the proposed manifolds. The numerical results reveal that, when the optimal solution has a known low-rank, the resulting algorithms present a clear complexity advantage when compared with state-of-the-art Euclidean and Riemannian approaches for graph clustering applications.",http://proceedings.mlr.press/v80/douik18a.html,http://proceedings.mlr.press/v80/douik18a/douik18a.pdf,ICML
1099,2018,Autoregressive Convolutional Neural Networks for Asynchronous Time Series,"Mikolaj Binkowski,         Gautier Marti,         Philippe Donnat","We propose Significance-Offset Convolutional Neural Network, a deep convolutional network architecture for regression of multivariate asynchronous time series. The model is inspired by standard autoregressive (AR) models and gating mechanisms used in recurrent neural networks. It involves an AR-like weighting system, where the final predictor is obtained as a weighted sum of adjusted regressors, while the weights are data-dependent functions learnt through a convolutional network. The architecture was designed for applications on asynchronous time series and is evaluated on such datasets: a hedge fund proprietary dataset of over 2 million quotes for a credit derivative index, an artificially generated noisy autoregressive series and UCI household electricity consumption dataset. The proposed architecture achieves promising results as compared to convolutional and recurrent neural networks.",http://proceedings.mlr.press/v80/binkowski18a.html,http://proceedings.mlr.press/v80/binkowski18a/binkowski18a.pdf,ICML
1100,2018,Shampoo: Preconditioned Stochastic Tensor Optimization,"Vineet Gupta,         Tomer Koren,         Yoram Singer","Preconditioned gradient methods are among the most general and powerful tools in optimization. However, preconditioning requires storing and manipulating prohibitively large matrices. We describe and analyze a new structure-aware preconditioning algorithm, called Shampoo, for stochastic optimization over tensor spaces. Shampoo maintains a set of preconditioning matrices, each of which operates on a single dimension, contracting over the remaining dimensions. We establish convergence guarantees in the stochastic convex setting, the proof of which builds upon matrix trace inequalities. Our experiments with state-of-the-art deep learning models show that Shampoo is capable of converging considerably faster than commonly used optimizers. Surprisingly, although it involves a more complex update rule, Shampoo’s runtime per step is comparable in practice to that of simple gradient methods such as SGD, AdaGrad, and Adam.",http://proceedings.mlr.press/v80/gupta18a.html,http://proceedings.mlr.press/v80/gupta18a/gupta18a.pdf,ICML
1101,2018,"Submodular Hypergraphs: p-Laplacians, Cheeger Inequalities and Spectral Clustering","Pan Li,         Olgica Milenkovic","We introduce submodular hypergraphs, a family of hypergraphs that have different submodular weights associated with different cuts of hyperedges. Submodular hypergraphs arise in cluster- ing applications in which higher-order structures carry relevant information. For such hypergraphs, we define the notion of p-Laplacians and derive corresponding nodal domain theorems and k-way Cheeger inequalities. We conclude with the description of algorithms for computing the spectra of 1- and 2-Laplacians that constitute the basis of new spectral hypergraph clustering methods.",http://proceedings.mlr.press/v80/li18e.html,http://proceedings.mlr.press/v80/li18e/li18e.pdf,ICML
1102,2018,Is Generator Conditioning Causally Related to GAN Performance?,"Augustus Odena,         Jacob Buckman,         Catherine Olsson,         Tom Brown,         Christopher Olah,         Colin Raffel,         Ian Goodfellow","Recent work suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks. We find that this Jacobian generally becomes ill-conditioned at the beginning of training. Moreover, we find that the average (across the latent space) conditioning of the generator is highly predictive of two other ad-hoc metrics for measuring the “quality” of trained GANs: the Inception Score and the Frechet Inception Distance. We then test the hypothesis that this relationship is causal by proposing a “regularization” technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean score for nearly all datasets on which we tested it. It also greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs.",http://proceedings.mlr.press/v80/odena18a.html,http://proceedings.mlr.press/v80/odena18a/odena18a.pdf,ICML
1103,2018,Learning Dynamics of Linear Denoising Autoencoders,"Arnu Pretorius,         Steve Kroon,         Herman Kamper","Denoising autoencoders (DAEs) have proven useful for unsupervised representation learning, but a thorough theoretical understanding is still lacking of how the input noise influences learning. Here we develop theory for how noise influences learning in DAEs. By focusing on linear DAEs, we are able to derive analytic expressions that exactly describe their learning dynamics. We verify our theoretical predictions with simulations as well as experiments on MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise allows DAEs to ignore low variance directions in the inputs while learning to reconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs to standard regularised autoencoders, we show that noise has a similar regularisation effect to weight decay, but with faster training dynamics. We also show that our theoretical predictions approximate learning dynamics on real-world data and qualitatively match observed dynamics in nonlinear DAEs.",http://proceedings.mlr.press/v80/pretorius18a.html,http://proceedings.mlr.press/v80/pretorius18a/pretorius18a.pdf,ICML
1104,2018,Online Learning with Abstention,"Corinna Cortes,         Giulia DeSalvo,         Claudio Gentile,         Mehryar Mohri,         Scott Yang","We present an extensive study of a key problem in online learning where the learner can opt to abstain from making a prediction, at a certain cost. In the adversarial setting, we show how existing online algorithms and guarantees can be adapted to this problem. In the stochastic setting, we first point out a bias problem that limits the straightforward extension of algorithms such as UCB-N to this context. Next, we give a new algorithm, UCB-GT, that exploits historical data and time-varying feedback graphs. We show that this algorithm benefits from more favorable regret guarantees than a natural extension of UCB-N . We further report the results of a series of experiments demonstrating that UCB-GT largely outperforms that extension of UCB-N, as well as other standard baselines.",http://proceedings.mlr.press/v80/cortes18a.html,http://proceedings.mlr.press/v80/cortes18a/cortes18a.pdf,ICML
1105,2018,Distributed Nonparametric Regression under Communication Constraints,"Yuancheng Zhu,         John Lafferty","This paper studies the problem of nonparametric estimation of a smooth function with data distributed across multiple machines. We assume an independent sample from a white noise model is collected at each machine, and an estimator of the underlying true function needs to be constructed at a central machine. We place limits on the number of bits that each machine can use to transmit information to the central machine. Our results give both asymptotic lower bounds and matching upper bounds on the statistical risk under various settings. We identify three regimes, depending on the relationship among the number of machines, the size of data available at each machine, and the communication budget. When the communication budget is small, the statistical risk depends solely on this communication bottleneck, regardless of the sample size. In the regime where the communication budget is large, the classic minimax risk in the non-distributed estimation setting is recovered. In an intermediate regime, the statistical risk depends on both the sample size and the communication budget.",http://proceedings.mlr.press/v80/zhu18a.html,http://proceedings.mlr.press/v80/zhu18a/zhu18a.pdf,ICML
1106,2018,Learning Representations and Generative Models for 3D Point Clouds,"Panos Achlioptas,         Olga Diamanti,         Ioannis Mitliagkas,         Leonidas Guibas","Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.",http://proceedings.mlr.press/v80/achlioptas18a.html,http://proceedings.mlr.press/v80/achlioptas18a/achlioptas18a.pdf,ICML
1107,2018,Tight Regret Bounds for Bayesian Optimization in One Dimension,Jonathan Scarlett,"We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time TTT behaves as Ω(T−−√)Ω(T)\Omega(\sqrt{T}) and O(TlogT−−−−−−√)O(Tlog⁡T)O(\sqrt{T\log T}). This gives a tight characterization up to a logT−−−−√log⁡T\sqrt{\log T} factor, and includes the first non-trivial lower bound for noisy BO. Our assumptions are satisfied, for example, by the squared exponential and Matérn-νν\nu kernels, with the latter requiring ν>2ν>2\nu > 2. Our results certify the near-optimality of existing bounds (Srinivas et al., 2009) for the SE kernel, while proving them to be strictly suboptimal for the Matérn kernel with ν>2ν>2\nu > 2.",http://proceedings.mlr.press/v80/scarlett18a.html,http://proceedings.mlr.press/v80/scarlett18a/scarlett18a.pdf,ICML
1108,2018,Best Arm Identification in Linear Bandits with Linear Dimension Dependency,"Chao Tao,         Saúl Blanco,         Yuan Zhou","We study the best arm identification problem in linear bandits, where the mean reward of each arm depends linearly on an unknown ddd-dimensional parameter vector θθ\theta, and the goal is to identify the arm with the largest expected reward. We first design and analyze a novel randomized θθ\theta estimator based on the solution to the convex relaxation of an optimal GGG-allocation experiment design problem. Using this estimator, we describe an algorithm whose sample complexity depends linearly on the dimension ddd, as well as an algorithm with sample complexity dependent on the reward gaps of the best ddd arms, matching the lower bound arising from the ordinary top-arm identification problem. We finally compare the empirical performance of our algorithms with other state-of-the-art algorithms in terms of both sample complexity and computational time.",http://proceedings.mlr.press/v80/tao18a.html,http://proceedings.mlr.press/v80/tao18a/tao18a.pdf,ICML
1109,2018,Revealing Common Statistical Behaviors in Heterogeneous Populations,"Andrey Zhitnikov,         Rotem Mulayoff,         Tomer Michaeli","In many areas of neuroscience and biological data analysis, it is desired to reveal common patterns among a group of subjects. Such analyses play important roles e.g., in detecting functional brain networks from fMRI scans and in identifying brain regions which show increased activity in response to certain stimuli. Group level techniques usually assume that all subjects in the group behave according to a single statistical model, or that deviations from the common model have simple parametric forms. Therefore, complex subject-specific deviations from the common model severely impair the performance of such methods. In this paper, we propose nonparametric algorithms for estimating the common covariance matrix and the common density function of several variables in a heterogeneous group of subjects. Our estimates converge to the true model as the number of subjects tends to infinity, under very mild conditions. We illustrate the effectiveness of our methods through extensive simulations as well as on real-data from fMRI scans and from arterial blood pressure and photoplethysmogram measurements.",http://proceedings.mlr.press/v80/zhitnikov18a.html,http://proceedings.mlr.press/v80/zhitnikov18a/zhitnikov18a.pdf,ICML
1110,2018,Batch Bayesian Optimization via Multi-objective Acquisition Ensemble for Automated Analog Circuit Design,"Wenlong Lyu,         Fan Yang,         Changhao Yan,         Dian Zhou,         Xuan Zeng","Bayesian optimization methods are promising for the optimization of black-box functions that are expensive to evaluate. In this paper, a novel batch Bayesian optimization approach is proposed. The parallelization is realized via a multi-objective ensemble of multiple acquisition functions. In each iteration, the multi-objective optimization of the multiple acquisition functions is performed to search for the Pareto front of the acquisition functions. The batch of inputs are then selected from the Pareto front. The Pareto front represents the best trade-off between the multiple acquisition functions. Such a policy for batch Bayesian optimization can significantly improve the efficiency of optimization. The proposed method is compared with several state-of-the-art batch Bayesian optimization algorithms using analytical benchmark functions and real-world analog integrated circuits. The experimental results show that the proposed method is competitive compared with the state-of-the-art algorithms.",http://proceedings.mlr.press/v80/lyu18a.html,http://proceedings.mlr.press/v80/lyu18a/lyu18a.pdf,ICML
1111,2018,To Understand Deep Learning We Need to Understand Kernel Learning,"Mikhail Belkin,         Siyuan Ma,         Soumik Mandal","Generalization performance of classifiers in deep learning has recently become a subject of intense study. Deep models, which are typically heavily over-parametrized, tend to fit the training data exactly. Despite this “overfitting"", they perform well on test data, a phenomenon not yet fully understood. The first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using six real-world and two synthetic datasets, we establish experimentally that kernel machines trained to have zero classification error or near zero regression error (interpolation) perform very well on test data. We proceed to give a lower bound on the norm of zero loss solutions for smooth kernels, showing that they increase nearly exponentially with data size. None of the existing bounds produce non-trivial results for interpolating solutions. We also show experimentally that (non-smooth) Laplacian kernels easily fit random labels, a finding that parallels results recently reported for ReLU neural networks. In contrast, fitting noisy data requires many more epochs for smooth Gaussian kernels. Similar performance of overfitted Laplacian and Gaussian classifiers on test, suggests that generalization is tied to the properties of the kernel function rather than the optimization process. Some key phenomena of deep learning are manifested similarly in kernel methods in the modern “overfitted"" regime. The combination of the experimental and theoretical results presented in this paper indicates a need for new theoretical ideas for understanding properties of classical kernel methods. We argue that progress on understanding deep learning will be difficult until more tractable “shallow” kernel methods are better understood.",http://proceedings.mlr.press/v80/belkin18a.html,http://proceedings.mlr.press/v80/belkin18a/belkin18a.pdf,ICML
1112,2018,Stability and Generalization of Learning Algorithms that Converge to Global Optima,"Zachary Charles,         Dimitris Papailiopoulos","We establish novel generalization bounds for learning algorithms that converge to global minima. We derive black-box stability results that only depend on the convergence of a learning algorithm and the geometry around the minimizers of the empirical risk function. The results are shown for non-convex loss functions satisfying the Polyak-Lojasiewicz (PL) and the quadratic growth (QG) conditions, which we show arise for 1-layer neural networks with leaky ReLU activations and deep neural networks with linear activations. We use our results to establish the stability of first-order methods such as stochastic gradient descent (SGD), gradient descent (GD), randomized coordinate descent (RCD), and the stochastic variance reduced gradient method (SVRG), in both the PL and the strongly convex setting. Our results match or improve state-of-the-art generalization bounds and can easily extend to similar optimization algorithms. Finally, although our results imply comparable stability for SGD and GD in the PL setting, we show that there exist simple quadratic models with multiple local minima where SGD is stable but GD is not.",http://proceedings.mlr.press/v80/charles18a.html,http://proceedings.mlr.press/v80/charles18a/charles18a.pdf,ICML
1113,2018,Neural Autoregressive Flows,"Chin-Wei Huang,         David Krueger,         Alexandre Lacoste,         Aaron Courville","Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) (Papamakarios et al., 2017), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time (Oord et al., 2017), via Inverse Autoregressive Flows (IAF) (Kingma et al., 2016). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.",http://proceedings.mlr.press/v80/huang18d.html,http://proceedings.mlr.press/v80/huang18d/huang18d.pdf,ICML
1114,2018,Modeling Others using Oneself in Multi-Agent Reinforcement Learning,"Roberta Raileanu,         Emily Denton,         Arthur Szlam,         Rob Fergus","We consider the multi-agent reinforcement learning setting with imperfect information. The reward function depends on the hidden goals of both agents, so the agents must infer the other players’ goals from their observed behavior in order to maximize their returns. We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent’s actions and update its belief of their hidden goal in an online manner. We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players’ goals, in both cooperative and competitive settings.",http://proceedings.mlr.press/v80/raileanu18a.html,http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf,ICML
1115,2018,Stein Points,"Wilson Ye Chen,         Lester Mackey,         Jackson Gorham,         Francois-Xavier Briol,         Chris Oates","An important task in computational statistics and machine learning is to approximate a posterior distribution p(x)p(x)p(x) with an empirical measure supported on a set of representative points {xi}ni=1{xi}i=1n\{x_i\}_{i=1}^n. This paper focuses on methods where the selection of points is essentially deterministic, with an emphasis on achieving accurate approximation when nnn is small. To this end, we present Stein Points. The idea is to exploit either a greedy or a conditional gradient method to iteratively minimise a kernel Stein discrepancy between the empirical measure and p(x)p(x)p(x). Our empirical results demonstrate that Stein Points enable accurate approximation of the posterior at modest computational cost. In addition, theoretical results are provided to establish convergence of the method.",http://proceedings.mlr.press/v80/chen18f.html,http://proceedings.mlr.press/v80/chen18f/chen18f.pdf,ICML
1116,2018,Path Consistency Learning in Tsallis Entropy Regularized MDPs,"Yinlam Chow,         Ofir Nachum,         Mohammad Ghavamzadeh","We study the sparse entropy-regularized reinforcement learning (ERL) problem in which the entropy term is a special form of the Tsallis entropy. The optimal policy of this formulation is sparse, i.e., at each state, it has non-zero probability for only a small number of actions. This addresses the main drawback of the standard Shannon entropy-regularized RL (soft ERL) formulation, in which the optimal policy is softmax, and thus, may assign a non-negligible probability mass to non-optimal actions. This problem is aggravated as the number of actions is increased. In this paper, we follow the work of Nachum et al. (2017) in the soft ERL setting, and propose a class of novel path consistency learning (PCL) algorithms, called sparse PCL, for the sparse ERL problem that can work with both on-policy and off-policy data. We first derive a sparse consistency equation that specifies a relationship between the optimal value function and policy of the sparse ERL along any system trajectory. Crucially, a weak form of the converse is also true, and we quantify the sub-optimality of a policy which satisfies sparse consistency, and show that as we increase the number of actions, this sub-optimality is better than that of the soft ERL optimal policy. We then use this result to derive the sparse PCL algorithms. We empirically compare sparse PCL with its soft counterpart, and show its advantage, especially in problems with a large number of actions.",http://proceedings.mlr.press/v80/chow18a.html,http://proceedings.mlr.press/v80/chow18a/chow18a.pdf,ICML
1117,2018,Investigating Human Priors for Playing Video Games,"Rachit Dubey,         Pulkit Agrawal,         Deepak Pathak,         Tom Griffiths,         Alexei Efros","What makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at https://rach0012.github.io/humanRL_website/",http://proceedings.mlr.press/v80/dubey18a.html,http://proceedings.mlr.press/v80/dubey18a/dubey18a.pdf,ICML
1118,2018,Bilevel Programming for Hyperparameter Optimization and Meta-Learning,"Luca Franceschi,         Paolo Frasconi,         Saverio Salzo,         Riccardo Grazzi,         Massimiliano Pontil","We introduce a framework based on bilevel programming that unifies gradient-based hyperparameter optimization and meta-learning. We show that an approximate version of the bilevel problem can be solved by taking into explicit account the optimization dynamics for the inner objective. Depending on the specific setting, the outer variables take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner. We provide sufficient conditions under which solutions of the approximate problem converge to those of the exact problem. We instantiate our approach for meta-learning in the case of deep learning where representation layers are treated as hyperparameters shared across a set of training episodes. In experiments, we confirm our theoretical findings, present encouraging results for few-shot learning and contrast the bilevel approach against classical approaches for learning-to-learn.",http://proceedings.mlr.press/v80/franceschi18a.html,http://proceedings.mlr.press/v80/franceschi18a/franceschi18a.pdf,ICML
1119,2018,Regret Minimization for Partially Observable Deep Reinforcement Learning,"Peter Jin,         Kurt Keutzer,         Sergey Levine","Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial observations by using finite length observation histories or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to an advantage-like function and is robust to partially observed state. We demonstrate that this new algorithm can substantially outperform strong baseline methods on several partially observed reinforcement learning tasks: learning first-person 3D navigation in Doom and Minecraft, and acting in the presence of partially observed objects in Doom and Pong.",http://proceedings.mlr.press/v80/jin18c.html,http://proceedings.mlr.press/v80/jin18c/jin18c.pdf,ICML
1120,2018,CoVeR: Learning Covariate-Specific Vector Representations with Tensor Decompositions,"Kevin Tian,         Teng Zhang,         James Zou","Word embedding is a useful approach to capture co-occurrence structures in large text corpora. However, in addition to the text data itself, we often have additional covariates associated with individual corpus documents—e.g. the demographic of the author, time and venue of publication—and we would like the embedding to naturally capture this information. We propose CoVeR, a new tensor decomposition model for vector embeddings with covariates. CoVeR jointly learns a base embedding for all the words as well as a weighted diagonal matrix to model how each covariate affects the base embedding. To obtain author or venue-specific embedding, for example, we can then simply multiply the base embedding by the associated transformation matrix. The main advantages of our approach are data efficiency and interpretability of the covariate transformation. Our experiments demonstrate that our joint model learns substantially better covariate-specific embeddings compared to the standard approach of learning a separate embedding for each covariate using only the relevant subset of data, as well as other related methods. Furthermore, CoVeR encourages the embeddings to be “topic-aligned” in that the dimensions have specific independent meanings. This allows our covariate-specific embeddings to be compared by topic, enabling downstream differential analysis. We empirically evaluate the benefits of our algorithm on datasets, and demonstrate how it can be used to address many natural questions about covariate effects.",http://proceedings.mlr.press/v80/tian18a.html,http://proceedings.mlr.press/v80/tian18a/tian18a.pdf,ICML
1121,2018,Firing Bandits: Optimizing Crowdfunding,"Lalit Jain,         Kevin Jamieson","In this paper, we model the problem of optimizing crowdfunding platforms, such as the non-profit Kiva or for-profit KickStarter, as a variant of the multi-armed bandit problem. In our setting, Bernoulli arms emit no rewards until their cumulative number of successes over any number of trials exceeds a fixed threshold and then provides no additional reward for any additional trials - a process reminiscent to that of a neuron firing once it reaches the action potential and then saturates. In the spirit of an infinite armed bandit problem, the player can add new arms whose expected probability of success is drawn iid from an unknown distribution – this endless supply of projects models the harsh reality that the number of projects seeking funding greatly exceeds the total capital available by lenders. Crowdfunding platforms naturally fall under this setting where the arms are potential projects, and their probability of success is the probability that a potential funder decides to fund it after reviewing it. The goal is to play arms (prioritize the display of projects on a webpage) to maximize the number of arms that reach the firing threshold (meet their goal amount) using as few total trials (number of impressions) as possible over all the played arms. We provide an algorithm for this setting and prove sublinear regret bounds.",http://proceedings.mlr.press/v80/jain18a.html,http://proceedings.mlr.press/v80/jain18a/jain18a.pdf,ICML
1122,2018,Scalable Gaussian Processes with Grid-Structured Eigenfunctions (GP-GRIEF),"Trefor Evans,         Prasanth Nair","We introduce a kernel approximation strategy that enables computation of the Gaussian process log marginal likelihood and all hyperparameter derivatives in O(p) time. Our GRIEF kernel consists of p eigenfunctions found using a Nystrom approximation from a dense Cartesian product grid of inducing points. By exploiting algebraic properties of Kronecker and Khatri-Rao tensor products, computational complexity of the training procedure can be practically independent of the number of inducing points. This allows us to use arbitrarily many inducing points to achieve a globally accurate kernel approximation, even in high-dimensional problems. The fast likelihood evaluation enables type-I or II Bayesian inference on large-scale datasets. We benchmark our algorithms on real-world problems with up to two-million training points and 10^33 inducing points.",http://proceedings.mlr.press/v80/evans18a.html,http://proceedings.mlr.press/v80/evans18a/evans18a.pdf,ICML
1123,2018,Deep Models of Interactions Across Sets,"Jason Hartford,         Devon Graham,         Kevin Leyton-Brown,         Siamak Ravanbakhsh","We use deep learning to model interactions across two or more sets of objects, such as user{–}movie ratings or protein{–}drug bindings. The canonical representation of such interactions is a matrix (or tensor) with an exchangeability property: the encoding’s meaning is not changed by permuting rows or columns. We argue that models should hence be Permutation Equivariant (PE): constrained to make the same predictions across such permutations. We present a parameter-sharing scheme and prove that it is maximally expressive under the PE constraint. This scheme yields three benefits. First, we demonstrate performance competitive with the state of the art on multiple matrix completion benchmarks. Second, our models require a number of parameters independent of the numbers of objects and thus scale well to large datasets. Third, models can be queried about new objects that were not available at training time, but for which interactions have since been observed. We observed surprisingly good generalization performance on this matrix extrapolation task, both within domains (e.g., new users and new movies drawn from the same distribution used for training) and even across domains (e.g., predicting music ratings after training on movie ratings).",http://proceedings.mlr.press/v80/hartford18a.html,http://proceedings.mlr.press/v80/hartford18a/hartford18a.pdf,ICML
1124,2018,Streaming Principal Component Analysis in Noisy Setting,"Teodor Vanislavov Marinov,         Poorya Mianjy,         Raman Arora","We study streaming algorithms for principal component analysis (PCA) in noisy settings. We present computationally efficient algorithms with sub-linear regret bounds for PCA in the presence of noise, missing data, and gross outliers.",http://proceedings.mlr.press/v80/marinov18a.html,http://proceedings.mlr.press/v80/marinov18a/marinov18a.pdf,ICML
1125,2018,Approximate message passing for amplitude based optimization,"Junjie Ma,         Ji Xu,         Arian Maleki","We consider an ℓ2\ell_2-regularized non-convex optimization problem for recovering signals from their noisy phaseless observations. We design and study the performance of a message passing algorithm that aims to solve this optimization problem. We consider the asymptotic setting m,n→∞m,n \rightarrow \infty, m/n→δm/n \rightarrow \delta and obtain sharp performance bounds, where mm is the number of measurements and nn is the signal dimension. We show that for complex signals the algorithm can perform accurate recovery with only m=(64π2−4)n≈2.5nm=\left ( \frac{64}{\pi^2}-4\right)n\approx 2.5n measurements. Also, we provide sharp analysis on the sensitivity of the algorithm to noise. We highlight the following facts about our message passing algorithm: (i) Adding ℓ2\ell_2 regularization to the non-convex loss function can be beneficial even in the noiseless setting; (ii) spectral initialization has marginal impact on the performance of the algorithm.",http://proceedings.mlr.press/v80/ma18e.html,http://proceedings.mlr.press/v80/ma18e/ma18e.pdf,ICML
1126,2018,SMAC: Simultaneous Mapping and Clustering Using Spectral Decompositions,"Chandrajit Bajaj,         Tingran Gao,         Zihang He,         Qixing Huang,         Zhenxiao Liang","We introduce a principled approach for simultaneous mapping and clustering (SMAC) for establishing consistent maps across heterogeneous object collections (e.g., 2D images or 3D shapes). Our approach takes as input a heterogeneous object collection and a set of maps computed between some pairs of objects, and outputs a homogeneous object clustering together with a new set of maps possessing optimal intra- and inter-cluster consistency. Our approach is based on the spectral decomposition of a data matrix storing all pairwise maps in its blocks. We additionally provide tight theoretical guarantees on the exactness of SMAC under established noise models. We also demonstrate the usefulness of the approach on synthetic and real datasets.",http://proceedings.mlr.press/v80/bajaj18a.html,http://proceedings.mlr.press/v80/bajaj18a/bajaj18a.pdf,ICML
1127,2018,Design of Experiments for Model Discrimination Hybridising Analytical and Data-Driven Approaches,"Simon Olofsson,         Marc Deisenroth,         Ruth Misener","Healthcare companies must submit pharmaceutical drugs or medical device to regulatory bodies before marketing new technology. Regulatory bodies frequently require transparent and interpretable computational modelling to justify a new healthcare technology, but researchers may have several competing models for a biological system and too little data to discriminate between the models. In design of experiments for model discrimination, where the goal is to design maximally informative physical experiments in order to discriminate between rival predictive models, research has focused either on analytical approaches, which cannot manage all functions, or on data-driven approaches, which may have computational difficulties or lack interpretable marginal predictive distributions. We develop a methodology for introducing Gaussian process surrogates in lieu of the original mechanistic models. This allows us to extend existing design and model discrimination methods developed for analytical models to cases of non-analytical models.",http://proceedings.mlr.press/v80/olofsson18a.html,http://proceedings.mlr.press/v80/olofsson18a/olofsson18a.pdf,ICML
1128,2018,Learning Low-Dimensional Temporal Representations,"Bing Su,         Ying Wu","Low-dimensional discriminative representations enhance machine learning methods in both performance and complexity, motivating supervised dimensionality reduction (DR) that transforms high-dimensional data to a discriminative subspace. Most DR methods require data to be i.i.d., however, in some domains, data naturally come in sequences, where the observations are temporally correlated. We propose a DR method called LT-LDA to learn low-dimensional temporal representations. We construct the separability among sequence classes by lifting the holistic temporal structures, which are established based on temporal alignments and may change in different subspaces. We jointly learn the subspace and the associated alignments by optimizing an objective which favors easily-separable temporal structures, and show that this objective is connected to the inference of alignments, thus allows an iterative solution. We provide both theoretical insight and empirical evaluation on real-world sequence datasets to show the interest of our method.",http://proceedings.mlr.press/v80/su18a.html,http://proceedings.mlr.press/v80/su18a/su18a.pdf,ICML
1129,2018,Greed is Still Good: Maximizing Monotone Submodular+Supermodular (BP) Functions,"Wenruo Bai,         Jeff Bilmes","We analyze the performance of the greedy algorithm, and also a discrete semi-gradient based algorithm, for maximizing the sum of a suBmodular and suPermodular (BP) function (both of which are non-negative monotone non-decreasing) under two types of constraints, either a cardinality constraint or p≥1p\geq 1 matroid independence constraints. These problems occur naturally in several real-world applications in data science, machine learning, and artificial intelligence. The problems are ordinarily inapproximable to any factor. Using the curvature \curvf\curv_f of the submodular term, and introducing \curvg\curv^g for the supermodular term (a natural dual curvature for supermodular functions), however, both of which are computable in linear time, we show that BP maximization can be efficiently approximated by both the greedy and the semi-gradient based algorithm. The algorithms yield multiplicative guarantees of 1\curvf[1−e−(1−\curvg)\curvf]\frac{1}{\curv_f}\left[1-e^{-(1-\curv^g)\curv_f}\right] and 1−\curvg(1−\curvg)\curvf+p\frac{1-\curv^g}{(1-\curv^g)\curv_f + p} for the two types of constraints respectively. For pure monotone supermodular constrained maximization, these yield 1−\curvg1-\curvg and (1−\curvg)/p(1-\curvg)/p for the two types of constraints respectively. We also analyze the hardness of BP maximization and show that our guarantees match hardness by a constant factor and by O(ln(p))O(\ln(p)) respectively. Computational experiments are also provided supporting our analysis.",http://proceedings.mlr.press/v80/bai18a.html,http://proceedings.mlr.press/v80/bai18a/bai18a.pdf,ICML
1130,2018,Learning in Integer Latent Variable Models with Nested Automatic Differentiation,"Daniel Sheldon,         Kevin Winner,         Debora Sujono","We develop nested automatic differentiation (AD) algorithms for exact inference and learning in integer latent variable models. Recently, Winner, Sujono, and Sheldon showed how to reduce marginalization in a class of integer latent variable models to evaluating a probability generating function which contains many levels of nested high-order derivatives. We contribute faster and more stable AD algorithms for this challenging problem and a novel algorithm to compute exact gradients for learning. These contributions lead to significantly faster and more accurate learning algorithms, and are the first AD algorithms whose running time is polynomial in the number of levels of nesting.",http://proceedings.mlr.press/v80/sheldon18a.html,http://proceedings.mlr.press/v80/sheldon18a/sheldon18a.pdf,ICML
1131,2018,GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models,"Jiaxuan You,         Rex Ying,         Xiang Ren,         William Hamilton,         Jure Leskovec","Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.",http://proceedings.mlr.press/v80/you18a.html,http://proceedings.mlr.press/v80/you18a/you18a.pdf,ICML
1132,2018,Message Passing Stein Variational Gradient Descent,"Jingwei Zhuo,         Chang Liu,         Jiaxin Shi,         Jun Zhu,         Ning Chen,         Bo Zhang","Stein variational gradient descent (SVGD) is a recently proposed particle-based Bayesian inference method, which has attracted a lot of interest due to its remarkable approximation ability and particle efficiency compared to traditional variational inference and Markov Chain Monte Carlo methods. However, we observed that particles of SVGD tend to collapse to modes of the target distribution, and this particle degeneracy phenomenon becomes more severe with higher dimensions. Our theoretical analysis finds out that there exists a negative correlation between the dimensionality and the repulsive force of SVGD which should be blamed for this phenomenon. We propose Message Passing SVGD (MP-SVGD) to solve this problem. By leveraging the conditional independence structure of probabilistic graphical models (PGMs), MP-SVGD converts the original high-dimensional global inference problem into a set of local ones over the Markov blanket with lower dimensions. Experimental results show its advantages of preventing vanishing repulsive force in high-dimensional space over SVGD, and its particle efficiency and approximation flexibility over other inference methods on graphical models.",http://proceedings.mlr.press/v80/zhuo18a.html,http://proceedings.mlr.press/v80/zhuo18a/zhuo18a.pdf,ICML
1133,2018,Gradually Updated Neural Networks for Large-Scale Image Recognition,"Siyuan Qiao,         Zhishuai Zhang,         Wei Shen,         Bo Wang,         Alan Yuille","Depth is one of the keys that make neural networks succeed in the task of large-scale image recognition. The state-of-the-art network architectures usually increase the depths by cascading convolutional layers or building blocks. In this paper, we present an alternative method to increase the depth. Our method is by introducing computation orderings to the channels within convolutional layers or blocks, based on which we gradually compute the outputs in a channel-wise manner. The added orderings not only increase the depths and the learning capacities of the networks without any additional computation costs, but also eliminate the overlap singularities so that the networks are able to converge faster and perform better. Experiments show that the networks based on our method achieve the state-of-the-art performances on CIFAR and ImageNet datasets.",http://proceedings.mlr.press/v80/qiao18b.html,http://proceedings.mlr.press/v80/qiao18b/qiao18b.pdf,ICML
1134,2018,Policy Optimization with Demonstrations,"Bingyi Kang,         Zequn Jie,         Jiashi Feng","Exploration remains a significant challenge to reinforcement learning methods, especially in environments where reward signals are sparse. Recent methods of learning from demonstrations have shown to be promising in overcoming exploration difficulties but typically require considerable high-quality demonstrations that are difficult to collect. We propose to effectively leverage available demonstrations to guide exploration through enforcing occupancy measure matching between the learned policy and current demonstrations, and develop a novel Policy Optimization from Demonstration (POfD) method. We show that POfD induces implicit dynamic reward shaping and brings provable benefits for policy improvement. Furthermore, it can be combined with policy gradient methods to produce state-of-the-art results, as demonstrated experimentally on a range of popular benchmark sparse-reward tasks, even when the demonstrations are few and imperfect.",http://proceedings.mlr.press/v80/kang18a.html,http://proceedings.mlr.press/v80/kang18a/kang18a.pdf,ICML
1135,2018,LaVAN: Localized and Visible Adversarial Noise,"Danny Karmon,         Daniel Zoran,         Yoav Goldberg","Most works on adversarial examples for deep-learning based image classifiers use noise that, while small, covers the entire image. We explore the case where the noise is allowed to be visible but confined to a small, localized patch of the image, without covering any of the main object(s) in the image. We show that it is possible to generate localized adversarial noises that cover only 2% of the pixels in the image, none of them over the main object, and that are transferable across images and locations, and successfully fool a state-of-the-art Inception v3 model with very high success rates.",http://proceedings.mlr.press/v80/karmon18a.html,http://proceedings.mlr.press/v80/karmon18a/karmon18a.pdf,ICML
1136,2018,A Spectral Approach to Gradient Estimation for Implicit Distributions,"Jiaxin Shi,         Shengyang Sun,         Jun Zhu","Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein’s identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystr{ö}m method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-of-sample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystr{ö}m method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.",http://proceedings.mlr.press/v80/shi18a.html,http://proceedings.mlr.press/v80/shi18a/shi18a.pdf,ICML
1137,2018,Quasi-Monte Carlo Variational Inference,"Alexander Buchholz,         Florian Wenzel,         Stephan Mandt","Many machine learning problems involve Monte Carlo gradient estimators. As a prominent example, we focus on Monte Carlo variational inference (MCVI) in this paper. The performance of MCVI crucially depends on the variance of its stochastic gradients. We propose variance reduction by means of Quasi-Monte Carlo (QMC) sampling. QMC replaces N i.i.d. samples from a uniform probability distribution by a deterministic sequence of samples of length N. This sequence covers the underlying random variable space more evenly than i.i.d. draws, reducing the variance of the gradient estimator. With our novel approach, both the score function and the reparameterization gradient estimators lead to much faster convergence. We also propose a new algorithm for Monte Carlo objectives, where we operate with a constant learning rate and increase the number of QMC samples per iteration. We prove that this way, our algorithm can converge asymptotically at a faster rate than SGD . We furthermore provide theoretical guarantees on qmc for Monte Carlo objectives that go beyond MCVI , and support our findings by several experiments on large-scale data sets from various domains.",http://proceedings.mlr.press/v80/buchholz18a.html,http://proceedings.mlr.press/v80/buchholz18a/buchholz18a.pdf,ICML
1138,2018,Learning Longer-term Dependencies in RNNs with Auxiliary Losses,"Trieu Trinh,         Andrew Dai,         Thang Luong,         Quoc Le","Despite recent advances in training recurrent neural networks (RNNs), capturing long-term dependencies in sequences remains a fundamental challenge. Most approaches use backpropagation through time (BPTT), which is difficult to scale to very long sequences. This paper proposes a simple method that improves the ability to capture long term dependencies in RNNs by adding an unsupervised auxiliary loss to the original objective. This auxiliary loss forces RNNs to either reconstruct previous events or predict next events in a sequence, making truncated backpropagation feasible for long sequences and also improving full BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel image classification with sequence lengths up to 16000, and a real document classification benchmark. Our results highlight good performance and resource efficiency of this approach over competitive baselines, including other recurrent models and a comparable sized Transformer. Further analyses reveal beneficial effects of the auxiliary loss on optimization and regularization, as well as extreme cases where there is little to no backpropagation.",http://proceedings.mlr.press/v80/trinh18a.html,http://proceedings.mlr.press/v80/trinh18a/trinh18a.pdf,ICML
1139,2018,Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations,"Yiping Lu,         Aoxiao Zhong,         Quanzheng Li,         Bin Dong","Deep neural networks have become the state-of-the-art models in numerous machine learning tasks. However, general guidance to network architecture design is still missing. In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress (>50%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.",http://proceedings.mlr.press/v80/lu18d.html,http://proceedings.mlr.press/v80/lu18d/lu18d.pdf,ICML
1140,2018,Black-Box Variational Inference for Stochastic Differential Equations,"Tom Ryder,         Andrew Golightly,         A. Stephen McGough,         Dennis Prangle","Parameter inference for stochastic differential equations is challenging due to the presence of a latent diffusion process. Working with an Euler-Maruyama discretisation for the diffusion, we use variational inference to jointly learn the parameters and the diffusion paths. We use a standard mean-field variational approximation of the parameter posterior, and introduce a recurrent neural network to approximate the posterior for the diffusion paths conditional on the parameters. This neural network learns how to provide Gaussian state transitions which bridge between observations in a very similar way to the conditioned diffusion process. The resulting black-box inference method can be applied to any SDE system with light tuning requirements. We illustrate the method on a Lotka-Volterra system and an epidemic model, producing accurate parameter estimates in a few hours.",http://proceedings.mlr.press/v80/ryder18a.html,http://proceedings.mlr.press/v80/ryder18a/ryder18a.pdf,ICML
1141,2018,Out-of-sample extension of graph adjacency spectral embedding,"Keith Levin,         Fred Roosta,         Michael Mahoney,         Carey Priebe","Many popular dimensionality reduction procedures have out-of-sample extensions, which allow a practitioner to apply a learned embedding to observations not seen in the initial training sample. In this work, we consider the problem of obtaining an out-of-sample extension for the adjacency spectral embedding, a procedure for embedding the vertices of a graph into Euclidean space. We present two different approaches to this problem, one based on a least-squares objective and the other based on a maximum-likelihood formulation. We show that if the graph of interest is drawn according to a certain latent position model called a random dot product graph, then both of these out-of-sample extensions estimate the true latent position of the out-of-sample vertex with the same error rate. Further, we prove a central limit theorem for the least-squares-based extension, showing that the estimate is asymptotically normal about the truth in the large-graph limit.",http://proceedings.mlr.press/v80/levin18a.html,http://proceedings.mlr.press/v80/levin18a/levin18a.pdf,ICML
1142,2018,Equivalence of Multicategory SVM and Simplex Cone SVM: Fast Computations and Statistical Theory,Guillaume Pouliot,"The multicategory SVM (MSVM) of Lee et al. (2004) is a natural generalization of the classical, binary support vector machines (SVM). However, its use has been limited by computational difficulties. The simplex-cone SVM (SCSVM) of Mroueh et al. (2012) is a computationally efficient multicategory classifier, but its use has been limited by a seemingly opaque interpretation. We show that MSVM and SCSVM are in fact exactly equivalent, and provide a bijection between their tuning parameters. MSVM may then be entertained as both a natural and computationally efficient multicategory extension of SVM. We further provide a Donsker theorem for finite-dimensional kernel MSVM and partially answer the open question pertaining to the very competitive performance of One-vs-Rest methods against MSVM. Furthermore, we use the derived asymptotic covariance formula to develop an inverse-variance weighted classification rule which improves on the One-vs-Rest approach.",http://proceedings.mlr.press/v80/pouliot18a.html,http://proceedings.mlr.press/v80/pouliot18a/pouliot18a.pdf,ICML
1143,2018,StrassenNets: Deep Learning with a Multiplication Budget,"Michael Tschannen,         Aran Khanna,         Animashree Anandkumar","A large fraction of the arithmetic operations required to evaluate deep neural networks (DNNs) consists of matrix multiplications, in both convolution and fully connected layers. We perform end-to-end learning of low-cost approximations of matrix multiplications in DNN layers by casting matrix multiplications as 2-layer sum-product networks (SPNs) (arithmetic circuits) and learning their (ternary) edge weights from data. The SPNs disentangle multiplication and addition operations and enable us to impose a budget on the number of multiplication operations. Combining our method with knowledge distillation and applying it to image classification DNNs (trained on ImageNet) and language modeling DNNs (using LSTMs), we obtain a first-of-a-kind reduction in number of multiplications (over 99.5%) while maintaining the predictive performance of the full-precision models. Finally, we demonstrate that the proposed framework is able to rediscover Strassen’s matrix multiplication algorithm, learning to multiply 2×22×22 \times 2 matrices using only 7 multiplications instead of 8.",http://proceedings.mlr.press/v80/tschannen18a.html,http://proceedings.mlr.press/v80/tschannen18a/tschannen18a.pdf,ICML
1144,2018,Least-Squares Temporal Difference Learning for the Linear Quadratic Regulator,"Stephen Tu,         Benjamin Recht","Reinforcement learning (RL) has been successfully used to solve many continuous control tasks. Despite its impressive results however, fundamental questions regarding the sample complexity of RL on continuous problems remain open. We study the performance of RL in this setting by considering the behavior of the Least-Squares Temporal Difference (LSTD) estimator on the classic Linear Quadratic Regulator (LQR) problem from optimal control. We give the first finite-time analysis of the number of samples needed to estimate the value function for a fixed static state-feedback policy to within epsilon-relative error. In the process of deriving our result, we give a general characterization for when the minimum eigenvalue of the empirical covariance matrix formed along the sample path of a fast-mixing stochastic process concentrates above zero, extending a result by Koltchinskii and Mendelson in the independent covariates setting. Finally, we provide experimental evidence indicating that our analysis correctly captures the qualitative behavior of LSTD on several LQR instances.",http://proceedings.mlr.press/v80/tu18a.html,http://proceedings.mlr.press/v80/tu18a/tu18a.pdf,ICML
1145,2018,Improved nearest neighbor search using auxiliary information and priority functions,"Omid Keivani,         Kaushik Sinha","Nearest neighbor search using random projection trees has recently been shown to achieve superior performance, in terms of better accuracy while retrieving less number of data points, compared to locality sensitive hashing based methods. However, to achieve acceptable nearest neighbor search accuracy for large scale applications, where number of data points and/or number of features can be very large, it requires users to maintain, store and search through large number of such independent random projection trees, which may be undesirable for many practical applications. To address this issue, in this paper we present different search strategies to improve nearest neighbor search performance of a single random projection tree. Our approach exploits properties of single and multiple random projections, which allows us to store meaningful auxiliary information at internal nodes of a random projection tree as well as to design priority functions to guide the search process that results in improved nearest neighbor search performance. Empirical results on multiple real world datasets show that our proposed method improves the search accuracy of a single tree compared to baseline methods.",http://proceedings.mlr.press/v80/keivani18a.html,http://proceedings.mlr.press/v80/keivani18a/keivani18a.pdf,ICML
1146,2018,BOHB: Robust and Efficient Hyperparameter Optimization at Scale,"Stefan Falkner,         Aaron Klein,         Frank Hutter","Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.",http://proceedings.mlr.press/v80/falkner18a.html,http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf,ICML
1147,2018,Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents,"Kaiqing Zhang,         Zhuoran Yang,         Han Liu,         Tong Zhang,         Tamer Basar","We consider the fully decentralized multi-agent reinforcement learning (MARL) problem, where the agents are connected via a time-varying and possibly sparse communication network. Specifically, we assume that the reward functions of the agents might correspond to different tasks, and are only known to the corresponding agent. Moreover, each agent makes individual decisions based on both the information observed locally and the messages received from its neighbors over the network. To maximize the globally averaged return over the network, we propose two fully decentralized actor-critic algorithms, which are applicable to large-scale MARL problems in an online fashion. Convergence guarantees are provided when the value functions are approximated within the class of linear functions. Our work appears to be the first theoretical study of fully decentralized MARL algorithms for networked agents that use function approximation.",http://proceedings.mlr.press/v80/zhang18n.html,http://proceedings.mlr.press/v80/zhang18n/zhang18n.pdf,ICML
1148,2018,Coded Sparse Matrix Multiplication,"Sinong Wang,         Jiashang Liu,         Ness Shroff","In a large-scale and distributed matrix multiplication problem C=A⊺BC=A⊺BC=A^{\intercal}B, where C∈Rr×tC∈Rr×tC\in\mathbb{R}^{r\times t}, the coded computation plays an important role to effectively deal with “stragglers” (distributed computations that may get delayed due to few slow or faulty processors). However, existing coded schemes could destroy the significant sparsity that exists in large-scale machine learning problems, and could result in much higher computation overhead, i.e., O(rt)O(rt)O(rt) decoding time. In this paper, we develop a new coded computation strategy, we call sparse code, which achieves near optimal recovery threshold, low computation overhead, and linear decoding time O(nnz(C))O(nnz(C))O(nnz(C)). We implement our scheme and demonstrate the advantage of the approach over both uncoded and current fastest coded strategies.",http://proceedings.mlr.press/v80/wang18e.html,http://proceedings.mlr.press/v80/wang18e/wang18e.pdf,ICML
1149,2018,Weakly Consistent Optimal Pricing Algorithms in Repeated Posted-Price Auctions with Strategic Buyer,Alexey Drutsa,We study revenue optimization learning algorithms for repeated posted-price auctions where a seller interacts with a single strategic buyer that holds a fixed private valuation for a good and seeks to maximize his cumulative discounted surplus. We propose a novel algorithm that never decreases offered prices and has a tight strategic regret bound of Θ(loglogT)Θ(log⁡log⁡T)\Theta(\log\log T). This result closes the open research question on the existence of a no-regret horizon-independent weakly consistent pricing. We also show that the property of non-decreasing prices is nearly necessary for a weakly consistent algorithm to be a no-regret one.,http://proceedings.mlr.press/v80/drutsa18a.html,http://proceedings.mlr.press/v80/drutsa18a/drutsa18a.pdf,ICML
1150,2018,On Learning Sparsely Used Dictionaries from Incomplete Samples,"Thanh Nguyen,         Akshay Soni,         Chinmay Hegde","Existing algorithms for dictionary learning assume that the entries of the (high-dimensional) input data are fully observed. However, in several practical applications, only an incomplete fraction of the data entries may be available. For incomplete settings, no provably correct and polynomial-time algorithm has been reported in the dictionary learning literature. In this paper, we provide provable approaches for learning – from incomplete samples – a family of dictionaries whose atoms have sufficiently “spread-out” mass. First, we propose a descent-style iterative algorithm that linearly converges to the true dictionary when provided a sufficiently coarse initial estimate. Second, we propose an initialization algorithm that utilizes a small number of extra fully observed samples to produce such a coarse initial estimate. Finally, we theoretically analyze their performance and provide asymptotic statistical and computational guarantees.",http://proceedings.mlr.press/v80/nguyen18e.html,http://proceedings.mlr.press/v80/nguyen18e/nguyen18e.pdf,ICML
1151,2018,GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks,"Zhao Chen,         Vijay Badrinarayanan,         Chen-Yu Lee,         Andrew Rabinovich","Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter αα\alpha. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.",http://proceedings.mlr.press/v80/chen18a.html,http://proceedings.mlr.press/v80/chen18a/chen18a.pdf,ICML
1152,2018,Understanding Generalization and Optimization Performance of Deep CNNs,"Pan Zhou,         Jiashi Feng","This work aims to provide understandings on the remarkable success of deep convolutional neural networks (CNNs) by theoretically analyzing their generalization performance and establishing optimization guarantees for gradient descent based training algorithms. Specifically, for a CNN model consisting of lll convolutional layers and one fully connected layer, we prove that its generalization error is bounded by O(θϱ˜/n−−−−√)O(θϱ~/n)\mathcal{O}(\sqrt{\theta\widetilde{\varrho}/n}) where θθ\theta denotes freedom degree of the network parameters and ϱ˜=O(log(∏li=1bi(ki−si+1)/p)+log(bl+1))ϱ~=O(log⁡(∏i=1lbi(ki−si+1)/p)+log⁡(bl+1))\widetilde{\varrho}=\mathcal{O}(\log(\prod_{i=1}^{l}b_{i} (k_{i}-s_{i}+1)/p)+\log(b_{l+1})) encapsulates architecture parameters including the kernel size kikik_{i}, stride sisis_{i}, pooling size ppp and parameter magnitude bibib_{i}. To our best knowledge, this is the first generalization bound that only depends on O(log(∏l+1i=1bi))O(log⁡(∏i=1l+1bi))\mathcal{O}(\log(\prod_{i=1}^{l+1}b_{i})), tighter than existing ones that all involve an exponential term like O(∏l+1i=1bi)O(∏i=1l+1bi)\mathcal{O}(\prod_{i=1}^{l+1}b_{i}). Besides, we prove that for an arbitrary gradient descent algorithm, the computed approximate stationary point by minimizing empirical risk is also an approximate stationary point to the population risk. This well explains why gradient descent training algorithms usually perform sufficiently well in practice. Furthermore, we prove the one-to-one correspondence and convergence guarantees for the non-degenerate stationary points between the empirical and population risks. It implies that the computed local minimum for the empirical risk is also close to a local minimum for the population risk, thus ensuring that the optimized CNN model well generalizes to new data.",http://proceedings.mlr.press/v80/zhou18a.html,http://proceedings.mlr.press/v80/zhou18a/zhou18a.pdf,ICML
1153,2018,Adversarially Regularized Autoencoders,"Junbo Zhao,         Yoon Kim,         Kelly Zhang,         Alexander Rush,         Yann LeCun","Deep latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for representation learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a more flexible method for training deep latent variable models of discrete structures. Our approach is based on the recently proposed Wasserstein Autoencoder (WAE) which formalizes adversarial autoencoders as an optimal transport problem. We first extend this framework to model discrete sequences, and then further explore different learned priors targeting a controllable representation. Unlike many other latent variable generative models for text, this adversarially regularized autoencoder (ARAE) allows us to generate fluent textual outputs as well as perform manipulations in the latent space to induce change in the output space. Finally we show that the latent representation can be trained to perform unaligned textual style transfer, giving improvements both in automatic measures and human evaluation.",http://proceedings.mlr.press/v80/zhao18b.html,http://proceedings.mlr.press/v80/zhao18b/zhao18b.pdf,ICML
1154,2018,Efficient end-to-end learning for quantizable representations,"Yeonwoo Jeong,         Hyun Oh Song","Embedding representation learning via neural networks is at the core foundation of modern similarity based search. While much effort has been put in developing algorithms for learning binary hamming code representations for search efficiency, this still requires a linear scan of the entire dataset per each query and trades off the search accuracy through binarization. To this end, we consider the problem of directly learning a quantizable embedding representation and the sparse binary hash code end-to-end which can be used to construct an efficient hash table not only providing significant search reduction in the number of data but also achieving the state of the art search accuracy outperforming previous state of the art deep metric learning methods. We also show that finding the optimal sparse binary hash code in a mini-batch can be computed exactly in polynomial time by solving a minimum cost flow problem. Our results on Cifar-100 and on ImageNet datasets show the state of the art search accuracy in precision@k and NMI metrics while providing up to 98X and 478X search speedup respectively over exhaustive linear search. The source code is available at https://github.com/maestrojeong/Deep-Hash-Table-ICML18.",http://proceedings.mlr.press/v80/jeong18a.html,http://proceedings.mlr.press/v80/jeong18a/jeong18a.pdf,ICML
1155,2018,Semi-Amortized Variational Autoencoders,"Yoon Kim,         Sam Wiseman,         Andrew Miller,         David Sontag,         Alexander Rush","Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.",http://proceedings.mlr.press/v80/kim18e.html,http://proceedings.mlr.press/v80/kim18e/kim18e.pdf,ICML
1156,2018,Deep Asymmetric Multi-task Feature Learning,"Hae Beom Lee,         Eunho Yang,         Sung Ju Hwang","We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can learn deep representations shared across multiple tasks while effectively preventing negative transfer that may happen in the feature sharing process. Specifically, we introduce an asymmetric autoencoder term that allows reliable predictors for the easy tasks to have high contribution to the feature learning while suppressing the influences of unreliable predictors for more difficult tasks. This allows the learning of less noisy representations, and enables unreliable predictors to exploit knowledge from the reliable predictors via the shared latent features. Such asymmetric knowledge transfer through shared features is also more scalable and efficient than inter-task asymmetric transfer. We validate our Deep-AMTFL model on multiple benchmark datasets for multitask learning and image classification, on which it significantly outperforms existing symmetric and asymmetric multitask learning models, by effectively preventing negative transfer in deep feature learning.",http://proceedings.mlr.press/v80/lee18d.html,http://proceedings.mlr.press/v80/lee18d/lee18d.pdf,ICML
1157,2018,Learning to Explain: An Information-Theoretic Perspective on Model Interpretation,"Jianbo Chen,         Le Song,         Martin Wainwright,         Michael Jordan","We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.",http://proceedings.mlr.press/v80/chen18j.html,http://proceedings.mlr.press/v80/chen18j/chen18j.pdf,ICML
1158,2018,Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models,"Raj Agrawal,         Caroline Uhler,         Tamara Broderick","Learning a Bayesian network (BN) from data can be useful for decision-making or discovering causal relationships. However, traditional methods often fail in modern applications, which exhibit a larger number of observed variables than data points. The resulting uncertainty about the underlying network as well as the desire to incorporate prior information recommend a Bayesian approach to learning the BN, but the highly combinatorial structure of BNs poses a striking challenge for inference. The current state-of-the-art methods such as order MCMC are faster than previous methods but prevent the use of many natural structural priors and still have running time exponential in the maximum indegree of the true directed acyclic graph (DAG) of the BN. We here propose an alternative posterior approximation based on the observation that, if we incorporate empirical conditional independence tests, we can focus on a high-probability DAG associated with each order of the vertices. We show that our method allows the desired flexibility in prior specification, removes timing dependence on the maximum indegree, and yields provably good posterior approximations; in addition, we show that it achieves superior accuracy, scalability, and sampler mixing on several datasets.",http://proceedings.mlr.press/v80/agrawal18a.html,http://proceedings.mlr.press/v80/agrawal18a/agrawal18a.pdf,ICML
1159,2018,Dependent Relational Gamma Process Models for Longitudinal Networks,"Sikun Yang,         Heinz Koeppl","A probabilistic framework based on the covariate-dependent relational gamma process is developed to analyze relational data arising from longitudinal networks. The proposed framework characterizes networked nodes by nonnegative node-group memberships, which allow each node to belong to multiple latent groups simultaneously, and encodes edge probabilities between each pair of nodes using a Bernoulli Poisson link to the embedded latent space. Within the latent space, our framework models the birth and death dynamics of individual groups via a thinning function. Our framework also captures the evolution of individual node-group memberships over time using gamma Markov processes. Exploiting the recent advances in data augmentation and marginalization techniques, a simple and efficient Gibbs sampler is proposed for posterior computation. Experimental results on a simulation study and three real-world temporal network data sets demonstrate the model’s capability, competitive performance and scalability compared to state-of-the-art methods.",http://proceedings.mlr.press/v80/yang18b.html,http://proceedings.mlr.press/v80/yang18b/yang18b.pdf,ICML
1160,2018,State Abstractions for Lifelong Reinforcement Learning,"David Abel,         Dilip Arumugam,         Lucas Lehnert,         Michael Littman","In lifelong reinforcement learning, agents must effectively transfer knowledge across tasks while simultaneously addressing exploration, credit assignment, and generalization. State abstraction can help overcome these hurdles by compressing the representation used by an agent, thereby reducing the computational and statistical burdens of learning. To this end, we here develop theory to compute and use state abstractions in lifelong reinforcement learning. We introduce two new classes of abstractions: (1) transitive state abstractions, whose optimal form can be computed efficiently, and (2) PAC state abstractions, which are guaranteed to hold with respect to a distribution of tasks. We show that the joint family of transitive PAC abstractions can be acquired efficiently, preserve near optimal-behavior, and experimentally reduce sample complexity in simple domains, thereby yielding a family of desirable abstractions for use in lifelong reinforcement learning. Along with these positive results, we show that there are pathological cases where state abstractions can negatively impact performance.",http://proceedings.mlr.press/v80/abel18a.html,http://proceedings.mlr.press/v80/abel18a/abel18a.pdf,ICML
1161,2018,Stochastic Variance-Reduced Hamilton Monte Carlo Methods,"Difan Zou,         Pan Xu,         Quanquan Gu","We propose a fast stochastic Hamilton Monte Carlo (HMC) method, for sampling from a smooth and strongly log-concave distribution. At the core of our proposed method is a variance reduction technique inspired by the recent advance in stochastic optimization. We show that, to achieve ϵϵ\epsilon accuracy in 2-Wasserstein distance, our algorithm achieves O~(n+κ2d1/2/ϵ+κ4/3d1/3n2/3/ϵ2/3)O~(n+κ2d1/2/ϵ+κ4/3d1/3n2/3/ϵ2/3)\tilde O\big(n+\kappa^{2}d^{1/2}/\epsilon+\kappa^{4/3}d^{1/3}n^{2/3}/\epsilon^{2/3}\big) gradient complexity (i.e., number of component gradient evaluations), which outperforms the state-of-the-art HMC and stochastic gradient HMC methods in a wide regime. We also extend our algorithm for sampling from smooth and general log-concave distributions, and prove the corresponding gradient complexity as well. Experiments on both synthetic and real data demonstrate the superior performance of our algorithm.",http://proceedings.mlr.press/v80/zou18a.html,http://proceedings.mlr.press/v80/zou18a/zou18a.pdf,ICML
1162,2018,Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory,"Ron Amit,         Ron Meir","In meta-learning an agent extracts knowledge from observed tasks, aiming to facilitate learning of novel future tasks. Under the assumption that future tasks are ‘related’ to previous tasks, accumulated knowledge should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task. We present a framework for meta-learning that is based on generalization error bounds, allowing us to extend various PAC-Bayes bounds to meta-learning. Learning takes place through the construction of a distribution over hypotheses based on the observed tasks, and its utilization for learning a new task. Thus, prior knowledge is incorporated through setting an experience-dependent prior for novel tasks. We develop a gradient-based algorithm, and implement it for deep neural networks, based on minimizing an objective function derived from the bounds, and demonstrate its effectiveness numerically. In addition to establishing the improved performance available through meta-learning, we demonstrate the intuitive way by which prior information is manifested at different levels of the network.",http://proceedings.mlr.press/v80/amit18a.html,http://proceedings.mlr.press/v80/amit18a/amit18a.pdf,ICML
1163,2018,Data Summarization at Scale: A Two-Stage Submodular Approach,"Marko Mitrovic,         Ehsan Kazemi,         Morteza Zadimoghaddam,         Amin Karbasi","The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset. Fortunately, the vast majority of data summarization tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-optimal solutions in linear time. We focus on a two-stage submodular framework where the goal is to use some given training functions to reduce the ground set so that optimizing new functions (drawn from the same distribution) over the reduced set provides almost as much value as optimizing them over the entire ground set. In this paper, we develop the first streaming and distributed solutions to this problem. In addition to providing strong theoretical guarantees, we demonstrate both the utility and efficiency of our algorithms on real-world tasks including image summarization and ride-share optimization.",http://proceedings.mlr.press/v80/mitrovic18a.html,http://proceedings.mlr.press/v80/mitrovic18a/mitrovic18a.pdf,ICML
1164,2018,Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations,"Ting Chen,         Martin Renqiang Min,         Yizhou Sun","Conventional embedding methods directly associate each symbol with a continuous embedding vector, which is equivalent to applying a linear transformation based on a “one-hot” encoding of the discrete symbols. Despite its simplicity, such approach yields the number of parameters that grows linearly with the vocabulary size and can lead to overfitting. In this work, we propose a much more compact K-way D-dimensional discrete encoding scheme to replace the “one-hot"" encoding. In the proposed “KD encoding”, each symbol is represented by a DDD-dimensional code with a cardinality of KKK, and the final symbol embedding vector is generated by composing the code embedding vectors. To end-to-end learn semantically meaningful codes, we derive a relaxed discrete optimization approach based on stochastic gradient descent, which can be generally applied to any differentiable computational graph with an embedding layer. In our experiments with various applications from natural language processing to graph convolutional networks, the total size of the embedding layer can be reduced up to 98% while achieving similar or better performance.",http://proceedings.mlr.press/v80/chen18g.html,http://proceedings.mlr.press/v80/chen18g/chen18g.pdf,ICML
1165,2018,JointGAN: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets,"Yunchen Pu,         Shuyang Dai,         Zhe Gan,         Weiyao Wang,         Guoyin Wang,         Yizhe Zhang,         Ricardo Henao,         Lawrence Carin Duke","A new generative adversarial network is developed for joint distribution matching.Distinct from most existing approaches, that only learn conditional distributions, the proposed model aims to learn a joint distribution of multiple random variables (domains). This is achieved by learning to sample from conditional distributions between the domains, while simultaneously learning to sample from the marginals of each individual domain.The proposed framework consists of multiple generators and a single softmax-based critic, all jointly trained via adversarial learning.From a simple noise source, the proposed framework allows synthesis of draws from the marginals, conditional draws given observations from a subset of random variables, or complete draws from the full joint distribution. Most examples considered are for joint analysis of two domains, with examples for three domains also presented.",http://proceedings.mlr.press/v80/pu18a.html,http://proceedings.mlr.press/v80/pu18a/pu18a.pdf,ICML
1166,2018,Neural Dynamic Programming for Musical Self Similarity,"Christian Walder,         Dongwoo Kim","We present a neural sequence model designed specifically for symbolic music. The model is based on a learned edit distance mechanism which generalises a classic recursion from computer science, leading to a neural dynamic program. Repeated motifs are detected by learning the transformations between them. We represent the arising computational dependencies using a novel data structure, the edit tree; this perspective suggests natural approximations which afford the scaling up of our otherwise cubic time algorithm. We demonstrate our model on real and synthetic data; in all cases it out-performs a strong stacked long short-term memory benchmark.",http://proceedings.mlr.press/v80/walder18a.html,http://proceedings.mlr.press/v80/walder18a/walder18a.pdf,ICML
1167,2018,Does Distributionally Robust Supervised Learning Give Robust Classifiers?,"Weihua Hu,         Gang Niu,         Issei Sato,         Masashi Sugiyama","Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable machine learning systems. When machine learning is deployed in the real world, its performance can be significantly degraded because test data may follow a different distribution from training data. DRSL with f-divergences explicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss. In this paper, we analyze this DRSL, focusing on the classification scenario. Since the DRSL is explicitly formulated for a distribution shift scenario, we naturally expect it to give a robust classifier that can aggressively handle shifted distributions. However, surprisingly, we prove that the DRSL just ends up giving a classifier that exactly fits the given training distribution, which is too pessimistic. This pessimism comes from two sources: the particular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide. Motivated by our analysis, we propose simple DRSL that overcomes this pessimism and empirically demonstrate its effectiveness.",http://proceedings.mlr.press/v80/hu18a.html,http://proceedings.mlr.press/v80/hu18a/hu18a.pdf,ICML
1168,2018,Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression,"Haitao Liu,         Jianfei Cai,         Yi Wang,         Yew Soon Ong","In order to scale standard Gaussian process (GP) regression to large-scale datasets, aggregation models employ factorized training process and then combine predictions from distributed experts. The state-of-the-art aggregation models, however, either provide inconsistent predictions or require time-consuming aggregation process. We first prove the inconsistency of typical aggregations using disjoint or random data partition, and then present a consistent yet efficient aggregation model for large-scale GP. The proposed model inherits the advantages of aggregations, e.g., closed-form inference and aggregation, parallelization and distributed computing. Furthermore, theoretical and empirical analyses reveal that the new aggregation model performs better due to the consistent predictions that converge to the true underlying function when the training size approaches infinity.",http://proceedings.mlr.press/v80/liu18a.html,http://proceedings.mlr.press/v80/liu18a/liu18a.pdf,ICML
1169,2018,Which Training Methods for GANs do actually Converge?,"Lars Mescheder,         Andreas Geiger,         Sebastian Nowozin","Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distributions lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.",http://proceedings.mlr.press/v80/mescheder18a.html,http://proceedings.mlr.press/v80/mescheder18a/mescheder18a.pdf,ICML
1170,2018,Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints,"Ehsan Kazemi,         Morteza Zadimoghaddam,         Amin Karbasi","Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation? We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted or masked due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against any number of adversarial deletions. We extensively evaluate the performance of our algorithms on real-world applications, including (i) Uber-pick up locations with location privacy constraints; (ii) feature selection with fairness constraints for income prediction and crime rate prediction; and (iii) robust to deletion summarization of census data, consisting of 2,458,285 feature vectors. Our experiments show that our solution is robust against even 808080% of data deletion.",http://proceedings.mlr.press/v80/kazemi18a.html,http://proceedings.mlr.press/v80/kazemi18a/kazemi18a.pdf,ICML
1171,2018,"Optimization, fast and slow: optimally switching between local and Bayesian optimization","Mark McLeod,         Stephen Roberts,         Michael A. Osborne","We develop the first Bayesian Optimization algorithm, BLOSSOM, which selects between multiple alternative acquisition functions and traditional local optimization at each step. This is combined with a novel stopping condition based on expected regret. This pairing allows us to obtain the best characteristics of both local and Bayesian optimization, making efficient use of function evaluations while yielding superior convergence to the global minimum on a selection of optimization problems, and also halting optimization once a principled and intuitive stopping condition has been fulfilled.",http://proceedings.mlr.press/v80/mcleod18a.html,http://proceedings.mlr.press/v80/mcleod18a/mcleod18a.pdf,ICML
1172,2018,Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings,"Aviral Kumar,         Sunita Sarawagi,         Ujjwal Jain","Modern neural networks have recently been found to be poorly calibrated, primarily in the direction of over-confidence. Methods like entropy penalty and temperature smoothing improve calibration by clamping confidence, but in doing so compromise the many legitimately confident predictions. We propose a more principled fix that minimizes an explicit calibration error during training. We present MMCE, a RKHS kernel based measure of calibration that is efficiently trainable alongside the negative likelihood loss without careful hyper-parameter tuning. Theoretically too, MMCE is a sound measure of calibration that is minimized at perfect calibration, and whose finite sample estimates are consistent and enjoy fast convergence rates. Extensive experiments on several network architectures demonstrate that MMCE is a fast, stable, and accurate method to minimize calibration error while maximally preserving the number of high confidence predictions.",http://proceedings.mlr.press/v80/kumar18a.html,http://proceedings.mlr.press/v80/kumar18a/kumar18a.pdf,ICML
1173,2018,Distributed Clustering via LSH Based Data Partitioning,"Aditya Bhaskara,         Maheshakya Wijewardena","Given the importance of clustering in the analysisof large scale data, distributed algorithms for formulations such as k-means, k-median, etc. have been extensively studied. A successful approach here has been the “reduce and merge” paradigm, in which each machine reduces its input size to {Õ}(k), and this data reduction continues (possibly iteratively) until all the data fits on one machine, at which point the problem is solved locally. This approach has the intrinsic bottleneck that each machine must solve a problem of size ≥≥\geq k, and needs to communicate at least ΩΩ\Omega(k) points to the other machines. We propose a novel data partitioning idea to overcome this bottleneck, and in effect, have different machines focus on “finding different clusters”. Under the assumption that we know the optimum value of the objective up to a poly(n) factor (arbitrary polynomial), we establish worst-case approximation guarantees for our method. We see that our algorithm results in lower communication as well as a near-optimal number of ‘rounds’ of computation (in the popular MapReduce framework).",http://proceedings.mlr.press/v80/bhaskara18a.html,http://proceedings.mlr.press/v80/bhaskara18a/bhaskara18a.pdf,ICML
1174,2018,Efficient Neural Architecture Search via Parameters Sharing,"Hieu Pham,         Melody Guan,         Barret Zoph,         Quoc Le,         Jeff Dean","We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. ENAS constructs a large computational graph, where each subgraph represents a neural network architecture, hence forcing all architectures to share their parameters. A controller is trained with policy gradient to search for a subgraph that maximizes the expected reward on a validation set. Meanwhile a model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, whilst using much fewer GPU-hours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel architecture that achieves a test perplexity of 56.3, on par with the existing state-of-the-art among all methods without post-training processing. On CIFAR-10, ENAS finds a novel architecture that achieves 2.89% test error, which is on par with the 2.65% test error of NASNet (Zoph et al., 2018).",http://proceedings.mlr.press/v80/pham18a.html,http://proceedings.mlr.press/v80/pham18a/pham18a.pdf,ICML
1175,2018,Optimal Distributed Learning with Multi-pass Stochastic Gradient Methods,"Junhong Lin,         Volkan Cevher","We study generalization properties of distributed algorithms in the setting of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We investigate distributed stochastic gradient methods (SGM), with mini-batches and multi-passes over the data. We show that optimal generalization error bounds can be retained for distributed SGM provided that the partition level is not too large. Our results are superior to the state-of-the-art theory, covering the cases that the regression function may not be in the hypothesis spaces. Particularly, our results show that distributed SGM has a smaller theoretical computational complexity, compared with distributed kernel ridge regression (KRR) and classic SGM.",http://proceedings.mlr.press/v80/lin18a.html,http://proceedings.mlr.press/v80/lin18a/lin18a.pdf,ICML
1176,2018,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,"Tuomas Haarnoja,         Aurick Zhou,         Pieter Abbeel,         Sergey Levine","Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",http://proceedings.mlr.press/v80/haarnoja18b.html,http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf,ICML
1177,2018,A Probabilistic Theory of Supervised Similarity Learning for Pointwise ROC Curve Optimization,"Robin Vogel,         Aurélien Bellet,         Stéphan Clémençon","The performance of many machine learning techniques depends on the choice of an appropriate similarity or distance measure on the input space. Similarity learning (or metric learning) aims at building such a measure from training data so that observations with the same (resp. different) label are as close (resp. far) as possible. In this paper, similarity learning is investigated from the perspective of pairwise bipartite ranking, where the goal is to rank the elements of a database by decreasing order of the probability that they share the same label with some query data point, based on the similarity scores. A natural performance criterion in this setting is pointwise ROC optimization: maximize the true positive rate under a fixed false positive rate. We study this novel perspective on similarity learning through a rigorous probabilistic framework. The empirical version of the problem gives rise to a constrained optimization formulation involving U-statistics, for which we derive universal learning rates as well as faster rates under a noise assumption on the data distribution. We also address the large-scale setting by analyzing the effect of sampling-based approximations. Our theoretical results are supported by illustrative numerical experiments.",http://proceedings.mlr.press/v80/vogel18a.html,http://proceedings.mlr.press/v80/vogel18a/vogel18a.pdf,ICML
1178,2018,Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care,"Patrick Schwab,         Emanuela Keller,         Carl Muroi,         David J. Mack,         Christian Strässle,         Walter Karlen","Patients in the intensive care unit (ICU) require constant and close supervision. To assist clinical staff in this task, hospitals use monitoring systems that trigger audiovisual alarms if their algorithms indicate that a patient’s condition may be worsening. However, current monitoring systems are extremely sensitive to movement artefacts and technical errors. As a result, they typically trigger hundreds to thousands of false alarms per patient per day - drowning the important alarms in noise and adding to the exhaustion of clinical staff. In this setting, data is abundantly available, but obtaining trustworthy annotations by experts is laborious and expensive. We frame the problem of false alarm reduction from multivariate time series as a machine-learning task and address it with a novel multitask network architecture that utilises distant supervision through multiple related auxiliary tasks in order to reduce the number of expensive labels required for training. We show that our approach leads to significant improvements over several state-of-the-art baselines on real-world ICU data and provide new insights on the importance of task selection and architectural choices in distantly supervised multitask learning.",http://proceedings.mlr.press/v80/schwab18a.html,http://proceedings.mlr.press/v80/schwab18a/schwab18a.pdf,ICML
1179,2018,One-Shot Segmentation in Clutter,"Claudio Michaelis,         Matthias Bethge,         Alexander Ecker","We tackle the problem of one-shot segmentation: finding and segmenting a previously unseen object in a cluttered scene based on a single instruction example. We propose a novel dataset, which we call cluttered Omniglot. Using a baseline architecture combining a Siamese embedding for detection with a U-net for segmentation we show that increasing levels of clutter make the task progressively harder. Using oracle models with access to various amounts of ground-truth information, we evaluate different aspects of the problem and show that in this kind of visual search task, detection and segmentation are two intertwined problems, the solution to each of which helps solving the other. We therefore introduce MaskNet, an improved model that attends to multiple candidate locations, generates segmentation proposals to mask out background clutter and selects among the segmented objects. Our findings suggest that such image recognition models based on an iterative refinement of object detection and foreground segmentation may provide a way to deal with highly cluttered scenes.",http://proceedings.mlr.press/v80/michaelis18a.html,http://proceedings.mlr.press/v80/michaelis18a/michaelis18a.pdf,ICML
1180,2018,"Binary Classification with Karmic, Threshold-Quasi-Concave Metrics","Bowei Yan,         Sanmi Koyejo,         Kai Zhong,         Pradeep Ravikumar","Complex performance measures, beyond the popular measure of accuracy, are increasingly being used in the context of binary classification. These complex performance measures are typically not even decomposable, that is, the loss evaluated on a batch of samples cannot typically be expressed as a sum or average of losses evaluated at individual samples, which in turn requires new theoretical and methodological developments beyond standard treatments of supervised learning. In this paper, we advance this understanding of binary classification for complex performance measures by identifying two key properties: a so-called Karmic property, and a more technical threshold-quasi-concavity property, which we show is milder than existing structural assumptions imposed on performance measures. Under these properties, we show that the Bayes optimal classifier is a threshold function of the conditional probability of positive class. We then leverage this result to come up with a computationally practical plug-in classifier, via a novel threshold estimator, and further, provide a novel statistical analysis of classification error with respect to complex performance measures.",http://proceedings.mlr.press/v80/yan18b.html,http://proceedings.mlr.press/v80/yan18b/yan18b.pdf,ICML
1181,2018,Accurate Uncertainties for Deep Learning Using Calibrated Regression,"Volodymyr Kuleshov,         Nathan Fenner,         Stefano Ermon","Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate {—} for example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.",http://proceedings.mlr.press/v80/kuleshov18a.html,http://proceedings.mlr.press/v80/kuleshov18a/kuleshov18a.pdf,ICML
1182,2018,Functional Gradient Boosting based on Residual Network Perception,"Atsushi Nitanda,         Taiji Suzuki","Residual Networks (ResNets) have become state-of-the-art models in deep learning and several theoretical studies have been devoted to understanding why ResNet works so well. One attractive viewpoint on ResNet is that it is optimizing the risk in a functional space by consisting of an ensemble of effective features. In this paper, we adopt this viewpoint to construct a new gradient boosting method, which is known to be very powerful in data analysis. To do so, we formalize the boosting perspective of ResNet mathematically using the notion of functional gradients and propose a new method called ResFGB for classification tasks by leveraging ResNet perception. Two types of generalization guarantees are provided from the optimization perspective: one is the margin bound and the other is the expected risk bound by the sample-splitting technique. Experimental results show superior performance of the proposed method over state-of-the-art methods such as LightGBM.",http://proceedings.mlr.press/v80/nitanda18a.html,http://proceedings.mlr.press/v80/nitanda18a/nitanda18a.pdf,ICML
1183,2018,Learning Deep ResNet Blocks Sequentially using Boosting Theory,"Furong Huang,         Jordan Ash,         John Langford,         Robert Schapire","We prove a multi-channel telescoping sum boosting theory for the ResNet architectures which simultaneously creates a new technique for boosting over features (in contrast with labels) and provides a new algorithm for ResNet-style architectures. Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures. Our method only requires the relatively inexpensive sequential training of TTT “shallow ResNets”. We prove that the training error decays exponentially with the depth TTT if the weak module classifiers that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l1l1l_1 norm bounded weights.",http://proceedings.mlr.press/v80/huang18b.html,http://proceedings.mlr.press/v80/huang18b/huang18b.pdf,ICML
1184,2018,Tempered Adversarial Networks,"Mehdi S. M. Sajjadi,         Giambattista Parascandolo,         Arash Mehrjou,         Bernhard Schölkopf","Generative adversarial networks (GANs) have been shown to produce realistic samples from high-dimensional distributions, but training them is considered hard. A possible explanation for training instabilities is the inherent imbalance between the networks: While the discriminator is trained directly on both real and fake samples, the generator only has control over the fake samples it produces since the real data distribution is fixed by the choice of a given dataset. We propose a simple modification that gives the generator control over the real samples which leads to a tempered learning process for both generator and discriminator. The real data distribution passes through a lens before being revealed to the discriminator, balancing the generator and discriminator by gradually revealing more detailed features necessary to produce high-quality results. The proposed module automatically adjusts the learning process to the current strength of the networks, yet is generic and easy to add to any GAN variant. In a number of experiments, we show that this can improve quality, stability and/or convergence speed across a range of different GAN architectures (DCGAN, LSGAN, WGAN-GP).",http://proceedings.mlr.press/v80/sajjadi18a.html,http://proceedings.mlr.press/v80/sajjadi18a/sajjadi18a.pdf,ICML
1185,2018,Stochastic Variance-Reduced Policy Gradient,"Matteo Papini,         Damiano Binaghi,         Giuseppe Canonaco,         Matteo Pirotta,         Marcello Restelli","In this paper, we propose a novel reinforcement-learning algorithm consisting in a stochastic variance-reduced version of policy gradient for solving Markov Decision Processes (MDPs). Stochastic variance-reduced gradient (SVRG) methods have proven to be very successful in supervised learning. However, their adaptation to policy gradient is not straightforward and needs to account for I) a non-concave objective function; II) approximations in the full gradient computation; and III) a non-stationary sampling process. The result is SVRPG, a stochastic variance-reduced policy gradient algorithm that leverages on importance weights to preserve the unbiasedness of the gradient estimate. Under standard assumptions on the MDP, we provide convergence guarantees for SVRPG with a convergence rate that is linear under increasing batch sizes. Finally, we suggest practical variants of SVRPG, and we empirically evaluate them on continuous MDPs.",http://proceedings.mlr.press/v80/papini18a.html,http://proceedings.mlr.press/v80/papini18a/papini18a.pdf,ICML
1186,2018,Multi-Fidelity Black-Box Optimization with Hierarchical Partitions,"Rajat Sen,         Kirthevasan Kandasamy,         Sanjay Shakkottai","Motivated by settings such as hyper-parameter tuning and physical simulations, we consider the problem of black-box optimization of a function. Multi-fidelity techniques have become popular for applications where exact function evaluations are expensive, but coarse (biased) approximations are available at much lower cost. A canonical example is that of hyper-parameter selection in a learning algorithm. The learning algorithm can be trained for fewer iterations – this results in a lower cost, but its validation error is only coarsely indicative of the same if the algorithm had been trained till completion. We incorporate the multi-fidelity setup into the powerful framework of black-box optimization through hierarchical partitioning. We develop tree-search based multi-fidelity algorithms with theoretical guarantees on simple regret. We finally demonstrate the performance gains of our algorithms on both real and synthetic datasets.",http://proceedings.mlr.press/v80/sen18a.html,http://proceedings.mlr.press/v80/sen18a/sen18a.pdf,ICML
1187,2018,Exploiting the Potential of Standard Convolutional Autoencoders for Image Restoration by Evolutionary Search,"Masanori Suganuma,         Mete Ozay,         Takayuki Okatani","Researchers have applied deep neural networks to image restoration tasks, in which they proposed various network architectures, loss functions, and training methods. In particular, adversarial training, which is employed in recent studies, seems to be a key ingredient to success. In this paper, we show that simple convolutional autoencoders (CAEs) built upon only standard network components, i.e., convolutional layers and skip connections, can outperform the state-of-the-art methods which employ adversarial training and sophisticated loss functions. The secret is to search for good architectures using an evolutionary algorithm. All we did was to train the optimized CAEs by minimizing the l2 loss between reconstructed images and their ground truths using the ADAM optimizer. Our experimental results show that this approach achieves 27.8 dB peak signal to noise ratio (PSNR) on the CelebA dataset and 33.3 dB on the SVHN dataset, compared to 22.8 dB and 19.0 dB provided by the former state-of-the-art methods, respectively.",http://proceedings.mlr.press/v80/suganuma18a.html,http://proceedings.mlr.press/v80/suganuma18a/suganuma18a.pdf,ICML
1188,2018,Stagewise Safe Bayesian Optimization with Gaussian Processes,"Yanan Sui,          Zhuang,         Joel Burdick,         Yisong Yue","Enforcing safety is a key aspect of many problems pertaining to sequential decision making under uncertainty, which require the decisions made at every step to be both informative of the optimal decision and also safe. For example, we value both efficacy and comfort in medical therapy, and efficiency and safety in robotic control. We consider this problem of optimizing an unknown utility function with absolute feedback or preference feedback subject to unknown safety constraints. We develop an efficient safe Bayesian optimization algorithm, StageOpt, that separates safe region expansion and utility function maximization into two distinct stages. Compared to existing approaches which interleave between expansion and optimization, we show that StageOpt is more efficient and naturally applicable to a broader class of problems. We provide theoretical guarantees for both the satisfaction of safety constraints as well as convergence to the optimal utility value. We evaluate StageOpt on both a variety of synthetic experiments, as well as in clinical practice. We demonstrate that StageOpt is more effective than existing safe optimization approaches, and is able to safely and effectively optimize spinal cord stimulation therapy in our clinical experiments.",http://proceedings.mlr.press/v80/sui18a.html,http://proceedings.mlr.press/v80/sui18a/sui18a.pdf,ICML
1189,2018,Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope,"Eric Wong,         Zico Kolter","We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded ℓ∞ℓ∞\ell_\infty norm less than ϵ=0.1ϵ=0.1\epsilon = 0.1).",http://proceedings.mlr.press/v80/wong18a.html,http://proceedings.mlr.press/v80/wong18a/wong18a.pdf,ICML
1190,2018,Do Outliers Ruin Collaboration?,Mingda Qiao,"We consider the problem of learning a binary classifier from nnn different data sources, among which at most an ηη\eta fraction are adversarial. The overhead is defined as the ratio between the sample complexity of learning in this setting and that of learning the same hypothesis class on a single data distribution. We present an algorithm that achieves an O(ηn+lnn)O(ηn+ln⁡n)O(\eta n + \ln n) overhead, which is proved to be worst-case optimal. We also discuss the potential challenges to the design of a computationally efficient learning algorithm with a small overhead.",http://proceedings.mlr.press/v80/qiao18a.html,http://proceedings.mlr.press/v80/qiao18a/qiao18a.pdf,ICML
1191,2018,Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?,"Maithra Raghu,         Alex Irpan,         Jacob Andreas,         Bobby Kleinberg,         Quoc Le,         Jon Kleinberg","Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior, and correspondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Spencer, and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning. These games have a number of appealing features: they are challenging for current learning approaches, but they form (i) a low-dimensional, simply parametrized environment where (ii) there is a linear closed form solution for optimal behavior from any state, and (iii) the difficulty of the game can be tuned by changing environment parameters in an interpretable way. We use these Erdos-Selfridge-Spencer games not only to compare different algorithms, but test for generalization, make comparisons to supervised learning, analyse multiagent play, and even develop a self play algorithm.",http://proceedings.mlr.press/v80/raghu18a.html,http://proceedings.mlr.press/v80/raghu18a/raghu18a.pdf,ICML
1192,2018,Local Convergence Properties of SAGA/Prox-SVRG and Acceleration,"Clarice Poon,         Jingwei Liang,         Carola Schoenlieb","In this paper, we present a local convergence anal- ysis for a class of stochastic optimisation meth- ods: the proximal variance reduced stochastic gradient methods, and mainly focus on SAGA (Defazio et al., 2014) and Prox-SVRG (Xiao & Zhang, 2014). Under the assumption that the non-smooth component of the optimisation prob- lem is partly smooth relative to a smooth mani- fold, we present a unified framework for the local convergence analysis of SAGA/Prox-SVRG: (i) the sequences generated by the methods are able to identify the smooth manifold in a finite num- ber of iterations; (ii) then the sequence enters a local linear convergence regime. Furthermore, we discuss various possibilities for accelerating these algorithms, including adapting to better lo- cal parameters, and applying higher-order deter- ministic/stochastic optimisation methods which can achieve super-linear convergence. Several concrete examples arising from machine learning are considered to demonstrate the obtained result.",http://proceedings.mlr.press/v80/poon18a.html,http://proceedings.mlr.press/v80/poon18a/poon18a.pdf,ICML
1193,2018,A Classification-Based Study of Covariate Shift in GAN Distributions,"Shibani Santurkar,         Ludwig Schmidt,         Aleksander Madry","A basic, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether they are truly able to capture all the fundamental characteristics of the distributions they are trained on. In particular, evaluating the diversity of GAN distributions is challenging and existing methods provide only a partial understanding of this issue. In this paper, we develop quantitative and scalable tools for assessing the diversity of GAN distributions. Specifically, we take a classification-based perspective and view loss of diversity as a form of covariate shift introduced by GANs. We examine two specific forms of such shift: mode collapse and boundary distortion. In contrast to prior work, our methods need only minimal human supervision and can be readily applied to state-of-the-art GANs on large, canonical datasets. Examining popular GANs using our tools indicates that these GANs have significant problems in reproducing the more distributional properties of their training dataset.",http://proceedings.mlr.press/v80/santurkar18a.html,http://proceedings.mlr.press/v80/santurkar18a/santurkar18a.pdf,ICML
1194,2018,Learning to Branch,"Maria-Florina Balcan,         Travis Dick,         Tuomas Sandholm,         Ellen Vitercik","Tree search algorithms, such as branch-and-bound, are the most widely used tools for solving combinatorial problems. These algorithms recursively partition the search space to find an optimal solution. To keep the tree small, it is crucial to carefully decide, when expanding a tree node, which variable to branch on at that node to partition the remaining space. Many partitioning techniques have been proposed, but no theory describes which is optimal. We show how to use machine learning to determine an optimal weighting of any set of partitioning procedures for the instance distribution at hand using samples. Via theory and experiments, we show that learning to branch is both practical and hugely beneficial.",http://proceedings.mlr.press/v80/balcan18a.html,http://proceedings.mlr.press/v80/balcan18a/balcan18a.pdf,ICML
1195,2018,Conditional Neural Processes,"Marta Garnelo,         Dan Rosenbaum,         Christopher Maddison,         Tiago Ramalho,         David Saxton,         Murray Shanahan,         Yee Whye Teh,         Danilo Rezende,         S. M. Ali Eslami","Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet, GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification and image completion.",http://proceedings.mlr.press/v80/garnelo18a.html,http://proceedings.mlr.press/v80/garnelo18a/garnelo18a.pdf,ICML
1196,2018,Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising,"Borja Balle,         Yu-Xiang Wang","The Gaussian mechanism is an essential building block used in multitude of differentially private data analysis algorithms. In this paper we revisit the Gaussian mechanism and show that the original analysis has several important limitations. Our analysis reveals that the variance formula for the original mechanism is far from tight in the high privacy regime (ε→0ε→0\varepsilon \to 0) and it cannot be extended to the low privacy regime (ε→∞ε→∞\varepsilon \to \infty). We address these limitations by developing an optimal Gaussian mechanism whose variance is calibrated directly using the Gaussian cumulative density function instead of a tail bound approximation. We also propose to equip the Gaussian mechanism with a post-processing step based on adaptive estimation techniques by leveraging that the distribution of the perturbation is known. Our experiments show that analytical calibration removes at least a third of the variance of the noise compared to the classical Gaussian mechanism, and that denoising dramatically improves the accuracy of the Gaussian mechanism in the high-dimensional regime.",http://proceedings.mlr.press/v80/balle18a.html,http://proceedings.mlr.press/v80/balle18a/balle18a.pdf,ICML
1197,2018,Competitive Caching with Machine Learned Advice,"Thodoris Lykouris,         Sergei Vassilvtiskii","We develop a framework for augmenting online algorithms with a machine learned oracle to achieve competitive ratios that provably improve upon unconditional worst case lower bounds when the oracle has low error. Our approach treats the oracle as a complete black box, and is not dependent on its inner workings, or the exact distribution of its errors. We apply this framework to the traditional caching problem {—} creating an eviction strategy for a cache of size k. We demonstrate that naively following the oracle’s recommendations may lead to very poor performance, even when the average error is quite low. Instead we show how to modify the Marker algorithm to take into account the oracle’s predictions, and prove that this combined approach achieves a competitive ratio that both (i) decreases as the oracle’s error decreases, and (ii) is always capped by O(log k), which can be achieved without any oracle input. We complement our results with an empirical evaluation of our algorithm on real world datasets, and show that it performs well empirically even using simple off the shelf predictions.",http://proceedings.mlr.press/v80/lykouris18a.html,http://proceedings.mlr.press/v80/lykouris18a/lykouris18a.pdf,ICML
1198,2018,Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions,"Junru Wu,         Yue Wang,         Zhenyu Wu,         Zhangyang Wang,         Ashok Veeraraghavan,         Yingyan Lin","The current trend of pushing CNNs deeper with convolutions has created a pressing demand to achieve higher compression gains on CNNs where convolutions dominate the computation and parameter amount (e.g., GoogLeNet, ResNet and Wide ResNet). Further, the high energy consumption of convolutions limits its deployment on mobile devices. To this end, we proposed a simple yet effective scheme for compressing convolutions though applying k-means clustering on the weights, compression is achieved through weight-sharing, by only recording KKK cluster centers and weight assignment indexes. We then introduced a novel spectrally relaxed kkk-means regularization, which tends to make hard assignments of convolutional layer weights to KKK learned cluster centers during re-training. We additionally propose an improved set of metrics to estimate energy consumption of CNN hardware implementations, whose estimation results are verified to be consistent with previously proposed energy estimation tool extrapolated from actual hardware measurements. We finally evaluated Deep kkk-Means across several CNN models in terms of both compression ratio and energy consumption reduction, observing promising results without incurring accuracy loss. The code is available at https://github.com/Sandbox3aster/Deep-K-Means",http://proceedings.mlr.press/v80/wu18h.html,http://proceedings.mlr.press/v80/wu18h/wu18h.pdf,ICML
1199,2018,Constrained Interacting Submodular Groupings,"Andrew Cotter,         Mahdi Milani Fard,         Seungil You,         Maya Gupta,         Jeff Bilmes","We introduce the problem of grouping a finite ground set into blocks where each block is a subset of the ground set and where: (i) the blocks are individually highly valued by a submodular function (both robustly and in the average case) while satisfying block-specific matroid constraints; and (ii) block scores interact where blocks are jointly scored highly, thus making the blocks mutually non-redundant. Submodular functions are good models of information and diversity; thus, the above can be seen as grouping the ground set into matroid constrained blocks that are both intra- and inter-diverse. Potential applications include forming ensembles of classification/regression models, partitioning data for parallel processing, and summarization. In the non-robust case, we reduce the problem to non-monotone submodular maximization subject to multiple matroid constraints. In the mixed robust/average case, we offer a bi-criterion guarantee for a polynomial time deterministic algorithm and a probabilistic guarantee for randomized algorithm, as long as the involved submodular functions (including the inter-block interaction terms) are monotone. We close with a case study in which we use these algorithms to find high quality diverse ensembles of classifiers, showing good results.",http://proceedings.mlr.press/v80/cotter18a.html,http://proceedings.mlr.press/v80/cotter18a/cotter18a.pdf,ICML
1200,2018,The Hidden Vulnerability of Distributed Learning in Byzantium,"El Mahdi El Mhamdi,         Rachid Guerraoui,         Sébastien Rouault","While machine learning is going through an era of celebrated success, concerns have been raised about the vulnerability of its backbone: stochastic gradient descent (SGD). Recent approaches have been proposed to ensure the robustness of distributed SGD against adversarial (Byzantine) workers sending poisoned gradients during the training phase. Some of these approaches have been proven Byzantine–resilient: they ensure the convergence of SGD despite the presence of a minority of adversarial workers. We show in this paper that convergence is not enough. In high dimension d≫1d \gg 1, an adver\-sary can build on the loss function’s non–convexity to make SGD converge to ineffective models. More precisely, we bring to light that existing Byzantine–resilient schemes leave a margin of poisoning of \bigOmega(f(d))\bigOmega\left(f(d)\right), where f(d)f(d) increases at least like d−−√p\sqrt[p]{d }. Based on this leeway, we build a simple attack, and experimentally show its strong to utmost effectivity on CIFAR–10 and MNIST. We introduce Bulyan, and prove it significantly reduces the attackers leeway to a narrow \bigO(\sfrac1d−−√)\bigO\,( \sfrac{1}{\sqrt{d }}) bound. We empirically show that Bulyan does not suffer the fragility of existing aggregation rules and, at a reasonable cost in terms of required batch size, achieves convergence as if only non–Byzantine gradients had been used to update the model.",http://proceedings.mlr.press/v80/mhamdi18a.html,http://proceedings.mlr.press/v80/mhamdi18a/mhamdi18a.pdf,ICML
1201,2018,Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data for Future Prediction,"Siyuan Qi,         Baoxiong Jia,         Song-Chun Zhu","Future predictions on sequence data (e.g., videos or audios) require the algorithms to capture non-Markovian and compositional properties of high-level semantics. Context-free grammars are natural choices to capture such properties, but traditional grammar parsers (e.g., Earley parser) only take symbolic sentences as inputs. In this paper, we generalize the Earley parser to parse sequence data which is neither segmented nor labeled. This generalized Earley parser integrates a grammar parser with a classifier to find the optimal segmentation and labels, and makes top-down future predictions. Experiments show that our method significantly outperforms other approaches for future human activity prediction.",http://proceedings.mlr.press/v80/qi18a.html,http://proceedings.mlr.press/v80/qi18a/qi18a.pdf,ICML
1202,2018,PredRNN++: Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning,"Yunbo Wang,         Zhifeng Gao,         Mingsheng Long,         Jianmin Wang,         Philip S Yu","We present PredRNN++, a recurrent network for spatiotemporal predictive learning. In pursuit of a great modeling capability for short-term video dynamics, we make our network deeper in time by leveraging a new recurrent structure named Causal LSTM with cascaded dual memories. To alleviate the gradient propagation difficulties in deep predictive models, we propose a Gradient Highway Unit, which provides alternative quick routes for the gradient flows from outputs back to long-range previous inputs. The gradient highway units work seamlessly with the causal LSTMs, enabling our model to capture the short-term and the long-term video dependencies adaptively. Our model achieves state-of-the-art prediction results on both synthetic and real video datasets, showing its power in modeling entangled motions.",http://proceedings.mlr.press/v80/wang18b.html,http://proceedings.mlr.press/v80/wang18b/wang18b.pdf,ICML
1203,2018,INSPECTRE: Privately Estimating the Unseen,"Jayadev Acharya,         Gautam Kamath,         Ziteng Sun,         Huanyu Zhang","We develop differentially private methods for estimating various distributional properties. Given a sample from a discrete distribution p, some functional f, and accuracy and privacy parameters alpha and epsilon, the goal is to estimate f(p) up to accuracy alpha, while maintaining epsilon-differential privacy of the sample. We prove almost-tight bounds on the sample size required for this problem for several functionals of interest, including support size, support coverage, and entropy. We show that the cost of privacy is negligible in a variety of settings, both theoretically and experimentally. Our methods are based on a sensitivity analysis of several state-of-the-art methods for estimating these properties with sublinear sample complexities",http://proceedings.mlr.press/v80/acharya18a.html,http://proceedings.mlr.press/v80/acharya18a/acharya18a.pdf,ICML
1204,2018,Learning Localized Spatio-Temporal Models From Streaming Data,"Muhammad Osama,         Dave Zachariah,         Thomas Schön","We address the problem of predicting spatio-temporal processes with temporal patterns that vary across spatial regions, when data is obtained as a stream. That is, when the training dataset is augmented sequentially. Specifically, we develop a localized spatio-temporal covariance model of the process that can capture spatially varying temporal periodicities in the data. We then apply a covariance-fitting methodology to learn the model parameters which yields a predictor that can be updated sequentially with each new data point. The proposed method is evaluated using both synthetic and real climate data which demonstrate its ability to accurately predict data missing in spatial regions over time.",http://proceedings.mlr.press/v80/osama18a.html,http://proceedings.mlr.press/v80/osama18a/osama18a.pdf,ICML
1205,2018,Learning Equations for Extrapolation and Control,"Subham Sahoo,         Christoph Lampert,         Georg Martius","We present an approach to identify concise equations from data using a shallow neural network approach. In contrast to ordinary black-box regression, this approach allows understanding functional relations and generalizing them from observed data to unseen parts of the parameter space. We show how to extend the class of learnable equations for a recently proposed equation learning network to include divisions, and we improve the learning and model selection strategy to be useful for challenging real-world data. For systems governed by analytical expressions, our method can in many cases identify the true underlying equation and extrapolate to unseen domains. We demonstrate its effectiveness by experiments on a cart-pendulum system, where only 2 random rollouts are required to learn the forward dynamics and successfully achieve the swing-up task.",http://proceedings.mlr.press/v80/sahoo18a.html,http://proceedings.mlr.press/v80/sahoo18a/sahoo18a.pdf,ICML
1206,2018,Learning and Memorization,Satrajit Chatterjee,"In the machine learning research community, it is generally believed that there is a tension between memorization and generalization. In this work we examine to what extent this tension exists by exploring if it is possible to generalize by memorizing alone. Although direct memorization with a lookup table obviously does not generalize, we find that introducing depth in the form of a network of support-limited lookup tables leads to generalization that is significantly above chance and closer to those obtained by standard learning algorithms on several tasks derived from MNIST and CIFAR-10. Furthermore, we demonstrate through a series of empirical results that our approach allows for a smooth tradeoff between memorization and generalization and exhibits some of the most salient characteristics of neural networks: depth improves performance; random data can be memorized and yet there is generalization on real data; and memorizing random data is harder in a certain sense than memorizing real data. The extreme simplicity of the algorithm and potential connections with generalization theory point to several interesting directions for future research.",http://proceedings.mlr.press/v80/chatterjee18a.html,http://proceedings.mlr.press/v80/chatterjee18a/chatterjee18a.pdf,ICML
1207,2018,Learning Maximum-A-Posteriori Perturbation Models for Structured Prediction in Polynomial Time,"Asish Ghoshal,         Jean Honorio","MAP perturbation models have emerged as a powerful framework for inference in structured prediction. Such models provide a way to efficiently sample from the Gibbs distribution and facilitate predictions that are robust to random noise. In this paper, we propose a provably polynomial time randomized algorithm for learning the parameters of perturbed MAP predictors. Our approach is based on minimizing a novel Rademacher-based generalization bound on the expected loss of a perturbed MAP predictor, which can be computed in polynomial time. We obtain conditions under which our randomized learning algorithm can guarantee generalization to unseen examples.",http://proceedings.mlr.press/v80/ghoshal18a.html,http://proceedings.mlr.press/v80/ghoshal18a/ghoshal18a.pdf,ICML
1208,2018,Differentiable plasticity: training plastic neural networks with backpropagation,"Thomas Miconi,         Kenneth Stanley,         Jeff Clune","How can we build agents that keep learning from experience, quickly and efficiently, after their initial training? Here we take inspiration from the main mechanism of learning in biological brains: synaptic plasticity, carefully tuned by evolution to produce efficient lifelong learning. We show that plasticity, just like connection weights, can be optimized by gradient descent in large (millions of parameters) recurrent networks with Hebbian plastic connections. First, recurrent plastic networks with more than two million parameters can be trained to memorize and reconstruct sets of novel, high-dimensional (1000+ pixels) natural images not seen during training. Crucially, traditional non-plastic recurrent networks fail to solve this task. Furthermore, trained plastic networks can also solve generic meta-learning tasks such as the Omniglot task, with competitive results and little parameter overhead. Finally, in reinforcement learning settings, plastic networks outperform non-plastic equivalent in a maze exploration task. We conclude that differentiable plasticity may provide a powerful novel approach to the learning-to-learn problem.",http://proceedings.mlr.press/v80/miconi18a.html,http://proceedings.mlr.press/v80/miconi18a/miconi18a.pdf,ICML
1209,2018,Automatic Goal Generation for Reinforcement Learning Agents,"Carlos Florensa,         David Held,         Xinyang Geng,         Pieter Abbeel","Reinforcement learning (RL) is a powerful technique to train an agent to perform a task; however, an agent that is trained using RL is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment. We use a generator network to propose tasks for the agent to try to accomplish, each task being specified as reaching a certain parametrized subset of the state-space. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent, thus automatically producing a curriculum. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment, even when only sparse rewards are available. Videos and code available at https://sites.google.com/view/goalgeneration4rl.",http://proceedings.mlr.press/v80/florensa18a.html,http://proceedings.mlr.press/v80/florensa18a/florensa18a.pdf,ICML
1210,2018,Goodness-of-Fit Testing for Discrete Distributions via Stein Discrepancy,"Jiasen Yang,         Qiang Liu,         Vinayak Rao,         Jennifer Neville","Recent work has combined Stein’s method with reproducing kernel Hilbert space theory to develop nonparametric goodness-of-fit tests for un-normalized probability distributions. However, the currently available tests apply exclusively to distributions with smooth density functions. In this work, we introduce a kernelized Stein discrepancy measure for discrete spaces, and develop a nonparametric goodness-of-fit test for discrete distributions with intractable normalization constants. Furthermore, we propose a general characterization of Stein operators that encompasses both discrete and continuous distributions, providing a recipe for constructing new Stein operators. We apply the proposed goodness-of-fit test to three statistical models involving discrete distributions, and our experiments show that the proposed test typically outperforms a two-sample test based on the maximum mean discrepancy.",http://proceedings.mlr.press/v80/yang18c.html,http://proceedings.mlr.press/v80/yang18c/yang18c.pdf,ICML
1211,2018,On the Spectrum of Random Features Maps of High Dimensional Data,"Zhenyu Liao,         Romain Couillet","Random feature maps are ubiquitous in modern statistical machine learning, where they generalize random projections by means of powerful, yet often difficult to analyze nonlinear operators. In this paper we leverage the ""concentration"" phenomenon induced by random matrix theory to perform a spectral analysis on the Gram matrix of these random feature maps, here for Gaussian mixture models of simultaneously large dimension and size. Our results are instrumental to a deeper understanding on the interplay of the nonlinearity and the statistics of the data, thereby allowing for a better tuning of random feature-based techniques.",http://proceedings.mlr.press/v80/liao18a.html,http://proceedings.mlr.press/v80/liao18a/liao18a.pdf,ICML
1212,2018,TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service,"Amartya Sanyal,         Matt Kusner,         Adria Gascon,         Varun Kanade","Machine learning methods are widely used for a variety of prediction problems. Prediction as a service is a paradigm in which service providers with technological expertise and computational resources may perform predictions for clients. However, data privacy severely restricts the applicability of such services, unless measures to keep client data private (even from the service provider) are designed. Equally important is to minimize the nature of computation and amount of communication required between client and server. Fully homomorphic encryption offers a way out, whereby clients may encrypt their data, and on which the server may perform arithmetic computations. The one drawback of using fully homomorphic encryption is the amount of time required to evaluate large machine learning models on encrypted data. We combine several ideas from the machine learning literature, particularly work on quantization and sparsification of neural networks, together with algorithmic tools to speed-up and parallelize computation using encrypted data.",http://proceedings.mlr.press/v80/sanyal18a.html,http://proceedings.mlr.press/v80/sanyal18a/sanyal18a.pdf,ICML
1213,2018,Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement,"Andre Barreto,         Diana Borsa,         John Quan,         Tom Schaul,         David Silver,         Matteo Hessel,         Daniel Mankowitz,         Augustin Zidek,         Remi Munos","The ability to transfer skills across tasks has the potential to scale up reinforcement learning (RL) agents to environments currently out of reach. Recently, a framework based on two ideas, successor features (SFs) and generalised policy improvement (GPI), has been introduced as a principled way of transferring skills. In this paper we extend the SF&GPI framework in two ways. One of the basic assumptions underlying the original formulation of SF&GPI is that rewards for all tasks of interest can be computed as linear combinations of a fixed set of features. We relax this constraint and show that the theoretical guarantees supporting the framework can be extended to any set of tasks that only differ in the reward function. Our second contribution is to show that one can use the reward functions themselves as features for future tasks, without any loss of expressiveness, thus removing the need to specify a set of features beforehand. This makes it possible to combine SF&GPI with deep learning in a more stable way. We empirically verify this claim on a complex 3D environment where observations are images from a first-person perspective. We show that the transfer promoted by SF&GPI leads to very good policies on unseen tasks almost instantaneously. We also describe how to learn policies specialised to the new tasks in a way that allows them to be added to the agent’s set of skills, and thus be reused in the future.",http://proceedings.mlr.press/v80/barreto18a.html,http://proceedings.mlr.press/v80/barreto18a/barreto18a.pdf,ICML
1214,2018,Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks,"Minmin Chen,         Jeffrey Pennington,         Samuel Schoenholz","Recurrent neural networks have gained widespread use in modeling sequence data across various domains. While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood. We develop a theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory. To simplify our discussion, we introduce a new RNN cell with a simple gating mechanism that we call the minimalRNN and compare it with vanilla RNNs. Our theory allows us to define a maximum timescale over which RNNs can remember an input. We show that this theory predicts trainability for both recurrent architectures. We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent experimental findings. Finally, we develop a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs. We show that this results in significantly improved training dynamics. Finally, we demonstrate that the minimalRNN achieves comparable performance to its more complex counterparts, such as LSTMs or GRUs, on a language modeling task.",http://proceedings.mlr.press/v80/chen18i.html,http://proceedings.mlr.press/v80/chen18i/chen18i.pdf,ICML
1215,2018,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,"Lasse Espeholt,         Hubert Soyer,         Remi Munos,         Karen Simonyan,         Vlad Mnih,         Tom Ward,         Yotam Doron,         Vlad Firoiu,         Tim Harley,         Iain Dunning,         Shane Legg,         Koray Kavukcuoglu","In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",http://proceedings.mlr.press/v80/espeholt18a.html,http://proceedings.mlr.press/v80/espeholt18a/espeholt18a.pdf,ICML
1216,2018,Implicit Quantile Networks for Distributional Reinforcement Learning,"Will Dabney,         Georg Ostrovski,         David Silver,         Remi Munos","In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm’s implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.",http://proceedings.mlr.press/v80/dabney18a.html,http://proceedings.mlr.press/v80/dabney18a/dabney18a.pdf,ICML
1217,2018,Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning,"Stefan Depeweg,         Jose-Miguel Hernandez-Lobato,         Finale Doshi-Velez,         Steffen Udluft","Bayesian neural networks with latent variables are scalable and flexible probabilistic models: they account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data. Using these models we show how to perform and utilize a decomposition of uncertainty in aleatoric and epistemic components for decision making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learningto identify policies that balance expected cost, model-bias and noise aversion.",http://proceedings.mlr.press/v80/depeweg18a.html,http://proceedings.mlr.press/v80/depeweg18a/depeweg18a.pdf,ICML
1218,2018,Differentiable Dynamic Programming for Structured Prediction and Attention,"Arthur Mensch,         Mathieu Blondel","Dynamic programming (DP) solves a variety of structured combinatorial problems by iteratively breaking them down into smaller subproblems. In spite of their versatility, many DP algorithms are non-differentiable, which hampers their use as a layer in neural networks trained by backpropagation. To address this issue, we propose to smooth the max operator in the dynamic programming recursion, using a strongly convex regularizer. This allows to relax both the optimal value and solution of the original combinatorial problem, and turns a broad class of DP algorithms into differentiable operators. Theoretically, we provide a new probabilistic perspective on backpropagating through these DP operators, and relate them to inference in graphical models. We derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for time-series alignment. We showcase these instantiations on structured prediction (audio-to-score alignment, NER) and on structured and sparse attention for translation.",http://proceedings.mlr.press/v80/mensch18a.html,http://proceedings.mlr.press/v80/mensch18a/mensch18a.pdf,ICML
1219,2018,Nonconvex Optimization for Regression with Fairness Constraints,"Junpei Komiyama,         Akiko Takeda,         Junya Honda,         Hajime Shimao","The unfairness of a regressor is evaluated by measuring the correlation between the estimator and the sensitive attribute (e.g., race, gender, age), and the coefficient of determination (CoD) is a natural extension of the correlation coefficient when more than one sensitive attribute exists. As is well known, there is a trade-off between fairness and accuracy of a regressor, which implies a perfectly fair optimizer does not always yield a useful prediction. Taking this into consideration, we optimize the accuracy of the estimation subject to a user-defined level of fairness. However, a fairness level as a constraint induces a nonconvexity of the feasible region, which disables the use of an off-the-shelf convex optimizer. Despite such nonconvexity, we show an exact solution is available by using tools of global optimization theory. Furthermore, we propose a nonlinear extension of the method by kernel representation. Unlike most of existing fairness-aware machine learning methods, our method allows us to deal with numeric and multiple sensitive attributes.",http://proceedings.mlr.press/v80/komiyama18a.html,http://proceedings.mlr.press/v80/komiyama18a/komiyama18a.pdf,ICML
1220,2018,Training Neural Machines with Trace-Based Supervision,"Matthew Mirman,         Dimitar Dimitrov,         Pavle Djordjevic,         Timon Gehr,         Martin Vechev","We investigate the effectiveness of trace-based supervision methods for training existing neural abstract machines. To define the class of neural machines amenable to trace-based supervision, we introduce the concept of a differential neural computational machine (dNCM) and show that several existing architectures (NTMs, NRAMs) can be described as dNCMs. We performed a detailed experimental evaluation with NTM and NRAM machines, showing that additional supervision on the interpretable portions of these architectures leads to better convergence and generalization capabilities of the learning phase than standard training, in both noise-free and noisy scenarios.",http://proceedings.mlr.press/v80/mirman18a.html,http://proceedings.mlr.press/v80/mirman18a/mirman18a.pdf,ICML
1221,2018,Practical Contextual Bandits with Regression Oracles,"Dylan Foster,         Alekh Agarwal,         Miroslav Dudik,         Haipeng Luo,         Robert Schapire","A major challenge in contextual bandits is to design general-purpose algorithms that are both practically useful and theoretically well-founded. We present a new technique that has the empirical and computational advantages of realizability-based approaches combined with the flexibility of agnostic methods. Our algorithms leverage the availability of a regression oracle for the value-function class, a more realistic and reasonable oracle than the classification oracles over policies typically assumed by agnostic methods. Our approach generalizes both UCB and LinUCB to far more expressive possible model classes and achieves low regret under certain distributional assumptions. In an extensive empirical evaluation, we find that our approach typically matches or outperforms both realizability-based and agnostic baselines.",http://proceedings.mlr.press/v80/foster18a.html,http://proceedings.mlr.press/v80/foster18a/foster18a.pdf,ICML
1222,2018,Differentially Private Identity and Equivalence Testing of Discrete Distributions,"Maryam Aliakbarpour,         Ilias Diakonikolas,         Ronitt Rubinfeld","We study the fundamental problems of identity and equivalence testing over a discrete population from random samples. Our goal is to develop efficient testers while guaranteeing differential privacy to the individuals of the population. We provide sample-efficient differentially private testers for these problems. Our theoretical results significantly improve over the best known algorithms for identity testing, and are the first results for private equivalence testing. The conceptual message of our work is that there exist private hypothesis testers that are nearly as sample-efficient as their non-private counterparts. We perform an experimental evaluation of our algorithms on synthetic data. Our experiments illustrate that our private testers achieve small type I and type II errors with sample size sublinear in the domain size of the underlying distributions.",http://proceedings.mlr.press/v80/aliakbarpour18a.html,http://proceedings.mlr.press/v80/aliakbarpour18a/aliakbarpour18a.pdf,ICML
1223,2018,Rectify Heterogeneous Models with Semantic Mapping,"Han-Jia Ye,         De-Chuan Zhan,         Yuan Jiang,         Zhi-Hua Zhou","On the way to the robust learner for real-world applications, there are still great    challenges, including considering unknown environments with limited data. Learnware (Zhou; 2016) describes a novel perspective, and claims that learning models should have reusable and evolvable properties. We propose to Encode Meta InformaTion of features (EMIT), as the model specification for characterizing the changes, which grants the model evolvability to bridge heterogeneous feature spaces. Then, pre-trained models from related tasks can be Reused by our REctiFy via heterOgeneous pRedictor Mapping (REFORM}) framework. In summary, the pre-trained model is adapted to a new environment with different features, through model refining on only a small amount of training data in the current task. Experimental results over both synthetic and real-world tasks with diverse feature configurations validate the effectiveness and practical utility of the proposed framework.",http://proceedings.mlr.press/v80/ye18c.html,http://proceedings.mlr.press/v80/ye2018c/ye2018c.pdf,ICML
1224,2018,Learning One Convolutional Layer with Overlapping Patches,"Surbhi Goel,         Adam Klivans,         Raghu Meka","We give the first provably efficient algorithm for learning a one hidden layer convolutional network with respect to a general class of (potentially overlapping) patches under mild conditions on the underlying distribution. We prove that our framework captures commonly used schemes from computer vision, including one-dimensional and two-dimensional “patch and stride” convolutions. Our algorithm– Convotron– is inspired by recent work applying isotonic regression to learning neural networks. Convotron uses a simple, iterative update rule that is stochastic in nature and tolerant to noise (requires only that the conditional mean function is a one layer convolutional network, as opposed to the realizable setting). In contrast to gradient descent, Convotron requires no special initialization or learning-rate tuning to converge to the global optimum. We also point out that learning one hidden convolutional layer with respect to a Gaussian distribution and just one disjoint patch PP (the other patches may be arbitrary) is easy in the following sense: Convotron can efficiently recover the hidden weight vector by updating only in the direction of PP.",http://proceedings.mlr.press/v80/goel18a.html,http://proceedings.mlr.press/v80/goel18a/goel18a.pdf,ICML
1225,2018,Approximation Algorithms for Cascading Prediction Models,Matthew Streeter,"We present an approximation algorithm that takes a pool of pre-trained models as input and produces from it a cascaded model with similar accuracy but lower average-case cost. Applied to state-of-the-art ImageNet classification models, this yields up to a 2x reduction in floating point multiplications, and up to a 6x reduction in average-case memory I/O. The auto-generated cascades exhibit intuitive properties, such as using lower-resolution input for easier images and requiring higher prediction confidence when using a computationally cheaper model.",http://proceedings.mlr.press/v80/streeter18a.html,http://proceedings.mlr.press/v80/streeter18a/streeter18a.pdf,ICML
1226,2018,Discovering Interpretable Representations for Both Deep Generative and Discriminative Models,"Tameem Adel,         Zoubin Ghahramani,         Adrian Weller","Interpretability of representations in both deep generative and discriminative models is highly desirable. Current methods jointly optimize an objective combining accuracy and interpretability. However, this may reduce accuracy, and is not applicable to already trained models. We propose two interpretability frameworks. First, we provide an interpretable lens for an existing model. We use a generative model which takes as input the representation in an existing (generative or discriminative) model, weakly supervised by limited side information. Applying a flexible and invertible transformation to the input leads to an interpretable representation with no loss in accuracy. We extend the approach using an active learning strategy to choose the most useful side information to obtain, allowing a human to guide what ""interpretable"" means. Our second framework relies on joint optimization for a representation which is both maximally informative about the side information and maximally compressive about the non-interpretable data factors. This leads to a novel perspective on the relationship between compression and regularization. We also propose a new interpretability evaluation metric based on our framework. Empirically, we achieve state-of-the-art results on three datasets using the two proposed algorithms.",http://proceedings.mlr.press/v80/adel18a.html,http://proceedings.mlr.press/v80/adel18a/adel18a.pdf,ICML
1227,2018,Compressing Neural Networks using the Variational Information Bottleneck,"Bin Dai,         Chen Zhu,         Baining Guo,         David Wipf","Neural networks can be compressed to reduce memory and computational requirements, or to increase accuracy by facilitating the use of a larger base architecture. In this paper we focus on pruning individual neurons, which can simultaneously trim model size, FLOPs, and run-time memory. To improve upon the performance of existing compression algorithms we utilize the information bottleneck principle instantiated via a tractable variational bound. Minimization of this information theoretic bound reduces the redundancy between adjacent layers by aggregating useful information into a subset of neurons that can be preserved. In contrast, the activations of disposable neurons are shut off via an attractive form of sparse regularization that emerges naturally from this framework, providing tangible advantages over traditional sparsity penalties without contributing additional tuning parameters to the energy landscape. We demonstrate state-of-the-art compression rates across an array of datasets and network architectures.",http://proceedings.mlr.press/v80/dai18d.html,http://proceedings.mlr.press/v80/dai18d/dai18d.pdf,ICML
1228,2018,Efficient and Consistent Adversarial Bipartite Matching,"Rizal Fathony,         Sima Behpour,         Xinhua Zhang,         Brian Ziebart","Many important structured prediction problems, including learning to rank items, correspondence-based natural language processing, and multi-object tracking, can be formulated as weighted bipartite matching optimizations. Existing structured prediction approaches have significant drawbacks when applied under the constraints of perfect bipartite matchings. Exponential family probabilistic models, such as the conditional random field (CRF), provide statistical consistency guarantees, but suffer computationally from the need to compute the normalization term of its distribution over matchings, which is a #P-hard matrix permanent computation. In contrast, the structured support vector machine (SSVM) provides computational efficiency, but lacks Fisher consistency, meaning that there are distributions of data for which it cannot learn the optimal matching even under ideal learning conditions (i.e., given the true distribution and selecting from all measurable potential functions). We propose adversarial bipartite matching to avoid both of these limitations. We develop this approach algorithmically, establish its computational efficiency and Fisher consistency properties, and apply it to matching problems that demonstrate its empirical benefits.",http://proceedings.mlr.press/v80/fathony18a.html,http://proceedings.mlr.press/v80/fathony18a/fathony18a.pdf,ICML
1229,2018,QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning,"Tabish Rashid,         Mikayel Samvelyan,         Christian Schroeder,         Gregory Farquhar,         Jakob Foerster,         Shimon Whiteson","In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.",http://proceedings.mlr.press/v80/rashid18a.html,http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf,ICML
1230,2018,Clipped Action Policy Gradient,"Yasuhiro Fujita,         Shin-ichi Maeda","Many continuous control tasks have bounded action spaces. When policy gradient methods are applied to such tasks, out-of-bound actions need to be clipped before execution, while policies are usually optimized as if the actions are not clipped. We propose a policy gradient estimator that exploits the knowledge of actions being clipped to reduce the variance in estimation. We prove that our estimator, named clipped action policy gradient (CAPG), is unbiased and achieves lower variance than the conventional estimator that ignores action bounds. Experimental results demonstrate that CAPG generally outperforms the conventional estimator, indicating that it is a better policy gradient estimator for continuous control tasks. The source code is available at https://github.com/pfnet-research/capg.",http://proceedings.mlr.press/v80/fujita18a.html,http://proceedings.mlr.press/v80/fujita18a/fujita18a.pdf,ICML
1231,2018,Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks,"Peter Bartlett,         Dave Helmbold,         Philip Long","We analyze algorithms for approximating a function f(x)=Φxf(x)=Φxf(x) = \Phi x mapping ℜd\Re^d to ℜd\Re^d using deep linear neural networks, i.e. that learn a function hh parameterized by matrices Θ1,...,ΘL\Theta_1,...,\Theta_L and defined by h(x)=ΘLΘL−1...Θ1xh(x) = \Theta_L \Theta_{L-1} ... \Theta_1 x. We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic. We provide polynomial bounds on the number of iterations for gradient descent to approximate the least squares matrix Φ\Phi, in the case where the initial hypothesis Θ1=...=ΘL=I\Theta_1 = ... = \Theta_L = I has excess loss bounded by a small enough constant. On the other hand, we show that gradient descent fails to converge for Φ\Phi whose distance from the identity is a larger constant, and we show that some forms of regularization toward the identity in each layer do not help. If Φ\Phi is symmetric positive definite, we show that an algorithm that initializes Θi=I\Theta_i = I learns an ϵ\epsilon-approximation of ff using a number of updates polynomial in LL, the condition number of Φ\Phi, and log(d/ϵ)\log(d/\epsilon). In contrast, we show that if the least squares matrix Φ\Phi is symmetric and has a negative eigenvalue, then all members of a class of algorithms that perform gradient descent with identity initialization, and optionally regularize toward the identity in each layer, fail to converge. We analyze an algorithm for the case that Φ\Phi satisfies u⊤Φu>0u^{\top} \Phi u > 0 for all uu, but may not be symmetric. This algorithm uses two regularizers: one that maintains the invariant u⊤ΘLΘL−1...Θ1u>0u^{\top} \Theta_L \Theta_{L-1} ... \Theta_1 u > 0 for all uu, and another that ""balances"" Θ1,...,ΘL\Theta_1, ..., \Theta_L so that they have the same singular values.",http://proceedings.mlr.press/v80/bartlett18a.html,http://proceedings.mlr.press/v80/bartlett18a/bartlett18a.pdf,ICML
1232,2018,Predict and Constrain: Modeling Cardinality in Deep Structured Prediction,"Nataly Brukhim,         Amir Globerson","Many machine learning problems require the prediction of multi-dimensional labels. Such structured prediction models can benefit from modeling dependencies between labels. Recently, several deep learning approaches to structured prediction have been proposed. Here we focus on capturing cardinality constraints in such models. Namely, constraining the number of non-zero labels that the model outputs. Such constraints have proven very useful in previous structured prediction methods, but it is a challenge to introduce them into a deep learning approach. Here we show how to do this via a novel deep architecture. Our approach outperforms strong baselines, achieving state-of-the-art results on multi-label classification benchmarks.",http://proceedings.mlr.press/v80/brukhim18a.html,http://proceedings.mlr.press/v80/brukhim18a/brukhim18a.pdf,ICML
1233,2018,Tree Edit Distance Learning via Adaptive Symbol Embeddings,"Benjamin Paaßen,         Claudio Gallicchio,         Alessio Micheli,         Barbara Hammer","Metric learning has the aim to improve classification accuracy by learning a distance measure which brings data points from the same class closer together and pushes data points from different classes further apart. Recent research has demonstrated that metric learning approaches can also be applied to trees, such as molecular structures, abstract syntax trees of computer programs, or syntax trees of natural language, by learning the cost function of an edit distance, i.e. the costs of replacing, deleting, or inserting nodes in a tree. However, learning such costs directly may yield an edit distance which violates metric axioms, is challenging to interpret, and may not generalize well. In this contribution, we propose a novel metric learning approach for trees which we call embedding edit distance learning (BEDL) and which learns an edit distance indirectly by embedding the tree nodes as vectors, such that the Euclidean distance between those vectors supports class discrimination. We learn such embeddings by reducing the distance to prototypical trees from the same class and increasing the distance to prototypical trees from different classes. In our experiments, we show that BEDL improves upon the state-of-the-art in metric learning for trees on six benchmark data sets, ranging from computer science over biomedical data to a natural-language processing data set containing over 300,000 nodes.",http://proceedings.mlr.press/v80/paassen18a.html,http://proceedings.mlr.press/v80/paassen18a/paassen18a.pdf,ICML
1234,2018,Adversarial Regression with Multiple Learners,"Liang Tong,         Sixie Yu,         Scott Alfeld,          vorobeychik","Despite the considerable success enjoyed by machine learning techniques in practice, numerous studies demonstrated that many approaches are vulnerable to attacks. An important class of such attacks involves adversaries changing features at test time to cause incorrect predictions. Previous investigations of this problem pit a single learner against an adversary. However, in many situations an adversary’s decision is aimed at a collection of learners, rather than specifically targeted at each independently. We study the problem of adversarial linear regression with multiple learners. We approximate the resulting game by exhibiting an upper bound on learner loss functions, and show that the resulting game has a unique symmetric equilibrium. We present an algorithm for computing this equilibrium, and show through extensive experiments that equilibrium models are significantly more robust than conventional regularized linear regression.",http://proceedings.mlr.press/v80/tong18a.html,http://proceedings.mlr.press/v80/tong18a/tong18a.pdf,ICML
1235,2018,Racing Thompson: an Efficient Algorithm for Thompson Sampling with Non-conjugate Priors,"Yichi Zhou,         Jun Zhu,         Jingwei Zhuo","Thompson sampling has impressive empirical performance for many multi-armed bandit problems. But current algorithms for Thompson sampling only work for the case of conjugate priors since they require to perform online Bayesian posterior inference, which is a difficult task when the prior is not conjugate. In this paper, we propose a novel algorithm for Thompson sampling which only requires to draw samples from a tractable proposal distribution. So our algorithm is efficient even when the prior is non-conjugate. To do this, we reformulate Thompson sampling as an optimization proplem via the Gumbel-Max trick. After that we construct a set of random variables and our goal is to identify the one with highest mean which is an instance of best arm identification problems. Finally, we solve it with techniques in best arm identification. Experiments show that our algorithm works well in practice.",http://proceedings.mlr.press/v80/zhou18e.html,http://proceedings.mlr.press/v80/zhou18e/zhou18e.pdf,ICML
1236,2018,A probabilistic framework for multi-view feature learning with many-to-many associations via neural networks,"Akifumi Okuno,         Tetsuya Hada,         Hidetoshi Shimodaira","A simple framework Probabilistic Multi-view Graph Embedding (PMvGE) is proposed for multi-view feature learning with many-to-many associations so that it generalizes various existing multi-view methods. PMvGE is a probabilistic model for predicting new associations via graph embedding of the nodes of data vectors with links of their associations. Multi-view data vectors with many-to-many associations are transformed by neural networks to feature vectors in a shared space, and the probability of new association between two data vectors is modeled by the inner product of their feature vectors. While existing multi-view feature learning techniques can treat only either of many-to-many association or non-linear transformation, PMvGE can treat both simultaneously. By combining Mercer’s theorem and the universal approximation theorem, we prove that PMvGE learns a wide class of similarity measures across views. Our likelihood-based estimator enables efficient computation of non-linear transformations of data vectors in large-scale datasets by minibatch SGD, and numerical experiments illustrate that PMvGE outperforms existing multi-view methods.",http://proceedings.mlr.press/v80/okuno18a.html,http://proceedings.mlr.press/v80/okuno18a/okuno18a.pdf,ICML
1237,2018,"Dropout Training, Data-dependent Regularization, and Generalization Bounds","Wenlong Mou,         Yuchen Zhou,         Jun Gao,         Liwei Wang","We study the problem of generalization guarantees for dropout training. A general framework is first proposed for learning procedures with random perturbation on model parameters. The generalization error is bounded by sum of two offset Rademacher complexities: the main term is Rademacher complexity of the hypothesis class with minus offset induced by the perturbation variance, which characterizes data-dependent regularization by the random perturbation; the auxiliary term is offset Rademacher complexity for the variance class, controlling the degree to which this regularization effect can be weakened. For neural networks, we estimate upper and lower bounds for the variance induced by truthful dropout, a variant of dropout that we propose to ensure unbiased output and fit into our framework, and the variance bounds exhibits connection to adaptive regularization methods. By applying our framework to ReLU networks with one hidden layer, a generalization upper bound is derived with no assumptions on the parameter norms or data distribution, with O(1/n)O(1/n)O(1/n) fast rate and adaptivity to geometry of data points being achieved at the same time.",http://proceedings.mlr.press/v80/mou18a.html,http://proceedings.mlr.press/v80/mou18a/mou18a.pdf,ICML
1238,2018,Unbiased Objective Estimation in Predictive Optimization,"Shinji Ito,         Akihiro Yabe,         Ryohei Fujimaki","For data-driven decision-making, one promising approach, called predictive optimization, is to solve maximization problems i n which the objective function to be maximized is estimated from data. Predictive optimization, however, suffers from the problem of a calculated optimal solution’s being evaluated too optimistically, i.e., the value of the objective function is overestimated. This paper investigates such optimistic bias and presents two methods for correcting it. The first, which is analogous to cross-validation, successfully corrects the optimistic bias but results in underestimation of the true value. Our second method employs resampling techniques to avoid both overestimation and underestimation. We show that the second method, referred to as the parameter perturbation method, achieves asymptotically unbiased estimation. Empirical results for both artificial and real-world datasets demonstrate that our proposed approach successfully corrects the optimistic bias.",http://proceedings.mlr.press/v80/ito18a.html,http://proceedings.mlr.press/v80/ito18a/ito18a.pdf,ICML
1239,2018,Network Global Testing by Counting Graphlets,"Jiashun Jin,         Zheng Ke,         Shengming Luo","Consider a large social network with possibly severe degree heterogeneity and mixed-memberships. We are interested in testing whether the network has only one community or there are more than one communities. The problem is known to be non-trivial, partially due to the presence of severe degree heterogeneity. We construct a class of test statistics using the numbers of short paths and short cycles, and the key to our approach is a general framework for canceling the effects of degree heterogeneity. The tests compare favorably with existing methods. We support our methods with careful analysis and numerical study with simulated data and a real data example.",http://proceedings.mlr.press/v80/jin18b.html,http://proceedings.mlr.press/v80/jin18b/jin18b.pdf,ICML
1240,2018,Markov Modulated Gaussian Cox Processes for Semi-Stationary Intensity Modeling of Events Data,Minyoung Kim,"The Cox process is a flexible event model that can account for uncertainty of the intensity function in the Poisson process. However, previous approaches make strong assumptions in terms of time stationarity, potentially failing to generalize when the data do not conform to the assumed stationarity conditions. In this paper we bring up two most popular Cox models representing two extremes, and propose a novel semi-stationary Cox process model that can take benefits from both models. Our model has a set of Gaussian process latent functions governed by a latent stationary Markov process where we provide analytic derivations for the variational inference. Empirical evaluations on several synthetic and real-world events data including the football shot attempts and daily earthquakes, demonstrate that the proposed model is promising, can yield improved generalization performance over existing approaches.",http://proceedings.mlr.press/v80/kim18a.html,http://proceedings.mlr.press/v80/kim18a/kim18a.pdf,ICML
1241,2018,Neural Program Synthesis from Diverse Demonstration Videos,"Shao-Hua Sun,         Hyeonwoo Noh,         Sriram Somasundaram,         Joseph Lim","Interpreting decision making logic in demonstration videos is key to collaborating with and mimicking humans. To empower machines with this ability, we propose a neural program synthesizer that is able to explicitly synthesize underlying programs from behaviorally diverse and visually complicated demonstration videos. We introduce a summarizer module as part of our model to improve the network’s ability to integrate multiple demonstrations varying in behavior. We also employ a multi-task objective to encourage the model to learn meaningful intermediate representations for end-to-end training. We show that our model is able to reliably synthesize underlying programs as well as capture diverse behaviors exhibited in demonstrations. The code is available at https://shaohua0116.github.io/demo2program.",http://proceedings.mlr.press/v80/sun18a.html,http://proceedings.mlr.press/v80/sun18a/sun18a.pdf,ICML
1242,2018,Residual Unfairness in Fair Machine Learning from Prejudiced Data,"Nathan Kallus,         Angela Zhou","Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairness-adjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-the-art fair machine learning can have a ""bias in, bias out"" property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.",http://proceedings.mlr.press/v80/kallus18a.html,http://proceedings.mlr.press/v80/kallus18a/kallus18a.pdf,ICML
1243,2018,An Efficient Semismooth Newton based Algorithm for Convex Clustering,"Yancheng Yuan,         Defeng Sun,         Kim-Chuan Toh","Clustering is a fundamental problem in unsupervised learning. Popular methods like K-means, may suffer from instability as they are prone to get stuck in its local minima. Recently, the sumof-norms (SON) model (also known as clustering path), which is a convex relaxation of hierarchical clustering model, has been proposed in (Lindsten et al., 2011) and (Hocking et al., 2011). Although numerical algorithms like alternating direction method of multipliers (ADMM) and alternating minimization algorithm (AMA) have been proposed to solve convex clustering model (Chi & Lange, 2015), it is known to be very challenging to solve large-scale problems. In this paper, we propose a semismooth Newton based augmented Lagrangian method for large-scale convex clustering problems. Extensive numerical experiments on both simulated and real data demonstrate that our algorithm is highly efficient and robust for solving large-scale problems. Moreover, the numerical results also show the superior performance and scalability of our algorithm comparing to existing first-order methods.",http://proceedings.mlr.press/v80/yuan18a.html,http://proceedings.mlr.press/v80/yuan18a/yuan18a.pdf,ICML
1244,2018,Online Linear Quadratic Control,"Alon Cohen,         Avinatan Hasidim,         Tomer Koren,         Nevena Lazic,         Yishay Mansour,         Kunal Talwar","We study the problem of controlling linear time-invariant systems with known noisy dynamics and adversarially chosen quadratic losses. We present the first efficient online learning algorithms in this setting that guarantee O(T−−√)O(T)O(\sqrt{T}) regret under mild assumptions, where TTT is the time horizon. Our algorithms rely on a novel SDP relaxation for the steady-state distribution of the system. Crucially, and in contrast to previously proposed relaxations, the feasible solutions of our SDP all correspond to “strongly stable” policies that mix exponentially fast to a steady state.",http://proceedings.mlr.press/v80/cohen18b.html,http://proceedings.mlr.press/v80/cohen18b/cohen18b.pdf,ICML
1245,2018,Information Theoretic Guarantees for Empirical Risk Minimization with Applications to Model Selection and Large-Scale Optimization,Ibrahim Alabdulmohsin,"In this paper, we derive bounds on the mutual information of the empirical risk minimization (ERM) procedure for both 0-1 and strongly-convex loss classes. We prove that under the Axiom of Choice, the existence of an ERM learning rule with a vanishing mutual information is equivalent to the assertion that the loss class has a finite VC dimension, thus bridging information theory with statistical learning theory. Similarly, an asymptotic bound on the mutual information is established for strongly-convex loss classes in terms of the number of model parameters. The latter result rests on a central limit theorem (CLT) that we derive in this paper. In addition, we use our results to analyze the excess risk in stochastic convex optimization and unify previous works. Finally, we present two important applications. First, we show that the ERM of strongly-convex loss classes can be trivially scaled to big data using a naive parallelization algorithm with provable guarantees. Second, we propose a simple information criterion for model selection and demonstrate experimentally that it outperforms the popular Akaike’s information criterion (AIC) and Schwarz’s Bayesian information criterion (BIC).",http://proceedings.mlr.press/v80/alabdulmohsin18a.html,http://proceedings.mlr.press/v80/alabdulmohsin18a/alabdulmohsin18a.pdf,ICML
1246,2018,Differentiable Compositional Kernel Learning for Gaussian Processes,"Shengyang Sun,         Guodong Zhang,         Chaoqi Wang,         Wenyuan Zeng,         Jiaman Li,         Roger Grosse","The generalization properties of Gaussian processes depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network (NKN), a flexible family of kernels represented by a neural network. The NKN’s architecture is based on the composition rules for kernels, so that each unit of the network corresponds to a valid kernel. It can compactly approximate compositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradient- based optimization. We show that the NKN is universal for the class of stationary kernels. Empirically we demonstrate NKN’s pattern discovery and extrapolation abilities on several tasks that depend crucially on identifying the underlying structure, including time series and texture extrapolation, as well as Bayesian optimization.",http://proceedings.mlr.press/v80/sun18e.html,http://proceedings.mlr.press/v80/sun18e/sun18e.pdf,ICML
1247,2018,Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global,"Thomas Laurent,         James Brecht","We consider deep linear networks with arbitrary convex differentiable loss. We provide a short and elementary proof of the fact that all local minima are global minima if the hidden layers are either 1) at least as wide as the input layer, or 2) at least as wide as the output layer. This result is the strongest possible in the following sense: If the loss is convex and Lipschitz but not differentiable then deep linear networks can have sub-optimal local minima.",http://proceedings.mlr.press/v80/laurent18a.html,http://proceedings.mlr.press/v80/laurent18a/laurent18a.pdf,ICML
1248,2018,The Hierarchical Adaptive Forgetting Variational Filter,Vincent Moens,"A common problem in Machine Learning and statistics consists in detecting whether the current sample in a stream of data belongs to the same distribution as previous ones, is an isolated outlier or inaugurates a new distribution of data. We present a hierarchical Bayesian algorithm that aims at learning a time-specific approximate posterior distribution of the parameters describing the distribution of the data observed. We derive the update equations of the variational parameters of the approximate posterior at each time step for models from the exponential family, and show that these updates find interesting correspondents in Reinforcement Learning (RL). In this perspective, our model can be seen as a hierarchical RL algorithm that learns a posterior distribution according to a certain stability confidence that is, in turn, learned according to its own stability confidence. Finally, we show some applications of our generic model, first in a RL context, next with an adaptive Bayesian Autoregressive model, and finally in the context of Stochastic Gradient Descent optimization.",http://proceedings.mlr.press/v80/moens18a.html,http://proceedings.mlr.press/v80/moens18a/moens18a.pdf,ICML
1249,2018,Using Inherent Structures to design Lean 2-layer RBMs,"Abhishek Bansal,         Abhinav Anand,         Chiranjib Bhattacharyya","Understanding the representational power of Restricted Boltzmann Machines (RBMs) with multiple layers is an ill-understood problem and is an area of active research. Motivated from the approach of Inherent Structure formalism (Stillinger & Weber, 1982), extensively used in analysing Spin Glasses, we propose a novel measure called Inherent Structure Capacity (ISC), which characterizes the representation capacity of a fixed architecture RBM by the expected number of modes of distributions emanating from the RBM with parameters drawn from a prior distribution. Though ISC is intractable, we show that for a single layer RBM architecture ISC approaches a finite constant as number of hidden units are increased and to further improve the ISC, one needs to add a second layer. Furthermore, we introduce Lean RBMs, which are multi-layer RBMs where each layer can have at-most O(n) units with the number of visible units being n. We show that for every single layer RBM with Omega(n^{2+r}), r >= 0, hidden units there exists a two-layered lean RBM with Theta(n^2) parameters with the same ISC, establishing that 2 layer RBMs can achieve the same representational power as single-layer RBMs but using far fewer number of parameters. To the best of our knowledge, this is the first result which quantitatively establishes the need for layering.",http://proceedings.mlr.press/v80/bansal18a.html,http://proceedings.mlr.press/v80/bansal18a/bansal18a.pdf,ICML
1250,2018,CRVI: Convex Relaxation for Variational Inference,"Ghazal Fazelnia,         John Paisley","We present a new technique for solving non-convex variational inference optimization problems. Variational inference is a widely used method for posterior approximation in which the inference problem is transformed into an optimization problem. For most models, this optimization is highly non-convex and so hard to solve. In this paper, we introduce a new approach to solving the variational inference optimization based on convex relaxation and semidefinite programming. Our theoretical results guarantee very tight relaxation bounds that get nearer to the global optimal solution than traditional coordinate ascent. We evaluate the performance of our approach on regression and sparse coding.",http://proceedings.mlr.press/v80/fazelnia18a.html,http://proceedings.mlr.press/v80/fazelnia18a/fazelnia18a.pdf,ICML
1251,2018,Improving Sign Random Projections With Additional Information,"Keegan Kang,         Weipin Wong","Sign random projections (SRP) is a technique which allows the user to quickly estimate the angular similarity and inner products between data. We propose using additional information to improve these estimates which is easy to implement and cost efficient. We prove that the variance of our estimator is lower than the variance of SRP. Our proposed method can also be used together with other modifications of SRP, such as Super-Bit LSH (SBLSH). We demonstrate the effectiveness of our method on the MNIST test dataset and the Gisette dataset. We discuss how our proposed method can be extended to random projections or even other hashing algorithms.",http://proceedings.mlr.press/v80/kang18b.html,http://proceedings.mlr.press/v80/kang18b/kang18b.pdf,ICML
1252,2018,Characterizing and Learning Equivalence Classes of Causal DAGs under Interventions,"Karren Yang,         Abigail Katcoff,         Caroline Uhler","We consider the problem of learning causal DAGs in the setting where both observational and interventional data is available. This setting is common in biology, where gene regulatory networks can be intervened on using chemical reagents or gene deletions. Hauser & Buhlmann (2012) previously characterized the identifiability of causal DAGs under perfect interventions, which eliminate dependencies between targeted variables and their direct causes. In this paper, we extend these identifiability results to general interventions, which may modify the dependencies between targeted variables and their causes without eliminating them. We define and characterize the interventional Markov equivalence class that can be identified from general (not necessarily perfect) intervention experiments. We also propose the first provably consistent algorithm for learning DAGs in this setting and evaluate our algorithm on simulated and biological datasets.",http://proceedings.mlr.press/v80/yang18a.html,http://proceedings.mlr.press/v80/yang18a/yang18a.pdf,ICML
1253,2018,Bounds on the Approximation Power of Feedforward Neural Networks,"Mohammad Mehrabi,         Aslan Tchamkerten,         MANSOOR YOUSEFI","The approximation power of general feedforward neural networks with piecewise linear activation functions is investigated. First, lower bounds on the size of a network are established in terms of the approximation error and network depth and width. These bounds improve upon state-of-the-art bounds for certain classes of functions, such as strongly convex functions. Second, an upper bound is established on the difference of two neural networks with identical weights but different activation functions.",http://proceedings.mlr.press/v80/mehrabi18a.html,http://proceedings.mlr.press/v80/mehrabi18a/mehrabi18a.pdf,ICML
1254,2018,Error Compensated Quantized SGD and its Applications to Large-scale Distributed Optimization,"Jiaxiang Wu,         Weidong Huang,         Junzhou Huang,         Tong Zhang","Large-scale distributed optimization is of great importance in various applications. For data-parallel based distributed learning, the inter-node gradient communication often becomes the performance bottleneck. In this paper, we propose the error compensated quantized stochastic gradient descent algorithm to improve the training efficiency. Local gradients are quantized to reduce the communication overhead, and accumulated quantization error is utilized to speed up the convergence. Furthermore, we present theoretical analysis on the convergence behaviour, and demonstrate its advantage over competitors. Extensive experiments indicate that our algorithm can compress gradients by a factor of up to two magnitudes without performance degradation.",http://proceedings.mlr.press/v80/wu18d.html,http://proceedings.mlr.press/v80/wu18d/wu18d.pdf,ICML
1255,2018,Latent Space Policies for Hierarchical Reinforcement Learning,"Tuomas Haarnoja,         Kristian Hartikainen,         Pieter Abbeel,         Sergey Levine","We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer’s policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.",http://proceedings.mlr.press/v80/haarnoja18a.html,http://proceedings.mlr.press/v80/haarnoja18a/haarnoja18a.pdf,ICML
1256,2018,Tighter Variational Bounds are Not Necessarily Better,"Tom Rainforth,         Adam Kosiorek,         Tuan Anh Le,         Chris Maddison,         Maximilian Igl,         Frank Wood,         Yee Whye Teh","We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted autoencoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.",http://proceedings.mlr.press/v80/rainforth18b.html,http://proceedings.mlr.press/v80/rainforth18b/rainforth18b.pdf,ICML
1257,2018,Compiling Combinatorial Prediction Games,Frederic Koriche,"In online optimization, the goal is to iteratively choose solutions from a decision space, so as to minimize the average cost over time. As long as this decision space is described by combinatorial constraints, the problem is generally intractable. In this paper, we consider the paradigm of compiling the set of combinatorial constraints into a deterministic and Decomposable Negation Normal Form (dDNNF) circuit, for which the tasks of linear optimization and solution sampling take linear time. Based on this framework, we provide efficient characterizations of existing combinatorial prediction strategies, with a particular attention to mirror descent techniques. These strategies are compared on several real-world benchmarks for which the set of Boolean constraints is preliminarily compiled into a dDNNF circuit.",http://proceedings.mlr.press/v80/koriche18a.html,http://proceedings.mlr.press/v80/koriche18a/koriche18a.pdf,ICML
1258,2018,Learning Semantic Representations for Unsupervised Domain Adaptation,"Shaoan Xie,         Zibin Zheng,         Liang Chen,         Chuan Chen","It is important to transfer the knowledge from label-rich source domain to unlabeled target domain due to the expensive cost of manual labeling efforts. Prior domain adaptation methods address this problem through aligning the global distribution statistics between source domain and target domain, but a drawback of prior methods is that they ignore the semantic information contained in samples, e.g., features of backpacks in target domain might be mapped near features of cars in source domain. In this paper, we present moving semantic transfer network, which learn semantic representations for unlabeled target samples by aligning labeled source centroid and pseudo-labeled target centroid. Features in same class but different domains are expected to be mapped nearby, resulting in an improved target classification accuracy. Moving average centroid alignment is cautiously designed to compensate the insufficient categorical information within each mini batch. Experiments testify that our model yields state of the art results on standard datasets.",http://proceedings.mlr.press/v80/xie18c.html,http://proceedings.mlr.press/v80/xie18c/xie18c.pdf,ICML
1259,2018,Non-convex Conditional Gradient Sliding,"Chao Qu,         Yan Li,         Huan Xu","We investigate a projection free optimization method, namely non-convex conditional gradient sliding (NCGS) for non-convex optimization problems on the batch, stochastic and finite-sum settings. Conditional gradient sliding (CGS) method, by integrating Nesterov’s accelerated gradient method with Frank-Wolfe (FW) method in a smart way, outperforms FW for convex optimization, by reducing the amount of gradient computations. However, the study of CGS in the non-convex setting is limited. In this paper, we propose the non-convex conditional gradient sliding (NCGS) methods and analyze their convergence properties. We also leverage the idea of variance reduction from the recent progress in convex optimization to obtain a new algorithm termed variance reduced NCGS (NCGS-VR), and obtain faster convergence rate than the batch NCGS in the finite-sum setting. We show that NCGS algorithms outperform their Frank-Wolfe counterparts both in theory and in practice, for all three settings, namely the batch, stochastic and finite-sum setting. This significantly improves our understanding of optimizing non-convex functions with complicated feasible sets (where projection is prohibitively expensive).",http://proceedings.mlr.press/v80/qu18a.html,http://proceedings.mlr.press/v80/qu18a/qu18a.pdf,ICML
1260,2018,Crowdsourcing with Arbitrary Adversaries,"Matthaeus Kleindessner,         Pranjal Awasthi","Most existing works on crowdsourcing assume that the workers follow the Dawid-Skene model, or the one-coin model as its special case, where every worker makes mistakes independently of other workers and with the same error probability for every task. We study a significant extension of this restricted model. We allow almost half of the workers to deviate from the one-coin model and for those workers, their probabilities of making an error to be task-dependent and to be arbitrarily correlated. In other words, we allow for arbitrary adversaries, for which not only error probabilities can be high, but which can also perfectly collude. In this adversarial scenario, we design an efficient algorithm to consistently estimate the workers’ error probabilities.",http://proceedings.mlr.press/v80/kleindessner18a.html,http://proceedings.mlr.press/v80/kleindessner18a/kleindessner18a.pdf,ICML
1261,2018,RLlib: Abstractions for Distributed Reinforcement Learning,"Eric Liang,         Richard Liaw,         Robert Nishihara,         Philipp Moritz,         Roy Fox,         Ken Goldberg,         Joseph Gonzalez,         Michael Jordan,         Ion Stoica","Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RL. These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available as part of the open source Ray project at http://rllib.io/.",http://proceedings.mlr.press/v80/liang18b.html,http://proceedings.mlr.press/v80/liang18b/liang18b.pdf,ICML
1262,2018,Distributed Asynchronous Optimization with Unbounded Delays: How Slow Can You Go?,"Zhengyuan Zhou,         Panayotis Mertikopoulos,         Nicholas Bambos,         Peter Glynn,         Yinyu Ye,         Li-Jia Li,         Li Fei-Fei","One of the most widely used optimization methods for large-scale machine learning problems is distributed asynchronous stochastic gradient descent (DASGD). However, a key issue that arises here is that of delayed gradients: when a “worker” node asynchronously contributes a gradient update to the “master”, the global model parameter may have changed, rendering this information stale. In massively parallel computing grids, these delays can quickly add up if the computational throughput of a node is saturated, so the convergence of DASGD is uncertain under these conditions. Nevertheless, by using a judiciously chosen quasilinear step-size sequence, we show that it is possible to amortize these delays and achieve global convergence with probability 1, even when the delays grow at a polynomial rate. In this way, our results help reaffirm the successful application of DASGD to large-scale optimization problems.",http://proceedings.mlr.press/v80/zhou18b.html,http://proceedings.mlr.press/v80/zhou18b/zhou18b.pdf,ICML
1263,2018,Invariance of Weight Distributions in Rectified MLPs,"Russell Tsuchida,         Fred Roosta,         Marcus Gallagher","An interesting approach to analyzing neural networks that has received renewed attention is to examine the equivalent kernel of the neural network. This is based on the fact that a fully connected feedforward network with one hidden layer, a certain weight distribution, an activation function, and an infinite number of neurons can be viewed as a mapping into a Hilbert space. We derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for all rotationally-invariant weight distributions, generalizing a previous result that required Gaussian weight distributions. Additionally, the Central Limit Theorem is used to show that for certain activation functions, kernels corresponding to layers with weight distributions having 000 mean and finite absolute third moment are asymptotically universal, and are well approximated by the kernel corresponding to layers with spherical Gaussian weights. In deep networks, as depth increases the equivalent kernel approaches a pathological fixed point, which can be used to argue why training randomly initialized networks can be difficult. Our results also have implications for weight initialization.",http://proceedings.mlr.press/v80/tsuchida18a.html,http://proceedings.mlr.press/v80/tsuchida18a/tsuchida18a.pdf,ICML
1264,2018,Disentangled Sequential Autoencoder,"Li Yingzhen,         Stephan Mandt","We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.",http://proceedings.mlr.press/v80/yingzhen18a.html,http://proceedings.mlr.press/v80/yingzhen18a/yingzhen18a.pdf,ICML
1265,2018,Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations,"Xingyu Wang,         Diego Klabjan","This paper considers the problem of inverse reinforcement learning in zero-sum stochastic games when expert demonstrations are known to be suboptimal. Compared to previous works that decouple agents in the game by assuming optimality in expert policies, we introduce a new objective function that directly pits experts against Nash Equilibrium policies, and we design an algorithm to solve for the reward function in the context of inverse reinforcement learning with deep neural networks as model approximations. To ?nd Nash Equilibrium in large-scale games, we also propose an adversarial training algorithm for zero-sum stochastic games, and show the theoretical appeal of non-existence of local optima in its objective function. In numerical experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement learning algorithms address games that are not amenable to existing benchmark algorithms. Moreover, our algorithm successfully recovers reward and policy functions regardless of the quality of the sub-optimal expert demonstration set.",http://proceedings.mlr.press/v80/wang18d.html,http://proceedings.mlr.press/v80/wang18d/wang18d.pdf,ICML
1266,2018,Image Transformer,"Niki Parmar,         Ashish Vaswani,         Jakob Uszkoreit,         Lukasz Kaiser,         Noam Shazeer,         Alexander Ku,         Dustin Tran","Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.",http://proceedings.mlr.press/v80/parmar18a.html,http://proceedings.mlr.press/v80/parmar18a/parmar18a.pdf,ICML
1267,2018,Frank-Wolfe with Subsampling Oracle,"Thomas Kerdreux,         Fabian Pedregosa,         Alexandre d’Aspremont","We analyze two novel randomized variants of the Frank-Wolfe (FW) or conditional gradient algorithm. While classical FW algorithms require solving a linear minimization problem over the domain at each iteration, the proposed method only requires to solve a linear minimization problem over a small subset of the original domain. The first algorithm that we propose is a randomized variant of the original FW algorithm and achieves a O(1/t)O(1/t)\mathcal{O}(1/t) sublinear convergence rate as in the deterministic counterpart. The second algorithm is a randomized variant of the Away-step FW algorithm, and again as its deterministic counterpart, reaches linear (i.e., exponential) convergence rate making it the first provably convergent randomized variant of Away-step FW. In both cases, while subsampling reduces the convergence rate by a constant factor, the linear minimization step can be a fraction of the cost of that of the deterministic versions, especially when the data is streamed. We illustrate computational gains of both algorithms on regression problems, involving both ℓ1ℓ1\ell_1 and latent group lasso penalties.",http://proceedings.mlr.press/v80/kerdreux18a.html,http://proceedings.mlr.press/v80/kerdreux18a/kerdreux18a.pdf,ICML
1268,2018,Mix & Match Agent Curricula for Reinforcement Learning,"Wojciech Czarnecki,         Siddhant Jayakumar,         Max Jaderberg,         Leonard Hasenclever,         Yee Whye Teh,         Nicolas Heess,         Simon Osindero,         Razvan Pascanu","We introduce Mix and match (M&M) – a training framework designed to facilitate rapid and effective learning in RL agents that would be too slow or too challenging to train otherwise.The key innovation is a procedure that allows us to automatically form a curriculum over agents. Through such a curriculum we can progressively train more complex agents by, effectively, bootstrapping from solutions found by simpler agents.In contradistinction to typical curriculum learning approaches, we do not gradually modify the tasks or environments presented, but instead use a process to gradually alter how the policy is represented internally.We show the broad applicability of our method by demonstrating significant performance gains in three different experimental setups: (1) We train an agent able to control more than 700 actions in a challenging 3D first-person task; using our method to progress through an action-space curriculum we achieve both faster training and better final performance than one obtains using traditional methods.(2) We further show that M&M can be used successfully to progress through a curriculum of architectural variants defining an agents internal state. (3) Finally, we illustrate how a variant of our method can be used to improve agent performance in a multitask setting.",http://proceedings.mlr.press/v80/czarnecki18a.html,http://proceedings.mlr.press/v80/czarnecki18a/czarnecki18a.pdf,ICML
1269,2018,Probabilistic Boolean Tensor Decomposition,"Tammo Rukat,         Chris Holmes,         Christopher Yau","Boolean tensor decomposition approximates data of multi-way binary relationships as product of interpretable low-rank binary factors, following the rules Boolean algebra. Here, we present its first probabilistic treatment. We facilitate scalable sampling-based posterior inference by exploitation of the combinatorial structure of the factor conditionals. Maximum a posteriori estimates consistently outperform existing non-probabilistic approaches. We show that our performance gains can partially be explained by convergence to solutions that occupy relatively large regions of the parameter space, as well as by implicit model averaging. Moreover, the Bayesian treatment facilitates model selection with much greater accuracy than the previously suggested minimum description length based approach. We investigate three real-world data sets. First, temporal interaction networks and behavioural data of university students demonstrate the inference of instructive latent patterns. Next, we decompose a tensor with more than 10 Billion data points, indicating relations of gene expression in cancer patients. Not only does this demonstrate scalability, it also provides an entirely novel perspective on relational properties of continuous data and, in the present example, on the molecular heterogeneity of cancer. Our implementation is available on GitHub: https://github.com/TammoR/LogicalFactorisationMachines",http://proceedings.mlr.press/v80/rukat18a.html,http://proceedings.mlr.press/v80/rukat18a/rukat18a.pdf,ICML
1270,2018,Accelerating Greedy Coordinate Descent Methods,"Haihao Lu,         Robert Freund,         Vahab Mirrokni","We introduce and study two algorithms to accelerate greedy coordinate descent in theory and in practice: Accelerated Semi-Greedy Coordinate Descent (ASCD) and Accelerated Greedy Coordinate Descent (AGCD). On the theory side, our main results are for ASCD: we show that ASCD achieves O(1/k2)O(1/k2)O(1/k^2) convergence, and it also achieves accelerated linear convergence for strongly convex functions. On the empirical side, while both AGCD and ASCD outperform Accelerated Randomized Coordinate Descent on most instances in our numerical experiments, we note that AGCD significantly outperforms the other two methods in our experiments, in spite of a lack of theoretical guarantees for this method. To complement this empirical finding for AGCD, we present an explanation why standard proof techniques for acceleration cannot work for AGCD, and we further introduce a technical condition under which AGCD is guaranteed to have accelerated convergence. Finally, we confirm that this technical condition holds in our numerical experiments.",http://proceedings.mlr.press/v80/lu18b.html,http://proceedings.mlr.press/v80/lu18b/lu18b.pdf,ICML
1271,2018,Loss Decomposition for Fast Learning in Large Output Spaces,"Ian En-Hsu Yen,         Satyen Kale,         Felix Yu,         Daniel Holtmann-Rice,         Sanjiv Kumar,         Pradeep Ravikumar","For problems with large output spaces, evaluation of the loss function and its gradient are expensive, typically taking linear time in the size of the output space. Recently, methods have been developed to speed up learning via efficient data structures for Nearest-Neighbor Search (NNS) or Maximum Inner-Product Search (MIPS). However, the performance of such data structures typically degrades in high dimensions. In this work, we propose a novel technique to reduce the intractable high dimensional search problem to several much more tractable lower dimensional ones via dual decomposition of the loss function. At the same time, we demonstrate guaranteed convergence to the original loss via a greedy message passing procedure. In our experiments on multiclass and multilabel classification with hundreds of thousands of classes, as well as training skip-gram word embeddings with a vocabulary size of half a million, our technique consistently improves the accuracy of search-based gradient approximation methods and outperforms sampling-based gradient approximation methods by a large margin.",http://proceedings.mlr.press/v80/yen18a.html,http://proceedings.mlr.press/v80/yen18a/yen18a.pdf,ICML
1272,2018,Transfer Learning via Learning to Transfer,"Ying WEI,         Yu Zhang,         Junzhou Huang,         Qiang Yang","In transfer learning, what and how to transfer are two primary issues to be addressed, as different transfer learning algorithms applied between a source and a target domain result in different knowledge transferred and thereby the performance improvement in the target domain. Determining the optimal one that maximizes the performance improvement requires either exhaustive exploration or considerable expertise. Meanwhile, it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices. Motivated by this, we propose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences. We establish the L2T framework in two stages: 1) we learn a reflection function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer are the best for a future pair of domains by optimizing the reflection function. We also theoretically analyse the algorithmic stability and generalization bound of L2T, and empirically demonstrate its superiority over several state-of-the-art transfer learning algorithms.",http://proceedings.mlr.press/v80/wei18a.html,http://proceedings.mlr.press/v80/wei18a/wei18a.pdf,ICML
1273,2018,Mitigating Bias in Adaptive Data Gathering via Differential Privacy,"Seth Neel,         Aaron Roth","Data that is gathered adaptively — via bandit algorithms, for example — exhibits bias. This is true both when gathering simple numeric valued data — the empirical means kept track of by stochastic bandit algorithms are biased downwards — and when gathering more complicated data — running hypothesis tests on complex data gathered via contextual bandit algorithms leads to false discovery. In this paper, we show that this problem is mitigated if the data collection procedure is differentially private. This lets us both bound the bias of simple numeric valued quantities (like the empirical means of stochastic bandit algorithms), and correct the p-values of hypothesis tests run on the adaptively gathered data. Moreover, there exist differentially private bandit algorithms with near optimal regret bounds: we apply existing theorems in the simple stochastic case, and give a new analysis for linear contextual bandits. We complement our theoretical results with experiments validating our theory.",http://proceedings.mlr.press/v80/neel18a.html,http://proceedings.mlr.press/v80/neel18a/neel18a.pdf,ICML
1274,2018,Limits of Estimating Heterogeneous Treatment Effects: Guidelines for Practical Algorithm Design,"Ahmed Alaa,         Mihaela Schaar","Estimating heterogeneous treatment effects from observational data is a central problem in many domains. Because counterfactual data is inaccessible, the problem differs fundamentally from supervised learning, and entails a more complex set of modeling choices. Despite a variety of recently proposed algorithmic solutions, a principled guideline for building estimators of treatment effects using machine learning algorithms is still lacking. In this paper, we provide such a guideline by characterizing the fundamental limits of estimating heterogeneous treatment effects, and establishing conditions under which these limits can be achieved. Our analysis reveals that the relative importance of the different aspects of observational data vary with the sample size. For instance, we show that selection bias matters only in small-sample regimes, whereas with a large sample size, the way an algorithm models the control and treated outcomes is what bottlenecks its performance. Guided by our analysis, we build a practical algorithm for estimating treatment effects using a non-stationary Gaussian processes with doubly-robust hyperparameters. Using a standard semi-synthetic simulation setup, we show that our algorithm outperforms the state-of-the-art, and that the behavior of existing algorithms conforms with our analysis.",http://proceedings.mlr.press/v80/alaa18a.html,http://proceedings.mlr.press/v80/alaa18a/alaa18a.pdf,ICML
1275,2018,DVAE++: Discrete Variational Autoencoders with Overlapping Transformations,"Arash Vahdat,         William Macready,         Zhengbing Bian,         Amir Khoshaman,         Evgeny Andriyash","Training of discrete latent variable models remains challenging because passing gradient information through discrete units is difficult. We propose a new class of smoothing transformations based on a mixture of two overlapping distributions, and show that the proposed transformation can be used for training binary latent models with either directed or undirected priors. We derive a new variational bound to efficiently train with Boltzmann machine priors. Using this bound, we develop DVAE++, a generative model with a global discrete prior and a hierarchy of convolutional continuous variables. Experiments on several benchmarks show that overlapping transformations outperform other recent continuous relaxations of discrete latent variables including Gumbel-Softmax (Maddison et al., 2016; Jang et al., 2016), and discrete variational autoencoders (Rolfe 2016).",http://proceedings.mlr.press/v80/vahdat18a.html,http://proceedings.mlr.press/v80/vahdat18a/vahdat18a.pdf,ICML
1276,2018,SAFFRON: an Adaptive Algorithm for Online Control of the False Discovery Rate,"Aaditya Ramdas,         Tijana Zrnic,         Martin Wainwright,         Michael Jordan","In the online false discovery rate (FDR) problem, one observes a possibly infinite sequence of ppp-values P1,P2,…P1,P2,…P_1,P_2,…, each testing a different null hypothesis, and an algorithm must pick a sequence of rejection thresholds α1,α2,…α1,α2,…\alpha_1,\alpha_2,… in an online fashion, effectively rejecting the kkk-th null hypothesis whenever Pk≤αkPk≤αkP_k \leq \alpha_k. Importantly, αkαk\alpha_k must be a function of the past, and cannot depend on PkPkP_k or any of the later unseen ppp-values, and must be chosen to guarantee that for any time ttt, the FDR up to time ttt is less than some pre-determined quantity α∈(0,1)α∈(0,1)\alpha \in (0,1). In this work, we present a powerful new framework for online FDR control that we refer to as “SAFFRON”. Like older alpha-investing algorithms, SAFFRON starts off with an error budget (called alpha-wealth) that it intelligently allocates to different tests over time, earning back some alpha-wealth whenever it makes a new discovery. However, unlike older methods, SAFFRON’s threshold sequence is based on a novel estimate of the alpha fraction that it allocates to true null hypotheses. In the offline setting, algorithms that employ an estimate of the proportion of true nulls are called “adaptive”, hence SAFFRON can be seen as an online analogue of the offline Storey-BH adaptive procedure. Just as Storey-BH is typically more powerful than the Benjamini-Hochberg (BH) procedure under independence, we demonstrate that SAFFRON is also more powerful than its non-adaptive counterparts such as LORD.",http://proceedings.mlr.press/v80/ramdas18a.html,http://proceedings.mlr.press/v80/ramdas18a/ramdas18a.pdf,ICML
1277,2018,A Semantic Loss Function for Deep Learning with Symbolic Knowledge,"Jingyi Xu,         Zilu Zhang,         Tal Friedman,         Yitao Liang,         Guy Broeck","This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that it effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.",http://proceedings.mlr.press/v80/xu18h.html,http://proceedings.mlr.press/v80/xu18h/xu18h.pdf,ICML
1278,2018,End-to-end Active Object Tracking via Reinforcement Learning,"Wenhan Luo,         Peng Sun,         Fangwei Zhong,         Wei Liu,         Tong Zhang,         Yizhou Wang","We study active object tracking, where a tracker takes as input the visual observation (i.e. frame sequence) and produces the camera control signal (e.g., move forward, turn left, etc). Conventional methods tackle the tracking and the camera control separately, which is challenging to tune jointly. It also incurs many human efforts for labeling and many expensive trial-and-errors in real-world. To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning, where a ConvNet-LSTM function approximator is adopted for the direct frame-to-action prediction. We further propose an environment augmentation technique and a customized reward function, which are crucial for a successful training. The tracker trained in simulators (ViZDoom, Unreal Engine) shows good generalization in the case of unseen object moving path, unseen object appearance, unseen background, and distracting object. It can restore tracking when occasionally losing the target. With the experiments over the VOT dataset, we also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios.",http://proceedings.mlr.press/v80/luo18a.html,http://proceedings.mlr.press/v80/luo18a/luo18a.pdf,ICML
1279,2018,Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data,"Amjad Almahairi,         Sai Rajeshwar,         Alessandro Sordoni,         Philip Bachman,         Aaron Courville","Learning inter-domain mappings from unpaired data can improve performance in structured prediction tasks, such as image segmentation, by reducing the need for paired data. CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-to-one. This assumption renders the model ineffective for tasks requiring flexible, many-to-many mappings. We propose a new model, called Augmented CycleGAN, which learns many-to-many mappings between domains. We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.",http://proceedings.mlr.press/v80/almahairi18a.html,http://proceedings.mlr.press/v80/almahairi18a/almahairi18a.pdf,ICML
1280,2018,The Mechanics of n-Player Differentiable Games,"David Balduzzi,         Sebastien Racaniere,         James Martens,         Jakob Foerster,         Karl Tuyls,         Thore Graepel","The cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood – and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new techniques to understand and control the dynamics in general games. The key result is to decompose the second-order dynamics into two components. The first is related to potential games, which reduce to gradient descent on an implicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in general games. Basic experiments show SGA is competitive with recently proposed algorithms for finding local Nash equilibria in GANs – whilst at the same time being applicable to – and having guarantees in – much more general games.",http://proceedings.mlr.press/v80/balduzzi18a.html,http://proceedings.mlr.press/v80/balduzzi18a/balduzzi18a.pdf,ICML
1281,2018,Improved Regret Bounds for Thompson Sampling in Linear Quadratic Control Problems,"Marc Abeille,         Alessandro Lazaric","Thompson sampling (TS) is an effective approach to trade off exploration and exploration in reinforcement learning. Despite its empirical success and recent advances, its theoretical analysis is often limited to the Bayesian setting, finite state-action spaces, or finite-horizon problems. In this paper, we study an instance of TS in the challenging setting of the infinite-horizon linear quadratic (LQ) control, which models problems with continuous state-action variables, linear dynamics, and quadratic cost. In particular, we analyze the regret in the frequentist sense (i.e., for a fixed unknown environment) in one-dimensional systems. We derive the first O(T−−√)O(T)O(\sqrt{T}) frequentist regret bound for this problem, thus significantly improving the O(T2/3)O(T2/3)O(T^{2/3}) bound of Abeille & Lazaric (2017) and matching the frequentist performance derived by Abbasi-Yadkori & Szepesvári (2011) for an optimistic approach and the Bayesian result Ouyang et al. (2017) We obtain this result by developing a novel bound on the regret due to policy switches, which holds for LQ systems of any dimensionality and it allows updating the parameters and the policy at each step, thus overcoming previous limitations due to lazy updates. Finally, we report numerical simulations supporting the conjecture that our result extends to multi-dimensional systems.",http://proceedings.mlr.press/v80/abeille18a.html,http://proceedings.mlr.press/v80/abeille18a/abeille18a.pdf,ICML
1282,2018,Selecting Representative Examples for Program Synthesis,"Yewen Pu,         Zachery Miranda,         Armando Solar-Lezama,         Leslie Kaelbling","Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, mapping the inputs to their corresponding outputs exactly. Due to its precise and combinatorial nature, program synthesis is commonly formulated as a constraint satisfaction problem, where input-output examples are encoded as constraints and solved with a constraint solver. A key challenge of this formulation is scalability: while constraint solvers work well with a few well-chosen examples, a large set of examples can incur significant overhead in both time and memory. We describe a method to discover a subset of examples that is both small and representative: the subset is constructed iteratively, using a neural network to predict the probability of unchosen examples conditioned on the chosen examples in the subset, and greedily adding the least probable example. We empirically evaluate the representativeness of the subsets constructed by our method, and demonstrate such subsets can significantly improve synthesis time and stability.",http://proceedings.mlr.press/v80/pu18b.html,http://proceedings.mlr.press/v80/pu18b/pu18b.pdf,ICML
1283,2018,Convergent Tree Backup and Retrace with Function Approximation,"Ahmed Touati,         Pierre-Luc Bacon,         Doina Precup,         Pascal Vincent","Off-policy learning is key to scaling up reinforcement learning as it allows to learn about a target policy from the experience generated by a different behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping in a way that leads to both stable and efficient algorithms. In this work, we show that the Tree Backup and Retrace algorithms are unstable with linear function approximation, both in theory and in practice with specific examples. Based on our analysis, we then derive stable and efficient gradient-based algorithms using a quadratic convex-concave saddle-point formulation. By exploiting the problem structure proper to these algorithms, we are able to provide convergence guarantees and finite-sample bounds. The applicability of our new analysis also goes beyond Tree Backup and Retrace and allows us to provide new convergence rates for the GTD and GTD2 algorithms without having recourse to projections or Polyak averaging.",http://proceedings.mlr.press/v80/touati18a.html,http://proceedings.mlr.press/v80/touati18a/touati18a.pdf,ICML
1284,2018,Fast Information-theoretic Bayesian Optimisation,"Binxin Ru,         Michael A. Osborne,         Mark Mcleod,         Diego Granziol","Information-theoretic Bayesian optimisation techniques have demonstrated state-of-the-art performance in tackling important global optimisation problems. However, current information-theoretic approaches require many approximations in implementation, introduce often-prohibitive computational overhead and limit the choice of kernels available to model the objective. We develop a fast information-theoretic Bayesian Optimisation method, FITBO, that avoids the need for sampling the global minimiser, thus significantly reducing computational overhead. Moreover, in comparison with existing approaches, our method faces fewer constraints on kernel choice and enjoys the merits of dealing with the output space. We demonstrate empirically that FITBO inherits the performance associated with information-theoretic Bayesian optimisation, while being even faster than simpler Bayesian optimisation approaches, such as Expected Improvement.",http://proceedings.mlr.press/v80/ru18a.html,http://proceedings.mlr.press/v80/ru18a/ru18a.pdf,ICML
1285,2018,The Dynamics of Learning: A Random Matrix Approach,"Zhenyu Liao,         Romain Couillet","Understanding the learning dynamics of neural networks is one of the key issues for the improvement of optimization algorithms as well as for the theoretical comprehension of why deep neural nets work so well today. In this paper, we introduce a random matrix-based framework to analyze the learning dynamics of a single-layer linear network on a binary classification problem, for data of simultaneously large dimension and size, trained by gradient descent. Our results provide rich insights into common questions in neural nets, such as overfitting, early stopping and the initialization of training, thereby opening the door for future studies of more elaborate structures and models appearing in today’s neural networks.",http://proceedings.mlr.press/v80/liao18b.html,http://proceedings.mlr.press/v80/liao18b/liao18b.pdf,ICML
1286,2018,Fast Parametric Learning with Activation Memorization,"Jack Rae,         Chris Dyer,         Peter Dayan,         Timothy Lillicrap","Neural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times. In applications where most class labels are rare, such as language modelling, this can become a performance bottleneck. One potential remedy is to augment the network with a fast-learning non-parametric model which stores recent activations and class labels into an external memory. We explore a simplified architecture where we treat a subset of the model parameters as fast memory stores. This can help retain information over longer time intervals than a traditional memory, and does not require additional space or compute. In the case of image classification, we display faster binding of novel classes on an Omniglot image curriculum task. We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) - the latter achieving a state-of-the-art perplexity of 29.2.",http://proceedings.mlr.press/v80/rae18a.html,http://proceedings.mlr.press/v80/rae18a/rae18a.pdf,ICML
1287,2018,SQL-Rank: A Listwise Approach to Collaborative Ranking,"Liwei Wu,         Cho-Jui Hsieh,         James Sharpnack","In this paper, we propose a listwise approach for constructing user-specific rankings in recommendation systems in a collaborative fashion. We contrast the listwise approach to previous pointwise and pairwise approaches, which are based on treating either each rating or each pairwise comparison as an independent instance respectively. By extending the work of ListNet (Cao et al., 2007), we cast listwise collaborative ranking as maximum likelihood under a permutation model which applies probability mass to permutations based on a low rank latent score matrix. We present a novel algorithm called SQL-Rank, which can accommodate ties and missing data and can run in linear time. We develop a theoretical framework for analyzing listwise ranking methods based on a novel representation theory for the permutation model. Applying this framework to collaborative ranking, we derive asymptotic statistical rates as the number of users and items grow together. We conclude by demonstrating that our SQL-Rank method often outperforms current state-of-the-art algorithms for implicit feedback such as Weighted-MF and BPR and achieve favorable results when compared to explicit feedback algorithms such as matrix factorization and collaborative ranking.",http://proceedings.mlr.press/v80/wu18c.html,http://proceedings.mlr.press/v80/wu18c/wu18c.pdf,ICML
1288,2018,Self-Bounded Prediction Suffix Tree via Approximate String Matching,"Dongwoo Kim,         Christian Walder","Prediction suffix trees (PST) provide an effective tool for sequence modelling and prediction. Current prediction techniques for PSTs rely on exact matching between the suffix of the current sequence and the previously observed sequence. We present a provably correct algorithm for learning a PST with approximate suffix matching by relaxing the exact matching condition. We then present a self-bounded enhancement of our algorithm where the depth of suffix tree grows automatically in response to the model performance on a training sequence. Through experiments on synthetic datasets as well as three real-world datasets, we show that the approximate matching PST results in better predictive performance than the other variants of PST.",http://proceedings.mlr.press/v80/kim18c.html,http://proceedings.mlr.press/v80/kim18c/kim18c.pdf,ICML
1289,2018,Kronecker Recurrent Units,"Cijo Jose,         Moustapha Cisse,         Francois Fleuret","Our work addresses two important issues with recurrent neural networks: (1) they are over-parametrized, and (2) the recurrent weight matrix is ill-conditioned. The former increases the sample complexity of learning and the training time. The latter causes the vanishing and exploding gradient problem. We present a flexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achieves parameter efficiency in RNNs through a Kronecker factored recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by enforcing soft unitary constraints on the factors. Thanks to the small dimensionality of the factors, maintaining these constraints is computationally efficient. Our experimental results on seven standard data-sets reveal that KRU can reduce the number of parameters by three orders of magnitude in the recurrent weight matrix compared to the existing recurrent models, without trading the statistical performance. These results in particular show that while there are advantages in having a high dimensional recurrent space, the capacity of the recurrent part of the model can be dramatically reduced.",http://proceedings.mlr.press/v80/jose18a.html,http://proceedings.mlr.press/v80/jose18a/jose18a.pdf,ICML
1290,2018,Reinforcement Learning with Function-Valued Action Spaces for Partial Differential Equation Control,"Yangchen Pan,         Amir-massoud Farahmand,         Martha White,         Saleh Nabi,         Piyush Grover,         Daniel Nikovski","Recent work has shown that reinforcement learning (RL) is a promising approach to control dynamical systems described by partial differential equations (PDE). This paper shows how to use RL to tackle more general PDE control problems that have continuous high-dimensional action spaces with spatial relationship among action dimensions. In particular, we propose the concept of action descriptors, which encode regularities among spatially-extended action dimensions and enable the agent to control high-dimensional action PDEs. We provide theoretical evidence suggesting that this approach can be more sample efficient compared to a conventional approach that treats each action dimension separately and does not explicitly exploit the spatial regularity of the action space. The action descriptor approach is then used within the deep deterministic policy gradient algorithm. Experiments on two PDE control problems, with up to 256-dimensional continuous actions, show the advantage of the proposed approach over the conventional one.",http://proceedings.mlr.press/v80/pan18a.html,http://proceedings.mlr.press/v80/pan18a/pan18a.pdf,ICML
1291,2018,Learning Adversarially Fair and Transferable Representations,"David Madras,         Elliot Creager,         Toniann Pitassi,         Richard Zemel","In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.",http://proceedings.mlr.press/v80/madras18a.html,http://proceedings.mlr.press/v80/madras18a/madras18a.pdf,ICML
1292,2018,Semiparametric Contextual Bandits,"Akshay Krishnamurthy,         Zhiwei Steven Wu,         Vasilis Syrgkanis","This paper studies semiparametric contextual bandits, a generalization of the linear stochastic bandit problem where the reward for a chosen action is modeled as a linear function of known action features confounded by a non-linear action-independent term. We design new algorithms that achieve O~(dT−−√)O~(dT)\tilde{O}(d\sqrt{T}) regret over TTT rounds, when the linear function is ddd-dimensional, which matches the best known bounds for the simpler unconfounded case and improves on a recent result of Greenwald et al. (2017). Via an empirical evaluation, we show that our algorithms outperform prior approaches when there are non-linear confounding effects on the rewards. Technically, our algorithms use a new reward estimator inspired by doubly-robust approaches and our proofs require new concentration inequalities for self-normalized martingales.",http://proceedings.mlr.press/v80/krishnamurthy18a.html,http://proceedings.mlr.press/v80/krishnamurthy18a/krishnamurthy18a.pdf,ICML
1293,2018,Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks,"Daphna Weinshall,         Gad Cohen,         Dan Amir","We provide theoretical investigation of curriculum learning in the context of stochastic gradient descent when optimizing the convex linear regression loss. We prove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examples. Moreover, among all equally difficult points, convergence is faster when using points which incur higher loss with respect to the current hypothesis. We then analyze curriculum learning in the context of training a CNN. We describe a method which infers the curriculum by way of transfer learning from another network, pre-trained on a different task. While this approach can only approximate the ideal curriculum, we observe empirically similar behavior to the one predicted by the theory, namely, a significant boost in convergence speed at the beginning of training. When the task is made more difficult, improvement in generalization performance is also observed. Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.",http://proceedings.mlr.press/v80/weinshall18a.html,http://proceedings.mlr.press/v80/weinshall18a/weinshall18a.pdf,ICML
1294,2018,On the Relationship between Data Efficiency and Error for Uncertainty Sampling,"Stephen Mussmann,         Percy Liang","While active learning offers potential cost savings, the actual data efficiency—the reduction in amount of labeled data needed to obtain the same error rate—observed in practice is mixed. This paper poses a basic question: when is active learning actually helpful? We provide an answer for logistic regression with the popular active learning algorithm, uncertainty sampling. Empirically, on 21 datasets from OpenML, we find a strong inverse correlation between data efficiency and the error rate of the final classifier. Theoretically, we show that for a variant of uncertainty sampling, the asymptotic data efficiency is within a constant factor of the inverse error rate of the limiting classifier.",http://proceedings.mlr.press/v80/mussmann18a.html,http://proceedings.mlr.press/v80/mussmann18a/mussmann18a.pdf,ICML
1295,2018,Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity,"Lin Chen,         Christopher Harshaw,         Hamed Hassani,         Amin Karbasi","Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of non-convex optimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized. Although online methods have been introduced, they suffer from similar problems. In this work, we propose Meta-Frank-Wolfe, the first online projection-free algorithm that uses stochastic gradient estimates. The algorithm relies on a careful sampling of gradients in each round and achieves the optimal O(T−−√)O(T)O( \sqrt{T}) adversarial regret bounds for convex and continuous submodular optimization. We also propose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single stochastic gradient estimate in each round and achieves an O(T2/3)O(T2/3)O(T^{2/3}) stochastic regret bound for convex and continuous submodular optimization. We apply our methods to develop a novel ""lifting"" framework for the online discrete submodular maximization and also see that they outperform current state-of-the-art techniques on various experiments.",http://proceedings.mlr.press/v80/chen18c.html,http://proceedings.mlr.press/v80/chen18c/chen18c.pdf,ICML
1296,2018,A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music,"Adam Roberts,         Jesse Engel,         Colin Raffel,         Curtis Hawthorne,         Douglas Eck","The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the ""posterior collapse"" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a ""flat"" baseline model. An implementation of our ""MusicVAE"" is available online at https://goo.gl/magenta/musicvae-code.",http://proceedings.mlr.press/v80/roberts18a.html,http://proceedings.mlr.press/v80/roberts18a/roberts18a.pdf,ICML
1297,2018,Learning Long Term Dependencies via Fourier Recurrent Units,"Jiong Zhang,         Yibo Lin,         Zhao Song,         Inderjit Dhillon","It is a known fact that training recurrent neural networks for tasks that have long term dependencies is challenging. One of the main reasons is the vanishing or exploding gradient problem, which prevents gradient information from propagating to early layers. In this paper we propose a simple recurrent architecture, the Fourier Recurrent Unit (FRU), that stabilizes the gradients that arise in its training while giving us stronger expressive power. Specifically, FRU summarizes the hidden states h(t)h(t)h^{(t)} along the temporal dimension with Fourier basis functions. This allows gradients to easily reach any layer due to FRU’s residual learning structure and the global support of trigonometric functions. We show that FRU has gradient lower and upper bounds independent of temporal dimension. We also show the strong expressivity of sparse Fourier basis, from which FRU obtains its strong expressive power. Our experimental study also demonstrates that with fewer parameters the proposed architecture outperforms other recurrent architectures on many tasks.",http://proceedings.mlr.press/v80/zhang18h.html,http://proceedings.mlr.press/v80/zhang18h/zhang18h.pdf,ICML
1298,2018,Cut-Pursuit Algorithm for Regularizing Nonsmooth Functionals with Graph Total Variation,"Hugo Raguet,         Loic Landrieu","We present an extension of the cut-pursuit algorithm, introduced by Landrieu and Obozinski (2017), to the graph total-variation regularization of functions with a separable nondifferentiable part. We propose a modified algorithmic scheme as well as adapted proofs of convergence. We also present a heuristic approach for handling the cases in which the values associated to each vertex of the graph are multidimensional. The performance of our algorithm, which we demonstrate on difficult, ill-conditioned large-scale inverse and learning problems, is such that it may in practice extend the scope of application of the total-variation regularization.",http://proceedings.mlr.press/v80/raguet18a.html,http://proceedings.mlr.press/v80/raguet18a/raguet18a.pdf,ICML
1299,2018,Hierarchical Deep Generative Models for Multi-Rate Multivariate Time Series,"Zhengping Che,         Sanjay Purushotham,         Guangyu Li,         Bo Jiang,         Yan Liu","Multi-Rate Multivariate Time Series (MR-MTS) are the multivariate time series observations which come with various sampling rates and encode multiple temporal dependencies. State-space models such as Kalman filters and deep learning models such as deep Markov models are mainly designed for time series data with the same sampling rate and cannot capture all the dependencies present in the MR-MTS data. To address this challenge, we propose the Multi-Rate Hierarchical Deep Markov Model (MR-HDMM), a novel deep generative model which uses the latent hierarchical structure with a learnable switch mechanism to capture the temporal dependencies of MR-MTS. Experimental results on two real-world datasets demonstrate that our MR-HDMM model outperforms the existing state-of-the-art deep learning and state-space models on forecasting and interpolation tasks. In addition, the latent hierarchies in our model provide a way to show and interpret the multiple temporal dependencies.",http://proceedings.mlr.press/v80/che18a.html,http://proceedings.mlr.press/v80/che18a/che18a.pdf,ICML
1300,2018,Communication-Computation Efficient Gradient Coding,"Min Ye,         Emmanuel Abbe","This paper develops coding techniques to reduce the running time of distributed learning tasks. It characterizes the fundamental tradeoff to compute gradients in terms of three parameters: computation load, straggler tolerance and communication cost. It further gives an explicit coding scheme that achieves the optimal tradeoff based on recursive polynomial constructions, coding both across data subsets and vector components. As a result, the proposed scheme allows to minimize the running time for gradient computations. Implementations are made on Amazon EC2 clusters using Python with mpi4py package. Results show that the proposed scheme maintains the same generalization error while reducing the running time by 323232% compared to uncoded schemes and 2323% compared to prior coded schemes focusing only on stragglers (Tandon et al., ICML 2017).",http://proceedings.mlr.press/v80/ye18a.html,http://proceedings.mlr.press/v80/ye18a/ye18a.pdf,ICML
1301,2018,Black-box Adversarial Attacks with Limited Queries and Information,"Andrew Ilyas,         Logan Engstrom,         Anish Athalye,         Jessy Lin","Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.",http://proceedings.mlr.press/v80/ilyas18a.html,http://proceedings.mlr.press/v80/ilyas18a/ilyas18a.pdf,ICML
1302,2018,Anonymous Walk Embeddings,"Sergey Ivanov,         Evgeny Burnaev","The task of representing entire graphs has seen a surge of prominent results, mainly due to learning convolutional neural networks (CNNs) on graph-structured data. While CNNs demonstrate state-of-the-art performance in graph classification task, such methods are supervised and therefore steer away from the original problem of network representation in task-agnostic manner. Here, we coherently propose an approach for embedding entire graphs and show that our feature representations with SVM classifier increase classification accuracy of CNN algorithms and traditional graph kernels. For this we describe a recently discovered graph object, anonymous walk, on which we design task-independent algorithms for learning graph representations in explicit and distributed way. Overall, our work represents a new scalable unsupervised learning of state-of-the-art representations of entire graphs.",http://proceedings.mlr.press/v80/ivanov18a.html,http://proceedings.mlr.press/v80/ivanov18a/ivanov18a.pdf,ICML
1303,2018,Learning Compact Neural Networks with Regularization,Samet Oymak,"Proper regularization is critical for speeding up training, improving generalization performance, and learning compact models that are cost efficient. We propose and analyze regularized gradient descent algorithms for learning shallow neural networks. Our framework is general and covers weight-sharing (convolutional networks), sparsity (network pruning), and low-rank constraints among others. We first introduce covering dimension to quantify the complexity of the constraint set and provide insights on the generalization properties. Then, we show that proposed algorithms become well-behaved and local linear convergence occurs once the amount of data exceeds the covering dimension. Overall, our results demonstrate that near-optimal sample complexity is sufficient for efficient learning and illustrate how regularization can be beneficial to learn over-parameterized networks.",http://proceedings.mlr.press/v80/oymak18a.html,http://proceedings.mlr.press/v80/oymak18a/oymak18a.pdf,ICML
1304,2018,The Mirage of Action-Dependent Baselines in Reinforcement Learning,"George Tucker,         Surya Bhupatiraju,         Shixiang Gu,         Richard Turner,         Zoubin Ghahramani,         Sergey Levine","Policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the source of the previously observed empirical gains. Furthermore, the variance decomposition highlights areas for improvement, which we demonstrate by illustrating a simple change to the typical value function parameterization that can significantly improve performance.",http://proceedings.mlr.press/v80/tucker18a.html,http://proceedings.mlr.press/v80/tucker18a/tucker18a.pdf,ICML
1305,2018,Understanding the Loss Surface of Neural Networks for Binary Classification,"SHIYU LIANG,         Ruoyu Sun,         Yixuan Li,         Rayadurgam Srikant","It is widely conjectured that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of appropriately chosen surrogate loss functions. Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcut-like connection, and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that, when these conditions are relaxed, the result may not hold.",http://proceedings.mlr.press/v80/liang18a.html,http://proceedings.mlr.press/v80/liang18a/liang18a.pdf,ICML
1306,2018,Dissipativity Theory for Accelerating Stochastic Variance Reduction: A Unified Analysis of SVRG and Katyusha Using Semidefinite Programs,"Bin Hu,         Stephen Wright,         Laurent Lessard","Techniques for reducing the variance of gradient estimates used in stochastic programming algorithms for convex finite-sum problems have received a great deal of attention in recent years. By leveraging dissipativity theory from control, we provide a new perspective on two important variance-reduction algorithms: SVRG and its direct accelerated variant Katyusha. Our perspective provides a physically intuitive understanding of the behavior of SVRG-like methods via a principle of energy conservation. The tools discussed here allow us to automate the convergence analysis of SVRG-like methods by capturing their essential properties in small semidefinite programs amenable to standard analysis and computational techniques. Our approach recovers existing convergence results for SVRG and Katyusha and generalizes the theory to alternative parameter choices. We also discuss how our approach complements the linear coupling technique. Our combination of perspectives leads to a better understanding of accelerated variance-reduced stochastic methods for finite-sum problems.",http://proceedings.mlr.press/v80/hu18b.html,http://proceedings.mlr.press/v80/hu18b/hu18b.pdf,ICML
1307,2018,Probably Approximately Metric-Fair Learning,"Gal Yona,         Guy Rothblum","The seminal work of Dwork et al. [ITCS 2012] introduced a metric-based notion of individual fairness: given a task-specific similarity metric, their notion required that every pair of similar individuals should be treated similarly. In the context of machine learning, however, individual fairness does not generalize from a training set to the underlying population. We show that this can lead to computational intractability even for simple fair-learning tasks. With this motivation in mind, we introduce and study a relaxed notion of approximate metric-fairness: for a random pair of individuals sampled from the population, with all but a small probability of error, if they are similar then they should be treated similarly. We formalize the goal of achieving approximate metric-fairness simultaneously with best-possible accuracy as Probably Approximately Correct and Fair (PACF) Learning. We show that approximate metric-fairness does generalize, and leverage these generalization guarantees to construct polynomial-time PACF learning algorithms for the classes of linear and logistic predictors.",http://proceedings.mlr.press/v80/yona18a.html,http://proceedings.mlr.press/v80/yona18a/yona18a.pdf,ICML
1308,2018,Policy and Value Transfer in Lifelong Reinforcement Learning,"David Abel,         Yuu Jinnai,         Sophie Yue Guo,         George Konidaris,         Michael Littman","We consider the problem of how best to use prior experience to bootstrap lifelong learning, where an agent faces a series of task instances drawn from some task distribution. First, we identify the initial policy that optimizes expected performance over the distribution of tasks for increasingly complex classes of policy and task distributions. We empirically demonstrate the relative performance of each policy class’ optimal element in a variety of simple task distributions. We then consider value-function initialization methods that preserve PAC guarantees while simultaneously minimizing the learning required in two learning algorithms, yielding MaxQInit, a practical new method for value-function-based transfer. We show that MaxQInit performs well in simple lifelong RL experiments.",http://proceedings.mlr.press/v80/abel18b.html,http://proceedings.mlr.press/v80/abel18b/abel18b.pdf,ICML
1309,2018,Fast Bellman Updates for Robust MDPs,"Chin Pang Ho,         Marek Petrik,         Wolfram Wiesemann","We describe two efficient, and exact, algorithms for computing Bellman updates in robust Markov decision processes (MDPs). The first algorithm uses a homotopy continuation method to compute updates for L1-constrained s,a-rectangular ambiguity sets. It runs in quasi-linear time for plain L1-norms and also generalizes to weighted L1-norms. The second algorithm uses bisection to compute updates for robust MDPs with s-rectangular ambiguity sets. This algorithm, when combined with the homotopy method, also has a quasi-linear runtime. Unlike previous methods, our algorithms compute the primal solution in addition to the optimal objective value, which makes them useful in policy iteration methods. Our experimental results indicate that the proposed methods are over 1,000 times faster than Gurobi, a state-of-the-art commercial optimization package, for small instances, and the performance gap grows considerably with problem size.",http://proceedings.mlr.press/v80/ho18a.html,http://proceedings.mlr.press/v80/ho18a/ho18a.pdf,ICML
1310,2018,Deep Density Destructors,"David Inouye,         Pradeep Ravikumar","We propose a unified framework for deep density models by formally defining density destructors. A density destructor is an invertible function that transforms a given density to the uniform density—essentially destroying any structure in the original density. This destructive transformation generalizes Gaussianization via ICA and more recent autoregressive models such as MAF and Real NVP. Informally, this transformation can be seen as a generalized whitening procedure or a multivariate generalization of the univariate CDF function. Unlike Gaussianization, our destructive transformation has the elegant property that the density function is equal to the absolute value of the Jacobian determinant. Thus, each layer of a deep density can be seen as a shallow density—uncovering a fundamental connection between shallow and deep densities. In addition, our framework provides a common interface for all previous methods enabling them to be systematically combined, evaluated and improved. Leveraging the connection to shallow densities, we also propose a novel tree destructor based on tree densities and an image-specific destructor based on pixel locality. We illustrate our framework on a 2D dataset, MNIST, and CIFAR-10. Code is available on first author’s website.",http://proceedings.mlr.press/v80/inouye18a.html,http://proceedings.mlr.press/v80/inouye18a/inouye18a.pdf,ICML
1311,2018,PDE-Net: Learning PDEs from Data,"Zichao Long,         Yiping Lu,         Xianzhong Ma,         Bin Dong","Partial differential equations (PDEs) play a prominent role in many disciplines of science and engineering. PDEs are commonly derived based on empirical observations. However, with the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. Comparing with existing approaches, our approach has the most flexibility by learning both differential operators and the nonlinear response function of the underlying PDE model. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.",http://proceedings.mlr.press/v80/long18a.html,http://proceedings.mlr.press/v80/long18a/long18a.pdf,ICML
1312,2018,PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos,"Paavo Parmas,         Carl Edward Rasmussen,         Jan Peters,         Kenji Doya","Previously, the exploding gradient problem has been explained to be central in deep learning and model-based reinforcement learning, because it causes numerical issues and instability in optimization. Our experiments in model-based reinforcement learning imply that the problem is not just a numerical issue, but it may be caused by a fundamental chaos-like nature of long chains of nonlinear computations. Not only do the magnitudes of the gradients become large, the direction of the gradients becomes essentially random. We show that reparameterization gradients suffer from the problem, while likelihood ratio gradients are robust. Using our insights, we develop a model-based policy search framework, Probabilistic Inference for Particle-Based Policy Search (PIPPS), which is easily extensible, and allows for almost arbitrary models and policies, while simultaneously matching the performance of previous data-efficient learning algorithms. Finally, we invent the total propagation algorithm, which efficiently computes a union over all pathwise derivative depths during a single backwards pass, automatically giving greater weight to estimators with lower variance, sometimes improving over reparameterization gradients by 10610610^6 times.",http://proceedings.mlr.press/v80/parmas18a.html,http://proceedings.mlr.press/v80/parmas18a/parmas18a.pdf,ICML
1313,2018,Deep Variational Reinforcement Learning for POMDPs,"Maximilian Igl,         Luisa Zintgraf,         Tuan Anh Le,         Frank Wood,         Shimon Whiteson","Many real-world sequential decision making problems are partially observable by nature, and the environment model is typically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of rewards and incomplete and noisy observations. In this paper, we propose deep variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and perform inference in that model to effectively aggregate the available information. We develop an n-step approximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the policy. This ensures that the latent state representation is suitable for the control task. In experiments on Mountain Hike and flickering Atari we show that our method outperforms previous approaches relying on recurrent neural networks to encode the past.",http://proceedings.mlr.press/v80/igl18a.html,http://proceedings.mlr.press/v80/igl18a/igl18a.pdf,ICML
1314,2018,Transformation Autoregressive Networks,"Junier Oliva,         Avinava Dubey,         Manzil Zaheer,         Barnabas Poczos,         Ruslan Salakhutdinov,         Eric Xing,         Jeff Schneider","The fundamental task of general density estimation p(x)p(x)p(x) has been of keen interest to machine learning. In this work, we attempt to systematically characterize methods for density estimation. Broadly speaking, most of the existing methods can be categorized into either using: a) autoregressive models to estimate the conditional factors of the chain rule, p(xi|xi−1,…)p(xi|xi−1,…)p(x_{i}\, |\, x_{i-1}, \ldots); or b) non-linear transformations of variables of a simple base distribution. Based on the study of the characteristics of these categories, we propose multiple novel methods for each category. For example we propose RNN based transformations to model non-Markovian dependencies. Further, through a comprehensive study over both real world and synthetic data, we show that jointly leveraging transformations of variables and autoregressive conditional models, results in a considerable improvement in performance. We illustrate the use of our models in outlier detection and image modeling. Finally we introduce a novel data driven framework for learning a family of distributions.",http://proceedings.mlr.press/v80/oliva18a.html,http://proceedings.mlr.press/v80/oliva18a/oliva18a.pdf,ICML
1315,2018,Variable Selection via Penalized Neural Network: a Drop-Out-One Loss Approach,"Mao Ye,         Yan Sun","We propose a variable selection method for high dimensional regression models, which allows for complex, nonlinear, and high-order interactions among variables. The proposed method approximates this complex system using a penalized neural network and selects explanatory variables by measuring their utility in explaining the variance of the response variable. This measurement is based on a novel statistic called Drop-Out-One Loss. The proposed method also allows (overlapping) group variable selection. We prove that the proposed method can select relevant variables and exclude irrelevant variables with probability one as the sample size goes to infinity, which is referred to as the Oracle Property. Experimental results on simulated and real world datasets show the efficiency of our method in terms of variable selection and prediction accuracy.",http://proceedings.mlr.press/v80/ye18b.html,http://proceedings.mlr.press/v80/ye18b/ye18b.pdf,ICML
1316,2018,Fast Stochastic AUC Maximization with O(1/n)O(1/n)O(1/n)-Convergence Rate,"Mingrui Liu,         Xiaoxuan Zhang,         Zaiyi Chen,         Xiaoyu Wang,         Tianbao Yang","In this paper, we consider statistical learning with AUC (area under ROC curve) maximization in the classical stochastic setting where one random data drawn from an unknown distribution is revealed at each iteration for updating the model. Although consistent convex surrogate losses for AUC maximization have been proposed to make the problem tractable, it remains an challenging problem to design fast optimization algorithms in the classical stochastic setting due to that the convex surrogate loss depends on random pairs of examples from positive and negative classes. Building on a saddle point formulation for a consistent square loss, this paper proposes a novel stochastic algorithm to improve the standard O(1/n−−√)O(1/n)O(1/\sqrt{n}) convergence rate to O˜(1/n)O~(1/n)\widetilde O(1/n) convergence rate without strong convexity assumption or any favorable statistical assumptions (e.g., low noise), where nnn is the number of random samples. To the best of our knowledge, this is the first stochastic algorithm for AUC maximization with a statistical convergence rate as fast as O(1/n)O(1/n)O(1/n) up to a logarithmic factor. Extensive experiments on eight large-scale benchmark data sets demonstrate the superior performance of the proposed algorithm comparing with existing stochastic or online algorithms for AUC maximization.",http://proceedings.mlr.press/v80/liu18g.html,http://proceedings.mlr.press/v80/liu18g/liu18g.pdf,ICML
1317,2018,Generative Temporal Models with Spatial Memory for Partially Observed Environments,"Marco Fraccaro,         Danilo Rezende,         Yori Zwols,         Alexander Pritzel,         S. M. Ali Eslami,         Fabio Viola","In model-based reinforcement learning, generative and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent’s representations during training or via use as part of an explicit planning mechanism. However, their application in practice has been limited to simplistic environments, due to the difficulty of training such models in larger, potentially partially-observed and 3D environments. In this work we introduce a novel action-conditioned generative model of such challenging environments. The model features a non-parametric spatial memory system in which we store learned, disentangled representations of the environment. Low-dimensional spatial updates are computed using a state-space model that makes use of knowledge on the prior dynamics of the moving agent, and high-dimensional visual observations are modelled with a Variational Auto-Encoder. The result is a scalable architecture capable of performing coherent predictions over hundreds of time steps across a range of partially observed 2D and 3D environments.",http://proceedings.mlr.press/v80/fraccaro18a.html,http://proceedings.mlr.press/v80/fraccaro18a/fraccaro18a.pdf,ICML
1318,2018,Inductive Two-Layer Modeling with Parametric Bregman Transfer,"Vignesh Ganapathiraman,         Zhan Shi,         Xinhua Zhang,         Yaoliang Yu","Latent prediction models, exemplified by multi-layer networks, employ hidden variables that automate abstract feature discovery. They typically pose nonconvex optimization problems and effective semi-definite programming (SDP) relaxations have been developed to enable global solutions (Aslan et al., 2014).However, these models rely on nonparametric training of layer-wise kernel representations, and are therefore restricted to transductive learning which slows down test prediction. In this paper, we develop a new inductive learning framework for parametric transfer functions using matching losses. The result for ReLU utilizes completely positive matrices, and the inductive learner not only delivers superior accuracy but also offers an order of magnitude speedup over SDP with constant approximation guarantees.",http://proceedings.mlr.press/v80/ganapathiraman18a.html,http://proceedings.mlr.press/v80/ganapathiraman18a/ganapathiraman18a.pdf,ICML
1319,2018,Rates of Convergence of Spectral Methods for Graphon Estimation,Jiaming Xu,"This paper studies the problem of estimating the graphon function – a generative mechanism for a class of random graphs that are useful approximations to real networks. Specifically, a graph of nnn vertices is generated such that each pair of two vertices iii and jjj are connected independently with probability ρn×f(xi,xj)ρn×f(xi,xj)\rho_n \times f(x_i,x_j), where xixix_i is the unknown ddd-dimensional label of vertex iii, fff is an unknown symmetric function, and ρnρn\rho_n, assumed to be Ω(logn/n)Ω(log⁡n/n)\Omega(\log n/n), is a scaling parameter characterizing the graph sparsity. The task is to estimate graphon fff given the graph. Recent studies have identified the minimax optimal estimation error rate for d=1d=1d=1. However, there exists a wide gap between the known error rates of polynomial-time estimators and the minimax optimal error rate. We improve on the previously known error rates of polynomial-time estimators, by analyzing a spectral method, namely universal singular value thresholding (USVT) algorithm. When fff belongs to either Hölder or Sobolev space with smoothness index αα\alpha, we show the error rates of USVT are at most (nρ)−2α/(2α+d)(nρ)−2α/(2α+d)(n\rho)^{ -2 \alpha / (2\alpha+d)}. These error rates approach the minimax optimal error rate log(nρ)/(nρ)log⁡(nρ)/(nρ)\log (n\rho)/(n\rho) proved in prior work for d=1d=1d=1, as αα\alpha increases, i.e., fff becomes smoother. Furthermore, when fff is analytic with infinitely many times differentiability, we show the error rate of USVT is at most logd(nρ)/(nρ)logd⁡(nρ)/(nρ)\log^d (n\rho)/(n\rho). When fff is a step function which corresponds to the stochastic block model with kkk blocks for some kkk, the error rate of USVT is at most k/(nρ)k/(nρ)k/(n\rho), which is larger than the minimax optimal error rate by at most a multiplicative factor k/logkk/log⁡kk/\log k. This coincides with the computational gap observed in community detection. A key ingredient of our analysis is to derive the eigenvalue decaying rate of the edge probability matrix using piecewise polynomial approximations of the graphon function fff.",http://proceedings.mlr.press/v80/xu18a.html,http://proceedings.mlr.press/v80/xu18a/xu18a.pdf,ICML
1320,2018,Covariate Adjusted Precision Matrix Estimation via Nonconvex Optimization,"Jinghui Chen,         Pan Xu,         Lingxiao Wang,         Jian Ma,         Quanquan Gu","We propose a nonconvex estimator for the covariate adjusted precision matrix estimation problem in the high dimensional regime, under sparsity constraints. To solve this estimator, we propose an alternating gradient descent algorithm with hard thresholding. Compared with existing methods along this line of research, which lack theoretical guarantees in optimization error and/or statistical error, the proposed algorithm not only is computationally much more efficient with a linear rate of convergence, but also attains the optimal statistical rate up to a logarithmic factor. Thorough experiments on both synthetic and real data support our theory.",http://proceedings.mlr.press/v80/chen18n.html,http://proceedings.mlr.press/v80/chen18n/chen18n.pdf,ICML
1321,2018,Fair and Diverse DPP-Based Data Summarization,"Elisa Celis,         Vijay Keswani,         Damian Straszak,         Amit Deshpande,         Tarun Kathuria,         Nisheeth Vishnoi","Sampling methods that choose a subset of the data proportional to its diversity in the feature space are popular for data summarization. However, recent studies have noted the occurrence of bias {–} e.g., under or over representation of a particular gender or ethnicity {–} in such data summarization methods. In this paper we initiate a study of the problem of outputting a diverse and fair summary of a given dataset. We work with a well-studied determinantal measure of diversity and corresponding distributions (DPPs) and present a framework that allows us to incorporate a general class of fairness constraints into such distributions. Designing efficient algorithms to sample from these constrained determinantal distributions, however, suffers from a complexity barrier; we present a fast sampler that is provably good when the input vectors satisfy a natural property. Our empirical results on both real-world and synthetic datasets show that the diversity of the samples produced by adding fairness constraints is not too far from the unconstrained case.",http://proceedings.mlr.press/v80/celis18a.html,http://proceedings.mlr.press/v80/celis18a/celis18a.pdf,ICML
1322,2018,Configurable Markov Decision Processes,"Alberto Maria Metelli,         Mirco Mutti,         Marcello Restelli","In many real-world problems, there is the possibility to configure, to a limited extent, some environmental parameters to improve the performance of a learning agent. In this paper, we propose a novel framework, Configurable Markov Decision Processes (Conf-MDPs), to model this new type of interaction with the environment. Furthermore, we provide a new learning algorithm, Safe Policy-Model Iteration (SPMI), to jointly and adaptively optimize the policy and the environment configuration. After having introduced our approach and derived some theoretical results, we present the experimental evaluation in two explicative problems to show the benefits of the environment configurability on the performance of the learned policy.",http://proceedings.mlr.press/v80/metelli18a.html,http://proceedings.mlr.press/v80/metelli18a/metelli18a.pdf,ICML
1323,2018,Analysis of Minimax Error Rate for Crowdsourcing and Its Application to Worker Clustering Model,"Hideaki Imamura,         Issei Sato,         Masashi Sugiyama","While crowdsourcing has become an important means to label data, there is great interest in estimating the ground truth from unreliable labels produced by crowdworkers. The Dawid and Skene (DS) model is one of the most well-known models in the study of crowdsourcing. Despite its practical popularity, theoretical error analysis for the DS model has been conducted only under restrictive assumptions on class priors, confusion matrices, or the number of labels each worker provides. In this paper, we derive a minimax error rate under more practical setting for a broader class of crowdsourcing models including the DS model as a special case. We further propose the worker clustering model, which is more practical than the DS model under real crowdsourcing settings. The wide applicability of our theoretical analysis allows us to immediately investigate the behavior of this proposed model, which can not be analyzed by existing studies. Experimental results showed that there is a strong similarity between the lower bound of the minimax error rate derived by our theoretical analysis and the empirical error of the estimated value.",http://proceedings.mlr.press/v80/imamura18a.html,http://proceedings.mlr.press/v80/imamura18a/imamura18a.pdf,ICML
1324,2018,Learning the Reward Function for a Misspecified Model,Erik Talvitie,"In model-based reinforcement learning it is typical to decouple the problems of learning the dynamics model and learning the reward function. However, when the dynamics model is flawed, it may generate erroneous states that would never occur in the true environment. It is not clear a priori what value the reward function should assign to such states. This paper presents a novel error bound that accounts for the reward model’s behavior in states sampled from the model. This bound is used to extend the existing Hallucinated DAgger-MC algorithm, which offers theoretical performance guarantees in deterministic MDPs that do not assume a perfect model can be learned. Empirically, this approach to reward learning can yield dramatic improvements in control performance when the dynamics model is flawed.",http://proceedings.mlr.press/v80/talvitie18a.html,http://proceedings.mlr.press/v80/talvitie18a/talvitie18a.pdf,ICML
1325,2018,On Nesting Monte Carlo Estimators,"Tom Rainforth,         Rob Cornish,         Hongseok Yang,         Andrew Warrington,         Frank Wood","Many problems in machine learning and statistics involve nested expectations and thus do not permit conventional Monte Carlo (MC) estimation. For such problems, one must nest estimators, such that terms in an outer estimator themselves involve calculation of a separate, nested, estimation. We investigate the statistical implications of nesting MC estimators, including cases of multiple levels of nesting, and establish the conditions under which they converge. We derive corresponding rates of convergence and provide empirical evidence that these rates are observed in practice. We further establish a number of pitfalls that can arise from naive nesting of MC estimators, provide guidelines about how these can be avoided, and lay out novel methods for reformulating certain classes of nested expectation problems into single expectations, leading to improved convergence rates. We demonstrate the applicability of our work by using our results to develop a new estimator for discrete Bayesian experimental design problems and derive error bounds for a class of variational objectives.",http://proceedings.mlr.press/v80/rainforth18a.html,http://proceedings.mlr.press/v80/rainforth18a/rainforth18a.pdf,ICML
1326,2018,Hierarchical Long-term Video Prediction without Supervision,"Nevan wichers,         Ruben Villegas,         Dumitru Erhan,         Honglak Lee","Much of recent research has been devoted to video prediction and generation, yet most of the previous works have demonstrated only limited success in generating videos on short-term horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state-of-the-art method for long-term video prediction, but their method is limited because it requires ground truth annotation of high-level structures (e.g., human joint landmarks) at training time. Our network encodes the input frame, predicts a high-level encoding into the future, and then a decoder with access to the first frame produces the predicted image from the predicted encoding. The decoder also produces a mask that outlines the predicted foreground object (e.g., person) as a by-product. Unlike Villegas et al. (2017), we develop a novel training method that jointly trains the encoder, the predictor, and the decoder together without highlevel supervision; we further improve upon this by using an adversarial loss in the feature space to train the predictor. Our method can predict about 20 seconds into the future and provides better results compared to Denton and Fergus (2018) and Finn et al. (2016) on the Human 3.6M dataset.",http://proceedings.mlr.press/v80/wichers18a.html,http://proceedings.mlr.press/v80/wichers18a/wichers18a.pdf,ICML
1327,2018,Theoretical Analysis of Image-to-Image Translation with Adversarial Learning,"Xudong Pan,         Mi Zhang,         Daizong Ding","Recently, a unified model for image-to-image translation tasks within adversarial learning framework has aroused widespread research interests in computer vision practitioners. Their reported empirical success however lacks solid theoretical interpretations for its inherent mechanism. In this paper, we reformulate their model from a brand-new geometrical perspective and have eventually reached a full interpretation on some interesting but unclear empirical phenomenons from their experiments. Furthermore, by extending the definition of generalization for generative adversarial nets to a broader sense, we have derived a condition to control the generalization capability of their model. According to our derived condition, several practical suggestions have also been proposed on model design and dataset construction as a guidance for further empirical researches.",http://proceedings.mlr.press/v80/pan18c.html,http://proceedings.mlr.press/v80/pan18c/pan18c.pdf,ICML
1328,2018,Adafactor: Adaptive Learning Rates with Sublinear Memory Cost,"Noam Shazeer,         Mitchell Stern","In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves.",http://proceedings.mlr.press/v80/shazeer18a.html,http://proceedings.mlr.press/v80/shazeer18a/shazeer18a.pdf,ICML
1329,2018,Stochastic Training of Graph Convolutional Networks with Variance Reduction,"Jianfei Chen,         Jun Zhu,         Le Song","Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially with the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop control variate based algorithms with new theoretical guarantee to converge to a local optimum of GCN regardless of the neighbor sampling size. Empirical results show that our algorithms enjoy similar convergence rate and model quality with the exact algorithm using only two neighbors per node. The running time of our algorithms on a large Reddit dataset is only one seventh of previous neighbor sampling algorithms.",http://proceedings.mlr.press/v80/chen18p.html,http://proceedings.mlr.press/v80/chen18p/chen18p.pdf,ICML
1330,2018,Lyapunov Functions for First-Order Methods: Tight Automated Convergence Guarantees,"Adrien Taylor,         Bryan Van Scoy,         Laurent Lessard","We present a novel way of generating Lyapunov functions for proving linear convergence rates of first-order optimization methods. Our approach provably obtains the fastest linear convergence rate that can be verified by a quadratic Lyapunov function (with given states), and only relies on solving a small-sized semidefinite program. Our approach combines the advantages of performance estimation problems (PEP, due to Drori and Teboulle (2014)) and integral quadratic constraints (IQC, due to Lessard et al. (2016)), and relies on convex interpolation (due to Taylor et al. (2017c;b)).",http://proceedings.mlr.press/v80/taylor18a.html,http://proceedings.mlr.press/v80/taylor18a/taylor18a.pdf,ICML
1331,2018,Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness,"Michael Kearns,         Seth Neel,         Aaron Roth,         Zhiwei Steven Wu","The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.",http://proceedings.mlr.press/v80/kearns18a.html,http://proceedings.mlr.press/v80/kearns18a/kearns18a.pdf,ICML
1332,2018,Learning to Explore via Meta-Policy Gradient,"Tianbing Xu,         Qiang Liu,         Liang Zhao,         Jian Peng","The performance of off-policy learning, including deep Q-learning and deep deterministic policy gradient (DDPG), critically depends on the choice of the exploration policy. Existing exploration methods are mostly based on adding noise to the on-going actor policy and can only explore local regions close to what the actor policy dictates. In this work, we develop a simple meta-policy gradient algorithm that allows us to adaptively learn the exploration policy in DDPG. Our algorithm allows us to train flexible exploration behaviors that are independent of the actor policy, yielding a global exploration that significantly speeds up the learning process. With an extensive study, we show that our method significantly improves the sample-efficiency of DDPG on a variety of reinforcement learning continuous control tasks.",http://proceedings.mlr.press/v80/xu18d.html,http://proceedings.mlr.press/v80/xu18d/xu18d.pdf,ICML
1333,2018,Optimization Landscape and Expressivity of Deep CNNs,"Quynh Nguyen,         Matthias Hein","We analyze the loss landscape and expressiveness of practical deep convolutional neural networks (CNNs) with shared weights and max pooling layers. We show that such CNNs produce linearly independent features at a “wide” layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer we show that almost every critical point of the empirical loss is a global minimum with zero training error. Our analysis suggests that both depth and width are very important in deep learning. While depth brings more representational power and allows the network to learn high level features, width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide network has a well-behaved loss surface with almost no bad local minima.",http://proceedings.mlr.press/v80/nguyen18a.html,http://proceedings.mlr.press/v80/nguyen18a/nguyen18a.pdf,ICML
1334,2018,A Reductions Approach to Fair Classification,"Alekh Agarwal,         Alina Beygelzimer,         Miroslav Dudik,         John Langford,         Hanna Wallach","We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.",http://proceedings.mlr.press/v80/agarwal18a.html,http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf,ICML
1335,2018,Bayesian Quadrature for Multiple Related Integrals,"Xiaoyue Xi,         Francois-Xavier Briol,         Mark Girolami","Bayesian probabilistic numerical methods are a set of tools providing posterior distributions on the output of numerical methods. The use of these methods is usually motivated by the fact that they can represent our uncertainty due to incomplete/finite information about the continuous mathematical problem being approximated. In this paper, we demonstrate that this paradigm can provide additional advantages, such as the possibility of transferring information between several numerical methods. This allows users to represent uncertainty in a more faithful manner and, as a by-product, provide increased numerical efficiency. We propose the first such numerical method by extending the well-known Bayesian quadrature algorithm to the case where we are interested in computing the integral of several related functions. We then prove convergence rates for the method in the well-specified and misspecified cases, and demonstrate its efficiency in the context of multi-fidelity models for complex engineering systems and a problem of global illumination in computer graphics.",http://proceedings.mlr.press/v80/xi18a.html,http://proceedings.mlr.press/v80/xi18a/xi18a.pdf,ICML
1336,2018,Riemannian Stochastic Recursive Gradient Algorithm,"Hiroyuki Kasai,         Hiroyuki Sato,         Bamdev Mishra","Stochastic variance reduction algorithms have recently become popular for minimizing the average of a large, but finite number of loss functions on a Riemannian manifold. The present paper proposes a Riemannian stochastic recursive gradient algorithm (R-SRG), which does not require the inverse of retraction between two distant iterates on the manifold. Convergence analyses of R-SRG are performed on both retraction-convex and non-convex functions under computationally efficient retraction and vector transport operations. The key challenge is analysis of the influence of vector transport along the retraction curve. Numerical evaluations reveal that R-SRG competes well with state-of-the-art Riemannian batch and stochastic gradient algorithms.",http://proceedings.mlr.press/v80/kasai18a.html,http://proceedings.mlr.press/v80/kasai18a/kasai18a.pdf,ICML
1337,2018,Theoretical Analysis of Sparse Subspace Clustering with Missing Entries,"Manolis Tsakiris,         Rene Vidal","Sparse Subspace Clustering (SSC) is a popular unsupervised machine learning method for clustering data lying close to an unknown union of low-dimensional linear subspaces; a problem with numerous applications in pattern recognition and computer vision. Even though the behavior of SSC for complete data is by now well-understood, little is known about its theoretical properties when applied to data with missing entries. In this paper we give theoretical guarantees for SSC with incomplete data, and provide theoretical evidence that projecting the zero-filled data onto the observation pattern of the point being expressed can lead to substantial improvement in performance; a phenomenon already known experimentally. The main insight of our analysis is that even though this projection induces additional missing entries, this is counterbalanced by the fact that the projected and zero-filled data are in effect incomplete points associated with the union of the corresponding projected subspaces, with respect to which the point being expressed is complete. The significance of this phenomenon potentially extends to the entire class of self-expressive methods.",http://proceedings.mlr.press/v80/tsakiris18a.html,http://proceedings.mlr.press/v80/tsakiris18a/tsakiris18a.pdf,ICML
1338,2018,NetGAN: Generating Graphs via Random Walks,"Aleksandar Bojchevski,         Oleksandr Shchur,         Daniel Zügner,         Stephan Günnemann","We propose NetGAN - the first implicit generative model for graphs able to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over the input graph. The proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the Wasserstein GAN objective. NetGAN is able to produce graphs that exhibit well-known network patterns without explicitly specifying them in the model definition. At the same time, our model exhibits strong generalization properties, as highlighted by its competitive link prediction performance, despite not being trained specifically for this task. Being the first approach to combine both of these desirable properties, NetGAN opens exciting avenues for further research.",http://proceedings.mlr.press/v80/bojchevski18a.html,http://proceedings.mlr.press/v80/bojchevski18a/bojchevski18a.pdf,ICML
1339,2018,Weightless: Lossy weight encoding for deep neural network compression,"Brandon Reagan,         Udit Gupta,         Bob Adolf,         Michael Mitzenmacher,         Alexander Rush,         Gu-Yeon Wei,         David Brooks","The large memory requirements of deep neural networks limit their deployment and adoption on many devices. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding co-designed with weight simplification techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, named Weightless, can compress weights by up to 496x without loss of model accuracy. This results in up to a 1.51x improvement over the state-of-the-art.",http://proceedings.mlr.press/v80/reagan18a.html,http://proceedings.mlr.press/v80/reagan18a/reagan18a.pdf,ICML
1340,2018,Augment and Reduce: Stochastic Inference for Large Categorical Distributions,"Francisco Ruiz,         Michalis Titsias,         Adji Bousso Dieng,         David Blei","Categorical distributions are ubiquitous in machine learning, e.g., in classification, language models, and recommendation systems. However, when the number of possible outcomes is very large, using categorical distributions becomes computationally expensive, as the complexity scales linearly with the number of outcomes. To address this problem, we propose augment and reduce (A&R), a method to alleviate the computational complexity. A&R uses two ideas: latent variable augmentation and stochastic variational inference. It maximizes a lower bound on the marginal likelihood of the data. Unlike existing methods which are specific to softmax, A&R is more general and is amenable to other categorical models, such as multinomial probit. On several large-scale classification problems, we show that A&R provides a tighter bound on the marginal likelihood and has better predictive performance than existing approaches.",http://proceedings.mlr.press/v80/ruiz18a.html,http://proceedings.mlr.press/v80/ruiz18a/ruiz18a.pdf,ICML
1341,2018,On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization,"Sanjeev Arora,         Nadav Cohen,         Elad Hazan","Conventional wisdom in deep learning states that increasing depth improves expressiveness but complicates optimization. This paper suggests that, sometimes, increasing depth can speed up optimization. The effect of depth on optimization is decoupled from expressiveness by focusing on settings where additional layers amount to overparameterization – linear neural networks, a well-studied model. Theoretical analysis, as well as experiments, show that here depth acts as a preconditioner which may accelerate convergence. Even on simple convex problems such as linear regression with ℓpℓp\ell_p loss, p>2p>2p>2, gradient descent can benefit from transitioning to a non-convex overparameterized objective, more than it would from some common acceleration schemes. We also prove that it is mathematically impossible to obtain the acceleration effect of overparametrization via gradients of any regularizer.",http://proceedings.mlr.press/v80/arora18a.html,http://proceedings.mlr.press/v80/arora18a/arora18a.pdf,ICML
1342,2018,Structured Variational Learning of Bayesian Neural Networks with Horseshoe Priors,"Soumya Ghosh,         Jiayu Yao,         Finale Doshi-Velez","Bayesian Neural Networks (BNNs) have recently received increasing attention for their ability to provide well-calibrated posterior uncertainties. However, model selection—even choosing the number of nodes—remains an open question. Recent work has proposed the use of a horseshoe prior over node pre-activations of a Bayesian neural network, which effectively turns off nodes that do not help explain the data. In this work, we propose several modeling and inference advances that consistently improve the compactness of the model learned while maintaining predictive performance, especially in smaller-sample settings including reinforcement learning.",http://proceedings.mlr.press/v80/ghosh18a.html,http://proceedings.mlr.press/v80/ghosh18a/ghosh18a.pdf,ICML
1343,2018,AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian Optimization with Structured Kernel Learning,"Ahmed Alaa,         Mihaela Schaar","Clinical prognostic models derived from largescale healthcare data can inform critical diagnostic and therapeutic decisions. To enable off-theshelf usage of machine learning (ML) in prognostic research, we developed AUTOPROGNOSIS: a system for automating the design of predictive modeling pipelines tailored for clinical prognosis. AUTOPROGNOSIS optimizes ensembles of pipeline configurations efficiently using a novel batched Bayesian optimization (BO) algorithm that learns a low-dimensional decomposition of the pipelines’ high-dimensional hyperparameter space in concurrence with the BO procedure. This is achieved by modeling the pipelines’ performances as a black-box function with a Gaussian process prior, and modeling the “similarities” between the pipelines’ baseline algorithms via a sparse additive kernel with a Dirichlet prior. Meta-learning is used to warmstart BO with external data from “similar” patient cohorts by calibrating the priors using an algorithm that mimics the empirical Bayes method. The system automatically explains its predictions by presenting the clinicians with logical association rules that link patients’ features to predicted risk strata. We demonstrate the utility of AUTOPROGNOSIS using 10 major patient cohorts representing various aspects of cardiovascular patient care.",http://proceedings.mlr.press/v80/alaa18b.html,http://proceedings.mlr.press/v80/alaa18b/alaa18b.pdf,ICML
1344,2018,A Robust Approach to Sequential Information Theoretic Planning,"Sue Zheng,         Jason Pacheco,         John Fisher","In many sequential planning applications a natural approach to generating high quality plans is to maximize an information reward such as mutual information (MI). Unfortunately, MI lacks a closed form in all but trivial models, and so must be estimated. In applications where the cost of plan execution is expensive, one desires planning estimates which admit theoretical guarantees. Through the use of robust M-estimators we obtain bounds on absolute deviation of estimated MI. Moreover, we propose a sequential algorithm which integrates inference and planning by maximally reusing particles in each stage. We validate the utility of using robust estimators in the sequential approach on a Gaussian Markov Random Field wherein information measures have a closed form. Lastly, we demonstrate the benefits of our integrated approach in the context of sequential experiment design for inferring causal regulatory networks from gene expression levels. Our method shows improvements over a recent method which selects intervention experiments based on the same MI objective.",http://proceedings.mlr.press/v80/zheng18b.html,http://proceedings.mlr.press/v80/zheng18b/zheng18b.pdf,ICML
1345,2018,A Primal-Dual Analysis of Global Optimality in Nonconvex Low-Rank Matrix Recovery,"Xiao Zhang,         Lingxiao Wang,         Yaodong Yu,         Quanquan Gu","We propose a primal-dual based framework for analyzing the global optimality of nonconvex low-rank matrix recovery. Our analysis are based on the restricted strongly convex and smooth conditions, which can be verified for a broad family of loss functions. In addition, our analytic framework can directly handle the widely-used incoherence constraints through the lens of duality. We illustrate the applicability of the proposed framework to matrix completion and one-bit matrix completion, and prove that all these problems have no spurious local minima. Our results not only improve the sample complexity required for characterizing the global optimality of matrix completion, but also resolve an open problem in Ge et al. (2017) regarding one-bit matrix completion. Numerical experiments show that primal-dual based algorithm can successfully recover the global optimum for various low-rank problems.",http://proceedings.mlr.press/v80/zhang18m.html,http://proceedings.mlr.press/v80/zhang18m/zhang18m.pdf,ICML
1346,2018,Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection,"Jeremias Knoblauch,         Theodoros Damoulas","Bayesian On-line Changepoint Detection is extended to on-line model selection and non-stationary spatio-temporal processes. We propose spatially structured Vector Autoregressions (VARs) for modelling the process between changepoints (CPs) and give an upper bound on the approximation error of such models. The resulting algorithm performs prediction, model selection and CP detection on-line. Its time complexity is linear and its space complexity constant, and thus it is two orders of magnitudes faster than its closest competitor. In addition, it outperforms the state of the art for multivariate data.",http://proceedings.mlr.press/v80/knoblauch18a.html,http://proceedings.mlr.press/v80/knoblauch18a/knoblauch18a.pdf,ICML
1347,2018,Hyperbolic Entailment Cones for Learning Hierarchical Embeddings,"Octavian Ganea,         Gary Becigneul,         Thomas Hofmann","Learning graph representations via low-dimensional embeddings that preserve relevant network properties is an important class of problems in machine learning. We here present a novel method to embed directed acyclic graphs. Following prior work, we first advocate for using hyperbolic spaces which provably model tree-like structures better than Euclidean geometry. Second, we view hierarchical relations as partial orders defined using a family of nested geodesically convex cones. We prove that these entailment cones admit an optimal shape with a closed form expression both in the Euclidean and hyperbolic spaces, and they canonically define the embedding learning process. Experiments show significant improvements of our method over strong recent baselines both in terms of representational capacity and generalization.",http://proceedings.mlr.press/v80/ganea18a.html,http://proceedings.mlr.press/v80/ganea18a/ganea18a.pdf,ICML
1348,2018,Parameterized Algorithms for the Matrix Completion Problem,"Robert Ganian,         Iyad Kanj,         Sebastian Ordyniak,         Stefan Szeider","We consider two matrix completion problems, in which we are given a matrix with missing entries and the task is to complete the matrix in a way that (1) minimizes the rank, or (2) minimizes the number of distinct rows. We study the parameterized complexity of the two aforementioned problems with respect to several parameters of interest, including the minimum number of matrix rows, columns, and rows plus columns needed to cover all missing entries. We obtain new algorithmic results showing that, for the bounded domain case, both problems are fixed-parameter tractable with respect to all aforementioned parameters. We complement these results with a lower-bound result for the unbounded domain case that rules out fixed-parameter tractability w.r.t. some of the parameters under consideration.",http://proceedings.mlr.press/v80/ganian18a.html,http://proceedings.mlr.press/v80/ganian18a/ganian18a.pdf,ICML
1349,2018,Towards Binary-Valued Gates for Robust LSTM Training,"Zhuohan Li,         Di He,         Fei Tian,         Wei Chen,         Tao Qin,         Liwei Wang,         Tieyan Liu","Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. It aims to use gates to control information flow (e.g., whether to skip some information or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal. In this paper, we propose a new way for LSTM training, which pushes the output values of the gates towards 0 or 1. By doing so, we can better control the information flow: the gates are mostly open or closed, instead of in a middle state, which makes the results more interpretable. Empirical studies show that (1) Although it seems that we restrict the model capacity, there is no performance drop: we achieve better or comparable performances due to its better generalization ability; (2) The outputs of gates are not sensitive to their inputs: we can easily compress the LSTM unit in multiple ways, e.g., low-rank approximation and low-precision approximation. The compressed models are even better than the baseline models without compression.",http://proceedings.mlr.press/v80/li18c.html,http://proceedings.mlr.press/v80/li18c/li18c.pdf,ICML
1350,2018,Minibatch Gibbs Sampling on Large Graphical Models,"Chris De Sa,         Vincent Chen,         Wing Wong","Gibbs sampling is the de facto Markov chain Monte Carlo method used for inference and learning on large scale graphical models. For complicated factor graphs with lots of factors, the performance of Gibbs sampling can be limited by the computational cost of executing a single update step of the Markov chain. This cost is proportional to the degree of the graph, the number of factors adjacent to each variable. In this paper, we show how this cost can be reduced by using minibatching: subsampling the factors to form an estimate of their sum. We introduce several minibatched variants of Gibbs, show that they can be made unbiased, prove bounds on their convergence rates, and show that under some conditions they can result in asymptotic single-update-run-time speedups over plain Gibbs sampling.",http://proceedings.mlr.press/v80/desa18a.html,http://proceedings.mlr.press/v80/desa18a/desa18a.pdf,ICML
1351,2018,SparseMAP: Differentiable Sparse Structured Inference,"Vlad Niculae,         Andre Martins,         Mathieu Blondel,         Claire Cardie","Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP, a new method for sparse structured inference, together with corresponding loss functions. SparseMAP inference is able to automatically select only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns probability mass to all structures, including implausible ones. Importantly, SparseMAP can be computed using only calls to a MAP oracle, hence it is applicable even to problems where marginal inference is intractable, such as linear assignment. Moreover, thanks to the solution sparsity, gradient backpropagation is efficient regardless of the structure. SparseMAP thus enables us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.",http://proceedings.mlr.press/v80/niculae18a.html,http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf,ICML
1352,2018,Modeling Sparse Deviations for Compressed Sensing using Generative Models,"Manik Dhar,         Aditya Grover,         Stefano Ermon","In compressed sensing, a small number of linear measurements can be used to reconstruct an unknown signal. Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a generative model. A domain-specific generative model can provide a stronger prior and thus allow for recovery with far fewer measurements. However, unlike sparsity-based approaches, existing methods based on generative models guarantee exact recovery only over their support, which is typically only a small subset of the space on which the signals are defined. We propose Sparse-Gen, a framework that allows for sparse deviations from the support set, thereby achieving the best of both worlds by using a domain specific prior and allowing reconstruction over the full space of signals. Theoretically, our framework provides a new class of signals that can be acquired using compressed sensing, reducing classic sparse vector recovery to a special case and avoiding the restrictive support due to a generative model prior. Empirically, we observe consistent improvements in reconstruction accuracy over competing approaches, especially in the more practical setting of transfer compressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain.",http://proceedings.mlr.press/v80/dhar18a.html,http://proceedings.mlr.press/v80/dhar18a/dhar18a.pdf,ICML
1353,2018,Noisin: Unbiased Regularization for Recurrent Neural Networks,"Adji Bousso Dieng,         Rajesh Ranganath,         Jaan Altosaar,         David Blei","Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased–it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.",http://proceedings.mlr.press/v80/dieng18a.html,http://proceedings.mlr.press/v80/dieng18a/dieng18a.pdf,ICML
1354,2018,Differentially Private Matrix Completion Revisited,"Prateek Jain,         Om Dipakbhai Thakkar,         Abhradeep Thakurta","We provide the first provably joint differentially private algorithm with formal utility guarantees for the problem of user-level privacy-preserving collaborative filtering. Our algorithm is based on the Frank-Wolfe method, and it consistently estimates the underlying preference matrix as long as the number of users mmm is ω(n5/4)ω(n5/4)\omega(n^{5/4}), where nnn is the number of items, and each user provides her preference for at least n−−√n\sqrt{n} randomly selected items. Along the way, we provide an optimal differentially private algorithm for singular vector computation, based on the celebrated Oja’s method, that provides significant savings in terms of space and time while operating on sparse matrices. We also empirically evaluate our algorithm on a suite of datasets, and show that it consistently outperforms the state-of-the-art private algorithms.",http://proceedings.mlr.press/v80/jain18b.html,http://proceedings.mlr.press/v80/jain18b/jain18b.pdf,ICML
1355,2018,A Two-Step Computation of the Exact GAN Wasserstein Distance,"Huidong Liu,         Xianfeng GU,         Dimitris Samaras","In this paper, we propose a two-step method to compute the Wasserstein distance in Wasserstein Generative Adversarial Networks (WGANs): 1) The convex part of our objective can be solved by linear programming; 2) The non-convex residual can be approximated by a deep neural network. We theoretically prove that the proposed formulation is equivalent to the discrete Monge-Kantorovich dual formulation. Furthermore, we give the approximation error bound of the Wasserstein distance and the error bound of generalizing the Wasserstein distance from discrete to continuous distributions. Our approach optimizes the exact Wasserstein distance, obviating the need for weight clipping previously used in WGANs. Results on synthetic data show that the our method computes the Wasserstein distance more accurately. Qualitative and quantitative results on MNIST, LSUN and CIFAR-10 datasets show that the proposed method is more efficient than state-of-the-art WGAN methods, and still produces images of comparable quality.",http://proceedings.mlr.press/v80/liu18d.html,http://proceedings.mlr.press/v80/liu18d/liu18d.pdf,ICML
1356,2018,An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks,"Qianxiao Li,         Shuji Hao","Deep learning is formulated as a discrete-time optimal control problem. This allows one to characterize necessary conditions for optimality and develop training algorithms that do not rely on gradients with respect to the trainable parameters. In particular, we introduce the discrete-time method of successive approximations (MSA), which is based on the Pontryagin’s maximum principle, for training neural networks. A rigorous error estimate for the discrete MSA is obtained, which sheds light on its dynamics and the means to stabilize the algorithm. The developed methods are applied to train, in a rather principled way, neural networks with weights that are constrained to take values in a discrete set. We obtain competitive performance and interestingly, very sparse weights in the case of ternary networks, which may be useful in model deployment in low-memory devices.",http://proceedings.mlr.press/v80/li18b.html,http://proceedings.mlr.press/v80/li18b/li18b.pdf,ICML
1357,2018,Stochastic Variance-Reduced Cubic Regularized Newton Methods,"Dongruo Zhou,         Pan Xu,         Quanquan Gu","We propose a stochastic variance-reduced cubic regularized Newton method (SVRC) for non-convex optimization. At the core of our algorithm is a novel semi-stochastic gradient along with a semi-stochastic Hessian, which are specifically designed for cubic regularization method. We show that our algorithm is guaranteed to converge to an (ϵ,ϵ√)(ϵ,ϵ)(\epsilon,\sqrt{\epsilon})-approximate local minimum within O~(n4/5/ϵ3/2)O~(n4/5/ϵ3/2)\tilde{O}(n^{4/5}/\epsilon^{3/2}) second-order oracle calls, which outperforms the state-of-the-art cubic regularization algorithms including subsampled cubic regularization. Our work also sheds light on the application of variance reduction technique to high-order non-convex optimization methods. Thorough experiments on various non-convex optimization problems support our theory.",http://proceedings.mlr.press/v80/zhou18d.html,http://proceedings.mlr.press/v80/zhou18d/zhou18d.pdf,ICML
1358,2018,Detecting and Correcting for Label Shift with Black Box Predictors,"Zachary Lipton,         Yu-Xiang Wang,         Alexander Smola","Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets), cause symptoms (observations), we focus on label shift, where the label marginal p(y) changes but the conditional p(x| y) does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution p(y). BBSE exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. While better predictors give tighter estimates, BBSE works even when predictors are biased, inaccurate, or uncalibrated, so long as their confusion matrices are invertible. We prove BBSE’s consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classifiers. Experiments demonstrate accurate estimates and improved prediction, even on high-dimensional datasets of natural images.",http://proceedings.mlr.press/v80/lipton18a.html,http://proceedings.mlr.press/v80/lipton18a/lipton18a.pdf,ICML
1359,2018,Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation,"Dane Corneil,         Wulfram Gerstner,         Johanni Brea","Modern reinforcement learning algorithms reach super-human performance on many board and video games, but they are sample inefficient, i.e. they typically require significantly more playing experience than humans to reach an equal performance level. To improve sample efficiency, an agent may build a model of the environment and use planning methods to update its policy. In this article we introduce Variational State Tabulation (VaST), which maps an environment with a high-dimensional state space (e.g. the space of visual inputs) to an abstract tabular model. Prioritized sweeping with small backups, a highly efficient planning method, can then be used to update state-action values. We show how VaST can rapidly learn to maximize reward in tasks like 3D navigation and efficiently adapt to sudden changes in rewards or transition probabilities.",http://proceedings.mlr.press/v80/corneil18a.html,http://proceedings.mlr.press/v80/corneil18a/corneil18a.pdf,ICML
1360,2018,Time Limits in Reinforcement Learning,"Fabio Pardo,         Arash Tavakoli,         Vitaly Levdik,         Petar Kormushev","In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state-aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent’s input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.",http://proceedings.mlr.press/v80/pardo18a.html,http://proceedings.mlr.press/v80/pardo18a/pardo18a.pdf,ICML
1361,2018,"High-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach","Tim Pearce,         Alexandra Brintrup,         Mohamed Zaki,         Andy Neely","This paper considers the generation of prediction intervals (PIs) by neural networks for quantifying uncertainty in regression tasks. It is axiomatic that high-quality PIs should be as narrow as possible, whilst capturing a specified portion of data. We derive a loss function directly from this axiom that requires no distributional assumption. We show how its form derives from a likelihood principle, that it can be used with gradient descent, and that model uncertainty is accounted for in ensembled form. Benchmark experiments show the method outperforms current state-of-the-art uncertainty quantification methods, reducing average PI width by over 10%.",http://proceedings.mlr.press/v80/pearce18a.html,http://proceedings.mlr.press/v80/pearce18a/pearce18a.pdf,ICML
1362,2018,Gradient Coding from Cyclic MDS Codes and Expander Graphs,"Netanel Raviv,         Rashish Tandon,         Alex Dimakis,         Itzhak Tamo","Gradient coding is a technique for straggler mitigation in distributed learning. In this paper we design novel gradient codes using tools from classical coding theory, namely, cyclic MDS codes, which compare favourably with existing solutions, both in the applicable range of parameters and in the complexity of the involved algorithms. Second, we introduce an approximate variant of the gradient coding problem, in which we settle for approximate gradient computation instead of the exact one. This approach enables graceful degradation, i.e., the ℓ2ℓ2\ell_2 error of the approximate gradient is a decreasing function of the number of stragglers. Our main result is that the normalized adjacency matrix of an expander graph can yield excellent approximate gradient codes, and that this approach allows us to perform significantly less computation compared to exact gradient coding. We experimentally test our approach on Amazon EC2, and show that the generalization error of approximate gradient coding is very close to the full gradient while requiring significantly less computation from the workers.",http://proceedings.mlr.press/v80/raviv18a.html,http://proceedings.mlr.press/v80/raviv18a/raviv18a.pdf,ICML
1363,2018,A Simple Stochastic Variance Reduced Algorithm with Fast Convergence Rates,"Kaiwen Zhou,         Fanhua Shang,         James Cheng","Recent years have witnessed exciting progress in the study of stochastic variance reduced gradient methods (e.g., SVRG, SAGA), their accelerated variants (e.g, Katyusha) and their extensions in many different settings (e.g., online, sparse, asynchronous, distributed). Among them, accelerated methods enjoy improved convergence rates but have complex coupling structures, which makes them hard to be extended to more settings (e.g., sparse and asynchronous) due to the existence of perturbation. In this paper, we introduce a simple stochastic variance reduced algorithm (MiG), which enjoys the best-known convergence rates for both strongly convex and non-strongly convex problems. Moreover, we also present its efficient sparse and asynchronous variants, and theoretically analyze its convergence rates in these settings. Finally, extensive experiments for various machine learning problems such as logistic regression are given to illustrate the practical improvement in both serial and asynchronous settings.",http://proceedings.mlr.press/v80/zhou18c.html,http://proceedings.mlr.press/v80/zhou18c/zhou18c.pdf,ICML
1364,2018,Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning,"Ronan Fruit,         Matteo Pirotta,         Alessandro Lazaric,         Ronald Ortner","We introduce SCAL, an algorithm designed to perform efficient exploration-exploration in any unknown weakly-communicating Markov Decision Process (MDP) for which an upper bound c on the span of the optimal bias function is known. For an MDP with SSS states, AAA actions and Γ≤SΓ≤S\Gamma \leq S possible next states, we prove a regret bound of O(cΓSAT−−−−−√)O(cΓSAT)O(c\sqrt{\Gamma SAT}), which significantly improves over existing algorithms (e.g., UCRL and PSRL), whose regret scales linearly with the MDP diameter DDD. In fact, the optimal bias span is finite and often much smaller than DDD (e.g., D=+∞D=+∞D=+\infty in non-communicating MDPs). A similar result was originally derived by Bartlett and Tewari (2009) for REGAL.C, for which no tractable algorithm is available. In this paper, we relax the optimization problem at the core of REGAL.C, we carefully analyze its properties, and we provide the first computationally efficient algorithm to solve it. Finally, we report numerical simulations supporting our theoretical findings and showing how SCAL significantly outperforms UCRL in MDPs with large diameter and small span.",http://proceedings.mlr.press/v80/fruit18a.html,http://proceedings.mlr.press/v80/fruit18a/fruit18a.pdf,ICML
1365,2018,Scalable approximate Bayesian inference for particle tracking data,"Ruoxi Sun,         Liam Paninski","Many important datasets in physics, chemistry, and biology consist of noisy sequences of images of multiple moving overlapping particles. In many cases, the observed particles are indistinguishable, leading to unavoidable uncertainty about nearby particles’ identities. Exact Bayesian inference is intractable in this setting, and previous approximate Bayesian methods scale poorly. Non-Bayesian approaches that output a single “best” estimate of the particle tracks (thus discarding important uncertainty information) are therefore dominant in practice. Here we propose a flexible and scalable amortized approach for Bayesian inference on this task. We introduce a novel neural network method to approximate the (intractable) filter-backward-sample-forward algorithm for Bayesian inference in this setting. By varying the simulated training data for the network, we can perform inference on a wide variety of data types. This approach is therefore highly flexible and improves on the state of the art in terms of accuracy; provides uncertainty estimates about the particle locations and identities; and has a test run-time that scales linearly as a function of the data length and number of particles, thus enabling Bayesian inference in arbitrarily large particle tracking datasets.",http://proceedings.mlr.press/v80/sun18b.html,http://proceedings.mlr.press/v80/sun18b/sun18b.pdf,ICML
1366,2018,DRACO: Byzantine-resilient Distributed Training via Redundant Gradients,"Lingjiao Chen,         Hongyi Wang,         Zachary Charles,         Dimitris Papailiopoulos","Distributed model training is vulnerable to byzantine system failures and adversarial compute nodes, i.e., nodes that use malicious updates to corrupt the global model stored at a parameter server (PS). To guarantee some form of robustness, recent work suggests using variants of the geometric median as an aggregation rule, in place of gradient averaging. Unfortunately, median-based rules can incur a prohibitive computational overhead in large-scale settings, and their convergence guarantees often require strong assumptions. In this work, we present DRACO, a scalable framework for robust distributed training that uses ideas from coding theory. In DRACO, each compute node evaluates redundant gradients that are used by the parameter server to eliminate the effects of adversarial updates. DRACO comes with problem-independent robustness guarantees, and the model that it trains is identical to the one trained in the adversary-free setup. We provide extensive experiments on real datasets and distributed setups across a variety of large-scale models, where we show that DRACO is several times, to orders of magnitude faster than median-based approaches.",http://proceedings.mlr.press/v80/chen18l.html,http://proceedings.mlr.press/v80/chen18l/chen18l.pdf,ICML
1367,2018,Neural Inverse Rendering for General Reflectance Photometric Stereo,"Tatsunori Taniai,         Takanori Maehara","We present a novel convolutional neural network architecture for photometric stereo (Woodham, 1980), a problem of recovering 3D object surface normals from multiple images observed under varying illuminations. Despite its long history in computer vision, the problem still shows fundamental challenges for surfaces with unknown general reflectance properties (BRDFs). Leveraging deep neural networks to learn complicated reflectance models is promising, but studies in this direction are very limited due to difficulties in acquiring accurate ground truth for training and also in designing networks invariant to permutation of input images. In order to address these challenges, we propose a physics based unsupervised learning framework where surface normals and BRDFs are predicted by the network and fed into the rendering equation to synthesize observed images. The network weights are optimized during testing by minimizing reconstruction loss between observed and synthesized images. Thus, our learning process does not require ground truth normals or even pre-training on external images. Our method is shown to achieve the state-of-the-art performance on a challenging real-world scene benchmark.",http://proceedings.mlr.press/v80/taniai18a.html,http://proceedings.mlr.press/v80/taniai18a/taniai18a.pdf,ICML
1368,2018,Synthesizing Robust Adversarial Examples,"Anish Athalye,         Logan Engstrom,         Andrew Ilyas,         Kevin Kwok","Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.",http://proceedings.mlr.press/v80/athalye18b.html,http://proceedings.mlr.press/v80/athalye18b/athalye18b.pdf,ICML
1369,2018,TACO: Learning Task Decomposition via Temporal Alignment for Control,"Kyriacos Shiarlis,         Markus Wulfmeier,         Sasha Salter,         Shimon Whiteson,         Ingmar Posner","Many advanced Learning from Demonstration (LfD) methods consider the decomposition of complex, real-world tasks into simpler sub-tasks. By reusing the corresponding sub-policies within and between tasks, we can provide training data for each policy from different high-level tasks and compose them to perform novel ones. Existing approaches to modular LfD focus either on learning a single high-level task or depend on domain knowledge and temporal segmentation. In contrast, we propose a weakly supervised, domain-agnostic approach based on task sketches, which include only the sequence of sub-tasks performed in each demonstration. Our approach simultaneously aligns the sketches with the observed demonstrations and learns the required sub-policies. This improves generalisation in comparison to separate optimisation procedures. We evaluate the approach on multiple domains, including a simulated 3D robot arm control task using purely image-based observations. The results show that our approach performs commensurately with fully supervised approaches, while requiring significantly less annotation effort.",http://proceedings.mlr.press/v80/shiarlis18a.html,http://proceedings.mlr.press/v80/shiarlis18a/shiarlis18a.pdf,ICML
1370,2018,Optimal Rates of Sketched-regularized Algorithms for Least-Squares Regression over Hilbert Spaces,"Junhong Lin,         Volkan Cevher","We investigate regularized algorithms combining with projection for least-squares regression problem over a Hilbert space, covering nonparametric regression over a reproducing kernel Hilbert space. We prove convergence results with respect to variants of norms, under a capacity assumption on the hypothesis space and a regularity condition on the target function. As a result, we obtain optimal rates for regularized algorithms with randomized sketches, provided that the sketch dimension is proportional to the effective dimension up to a logarithmic factor. As a byproduct, we obtain similar results for Nyström regularized algorithms. Our results provide optimal, distribution-dependent rates for sketched/Nyström regularized algorithms, considering both the attainable and non-attainable cases.",http://proceedings.mlr.press/v80/lin18b.html,http://proceedings.mlr.press/v80/lin18b/lin18b.pdf,ICML
1371,2018,Gated Path Planning Networks,"Lisa Lee,         Emilio Parisotto,         Devendra Singh Chaplot,         Eric Xing,         Ruslan Salakhutdinov","Value Iteration Networks (VINs) are effective differentiable path planning modules that can be used by agents to perform navigation while still maintaining end-to-end differentiability of the entire architecture. Despite their effectiveness, they suffer from several disadvantages including training instability, random seed sensitivity, and other optimization problems. In this work, we reframe VINs as recurrent-convolutional networks which demonstrates that VINs couple recurrent convolutions with an unconventional max-pooling activation. From this perspective, we argue that standard gated recurrent update equations could potentially alleviate the optimization issues plaguing VIN. The resulting architecture, which we call the Gated Path Planning Network, is shown to empirically outperform VIN on a variety of metrics such as learning speed, hyperparameter sensitivity, iteration count, and even generalization. Furthermore, we show that this performance gap is consistent across different maze transition types, maze sizes and even show success on a challenging 3D environment, where the planner is only provided with first-person RGB images.",http://proceedings.mlr.press/v80/lee18c.html,http://proceedings.mlr.press/v80/lee18c/lee18c.pdf,ICML
1372,2018,Orthogonality-Promoting Distance Metric Learning: Convex Relaxation and Theoretical Analysis,"Pengtao Xie,         Wei Wu,         Yichen Zhu,         Eric Xing","Distance metric learning (DML), which learns a distance metric from labeled ""similar"" and ""dissimilar"" data pairs, is widely utilized. Recently, several works investigate orthogonality-promoting regularization (OPR), which encourages the projection vectors in DML to be close to being orthogonal, to achieve three effects: (1) high balancedness – achieving comparable performance on both frequent and infrequent classes; (2) high compactness – using a small number of projection vectors to achieve a ""good"" metric; (3) good generalizability – alleviating overfitting to training data. While showing promising results, these approaches suffer three problems. First, they involve solving non-convex optimization problems where achieving the global optimal is NP-hard. Second, it lacks a theoretical understanding why OPR can lead to balancedness. Third, the current generalization error analysis of OPR is not directly on the regularizer. In this paper, we address these three issues by (1) seeking convex relaxations of the original nonconvex problems so that the global optimal is guaranteed to be achievable; (2) providing a formal analysis on OPR’s capability of promoting balancedness; (3) providing a theoretical analysis that directly reveals the relationship between OPR and generalization performance. Experiments on various datasets demonstrate that our convex methods are more effective in promoting balancedness, compactness, and generalization, and are computationally more efficient, compared with the nonconvex methods.",http://proceedings.mlr.press/v80/xie18a.html,http://proceedings.mlr.press/v80/xie18a/xie18a.pdf,ICML
1373,2018,Fast Variance Reduction Method with Stochastic Batch Size,"Xuanqing Liu,         Cho-Jui Hsieh","In this paper we study a family of variance reduction methods with randomized batch size—at each step, the algorithm first randomly chooses the batch size and then selects a batch of samples to conduct a variance-reduced stochastic update. We give the linear converge rate for this framework for composite functions, and show that the optimal strategy to achieve the best converge rate per data access is to always choose batch size equalling to 1, which is equivalent to the SAGA algorithm. However, due to the presence of cache/disk IO effect in computer architecture, number of data access cannot reflect the running time because of 1) random memory access is much slower than sequential access, 2) when data is too big to fit into memory, disk seeking takes even longer time. After taking these into account, choosing batch size equals to 1 is no longer optimal, so we propose a new algorithm called SAGA++ and theoretically show how to calculate the optimal average batch size. Our algorithm outperforms SAGA and other existing batch and stochastic solvers on real datasets. In addition, we also conduct a precise analysis to compare different update rules for variance reduction methods, showing that SAGA++ converges faster than SVRG in theory.",http://proceedings.mlr.press/v80/liu18f.html,http://proceedings.mlr.press/v80/liu18f/liu18f.pdf,ICML
1374,2017,Approximate Newton Methods and Their Local Convergence,"Haishan Ye,         Luo Luo,         Zhihua Zhang","Many machine learning models are reformulated as optimization problems. Thus, it is important to solve a large-scale optimization problem in big data applications. Recently, subsampled Newton methods have emerged to attract much attention for optimization due to their efficiency at each iteration, rectified a weakness in the ordinary Newton method of suffering a high cost in each iteration while commanding a high convergence rate. Other efficient stochastic second order methods are also proposed. However, the convergence properties of these methods are still not well understood. There are also several important gaps between the current convergence theory and performance in real applications. In this paper, we aim to fill these gaps. We propose a unifying framework to analyze local convergence properties of second order methods. Based on this framework, our theoretical analysis matches the performance in real applications.",http://proceedings.mlr.press/v70/ye17a.html,http://proceedings.mlr.press/v70/ye17a/ye17a.pdf,ICML
1375,2017,Gradient Projection Iterative Sketch for Large-Scale Constrained Least-Squares,"Junqi Tang,         Mohammad Golbabaee,         Mike E. Davies","We propose a randomized first order optimization algorithm Gradient Projection Iterative Sketch (GPIS) and an accelerated variant for efficiently solving large scale constrained Least Squares (LS). We provide the first theoretical convergence analysis for both algorithms. An efficient implementation using a tailored line-search scheme is also proposed. We demonstrate our methods’ computational efficiency compared to the classical accelerated gradient method, and the variance-reduced stochastic gradient methods through numerical experiments in various large synthetic/real data sets.",http://proceedings.mlr.press/v70/tang17a.html,http://proceedings.mlr.press/v70/tang17a/tang17a.pdf,ICML
1376,2017,Joint Dimensionality Reduction and Metric Learning: A Geometric Take,"Mehrtash Harandi,         Mathieu Salzmann,         Richard Hartley","To be tractable and robust to data noise, existing metric learning algorithms commonly rely on PCA as a pre-processing step. How can we know, however, that PCA, or any other specific dimensionality reduction technique, is the method of choice for the problem at hand? The answer is simple: We cannot! To address this issue, in this paper, we develop a Riemannian framework to jointly learn a mapping performing dimensionality reduction and a metric in the induced space. Our experiments evidence that, while we directly work on high-dimensional features, our approach yields competitive runtimes with and higher accuracy than state-of-the-art metric learning algorithms.",http://proceedings.mlr.press/v70/harandi17a.html,http://proceedings.mlr.press/v70/harandi17a/harandi17a.pdf,ICML
1377,2017,Large-Scale Evolution of Image Classifiers,"Esteban Real,         Sherry Moore,         Andrew Selle,         Saurabh Saxena,         Yutaka Leon Suematsu,         Jie Tan,         Quoc V. Le,         Alexey Kurakin","Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6\% (95.6\% for ensemble) and 77.0\%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.",http://proceedings.mlr.press/v70/real17a.html,http://proceedings.mlr.press/v70/real17a/real17a.pdf,ICML
1378,2017,Follow the Moving Leader in Deep Learning,"Shuai Zheng,         James T. Kwok","Deep networks are highly nonlinear and difficult to optimize. During training, the parameter iterate may move from one local basin to another, or the data distribution may even change. Inspired by the close connection between stochastic optimization and online learning, we propose a variant of the follow the regularized leader (FTRL) algorithm called follow the moving leader (FTML). Unlike the FTRL family of algorithms, the recent samples are weighted more heavily in each iteration and so FTML can adapt more quickly to changes. We show that FTML enjoys the nice properties of RMSprop and Adam, while avoiding their pitfalls. Experimental results on a number of deep learning models and tasks demonstrate that FTML converges quickly, and outperforms other state-of-the-art optimizers.",http://proceedings.mlr.press/v70/zheng17a.html,http://proceedings.mlr.press/v70/zheng17a/zheng17a.pdf,ICML
1379,2017,MEC: Memory-efficient Convolution for Deep Neural Network,"Minsik Cho,         Daniel Brand","Convolution is a critical component in modern deep neural networks, thus several algorithms for convolution have been developed. Direct convolution is simple but suffers from poor performance. As an alternative, multiple indirect methods have been proposed including im2col-based convolution, FFT-based convolution, or Winograd-based algorithm. However, all these indirect methods have high memory overhead, which creates performance degradation and offers a poor trade-off between performance and memory consumption. In this work, we propose a memory-efficient convolution or MEC with compact lowering, which reduces memory overhead substantially and accelerates convolution process. MEC lowers the input matrix in a simple yet efficient/compact way (i.e., much less memory overhead), and then executes multiple small matrix multiplications in parallel to get convolution completed. Additionally, the reduced memory footprint improves memory sub-system efficiency, improving performance. Our experimental results show that MEC reduces memory consumption significantly with good speedup on both mobile and server platforms, compared with other indirect convolution algorithms.",http://proceedings.mlr.press/v70/cho17a.html,http://proceedings.mlr.press/v70/cho17a/cho17a.pdf,ICML
1380,2017,Modular Multitask Reinforcement Learning with Policy Sketches,"Jacob Andreas,         Dan Klein,         Sergey Levine","We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them—specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor–critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.",http://proceedings.mlr.press/v70/andreas17a.html,http://proceedings.mlr.press/v70/andreas17a/andreas17a.pdf,ICML
1381,2017,Learning Latent Space Models with Angular Constraints,"Pengtao Xie,         Yuntian Deng,         Yi Zhou,         Abhimanu Kumar,         Yaoliang Yu,         James Zou,         Eric P. Xing","The large model capacity of latent space models (LSMs) enables them to achieve great performance on various applications, but meanwhile renders LSMs to be prone to overfitting. Several recent studies investigate a new type of regularization approach, which encourages components in LSMs to be diverse, for the sake of alleviating overfitting. While they have shown promising empirical effectiveness, in theory why larger “diversity” results in less overfitting is still unclear. To bridge this gap, we propose a new diversity-promoting approach that is both theoretically analyzable and empirically effective. Specifically, we use near-orthogonality to characterize “diversity” and impose angular constraints (ACs) on the components of LSMs to promote diversity. A generalization error analysis shows that larger diversity results in smaller estimation error and larger approximation error. An efficient ADMM algorithm is developed to solve the constrained LSM problems. Experiments demonstrate that ACs improve generalization performance of LSMs and outperform other diversity-promoting approaches.",http://proceedings.mlr.press/v70/xie17a.html,http://proceedings.mlr.press/v70/xie17a/xie17a.pdf,ICML
1382,2017,Conditional Accelerated Lazy Stochastic Gradient Descent,"Guanghui Lan,         Sebastian Pokutta,         Yi Zhou,         Daniel Zink","In this work we introduce a conditional accelerated lazy stochastic gradient descent algorithm with optimal number of calls to a stochastic first-order oracle and convergence rate O(1/ϵ2)O(1/ϵ2)O(1/\epsilon^2) improving over the projection-free, Online Frank-Wolfe based stochastic gradient descent of (Hazan and Kale, 2012) with convergence rate O(1/ϵ4)O(1/ϵ4)O(1/\epsilon^4).",http://proceedings.mlr.press/v70/lan17a.html,http://proceedings.mlr.press/v70/lan17a/lan17a.pdf,ICML
1383,2017,Safety-Aware Algorithms for Adversarial Contextual Bandit,"Wen Sun,         Debadeepta Dey,         Ashish Kapoor","In this work we study the safe sequential decision making problem under the setting of adversarial contextual bandits with sequential risk constraints. At each round, nature prepares a context, a cost for each arm, and additionally a risk for each arm. The learner leverages the context to pull an arm and receives the corresponding cost and risk associated with the pulled arm. In addition to minimizing the cumulative cost, for safety purposes, the learner needs to make safe decisions such that the average of the cumulative risk from all pulled arms should not be larger than a pre-defined threshold. To address this problem, we first study online convex programming in the full information setting where in each round the learner receives an adversarial convex loss and a convex constraint. We develop a meta algorithm leveraging online mirror descent for the full information setting and then extend it to contextual bandit with sequential risk constraints setting using expert advice. Our algorithms can achieve near-optimal regret in terms of minimizing the total cost, while successfully maintaining a sub-linear growth of accumulative risk constraint violation. We support our theoretical results by demonstrating our algorithm on a simple simulated robotics reactive control task.",http://proceedings.mlr.press/v70/sun17a.html,http://proceedings.mlr.press/v70/sun17a/sun17a.pdf,ICML
1384,2017,Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use,"Vatsal Sharan,         Gregory Valiant","The popular Alternating Least Squares (ALS) algorithm for tensor decomposition is efficient and easy to implement, but often converges to poor local optima—particularly when the weights of the factors are non-uniform. We propose a modification of the ALS approach that is as efficient as standard ALS, but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor. We demonstrate the significant practical superiority of our approach over traditional ALS for a variety of tasks on synthetic data—including tensor factorization on exact, noisy and over-complete tensors, as well as tensor completion—and for computing word embeddings from a third-order word tri-occurrence tensor.",http://proceedings.mlr.press/v70/sharan17a.html,http://proceedings.mlr.press/v70/sharan17a/sharan17a.pdf,ICML
1385,2017,From Patches to Images: A Nonparametric Generative Model,"Geng Ji,         Michael C. Hughes,         Erik B. Sudderth","We propose a hierarchical generative model that captures the self-similar structure of image regions as well as how this structure is shared across image collections. Our model is based on a novel, variational interpretation of the popular expected patch log-likelihood (EPLL) method as a model for randomly positioned grids of image patches. While previous EPLL methods modeled image patches with finite Gaussian mixtures, we use nonparametric Dirichlet process (DP) mixtures to create models whose complexity grows as additional images are observed. An extension based on the hierarchical DP then captures repetitive and self-similar structure via image-specific variations in cluster frequencies. We derive a structured variational inference algorithm that adaptively creates new patch clusters to more accurately model novel image textures. Our denoising performance on standard benchmarks is superior to EPLL and comparable to the state-of-the-art, and provides novel statistical justifications for common image processing heuristics. We also show accurate image inpainting results.",http://proceedings.mlr.press/v70/ji17a.html,http://proceedings.mlr.press/v70/ji17a/ji17a.pdf,ICML
1386,2017,"Coherence Pursuit: Fast, Simple, and Robust Subspace Recovery","Mostafa Rahmani,         George Atia","This paper presents a remarkably simple, yet powerful, algorithm for robust Principal Component Analysis (PCA). In the proposed approach, an outlier is set apart from an inlier by comparing their coherence with the rest of the data points. As inliers lie on a low dimensional subspace, they are likely to have strong mutual coherence provided there are enough inliers. By contrast, outliers do not typically admit low dimensional structures, wherefore an outlier is unlikely to bear strong resemblance with a large number of data points. The mutual coherences are computed by forming the Gram matrix of normalized data points. Subsequently, the subspace is recovered from the span of a small subset of the data points that exhibit strong coherence with the rest of the data. As coherence pursuit only involves one simple matrix multiplication, it is significantly faster than the state of-the-art robust PCA algorithms. We provide a mathematical analysis of the proposed algorithm under a random model for the distribution of the inliers and outliers. It is shown that the proposed method can recover the correct subspace even if the data is predominantly outliers. To the best of our knowledge, this is the first provable robust PCA algorithm that is simultaneously non-iterative, can tolerate a large number of outliers and is robust to linearly dependent outliers.",http://proceedings.mlr.press/v70/rahmani17a.html,http://proceedings.mlr.press/v70/rahmani17a/rahmani17a.pdf,ICML
1387,2017,Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs,"Rakshit Trivedi,         Hanjun Dai,         Yichen Wang,         Le Song","The availability of large scale event data with time stamps has given rise to dynamically evolving knowledge graphs that contain temporal information for each edge. Reasoning over time in such dynamic knowledge graphs is not yet well understood. To this end, we present Know-Evolve, a novel deep evolutionary knowledge network that learns non-linearly evolving entity representations over time. The occurrence of a fact (edge) is modeled as a multivariate point process whose intensity function is modulated by the score for that fact computed based on the learned entity embeddings. We demonstrate significantly improved performance over various relational learning approaches on two large scale real-world datasets. Further, our method effectively predicts occurrence or recurrence time of a fact which is novel compared to prior reasoning approaches in multi-relational setting.",http://proceedings.mlr.press/v70/trivedi17a.html,http://proceedings.mlr.press/v70/trivedi17a/trivedi17a.pdf,ICML
1388,2017,Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank,"Liang Zhao,         Siyu Liao,         Yanzhi Wang,         Zhe Li,         Jian Tang,         Bo Yuan","Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks. Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy. This paper gives theoretical study on LDR neural networks. First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure. Finally, we propose back-propagation based training algorithm for general LDR neural networks.",http://proceedings.mlr.press/v70/zhao17b.html,http://proceedings.mlr.press/v70/zhao17b/zhao17b.pdf,ICML
1389,2017,Learning to Generate Long-term Future via Hierarchical Prediction,"Ruben Villegas,         Jimei Yang,         Yuliang Zou,         Sungryull Sohn,         Xunyu Lin,         Honglak Lee","We propose a hierarchical approach for making long-term predictions of future frames. To avoid inherent compounding errors in recursive pixel-level prediction, we propose to first estimate high-level structure in the input frames, then predict how that structure evolves in the future, and finally by observing a single frame from the past and the predicted high-level structure, we construct the future frames without having to observe any of the pixel-level predictions. Long-term video prediction is difficult to perform by recurrently observing the predicted frames because the small errors in pixel space exponentially amplify as predictions are made deeper into the future. Our approach prevents pixel-level error propagation from happening by removing the need to observe the predicted frames. Our model is built with a combination of LSTM and analogy based encoder-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, our model is evaluated on the Human3.6M and Penn Action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art.",http://proceedings.mlr.press/v70/villegas17a.html,http://proceedings.mlr.press/v70/villegas17a/villegas17a.pdf,ICML
1390,2017,Learning to Aggregate Ordinal Labels by Maximizing Separating Width,"Guangyong Chen,         Shengyu Zhang,         Di Lin,         Hui Huang,         Pheng Ann Heng","While crowdsourcing has been a cost and time efficient method to label massive samples, one critical issue is quality control, for which the key challenge is to infer the ground truth from noisy or even adversarial data by various users. A large class of crowdsourcing problems, such as those involving age, grade, level, or stage, have an ordinal structure in their labels. Based on a technique of sampling estimated label from the posterior distribution, we define a novel separating width among the labeled observations to characterize the quality of sampled labels, and develop an efficient algorithm to optimize it through solving multiple linear decision boundaries and adjusting prior distributions. Our algorithm is empirically evaluated on several real world datasets, and demonstrates its supremacy over state-of-the-art methods.",http://proceedings.mlr.press/v70/chen17i.html,http://proceedings.mlr.press/v70/chen17i/chen17i.pdf,ICML
1391,2017,Gradient Coding: Avoiding Stragglers in Distributed Learning,"Rashish Tandon,         Qi Lei,         Alexandros G. Dimakis,         Nikos Karampatziakis","We propose a novel coding theoretic framework for mitigating stragglers in distributed learning. We show how carefully replicating data blocks and coding across gradients can provide tolerance to failures and stragglers for synchronous Gradient Descent. We implement our schemes in python (using MPI) to run on Amazon EC2, and show how we compare against baseline approaches in running time and generalization error.",http://proceedings.mlr.press/v70/tandon17a.html,http://proceedings.mlr.press/v70/tandon17a/tandon17a.pdf,ICML
1392,2017,Dance Dance Convolution,"Chris Donahue,         Zachary C. Lipton,         Julian McAuley","Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts. While many step charts are available in standardized packs, players may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograph. Given a raw audio track, the goal is to produce a new step chart. This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select. For the step placement task, we combine recurrent and convolutional neural networks to ingest spectrograms of low-level audio features to predict steps, conditioned on chart difficulty. For step selection, we present a conditional LSTM generative model that substantially outperforms n-gram and fixed-window approaches.",http://proceedings.mlr.press/v70/donahue17a.html,http://proceedings.mlr.press/v70/donahue17a/donahue17a.pdf,ICML
1393,2017,Device Placement Optimization with Reinforcement Learning,"Azalia Mirhoseini,         Hieu Pham,         Quoc V. Le,         Benoit Steiner,         Rasmus Larsen,         Yuefeng Zhou,         Naveen Kumar,         Mohammad Norouzi,         Samy Bengio,         Jeff Dean","The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs. Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions. In this paper, we propose a method which learns to optimize device placement for TensorFlow computational graphs. Key to our method is the use of a sequence-to-sequence model to predict which subsets of operations in a TensorFlow graph should run on which of the available devices. The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model. Our main result is that on Inception-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algo-rithmic methods.",http://proceedings.mlr.press/v70/mirhoseini17a.html,http://proceedings.mlr.press/v70/mirhoseini17a/mirhoseini17a.pdf,ICML
1394,2017,A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI,Justin Domke,"Two popular classes of methods for approximate inference are Markov chain Monte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run for a long enough time, while variational inference tends to give better approximations at shorter time horizons. However, the amount of time needed for MCMC to exceed the performance of variational methods can be quite high, motivating more fine-grained tradeoffs. This paper derives a distribution over variational parameters, designed to minimize a bound on the divergence between the resulting marginal distribution and the target, and gives an example of how to sample from this distribution in a way that interpolates between the behavior of existing methods based on Langevin dynamics and stochastic gradient variational inference (SGVI).",http://proceedings.mlr.press/v70/domke17a.html,http://proceedings.mlr.press/v70/domke17a/domke17a.pdf,ICML
1395,2017,Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees,"Haim Avron,         Michael Kapralov,         Cameron Musco,         Christopher Musco,         Ameya Velingker,         Amir Zandieh","Random Fourier features is one of the most popular techniques for scaling up kernel methods, such as kernel ridge regression. However, despite impressive empirical results, the statistical properties of random Fourier features are still not well understood. In this paper we take steps toward filling this gap. Specifically, we approach random Fourier features from a spectral matrix approximation point of view, give tight bounds on the number of Fourier features required to achieve a spectral approximation, and show how spectral matrix approximation bounds imply statistical guarantees for kernel ridge regression.",http://proceedings.mlr.press/v70/avron17a.html,http://proceedings.mlr.press/v70/avron17a/avron17a.pdf,ICML
1396,2017,An Infinite Hidden Markov Model With Similarity-Biased Transitions,"Colin Reimer Dawson,         Chaofan Huang,         Clayton T. Morrison","We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) which is able to encode prior information that state transitions are more likely between “nearby” states. This is accomplished by defining a similarity function on the state space and scaling transition probabilities by pairwise similarities, thereby inducing correlations among the transition distributions. We present an augmented data representation of the model as a Markov Jump Process in which: (1) some jump attempts fail, and (2) the probability of success is proportional to the similarity between the source and destination states. This augmentation restores conditional conjugacy and admits a simple Gibbs sampler. We evaluate the model and inference method on a speaker diarization task and a “harmonic parsing” task using four-part chorale data, as well as on several synthetic datasets, achieving favorable comparisons to existing models.",http://proceedings.mlr.press/v70/dawson17a.html,http://proceedings.mlr.press/v70/dawson17a/dawson17a.pdf,ICML
1397,2017,Statistical Inference for Incomplete Ranking Data: The Case of Rank-Dependent Coarsening,"Mohsen Ahmadi Fahandar,         Eyke Hüllermeier,         Inés Couso","We consider the problem of statistical inference for ranking data, specifically rank aggregation, under the assumption that samples are incomplete in the sense of not comprising all choice alternatives. In contrast to most existing methods, we explicitly model the process of turning a full ranking into an incomplete one, which we call the coarsening process. To this end, we propose the concept of rank-dependent coarsening, which assumes that incomplete rankings are produced by projecting a full ranking to a random subset of ranks. For a concrete instantiation of our model, in which full rankings are drawn from a Plackett-Luce distribution and observations take the form of pairwise preferences, we study the performance of various rank aggregation methods. In addition to predictive accuracy in the finite sample setting, we address the theoretical question of consistency, by which we mean the ability to recover a target ranking when the sample size goes to infinity, despite a potential bias in the observations caused by the (unknown) coarsening.",http://proceedings.mlr.press/v70/fahandar17a.html,http://proceedings.mlr.press/v70/fahandar17a/fahandar17a.pdf,ICML
1398,2017,Coresets for Vector Summarization with Applications to Network Graphs,"Dan Feldman,         Sedat Ozer,         Daniela Rus","We provide a deterministic data summarization algorithm that approximates the mean ˉp=1n∑p∈Pp\bar{p}=\frac{1}{n}\sum_{p\in P} p of a set PP of nn vectors in Rd\mathbb{R}^d, by a weighted mean ˜p\tilde{p} of a subset of O(1/ϵ)O(1/\epsilon) vectors, i.e., independent of both nn and dd. We prove that the squared Euclidean distance between ˉp\bar{p} and ˜p\tilde{p} is at most ϵ\epsilon multiplied by the variance of PP. We use this algorithm to maintain an approximated sum of vectors from an unbounded stream, using memory that is independent of dd, and logarithmic in the nn vectors seen so far. Our main application is to extract and represent in a compact way friend groups and activity summaries of users from underlying data exchanges. For example, in the case of mobile networks, we can use GPS traces to identify meetings; in the case of social networks, we can use information exchange to identify friend groups. Our algorithm provably identifies the Heavy Hitter entries in a proximity (adjacency) matrix. The Heavy Hitters can be used to extract and represent in a compact way friend groups and activity summaries of users from underlying data exchanges. We evaluate the algorithm on several large data sets.",http://proceedings.mlr.press/v70/feldman17a.html,http://proceedings.mlr.press/v70/feldman17a/feldman17a.pdf,ICML
1399,2017,Stochastic Convex Optimization: Faster Local Growth Implies Faster Global Convergence,"Yi Xu,         Qihang Lin,         Tianbao Yang","In this paper, a new theory is developed for first-order stochastic convex optimization, showing that the global convergence rate is sufficiently quantified by a local growth rate of the objective function in a neighborhood of the optimal solutions. In particular, if the objective function F(w)F(w)F(\mathbf{w}) in the ϵϵ\epsilon-sublevel set grows as fast as ‖w−w∗‖1/θ2∥w−w∗∥1/θ2\|\mathbf{w} - \mathbf{w}_*\|_2^{1/\theta}, where w∗w∗\mathbf{w}_* represents the closest optimal solution to ww\mathbf{w} and θ∈(0,1]θ∈(0,1]\theta\in(0,1] quantifies the local growth rate, the iteration complexity of first-order stochastic optimization for achieving an ϵϵ\epsilon-optimal solution can be ˜O(1/ϵ2(1−θ))O˜(1/ϵ2(1−θ))\widetilde O(1/\epsilon^{2(1-\theta)}), which is optimal at most up to a logarithmic factor. This result is fundamentally better in contrast with the previous works that either assume a global growth condition in the entire domain or achieve a local faster convergence under the local faster growth condition. To achieve the faster global convergence, we develop two different accelerated stochastic subgradient methods by iteratively solving the original problem approximately in a local region around a historical solution with the size of the local region gradually decreasing as the solution approaches the optimal set. Besides the theoretical improvements, this work also include new contributions towards making the proposed algorithms practical: (i) we present practical variants of accelerated stochastic subgradient methods that can run without the knowledge of multiplicative growth constant and even the growth rate θθ\theta; (ii) we consider a broad family of problems in machine learning to demonstrate that the proposed algorithms enjoy faster convergence than traditional stochastic subgradient method. For example, when applied to the ℓ1ℓ1\ell_1 regularized empirical polyhedral loss minimization (e.g., hinge loss, absolute loss), the proposed stochastic methods have a logarithmic iteration complexity.",http://proceedings.mlr.press/v70/xu17a.html,http://proceedings.mlr.press/v70/xu17a/xu17a.pdf,ICML
1400,2017,Partitioned Tensor Factorizations for Learning Mixed Membership Models,"Zilong Tan,         Sayan Mukherjee","We present an efficient algorithm for learning mixed membership models when the number of variables p is much larger than the number of hidden components k. This algorithm reduces the computational complexity of state-of-the-art tensor methods, which require decomposing an O(p3)O(p3)O(p^3) tensor, to factorizing O(p/k)O(p/k)O(p/k) sub-tensors each of size O(k3)O(k3)O(k^3). In addition, we address the issue of negative entries in the empirical method of moments based estimators. We provide sufficient conditions under which our approach has provable guarantees. Our approach obtains competitive empirical results on both simulated and real data.",http://proceedings.mlr.press/v70/tan17a.html,http://proceedings.mlr.press/v70/tan17a/tan17a.pdf,ICML
1401,2017,Frame-based Data Factorizations,"Sebastian Mair,         Ahcène Boubekki,         Ulf Brefeld","Archetypal Analysis is the method of choice to compute interpretable matrix factorizations. Every data point is represented as a convex combination of factors, i.e., points on the boundary of the convex hull of the data. This renders computation inefficient. In this paper, we show that the set of vertices of a convex hull, the so-called frame, can be efficiently computed by a quadratic program. We provide theoretical and empirical results for our proposed approach and make use of the frame to accelerate Archetypal Analysis. The novel method yields similar reconstruction errors as baseline competitors but is much faster to compute.",http://proceedings.mlr.press/v70/mair17a.html,http://proceedings.mlr.press/v70/mair17a/mair17a.pdf,ICML
1402,2017,Ordinal Graphical Models: A Tale of Two Approaches,"Arun Sai Suggala,         Eunho Yang,         Pradeep Ravikumar","Undirected graphical models or Markov random fields (MRFs) are widely used for modeling multivariate probability distributions. Much of the work on MRFs has focused on continuous variables, and nominal variables (that is, unordered categorical variables). However, data from many real world applications involve ordered categorical variables also known as ordinal variables, e.g., movie ratings on Netflix which can be ordered from 1 to 5 stars. With respect to univariate ordinal distributions, as we detail in the paper, there are two main categories of distributions; while there have been efforts to extend these to multivariate ordinal distributions, the resulting distributions are typically very complex, with either a large number of parameters, or with non-convex likelihoods. While there have been some work on tractable approximations, these do not come with strong statistical guarantees, and moreover are relatively computationally expensive. In this paper, we theoretically investigate two classes of graphical models for ordinal data, corresponding to the two main categories of univariate ordinal distributions. In contrast to previous work, our theoretical developments allow us to provide correspondingly two classes of estimators that are not only computationally efficient but also have strong statistical guarantees.",http://proceedings.mlr.press/v70/suggala17a.html,http://proceedings.mlr.press/v70/suggala17a/suggala17a.pdf,ICML
1403,2017,How to Escape Saddle Points Efficiently,"Chi Jin,         Rong Ge,         Praneeth Netrapalli,         Sham M. Kakade,         Michael I. Jordan","This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost “dimension-free”). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.",http://proceedings.mlr.press/v70/jin17a.html,http://proceedings.mlr.press/v70/jin17a/jin17a.pdf,ICML
1404,2017,Convex Phase Retrieval without Lifting via PhaseMax,"Tom Goldstein,         Christoph Studer","Semidefinite relaxation methods transform a variety of non-convex optimization problems into convex problems, but square the number of variables. We study a new type of convex relaxation for phase retrieval problems, called PhaseMax, that convexifies the underlying problem without lifting. The resulting problem formulation can be solved using standard convex optimization routines, while still working in the original, low-dimensional variable space. We prove, using a random spherical distribution measurement model, that PhaseMax succeeds with high probability for a sufficiently large number of measurements. We compare our approach to other phase retrieval methods and demonstrate that our theory accurately predicts the success of PhaseMax.",http://proceedings.mlr.press/v70/goldstein17a.html,http://proceedings.mlr.press/v70/goldstein17a/goldstein17a.pdf,ICML
1405,2017,An Alternative Softmax Operator for Reinforcement Learning,"Kavosh Asadi,         Michael L. Littman","A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one’s weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning. We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter. We show that the algorithm is convergent and that it performs favorably in practice.",http://proceedings.mlr.press/v70/asadi17a.html,http://proceedings.mlr.press/v70/asadi17a/asadi17a.pdf,ICML
1406,2017,Language Modeling with Gated Convolutional Networks,"Yann N. Dauphin,         Angela Fan,         Michael Auli,         David Grangier","The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al. (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.",http://proceedings.mlr.press/v70/dauphin17a.html,http://proceedings.mlr.press/v70/dauphin17a/dauphin17a.pdf,ICML
1407,2017,Multi-task Learning with Labeled and Unlabeled Tasks,"Anastasia Pentina,         Christoph H. Lampert","In multi-task learning, a learner is given a collection of prediction tasks and needs to solve all of them. In contrast to previous work, which required that annotated training data must be available for all tasks, we consider a new setting, in which for some tasks, potentially most of them, only unlabeled training data is provided. Consequently, to solve all tasks, information must be transferred between tasks with labels and tasks without labels. Focusing on an instance-based transfer method we analyze two variants of this setting: when the set of labeled tasks is fixed, and when it can be actively selected by the learner. We state and prove a generalization bound that covers both scenarios and derive from it an algorithm for making the choice of labeled tasks (in the active case) and for transferring information between the tasks in a principled way. We also illustrate the effectiveness of the algorithm on synthetic and real data.",http://proceedings.mlr.press/v70/pentina17a.html,http://proceedings.mlr.press/v70/pentina17a/pentina17a.pdf,ICML
1408,2017,ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices,"Chirag Gupta,         Arun Sai Suggala,         Ankit Goyal,         Harsha Vardhan Simhadri,         Bhargavi Paranjape,         Ashish Kumar,         Saurabh Goyal,         Raghavendra Udupa,         Manik Varma,         Prateek Jain","Several real-world applications require real-time prediction on resource-scarce devices such as an Internet of Things (IoT) sensor. Such applications demand prediction models with small storage and computational complexity that do not compromise significantly on accuracy. In this work, we propose ProtoNN, a novel algorithm that addresses the problem of real-time and accurate prediction on resource-scarce devices. ProtoNN is inspired by k-Nearest Neighbor (KNN) but has several orders lower storage and prediction complexity. ProtoNN models can be deployed even on devices with puny storage and computational power (e.g. an Arduino UNO with 2kB RAM) to get excellent prediction accuracy. ProtoNN derives its strength from three key ideas: a) learning a small number of prototypes to represent the entire training set, b) sparse low dimensional projection of data, c) joint discriminative learning of the projection and prototypes with explicit model size constraint. We conduct systematic empirical evaluation of ProtoNN on a variety of supervised learning tasks (binary, multi-class, multi-label classification) and show that it gives nearly state-of-the-art prediction accuracy on resource-scarce devices while consuming several orders lower storage, and using minimal working memory.",http://proceedings.mlr.press/v70/gupta17a.html,http://proceedings.mlr.press/v70/gupta17a/gupta17a.pdf,ICML
1409,2017,Active Learning for Accurate Estimation of Linear Models,"Carlos Riquelme,         Mohammad Ghavamzadeh,         Alessandro Lazaric","We explore the sequential decision making problem where the goal is to estimate uniformly well a number of linear models, given a shared budget of random contexts independently sampled from a known distribution. The decision maker must query one of the linear models for each incoming context, and receives an observation corrupted by noise levels that are unknown, and depend on the model instance. We present Trace-UCB, an adaptive allocation algorithm that learns the noise levels while balancing contexts accordingly across the different linear functions, and derive guarantees for simple regret in both expectation and high-probability. Finally, we extend the algorithm and its guarantees to high dimensional settings, where the number of linear models times the dimension of the contextual space is higher than the total budget of samples. Simulations with real data suggest that Trace-UCB is remarkably robust, outperforming a number of baselines even when its assumptions are violated.",http://proceedings.mlr.press/v70/riquelme17a.html,http://proceedings.mlr.press/v70/riquelme17a/riquelme17a.pdf,ICML
1410,2017,Selective Inference for Sparse High-Order Interaction Models,"Shinya Suzumura,         Kazuya Nakagawa,         Yuta Umezu,         Koji Tsuda,         Ichiro Takeuchi","Finding statistically significant high-order interactions in predictive modeling is important but challenging task because the possible number of high-order interactions is extremely large (e.g., >1017>1017> 10^{17}). In this paper we study feature selection and statistical inference for sparse high-order interaction models. Our main contribution is to extend recently developed selective inference framework for linear models to high-order interaction models by developing a novel algorithm for efficiently characterizing the selection event for the selective inference of high-order interactions. We demonstrate the effectiveness of the proposed algorithm by applying it to an HIV drug response prediction problem.",http://proceedings.mlr.press/v70/suzumura17a.html,http://proceedings.mlr.press/v70/suzumura17a/suzumura17a.pdf,ICML
1411,2017,GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization,"Li Shen,         Wei Liu,         Ganzhao Yuan,         Shiqian Ma","In this paper, we propose a fast Gauss-Seidel Operator Splitting (GSOS) algorithm for addressing multi-term nonsmooth convex composite optimization, which has wide applications in machine learning, signal processing and statistics. The proposed GSOS algorithm inherits the advantage of the Gauss-Seidel technique to accelerate the optimization procedure, and leverages the operator splitting technique to reduce the computational complexity. In addition, we develop a new technique to establish the global convergence of the GSOS algorithm. To be specific, we first reformulate the iterations of GSOS as a two-step iterations algorithm by employing the tool of operator optimization theory. Subsequently, we establish the convergence of GSOS based on the two-step iterations algorithm reformulation. At last, we apply the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems. Numerical experiments show that our proposed GSOS algorithm is superior to the state-of-the-art algorithms in terms of both efficiency and effectiveness.",http://proceedings.mlr.press/v70/shen17b.html,http://proceedings.mlr.press/v70/shen17b/shen17b.pdf,ICML
1412,2017,Connected Subgraph Detection with Mirror Descent on SDPs,"Cem Aksoylar,         Lorenzo Orecchia,         Venkatesh Saligrama","We propose a novel, computationally efficient mirror-descent based optimization framework for subgraph detection in graph-structured data. Our aim is to discover anomalous patterns present in a connected subgraph of a given graph. This problem arises in many applications such as detection of network intrusions, community detection, detection of anomalous events in surveillance videos or disease outbreaks. Since optimization over connected subgraphs is a combinatorial and computationally difficult problem, we propose a convex relaxation that offers a principled approach to incorporating connectivity and conductance constraints on candidate subgraphs. We develop a novel efficient algorithm to solve the relaxed problem, establish convergence guarantees and demonstrate its feasibility and performance with experiments on real and very large simulated networks.",http://proceedings.mlr.press/v70/aksoylar17a.html,http://proceedings.mlr.press/v70/aksoylar17a/aksoylar17a.pdf,ICML
1413,2017,Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation,"Yacine Jernite,         Anna Choromanska,         David Sontag","We consider multi-class classification where the predictor has a hierarchical structure that allows for a very large number of labels both at train and test time. The predictive power of such models can heavily depend on the structure of the tree, and although past work showed how to learn the tree structure, it expected that the feature vectors remained static. We provide a novel algorithm to simultaneously perform representation learning for the input data and learning of the hierarchical predictor. Our approach optimizes an objective function which favors balanced and easily-separable multi-way node partitions. We theoretically analyze this objective, showing that it gives rise to a boosting style property and a bound on classification error. We next show how to extend the algorithm to conditional density estimation. We empirically validate both variants of the algorithm on text classification and language modeling, respectively, and show that they compare favorably to common baselines in terms of accuracy and running time.",http://proceedings.mlr.press/v70/jernite17a.html,http://proceedings.mlr.press/v70/jernite17a/jernite17a.pdf,ICML
1414,2017,A Birth-Death Process for Feature Allocation,"Konstantina Palla,         David Knowles,         Zoubin Ghahramani","We propose a Bayesian nonparametric prior over feature allocations for sequential data, the birth-death feature allocation process (BDFP). The BDFP models the evolution of the feature allocation of a set of N objects across a covariate (e.g.~time) by creating and deleting features. A BDFP is exchangeable, projective, stationary and reversible, and its equilibrium distribution is given by the Indian buffet process (IBP). We show that the Beta process on an extended space is the de Finetti mixing distribution underlying the BDFP. Finally, we present the finite approximation of the BDFP, the Beta Event Process (BEP), that permits simplified inference. The utility of the BDFP as a prior is demonstrated on real world dynamic genomics and social network data.",http://proceedings.mlr.press/v70/palla17a.html,http://proceedings.mlr.press/v70/palla17a/palla17a.pdf,ICML
1415,2017,Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution,"Po-Wei Chou,         Daniel Maturana,         Sebastian Scherer","Recently, reinforcement learning with deep neural networks has achieved great success in challenging continuous control problems such as 3D locomotion and robotic manipulation. However, in real-world control problems, the actions one can take are bounded by physical constraints, which introduces a bias when the standard Gaussian distribution is used as the stochastic policy. In this work, we propose to use the Beta distribution as an alternative and analyze the bias and variance of the policy gradients of both policies. We show that the Beta policy is bias-free and provides significantly faster convergence and higher scores over the Gaussian policy when both are used with trust region policy optimization (TRPO) and actor critic with experience replay (ACER), the state-of-the-art on- and off-policy stochastic methods respectively, on OpenAI Gym’s and MuJoCo’s continuous control environments.",http://proceedings.mlr.press/v70/chou17a.html,http://proceedings.mlr.press/v70/chou17a/chou17a.pdf,ICML
1416,2017,High Dimensional Bayesian Optimization with Elastic Gaussian Process,"Santu Rana,         Cheng Li,         Sunil Gupta,         Vu Nguyen,         Svetha Venkatesh","Bayesian optimization is an efficient way to optimize expensive black-box functions such as designing a new product with highest quality or hyperparameter tuning of a machine learning algorithm. However, it has a serious limitation when the parameter space is high-dimensional as Bayesian optimization crucially depends on solving a global optimization of a surrogate utility function in the same sized dimensions. The surrogate utility function, known commonly as acquisition function is a continuous function but can be extremely sharp at high dimension - having only a few peaks marooned in a large terrain of almost flat surface. Global optimization algorithms such as DIRECT are infeasible at higher dimensions and gradient-dependent methods cannot move if initialized in the flat terrain. We propose an algorithm that enables local gradient-dependent algorithms to move through the flat terrain by using a sequence of gross-to-finer Gaussian process priors on the objective function as we leverage two underlying facts - a) there exists a large enough length-scales for which the acquisition function can be made to have a significant gradient at any location in the parameter space, and b) the extrema of the consecutive acquisition functions are close although they are different only due to a small difference in the length-scales. Theoretical guarantees are provided and experiments clearly demonstrate the utility of the proposed method at high dimension using both benchmark test functions and real-world case studies.",http://proceedings.mlr.press/v70/rana17a.html,http://proceedings.mlr.press/v70/rana17a/rana17a.pdf,ICML
1417,2017,Multilevel Clustering via Wasserstein Means,"Nhat Ho,         XuanLong Nguyen,         Mikhail Yurochkin,         Hung Hai Bui,         Viet Huynh,         Dinh Phung","We propose a novel approach to the problem of multilevel clustering, which aims to simultaneously partition data in each group and discover grouping patterns among groups in a potentially large hierarchically structured corpus of data. Our method involves a joint optimization formulation over several spaces of discrete probability measures, which are endowed with Wasserstein distance metrics. We propose a number of variants of this problem, which admit fast optimization algorithms, by exploiting the connection to the problem of finding Wasserstein barycenters. Consistency properties are established for the estimates of both local and global clusters. Finally, experiment results with both synthetic and real data are presented to demonstrate the flexibility and scalability of the proposed approach.",http://proceedings.mlr.press/v70/ho17a.html,http://proceedings.mlr.press/v70/ho17a/ho17a.pdf,ICML
1418,2017,On Context-Dependent Clustering of Bandits,"Claudio Gentile,         Shuai Li,         Purushottam Kar,         Alexandros Karatzoglou,         Giovanni Zappella,         Evans Etrue","We investigate a novel cluster-of-bandit algorithm CAB for collaborative recommendation tasks that implements the underlying feedback sharing mechanism by estimating user neighborhoods in a context-dependent manner. CAB makes sharp departures from the state of the art by incorporating collaborative effects into inference, as well as learning processes in a manner that seamlessly interleaves explore-exploit tradeoffs and collaborative steps. We prove regret bounds for CAB under various data-dependent assumptions which exhibit a crisp dependence on the expected number of clusters over the users, a natural measure of the statistical difficulty of the learning task. Experiments on production and real-world datasets show that CAB offers significantly increased prediction performance against a representative pool of state-of-the-art methods.",http://proceedings.mlr.press/v70/gentile17a.html,http://proceedings.mlr.press/v70/gentile17a/gentile17a.pdf,ICML
1419,2017,A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization,"Jianbo Ye,         James Z. Wang,         Jia Li","Learning under a Wasserstein loss, a.k.a. Wasserstein loss minimization (WLM), is an emerging research topic for gaining insights from a large set of structured objects. Despite being conceptually simple, WLM problems are computationally challenging because they involve minimizing over functions of quantities (i.e. Wasserstein distances) that themselves require numerical algorithms to compute. In this paper, we introduce a stochastic approach based on simulated annealing for solving WLMs. Particularly, we have developed a Gibbs sampler to approximate effectively and efficiently the partial gradients of a sequence of Wasserstein losses. Our new approach has the advantages of numerical stability and readiness for warm starts. These characteristics are valuable for WLM problems that often require multiple levels of iterations in which the oracle for computing the value and gradient of a loss function is embedded. We applied the method to optimal transport with Coulomb cost and the Wasserstein non-negative matrix factorization problem, and made comparisons with the existing method of entropy regularization.",http://proceedings.mlr.press/v70/ye17b.html,http://proceedings.mlr.press/v70/ye17b/ye17b.pdf,ICML
1420,2017,Robust Guarantees of Stochastic Greedy Algorithms,"Avinatan Hassidim,         Yaron Singer","In this paper we analyze the robustness of stochastic variants of the greedy algorithm for submodular maximization. Our main result shows that for maximizing a monotone submodular function under a cardinality constraint, iteratively selecting an element whose marginal contribution is approximately maximal in expectation is a sufficient condition to obtain the optimal approximation guarantee with exponentially high probability, assuming the cardinality is sufficiently large. One consequence of our result is that the linear-time STOCHASTIC-GREEDY algorithm recently proposed in (Mirzasoleiman et al.,2015) achieves the optimal running time while maintaining an optimal approximation guarantee. We also show that high probability guarantees cannot be obtained for stochastic greedy algorithms under matroid constraints, and prove an approximation guarantee which holds in expectation. In contrast to the guarantees of the greedy algorithm, we show that the approximation ratio of stochastic local search is arbitrarily bad, with high probability, as well as in expectation.",http://proceedings.mlr.press/v70/hassidim17a.html,http://proceedings.mlr.press/v70/hassidim17a/hassidim17a.pdf,ICML
1421,2017,Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis,"Dan Garber,         Ohad Shamir,         Nathan Srebro","We study the fundamental problem of Principal Component Analysis in a statistical distributed setting in which each machine out of m stores a sample of n points sampled i.i.d. from a single unknown distribution. We study algorithms for estimating the leading principal component of the population covariance matrix that are both communication-efficient and achieve estimation error of the order of the centralized ERM solution that uses all mn samples. On the negative side, we show that in contrast to results obtained for distributed estimation under convexity assumptions, for the PCA objective, simply averaging the local ERM solutions cannot guarantee error that is consistent with the centralized ERM. We show that this unfortunate phenomena can be remedied by performing a simple correction step which correlates between the individual solutions, and provides an estimator that is consistent with the centralized ERM for sufficiently-large n. We also introduce an iterative distributed algorithm that is applicable in any regime of n, which is based on distributed matrix-vector products. The algorithm gives significant acceleration in terms of communication rounds over previous distributed algorithms, in a wide regime of parameters.",http://proceedings.mlr.press/v70/garber17a.html,http://proceedings.mlr.press/v70/garber17a/garber17a.pdf,ICML
1422,2017,Maximum Selection and Ranking under Noisy Comparisons,"Moein Falahatgar,         Alon Orlitsky,         Venkatadheeraj Pichapati,         Ananda Theertha Suresh","We consider (ϵ,δ)(ϵ,δ)(\epsilon,\delta)-PAC maximum-selection and ranking using pairwise comparisons for general probabilistic models whose comparison probabilities satisfy strong stochastic transitivity and stochastic triangle inequality. Modifying the popular knockout tournament, we propose a simple maximum-selection algorithm that uses O(nϵ2(1+log1δ))O(nϵ2(1+log⁡1δ))\mathcal{O}\left(\frac{n}{\epsilon^2} \left(1+\log \frac1{\delta}\right)\right) comparisons, optimal up to a constant factor. We then derive a general framework that uses noisy binary search to speed up many ranking algorithms, and combine it with merge sort to obtain a ranking algorithm that uses O(nϵ2logn(loglogn)3)O(nϵ2log⁡n(log⁡log⁡n)3)\mathcal{O}\left(\frac n{\epsilon^2}\log n(\log \log n)^3\right) comparisons for δ=1nδ=1n\delta=\frac1n, optimal up to a (loglogn)3(log⁡log⁡n)3(\log \log n)^3 factor.",http://proceedings.mlr.press/v70/falahatgar17a.html,http://proceedings.mlr.press/v70/falahatgar17a/falahatgar17a.pdf,ICML
1423,2017,Iterative Machine Teaching,"Weiyang Liu,         Bo Dai,         Ahmad Humayun,         Charlene Tay,         Chen Yu,         Linda B. Smith,         James M. Rehg,         Le Song","In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner. We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers. We also validate our theoretical findings with extensive experiments on different data distribution and real image datasets.",http://proceedings.mlr.press/v70/liu17b.html,http://proceedings.mlr.press/v70/liu17b/liu17b.pdf,ICML
1424,2017,Latent Intention Dialogue Models,"Tsung-Hsien Wen,         Yishu Miao,         Phil Blunsom,         Steve Young","Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. The traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture the conversational stochasticity. In this paper, however, we propose a Latent Intention Dialogue Model that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. Additionally, in a goal-oriented dialogue scenario, the latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning. The experiments demonstrate the effectiveness of discrete latent variable models on learning goal-oriented dialogues, and the results outperform the published benchmarks on both corpus-based evaluation and human evaluation.",http://proceedings.mlr.press/v70/wen17a.html,http://proceedings.mlr.press/v70/wen17a/wen17a.pdf,ICML
1425,2017,Just Sort It! A Simple and Effective Approach to Active Preference Learning,"Lucas Maystre,         Matthias Grossglauser","We address the problem of learning a ranking by using adaptively chosen pairwise comparisons. Our goal is to recover the ranking accurately but to sample the comparisons sparingly. If all comparison outcomes are consistent with the ranking, the optimal solution is to use an efficient sorting algorithm, such as Quicksort. But how do sorting algorithms behave if some comparison outcomes are inconsistent with the ranking? We give favorable guarantees for Quicksort for the popular Bradley-Terry model, under natural assumptions on the parameters. Furthermore, we empirically demonstrate that sorting algorithms lead to a very simple and effective active learning strategy: repeatedly sort the items. This strategy performs as well as state-of-the-art methods (and much better than random sampling) at a minuscule fraction of the computational cost.",http://proceedings.mlr.press/v70/maystre17a.html,http://proceedings.mlr.press/v70/maystre17a/maystre17a.pdf,ICML
1426,2017,Multi-Class Optimal Margin Distribution Machine,"Teng Zhang,         Zhi-Hua Zhou","Recent studies disclose that maximizing the minimum margin like support vector machines does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution. Although it has been shown that for binary classification, characterizing the margin distribution by the first- and second-order statistics can achieve superior performance. It still remains open for multi-class classification, and due to the complexity of margin for multi-class classification, optimizing its distribution by mean and variance can also be difficult. In this paper, we propose mcODM (multi-class Optimal margin Distribution Machine), which can solve this problem efficiently. We also give a theoretical analysis for our method, which verifies the significance of margin distribution for multi-class classification. Empirical study further shows that mcODM always outperforms all four versions of multi-class SVMs on all experimental data sets.",http://proceedings.mlr.press/v70/zhang17h.html,http://proceedings.mlr.press/v70/zhang17h/zhang17h.pdf,ICML
1427,2017,Coordinated Multi-Agent Imitation Learning,"Hoang M. Le,         Yisong Yue,         Peter Carr,         Patrick Lucey","We study the problem of imitation learning from demonstrations of multiple coordinating agents. One key challenge in this setting is that learning a good model of coordination can be difficult, since coordination is often implicit in the demonstrations and must be inferred as a latent variable. We propose a joint approach that simultaneously learns a latent coordination model along with the individual policies. In particular, our method integrates unsupervised structure learning with conventional imitation learning. We illustrate the power of our approach on a difficult problem of learning multiple policies for fine-grained behavior modeling in team sports, where different players occupy different roles in the coordinated team strategy. We show that having a coordination model to infer the roles of players yields substantially improved imitation loss compared to conventional baselines.",http://proceedings.mlr.press/v70/le17a.html,http://proceedings.mlr.press/v70/le17a/le17a.pdf,ICML
1428,2017,Accelerating Eulerian Fluid Simulation With Convolutional Networks,"Jonathan Tompson,         Kristofer Schlachter,         Pablo Sprechmann,         Ken Perlin","Efficient simulation of the Navier-Stokes equations for fluid flow is a long standing problem in applied mathematics, for which state-of-the-art methods require large compute resources. In this work, we propose a data-driven approach that leverages the approximation power of deep-learning with the precision of standard solvers to obtain fast and highly realistic simulations. Our method solves the incompressible Euler equations using the standard operator splitting method, in which a large sparse linear system with many free parameters must be solved. We use a Convolutional Network with a highly tailored architecture, trained using a novel unsupervised learning framework to solve the linear system. We present real-time 2D and 3D simulations that outperform recently proposed data-driven methods; the obtained results are realistic and show good generalization properties.",http://proceedings.mlr.press/v70/tompson17a.html,http://proceedings.mlr.press/v70/tompson17a/tompson17a.pdf,ICML
1429,2017,Fractional Langevin Monte Carlo: Exploring Levy Driven Stochastic Differential Equations for Markov Chain Monte Carlo,Umut Şimşekli,"Along with the recent advances in scalable Markov Chain Monte Carlo methods, sampling techniques that are based on Langevin diffusions have started receiving increasing attention. These so called Langevin Monte Carlo (LMC) methods are based on diffusions driven by a Brownian motion, which gives rise to Gaussian proposal distributions in the resulting algorithms. Even though these approaches have proven successful in many applications, their performance can be limited by the light-tailed nature of the Gaussian proposals. In this study, we extend classical LMC and develop a novel Fractional LMC (FLMC) framework that is based on a family of heavy-tailed distributions, called alpha-stable Levy distributions. As opposed to classical approaches, the proposed approach can possess large jumps while targeting the correct distribution, which would be beneficial for efficient exploration of the state space. We develop novel computational methods that can scale up to large-scale problems and we provide formal convergence analysis of the proposed scheme. Our experiments support our theory: FLMC can provide superior performance in multi-modal settings, improved convergence rates, and robustness to algorithm parameters.",http://proceedings.mlr.press/v70/simsekli17a.html,http://proceedings.mlr.press/v70/simsekli17a/simsekli17a.pdf,ICML
1430,2017,Latent LSTM Allocation: Joint Clustering and Non-Linear Dynamic Modeling of Sequence Data,"Manzil Zaheer,         Amr Ahmed,         Alexander J. Smola","Recurrent neural networks, such as long-short term memory (LSTM) networks, are powerful tools for modeling sequential data like user browsing history (Tan et al., 2016; Korpusik et al., 2016) or natural language text (Mikolov et al., 2010). However, to generalize across different user types, LSTMs require a large number of parameters, notwithstanding the simplicity of the underlying dynamics, rendering it uninterpretable, which is highly undesirable in user modeling. The increase in complexity and parameters arises due to a large action space in which many of the actions have similar intent or topic. In this paper, we introduce Latent LSTM Allocation (LLA) for user modeling combining hierarchical Bayesian models with LSTMs. In LLA, each user is modeled as a sequence of actions, and the model jointly groups actions into topics and learns the temporal dynamics over the topic sequence, instead of action space directly. This leads to a model that is highly interpretable, concise, and can capture intricate dynamics. We present an efficient Stochastic EM inference algorithm for our model that scales to millions of users/documents. Our experimental evaluations show that the proposed model compares favorably with several state-of-the-art baselines.",http://proceedings.mlr.press/v70/zaheer17a.html,http://proceedings.mlr.press/v70/zaheer17a/zaheer17a.pdf,ICML
1431,2017,Neural Optimizer Search with Reinforcement Learning,"Irwan Bello,         Barret Zoph,         Vijay Vasudevan,         Quoc V. Le","We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a specific domain language that describes a mathematical update equation based on a list of primitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the performance of a model after a few epochs. On CIFAR-10, our method discovers several update rules that are better than many commonly used optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet model. These optimizers can also be transferred to perform well on different neural network architectures, including Google’s neural machine translation system.",http://proceedings.mlr.press/v70/bello17a.html,http://proceedings.mlr.press/v70/bello17a/bello17a.pdf,ICML
1432,2017,Online and Linear-Time Attention by Enforcing Monotonic Alignments,"Colin Raffel,         Minh-Thang Luong,         Peter J. Liu,         Ron J. Weiss,         Douglas Eck","Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.",http://proceedings.mlr.press/v70/raffel17a.html,http://proceedings.mlr.press/v70/raffel17a/raffel17a.pdf,ICML
1433,2017,SPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling,"Jun-ichiro Hirayama,         Aapo Hyvärinen,         Motoaki Kawanabe","We present a novel probabilistic framework for a hierarchical extension of independent component analysis (ICA), with a particular motivation in neuroscientific data analysis and modeling. The framework incorporates a general subspace pooling with linear ICA-like layers stacked recursively. Unlike related previous models, our generative model is fully tractable: both the likelihood and the posterior estimates of latent variables can readily be computed with analytically simple formulae. The model is particularly simple in the case of complex-valued data since the pooling can be reduced to taking the modulus of complex numbers. Experiments on electroencephalography (EEG) and natural images demonstrate the validity of the method.",http://proceedings.mlr.press/v70/hirayama17a.html,http://proceedings.mlr.press/v70/hirayama17a/hirayama17a.pdf,ICML
1434,2017,A Distributional Perspective on Reinforcement Learning,"Marc G. Bellemare,         Will Dabney,         Rémi Munos","In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.",http://proceedings.mlr.press/v70/bellemare17a.html,http://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf,ICML
1435,2017,Lazifying Conditional Gradient Algorithms,"Gábor Braun,         Sebastian Pokutta,         Daniel Zink","Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are popular due to their simplicity of only requiring a linear optimization oracle and more recently they also gained significant traction for online learning. While simple in principle, in many cases the actual implementation of the linear optimization oracle is costly. We show a general method to lazify various conditional gradient algorithms, which in actual computations leads to several orders of magnitude of speedup in wall-clock time. This is achieved by using a faster separation oracle instead of a linear optimization oracle, relying only on few linear optimization oracle calls.",http://proceedings.mlr.press/v70/braun17a.html,http://proceedings.mlr.press/v70/braun17a/braun17a.pdf,ICML
1436,2017,Diameter-Based Active Learning,"Christopher Tosh,         Sanjoy Dasgupta","To date, the tightest upper and lower-bounds for the active learning of general concept classes have been in terms of a parameter of the learning problem called the splitting index. We provide, for the first time, an efficient algorithm that is able to realize this upper bound, and we empirically demonstrate its good performance.",http://proceedings.mlr.press/v70/tosh17a.html,http://proceedings.mlr.press/v70/tosh17a/tosh17a.pdf,ICML
1437,2017,Axiomatic Attribution for Deep Networks,"Mukund Sundararajan,         Ankur Taly,         Qiqi Yan","We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.",http://proceedings.mlr.press/v70/sundararajan17a.html,http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf,ICML
1438,2017,Dictionary Learning Based on Sparse Distribution Tomography,"Pedram Pad,         Farnood Salehi,         Elisa Celis,         Patrick Thiran,         Michael Unser","We propose a new statistical dictionary learning algorithm for sparse signals that is based on an αα\alpha-stable innovation model. The parameters of the underlying model—that is, the atoms of the dictionary, the sparsity index αα\alpha and the dispersion of the transform-domain coefficients—are recovered using a new type of probability distribution tomography. Specifically, we drive our estimator with a series of random projections of the data, which results in an efficient algorithm. Moreover, since the projections are achieved using linear combinations, we can invoke the generalized central limit theorem to justify the use of our method for sparse signals that are not necessarily αα\alpha-stable. We evaluate our algorithm by performing two types of experiments: image in-painting and image denoising. In both cases, we find that our approach is competitive with state-of-the-art dictionary learning techniques. Beyond the algorithm itself, two aspects of this study are interesting in their own right. The first is our statistical formulation of the problem, which unifies the topics of dictionary learning and independent component analysis. The second is a generalization of a classical theorem about isometries of ℓpℓp\ell_p-norms that constitutes the foundation of our approach.",http://proceedings.mlr.press/v70/pad17a.html,http://proceedings.mlr.press/v70/pad17a/pad17a.pdf,ICML
1439,2017,Graph-based Isometry Invariant Representation Learning,"Renata Khasanova,         Pascal Frossard","Learning transformation invariant representations of visual data is an important problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classification tasks. However, they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we present a novel Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are inherently invariant to isometric transformations such as rotation and translation of input images. In particular, images are represented as signals on graphs, which permits to replace classical convolution and pooling layers in deep networks with graph spectral convolution and dynamic graph pooling layers that together contribute to invariance to isometric transformation. Our experiments show high performance on rotated and translated images from the test set compared to classical architectures that are very sensitive to transformations in the data. The inherent invariance properties of our framework provide key advantages, such as increased resiliency to data variability and sustained performance with limited training sets.",http://proceedings.mlr.press/v70/khasanova17a.html,http://proceedings.mlr.press/v70/khasanova17a/khasanova17a.pdf,ICML
1440,2017,Analytical Guarantees on Numerical Precision of Deep Neural Networks,"Charbel Sakr,         Yongjune Kim,         Naresh Shanbhag","The acclaimed successes of neural networks often overshadow their tremendous complexity. We focus on numerical precision – a key parameter defining the complexity of neural networks. First, we present theoretical bounds on the accuracy in presence of limited precision. Interestingly, these bounds can be computed via the back-propagation algorithm. Hence, by combining our theoretical analysis and the back-propagation algorithm, we are able to readily determine the minimum precision needed to preserve accuracy without having to resort to time-consuming fixed-point simulations. We provide numerical evidence showing how our approach allows us to maintain high accuracy but with lower complexity than state-of-the-art binary networks.",http://proceedings.mlr.press/v70/sakr17a.html,http://proceedings.mlr.press/v70/sakr17a/sakr17a.pdf,ICML
1441,2017,Equivariance Through Parameter-Sharing,"Siamak Ravanbakhsh,         Jeff Schneider,         Barnabás Póczos","We propose to study equivariance in deep neural networks through parameter symmetries. In particular, given a group G that acts discretely on the input and output of a standard neural network layer, we show that its equivariance is linked to the symmetry group of network parameters. We then propose two parameter-sharing scheme to induce the desirable symmetry on the parameters of the neural network. Under some conditions on the action of G, our procedure for tying the parameters achieves G-equivariance and guarantees sensitivity to all other permutation groups outside of G.",http://proceedings.mlr.press/v70/ravanbakhsh17a.html,http://proceedings.mlr.press/v70/ravanbakhsh17a/ravanbakhsh17a.pdf,ICML
1442,2017,Fast k-Nearest Neighbour Search via Prioritized DCI,"Ke Li,         Jitendra Malik","Most exact methods for k-nearest neighbour search suffer from the curse of dimensionality; that is, their query times exhibit exponential dependence on either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing (DCI) offers a promising way of circumventing the curse and successfully reduces the dependence of query time on intrinsic dimensionality from exponential to sublinear. In this paper, we propose a variant of DCI, which we call Prioritized DCI, and show a remarkable improvement in the dependence of query time on intrinsic dimensionality. In particular, a linear increase in intrinsic dimensionality, or equivalently, an exponential increase in the number of points near a query, can be mostly counteracted with just a linear increase in space. We also demonstrate empirically that Prioritized DCI significantly outperforms prior methods. In particular, relative to Locality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of distance evaluations by a factor of 14 to 116 and the memory consumption by a factor of 21.",http://proceedings.mlr.press/v70/li17d.html,http://proceedings.mlr.press/v70/li17d/li17d.pdf,ICML
1443,2017,Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics,"Ken Kansky,         Tom Silver,         David A. Mély,         Mohamed Eldawy,         Miguel Lázaro-Gredilla,         Xinghua Lou,         Nimrod Dorfman,         Szymon Sidor,         Scott Phoenix,         Dileep George","The recent adaptation of deep neural network-based methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an object-oriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.",http://proceedings.mlr.press/v70/kansky17a.html,http://proceedings.mlr.press/v70/kansky17a/kansky17a.pdf,ICML
1444,2017,Efficient Regret Minimization in Non-Convex Games,"Elad Hazan,         Karan Singh,         Cyril Zhang","We consider regret minimization in repeated games with non-convex loss functions. Minimizing the standard notion of regret is computationally intractable. Thus, we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum. We give gradient-based methods that achieve optimal regret, which in turn guarantee convergence to equilibrium in this framework.",http://proceedings.mlr.press/v70/hazan17a.html,http://proceedings.mlr.press/v70/hazan17a/hazan17a.pdf,ICML
1445,2017,Variational Inference for Sparse and Undirected Models,"John Ingraham,         Debora Marks","Undirected graphical models are applied in genomics, protein structure prediction, and neuroscience to identify sparse interactions that underlie discrete data. Although Bayesian methods for inference would be favorable in these contexts, they are rarely used because they require doubly intractable Monte Carlo sampling. Here, we develop a framework for scalable Bayesian inference of discrete undirected models based on two new methods. The first is Persistent VI, an algorithm for variational inference of discrete undirected models that avoids doubly intractable MCMC and approximations of the partition function. The second is Fadeout, a reparameterization approach for variational inference under sparsity-inducing priors that captures a posteriori correlations between parameters and hyperparameters with noncentered parameterizations. We find that, together, these methods for variational inference substantially improve learning of sparse undirected graphical models in simulated and real problems from physics and biology.",http://proceedings.mlr.press/v70/ingraham17a.html,http://proceedings.mlr.press/v70/ingraham17a/ingraham17a.pdf,ICML
1446,2017,Wasserstein Generative Adversarial Networks,"Martin Arjovsky,         Soumith Chintala,         Léon Bottou","We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.",http://proceedings.mlr.press/v70/arjovsky17a.html,http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf,ICML
1447,2017,Consistent On-Line Off-Policy Evaluation,"Assaf Hallak,         Shie Mannor","The problem of on-line off-policy evaluation (OPE) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme. However, most Temporal Difference (TD) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied. In this paper we propose the Consistent Off-Policy Temporal Difference (COP-TD(λλ\lambda, ββ\beta)) algorithm that addresses this issue and reduces this bias at some computational expense. We show that COP-TD(λλ\lambda, ββ\beta) can be designed to converge to the same value that would have been obtained by using on-policy TD(λλ\lambda) with the target policy. Subsequently, the proposed scheme leads to a related and promising heuristic we call log-COP-TD(λλ\lambda, ββ\beta). Both algorithms have favorable empirical results to the current state of the art on-line OPE algorithms. Finally, our formulation sheds some new light on the recently proposed Emphatic TD learning.",http://proceedings.mlr.press/v70/hallak17a.html,http://proceedings.mlr.press/v70/hallak17a/hallak17a.pdf,ICML
1448,2017,Automated Curriculum Learning for Neural Networks,"Alex Graves,         Marc G. Bellemare,         Jacob Menick,         Rémi Munos,         Koray Kavukcuoglu","We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.",http://proceedings.mlr.press/v70/graves17a.html,http://proceedings.mlr.press/v70/graves17a/graves17a.pdf,ICML
1449,2017,Sub-sampled Cubic Regularization for Non-convex Optimization,"Jonas Moritz Kohler,         Aurelien Lucchi","We consider the minimization of non-convex functions that typically arise in machine learning. Specifically, we focus our attention on a variant of trust region methods known as cubic regularization. This approach is particularly attractive because it escapes strict saddle points and it provides stronger convergence guarantees than first- and second-order as well as classical trust region methods. However, it suffers from a high computational complexity that makes it impractical for large-scale learning. Here, we propose a novel method that uses sub-sampling to lower this computational cost. By the use of concentration inequalities we provide a sampling scheme that gives sufficiently accurate gradient and Hessian approximations to retain the strong global and local convergence guarantees of cubically regularized methods. To the best of our knowledge this is the first work that gives global convergence guarantees for a sub-sampled variant of cubic regularization on non-convex functions. Furthermore, we provide experimental results supporting our theory.",http://proceedings.mlr.press/v70/kohler17a.html,http://proceedings.mlr.press/v70/kohler17a/kohler17a.pdf,ICML
1450,2017,Counterfactual Data-Fusion for Online Reinforcement Learners,"Andrew Forney,         Judea Pearl,         Elias Bareinboim","The Multi-Armed Bandit problem with Unobserved Confounders (MABUC) considers decision-making settings where unmeasured variables can influence both the agent’s decisions and received rewards (Bareinboim et al., 2015). Recent findings showed that unobserved confounders (UCs) pose a unique challenge to algorithms based on standard randomization (i.e., experimental data); if UCs are naively averaged out, these algorithms behave sub-optimally, possibly incurring infinite regret. In this paper, we show how counterfactual-based decision-making circumvents these problems and leads to a coherent fusion of observational and experimental data. We then demonstrate this new strategy in an enhanced Thompson Sampling bandit player, and support our findings’ efficacy with extensive simulations.",http://proceedings.mlr.press/v70/forney17a.html,http://proceedings.mlr.press/v70/forney17a/forney17a.pdf,ICML
1451,2017,Programming with a Differentiable Forth Interpreter,"Matko Bošnjak,         Tim Rocktäschel,         Jason Naradowsky,         Sebastian Riedel","Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.",http://proceedings.mlr.press/v70/bosnjak17a.html,http://proceedings.mlr.press/v70/bosnjak17a/bosnjak17a.pdf,ICML
1452,2017,Learning Algorithms for Active Learning,"Philip Bachman,         Alessandro Sordoni,         Adam Trischler","We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a prediction function. Our model uses the item selection heuristic to construct a labeled support set for training the prediction function. Using the Omniglot and MovieLens datasets, we test our model in synthetic and practical settings.",http://proceedings.mlr.press/v70/bachman17a.html,http://proceedings.mlr.press/v70/bachman17a/bachman17a.pdf,ICML
1453,2017,Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions,"Yichen Chen,         Dongdong Ge,         Mengdi Wang,         Zizhuo Wang,         Yinyu Ye,         Hao Yin","Consider the regularized sparse minimization problem, which involves empirical sums of loss functions for nnn data points (each of dimension ddd) and a nonconvex sparsity penalty. We prove that finding an O(nc1dc2)O(nc1dc2)\mathcal{O}(n^{c_1}d^{c_2})-optimal solution to the regularized sparse optimization problem is strongly NP-hard for any c1,c2∈[0,1)c1,c2∈[0,1)c_1, c_2\in [0,1) such that c1+c2<1c1+c2<1c_1+c_2<1. The result applies to a broad class of loss functions and sparse penalty functions. It suggests that one cannot even approximately solve the sparse optimization problem in polynomial time, unless P === NP.",http://proceedings.mlr.press/v70/chen17d.html,http://proceedings.mlr.press/v70/chen17d/chen17d.pdf,ICML
1454,2017,The Statistical Recurrent Unit,"Junier B. Oliva,         Barnabás Póczos,         Jeff Schneider","Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications. We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics. The SRU’s architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures’ hyperparameters for both synthetic and real-world tasks.",http://proceedings.mlr.press/v70/oliva17a.html,http://proceedings.mlr.press/v70/oliva17a/oliva17a.pdf,ICML
1455,2017,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,"Chelsea Finn,         Pieter Abbeel,         Sergey Levine","We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",http://proceedings.mlr.press/v70/finn17a.html,http://proceedings.mlr.press/v70/finn17a/finn17a.pdf,ICML
1456,2017,Variational Dropout Sparsifies Deep Neural Networks,"Dmitry Molchanov,         Arsenii Ashukha,         Dmitry Vetrov","We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.",http://proceedings.mlr.press/v70/molchanov17a.html,http://proceedings.mlr.press/v70/molchanov17a/molchanov17a.pdf,ICML
1457,2017,Adaptive Sampling Probabilities for Non-Smooth Optimization,"Hongseok Namkoong,         Aman Sinha,         Steve Yadlowsky,         John C. Duchi","Standard forms of coordinate and stochastic gradient methods do not adapt to structure in data; their good behavior under random sampling is predicated on uniformity in data. When gradients in certain blocks of features (for coordinate descent) or examples (for SGD) are larger than others, there is a natural structure that can be exploited for quicker convergence. Yet adaptive variants often suffer nontrivial computational overhead. We present a framework that discovers and leverages such structural properties at a low computational cost. We employ a bandit optimization procedure that “learns” probabilities for sampling coordinates or examples in (non-smooth) optimization problems, allowing us to guarantee performance close to that of the optimal stationary sampling distribution. When such structures exist, our algorithms achieve tighter convergence guarantees than their non-adaptive counterparts, and we complement our analysis with experiments on several datasets.",http://proceedings.mlr.press/v70/namkoong17a.html,http://proceedings.mlr.press/v70/namkoong17a/namkoong17a.pdf,ICML
1458,2017,DARLA: Improving Zero-Shot Transfer in Reinforcement Learning,"Irina Higgins,         Arka Pal,         Andrei Rusu,         Loic Matthey,         Christopher Burgess,         Alexander Pritzel,         Matthew Botvinick,         Charles Blundell,         Alexander Lerchner","Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA’s vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts – even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",http://proceedings.mlr.press/v70/higgins17a.html,http://proceedings.mlr.press/v70/higgins17a/higgins17a.pdf,ICML
1459,2017,Tensor Balancing on Statistical Manifold,"Mahito Sugiyama,         Hiroyuki Nakahara,         Koji Tsuda","We solve tensor balancing, rescaling an Nth order nonnegative tensor by multiplying N tensors of order N - 1 so that every fiber sums to one. This generalizes a fundamental process of matrix balancing used to compare matrices in a wide range of applications from biology to economics. We present an efficient balancing algorithm with quadratic convergence using Newton’s method and show in numerical experiments that the proposed algorithm is several orders of magnitude faster than existing ones. To theoretically prove the correctness of the algorithm, we model tensors as probability distributions in a statistical manifold and realize tensor balancing as projection onto a submanifold. The key to our algorithm is that the gradient of the manifold, used as a Jacobian matrix in Newton’s method, can be analytically obtained using the Möbius inversion formula, the essential of combinatorial mathematics. Our model is not limited to tensor balancing, but has a wide applicability as it includes various statistical and machine learning models such as weighted DAGs and Boltzmann machines.",http://proceedings.mlr.press/v70/sugiyama17a.html,http://proceedings.mlr.press/v70/sugiyama17a/sugiyama17a.pdf,ICML
1460,2017,Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks,"David Balduzzi,         Brian McWilliams,         Tony Butler-Yeoman","Modern convolutional networks, incorporating rectifiers and max-pooling, are neither smooth nor convex; standard guarantees therefore do not apply. Nevertheless, methods from convex optimization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the first convergence guarantee applicable to modern convnets, which furthermore matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation – a straightforward application of Taylor expansions to neural networks – and the associated Taylor loss. Experiments on a range of optimizers, layers, and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization. The second half of the paper applies the Taylor approximation to isolate the main difficulty in training rectifier nets – that gradients are shattered – and investigates the hypothesis that, by exploring the space of activation configurations more thoroughly, adaptive optimizers such as RMSProp and Adam are able to converge to better solutions.",http://proceedings.mlr.press/v70/balduzzi17c.html,http://proceedings.mlr.press/v70/balduzzi17c/balduzzi17c.pdf,ICML
1461,2017,Nearly Optimal Robust Matrix Completion,"Yeshwanth Cherapanamjeri,         Kartik Gupta,         Prateek Jain","In this paper, we consider the problem of Robust Matrix Completion (RMC) where the goal is to recover a low-rank matrix by observing a small number of its entries out of which a few can be arbitrarily corrupted. We propose a simple projected gradient descent-based method to estimate the low-rank matrix that alternately performs a projected gradient descent step and cleans up a few of the corrupted entries using hard-thresholding. Our algorithm solves RMC using nearly optimal number of observations while tolerating a nearly optimal number of corruptions. Our result also implies significant improvement over the existing time complexity bounds for the low-rank matrix completion problem. Finally, an application of our result to the robust PCA problem (low-rank+sparse matrix separation) leads to nearly linear time (in matrix dimensions) algorithm for the same; existing state-of-the-art methods require quadratic time. Our empirical results corroborate our theoretical results and show that even for moderate sized problems, our method for robust PCA is an order of magnitude faster than the existing methods.",http://proceedings.mlr.press/v70/cherapanamjeri17a.html,http://proceedings.mlr.press/v70/cherapanamjeri17a/cherapanamjeri17a.pdf,ICML
1462,2017,Robust Gaussian Graphical Model Estimation with Arbitrary Corruption,"Lingxiao Wang,         Quanquan Gu","We study the problem of estimating the high-dimensional Gaussian graphical model where the data are arbitrarily corrupted. We propose a robust estimator for the sparse precision matrix in the high-dimensional regime. At the core of our method is a robust covariance matrix estimator, which is based on truncated inner product. We establish the statistical guarantee of our estimator on both estimation error and model selection consistency. In particular, we show that provided that the number of corrupted samples n2n2n_2 for each variable satisfies n2≲n−−√/logd−−−−√n2≲n/log⁡dn_2 \lesssim \sqrt{n}/\sqrt{\log d}, where nnn is the sample size and ddd is the number of variables, the proposed robust precision matrix estimator attains the same statistical rate as the standard estimator for Gaussian graphical models. In addition, we propose a hypothesis testing procedure to assess the uncertainty of our robust estimator. We demonstrate the effectiveness of our method through extensive experiments on both synthetic data and real-world genomic data.",http://proceedings.mlr.press/v70/wang17d.html,http://proceedings.mlr.press/v70/wang17d/wang17d.pdf,ICML
1463,2017,Coupling Distributed and Symbolic Execution for Natural Language Queries,"Lili Mou,         Zhengdong Lu,         Hang Li,         Zhi Jin","Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in deep learning. An executor for table querying typically requires multiple steps of execution because queries may have complicated structures. In previous studies, researchers have developed either fully distributed executors or symbolic executors for table querying. A distributed executor can be trained in an end-to-end fashion, but is weak in terms of execution efficiency and explicit interpretability. A symbolic executor is efficient in execution, but is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries, where the symbolic executor is pretrained with the distributed executor’s intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms both distributed and symbolic executors, exhibiting high accuracy, high learning efficiency, high execution efficiency, and high interpretability.",http://proceedings.mlr.press/v70/mou17a.html,http://proceedings.mlr.press/v70/mou17a/mou17a.pdf,ICML
1464,2017,Attentive Recurrent Comparators,"Pranav Shyam,         Shubham Gupta,         Ambedkar Dukkipati","Rapid learning requires flexible representations to quickly adopt to new evidence. We develop a novel class of models called Attentive Recurrent Comparators (ARCs) that form representations of objects by cycling through them and making observations. Using the representations extracted by ARCs, we develop a way of approximating a dynamic representation space and use it for one-shot learning. In the task of one-shot classification on the Omniglot dataset, we achieve the state of the art performance with an error rate of 1.5\%. This represents the first super-human result achieved for this task with a generic model that uses only pixel information.",http://proceedings.mlr.press/v70/shyam17a.html,http://proceedings.mlr.press/v70/shyam17a/shyam17a.pdf,ICML
1465,2017,Coherent Probabilistic Forecasts for Hierarchical Time Series,"Souhaib Ben Taieb,         James W. Taylor,         Rob J. Hyndman","Many applications require forecasts for a hierarchy comprising a set of time series along with aggregates of subsets of these series. Hierarchical forecasting require not only good prediction accuracy at each level of the hierarchy, but also the coherency between different levels — the property that forecasts add up appropriately across the hierarchy. A fundamental limitation of prior research is the focus on forecasting the mean of each time series. We consider the situation where probabilistic forecasts are needed for each series in the hierarchy, and propose an algorithm to compute predictive distributions rather than mean forecasts only. Our algorithm has the advantage of synthesizing information from different levels in the hierarchy through a sparse forecast combination and a probabilistic hierarchical aggregation. We evaluate the accuracy of our forecasting algorithm on both simulated data and large-scale electricity smart meter data. The results show consistent performance gains compared to state-of-the art methods.",http://proceedings.mlr.press/v70/taieb17a.html,http://proceedings.mlr.press/v70/taieb17a/taieb17a.pdf,ICML
1466,2017,Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC,"Yulai Cong,         Bo Chen,         Hongwei Liu,         Mingyuan Zhou","It is challenging to develop stochastic gradient based scalable inference for deep discrete latent variable models (LVMs), due to the difficulties in not only computing the gradients, but also adapting the step sizes to different latent factors and hidden layers. For the Poisson gamma belief network (PGBN), a recently proposed deep discrete LVM, we derive an alternative representation that is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data augmentation and marginalization techniques, we derive a block-diagonal Fisher information matrix and its inverse for the simplex-constrained global model parameters of DLDA. Exploiting that Fisher information matrix with stochastic gradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. State-of-the-art results are demonstrated on big data sets.",http://proceedings.mlr.press/v70/cong17a.html,http://proceedings.mlr.press/v70/cong17a/cong17a.pdf,ICML
1467,2017,High-dimensional Non-Gaussian Single Index Models via Thresholded Score Function Estimation,"Zhuoran Yang,         Krishnakumar Balasubramanian,         Han Liu","We consider estimating the parametric component of single index models in high dimensions. Compared with existing work, we do not require the covariate to be normally distributed. Utilizing Stein’s Lemma, we propose estimators based on the score function of the covariate. Moreover, to handle score function and response variables that are heavy-tailed, our estimators are constructed via carefully thresholding their empirical counterparts. Under a bounded fourth moment condition, we establish optimal statistical rates of convergence for the proposed estimators. Extensive numerical experiments are provided to back up our theory.",http://proceedings.mlr.press/v70/yang17a.html,http://proceedings.mlr.press/v70/yang17a/yang17a.pdf,ICML
1468,2017,Doubly Greedy Primal-Dual Coordinate Descent for Sparse Empirical Risk Minimization,"Qi Lei,         Ian En-Hsu Yen,         Chao-yuan Wu,         Inderjit S. Dhillon,         Pradeep Ravikumar","We consider the popular problem of sparse empirical risk minimization with linear predictors and a large number of both features and observations. With a convex-concave saddle point objective reformulation, we propose a Doubly Greedy Primal-Dual Coordinate Descent algorithm that is able to exploit sparsity in both primal and dual variables. It enjoys a low cost per iteration and our theoretical analysis shows that it converges linearly with a good iteration complexity, provided that the set of primal variables is sparse. We then extend this algorithm further to leverage active sets. The resulting new algorithm is even faster, and experiments on large-scale Multi-class data sets show that our algorithm achieves up to 30 times speedup on several state-of-the-art optimization methods.",http://proceedings.mlr.press/v70/lei17b.html,http://proceedings.mlr.press/v70/lei17b/lei17b.pdf,ICML
1469,2017,Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning,"Oron Anschel,         Nir Baram,         Nahum Shimkin","Instability and variability of Deep Reinforcement Learning (DRL) algorithms tend to adversely affect their performance. Averaged-DQN is a simple extension to the DQN algorithm, based on averaging previously learned Q-values estimates, which leads to a more stable training procedure and improved performance by reducing approximation error variance in the target values. To understand the effect of the algorithm, we examine the source of value function estimation errors and provide an analytical comparison within a simplified model. We further present experiments on the Arcade Learning Environment benchmark that demonstrate significantly improved stability and performance due to the proposed extension.",http://proceedings.mlr.press/v70/anschel17a.html,http://proceedings.mlr.press/v70/anschel17a/anschel17a.pdf,ICML
1470,2017,Recurrent Highway Networks,"Julian Georg Zilly,         Rupesh Kumar Srivastava,         Jan Koutnı́k,         Jürgen Schmidhuber","Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with “deep” transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin’s circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.",http://proceedings.mlr.press/v70/zilly17a.html,http://proceedings.mlr.press/v70/zilly17a/zilly17a.pdf,ICML
1471,2017,Hierarchy Through Composition with Multitask LMDPs,"Andrew M. Saxe,         Adam C. Earle,         Benjamin Rosman","Hierarchical architectures are critical to the scalability of reinforcement learning methods. Most current hierarchical frameworks execute actions serially, with macro-actions comprising sequences of primitive actions. We propose a novel alternative to these control hierarchies based on concurrent execution of many actions in parallel. Our scheme exploits the guaranteed concurrent compositionality provided by the linearly solvable Markov decision process (LMDP) framework, which naturally enables a learning agent to draw on several macro-actions simultaneously to solve new tasks. We introduce the Multitask LMDP module, which maintains a parallel distributed representation of tasks and may be stacked to form deep hierarchies abstracted in space and time.",http://proceedings.mlr.press/v70/saxe17a.html,http://proceedings.mlr.press/v70/saxe17a/saxe17a.pdf,ICML
1472,2017,"The Shattered Gradients Problem: If resnets are the answer, then what is the question?","David Balduzzi,         Marcus Frean,         Lennox Leary,         J. P. Lewis,         Kurt Wan-Duo Ma,         Brian McWilliams","A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new “looks linear” (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.",http://proceedings.mlr.press/v70/balduzzi17b.html,http://proceedings.mlr.press/v70/balduzzi17b/balduzzi17b.pdf,ICML
1473,2017,“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions,"Yair Carmon,         John C. Duchi,         Oliver Hinder,         Aaron Sidford","We develop and analyze a variant of Nesterov’s accelerated gradient descent (AGD) for minimization of smooth non-convex functions. We prove that one of two cases occurs: either our AGD variant converges quickly, as if the function was convex, or we produce a certificate that the function is “guilty” of being non-convex. This non-convexity certificate allows us to exploit negative curvature and obtain deterministic, dimension-free acceleration of convergence for non-convex functions. For a function fff with Lipschitz continuous gradient and Hessian, we compute a point xxx with ∥∇f(x)∥≤ϵ‖∇f(x)‖≤ϵ\|\nabla f(x)\| \le \epsilon in O(ϵ−7/4log(1/ϵ))O(ϵ−7/4log⁡(1/ϵ))O(\epsilon^{-7/4} \log(1/ \epsilon) ) gradient and function evaluations. Assuming additionally that the third derivative is Lipschitz, we require only O(ϵ−5/3log(1/ϵ))O(ϵ−5/3log⁡(1/ϵ))O(\epsilon^{-5/3} \log(1/ \epsilon) ) evaluations.",http://proceedings.mlr.press/v70/carmon17a.html,http://proceedings.mlr.press/v70/carmon17a/carmon17a.pdf,ICML
1474,2017,Adaptive Consensus ADMM for Distributed Optimization,"Zheng Xu,         Gavin Taylor,         Hao Li,         Mário A. T. Figueiredo,         Xiaoming Yuan,         Tom Goldstein","The alternating direction method of multipliers (ADMM) is commonly used for distributed model fitting problems, but its performance and reliability depend strongly on user-defined penalty parameters. We study distributed ADMM methods that boost performance by using different fine-tuned algorithm parameters on each worker node. We present a O(1/k) convergence rate for adaptive ADMM methods with node-specific parameters, and propose adaptive consensus ADMM (ACADMM), which automatically tunes parameters without user oversight.",http://proceedings.mlr.press/v70/xu17c.html,http://proceedings.mlr.press/v70/xu17c/xu17c.pdf,ICML
1475,2017,Uncorrelation and Evenness: a New Diversity-Promoting Regularizer,"Pengtao Xie,         Aarti Singh,         Eric P. Xing","Latent space models (LSMs) provide a principled and effective way to extract hidden patterns from observed data. To cope with two challenges in LSMs: (1) how to capture infrequent patterns when pattern frequency is imbalanced and (2) how to reduce model size without sacrificing their expressiveness, several studies have been proposed to “diversify” LSMs, which design regularizers to encourage the components therein to be “diverse”. In light of the limitations of existing approaches, we design a new diversity-promoting regularizer by considering two factors: uncorrelation and evenness, which encourage the components to be uncorrelated and to play equally important roles in modeling data. Formally, this amounts to encouraging the covariance matrix of the components to have more uniform eigenvalues. We apply the regularizer to two LSMs and develop an efficient optimization algorithm. Experiments on healthcare, image and text data demonstrate the effectiveness of the regularizer.",http://proceedings.mlr.press/v70/xie17b.html,http://proceedings.mlr.press/v70/xie17b/xie17b.pdf,ICML
1476,2017,End-to-End Learning for Structured Prediction Energy Networks,"David Belanger,         Bishan Yang,         Andrew McCallum","Structured Prediction Energy Networks (SPENs) are a simple, yet expressive family of structured prediction models (Belanger and McCallum, 2016). An energy function over candidate structured outputs is given by a deep network, and predictions are formed by gradient-based optimization. This paper presents end-to-end learning for SPENs, where the energy function is discriminatively trained by back-propagating through gradient-based prediction. In our experience, the approach is substantially more accurate than the structured SVM method of Belanger and McCallum (2016), as it allows us to use more sophisticated non-convex energies. We provide a collection of techniques for improving the speed, accuracy, and memory requirements of end-to-end SPENs, and demonstrate the power of our method on 7-Scenes image denoising and CoNLL-2005 semantic role labeling tasks. In both, inexact minimization of non-convex SPEN energies is superior to baseline methods that use simplistic energy functions that can be minimized exactly.",http://proceedings.mlr.press/v70/belanger17a.html,http://proceedings.mlr.press/v70/belanger17a/belanger17a.pdf,ICML
1477,2017,Parseval Networks: Improving Robustness to Adversarial Examples,"Moustapha Cisse,         Piotr Bojanowski,         Edouard Grave,         Yann Dauphin,         Nicolas Usunier","We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than 111. Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation. The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art regarding accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples. Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.",http://proceedings.mlr.press/v70/cisse17a.html,http://proceedings.mlr.press/v70/cisse17a/cisse17a.pdf,ICML
1478,2017,Practical Gauss-Newton Optimisation for Deep Learning,"Aleksandar Botev,         Hippolyt Ritter,         David Barber","We present an efficient block-diagonal approximation to the Gauss-Newton matrix for feedforward neural networks. Our resulting algorithm is competitive against state-of-the-art first-order optimisation methods, with sometimes significant improvement in optimisation performance. Unlike first-order methods, for which hyperparameter tuning of the optimisation parameters is often a laborious process, our approach can provide good performance even when used with default settings. A side result of our work is that for piecewise linear transfer functions, the network objective function can have no differentiable local maxima, which may partially explain why such transfer functions facilitate effective optimisation.",http://proceedings.mlr.press/v70/botev17a.html,http://proceedings.mlr.press/v70/botev17a/botev17a.pdf,ICML
1479,2017,Online Learning to Rank in Stochastic Click Models,"Masrour Zoghi,         Tomas Tunys,         Mohammad Ghavamzadeh,         Branislav Kveton,         Csaba Szepesvari,         Zheng Wen","Online learning to rank is a core problem in information retrieval and machine learning. Many provably efficient algorithms have been recently proposed for this problem in specific click models. The click model is a model of how the user interacts with a list of documents. Though these results are significant, their impact on practice is limited, because all proposed algorithms are designed for specific click models and lack convergence guarantees in other models. In this work, we propose BatchRank, the first online learning to rank algorithm for a broad class of click models. The class encompasses two most fundamental click models, the cascade and position-based models. We derive a gap-dependent upper bound on the T-step regret of BatchRank and evaluate it on a range of web search queries. We observe that BatchRank outperforms ranked bandits and is more robust than CascadeKL-UCB, an existing algorithm for the cascade model.",http://proceedings.mlr.press/v70/zoghi17a.html,http://proceedings.mlr.press/v70/zoghi17a/zoghi17a.pdf,ICML
1480,2017,Conditional Image Synthesis with Auxiliary Classifier GANs,"Augustus Odena,         Christopher Olah,         Jonathon Shlens","In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128×128128×128128\times 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128×128128×128128\times 128 samples are more than twice as discriminable as artificially resized 32×3232×3232\times 32 samples. In addition, 84.7\% of the classes have samples exhibiting diversity comparable to real ImageNet data.",http://proceedings.mlr.press/v70/odena17a.html,http://proceedings.mlr.press/v70/odena17a/odena17a.pdf,ICML
1481,2017,Differentially Private Ordinary Least Squares,Or Sheffet,"Linear regression is one of the most prevalent techniques in machine learning; however, it is also common to use linear regression for its explanatory capabilities rather than label prediction. Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. income) in the presence of other (potentially correlated) features. OLS assumes a particular model that randomly generates the data, and derives t-values — representing the likelihood of each real value to be the true correlation. Using t-values, OLS can release a confidence interval, which is an interval on the reals that is likely to contain the true correlation; and when this interval does not intersect the origin, we can reject the null hypothesis as it is likely that the true correlation is non-zero. Our work aims at achieving similar guarantees on data under differentially private estimators. First, we show that for well-spread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good approximation of t-values; secondly, when JLT approximates Ridge regression (linear regression with l2l2l_2-regularization) we derive, under certain conditions, confidence intervals using the projected data; lastly, we derive, under different conditions, confidence intervals for the “Analyze Gauss” algorithm (Dwork et al 2014).",http://proceedings.mlr.press/v70/sheffet17a.html,http://proceedings.mlr.press/v70/sheffet17a/sheffet17a.pdf,ICML
1482,2017,Distributed Batch Gaussian Process Optimization,"Erik A. Daxberger,         Bryan Kian Hsiang Low","This paper presents a novel distributed batch Gaussian process upper confidence bound (DB-GP-UCB) algorithm for performing batch Bayesian optimization (BO) of highly complex, costly-to-evaluate black-box objective functions. In contrast to existing batch BO algorithms, DB-GP-UCB can jointly optimize a batch of inputs (as opposed to selecting the inputs of a batch one at a time) while still preserving scalability in the batch size. To realize this, we generalize GP-UCB to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent distributed constraint optimization problem in order to fully exploit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size. Our DB-GP-UCB algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order. We provide a theoretical guarantee for the convergence rate of DB-GP-UCB via bounds on its cumulative regret. Empirical evaluation on synthetic benchmark objective functions and a real-world optimization problem shows that DB-GP-UCB outperforms the state-of-the-art batch BO algorithms.",http://proceedings.mlr.press/v70/daxberger17a.html,http://proceedings.mlr.press/v70/daxberger17a/daxberger17a.pdf,ICML
1483,2017,Stochastic Gradient MCMC Methods for Hidden Markov Models,"Yi-An Ma,         Nicholas J. Foti,         Emily B. Fox","Stochastic gradient MCMC (SG-MCMC) algorithms have proven useful in scaling Bayesian inference to large datasets under an assumption of i.i.d data. We instead develop an SG-MCMC algorithm to learn the parameters of hidden Markov models (HMMs) for time-dependent data. There are two challenges to applying SG-MCMC in this setting: The latent discrete states, and needing to break dependencies when considering minibatches. We consider a marginal likelihood representation of the HMM and propose an algorithm that harnesses the inherent memory decay of the process. We demonstrate the effectiveness of our algorithm on synthetic experiments and an ion channel recording data, with runtimes significantly outperforming batch MCMC.",http://proceedings.mlr.press/v70/ma17a.html,http://proceedings.mlr.press/v70/ma17a/ma17a.pdf,ICML
1484,2017,Estimating individual treatment effect: generalization bounds and algorithms,"Uri Shalit,         Fredrik D. Johansson,         David Sontag","There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a “balanced” representation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.",http://proceedings.mlr.press/v70/shalit17a.html,http://proceedings.mlr.press/v70/shalit17a/shalit17a.pdf,ICML
1485,2017,Deep Voice: Real-time Neural Text-to-Speech,"Sercan Ö. Arık,         Mike Chrzanowski,         Adam Coates,         Gregory Diamos,         Andrew Gibiansky,         Yongguo Kang,         Xian Li,         John Miller,         Andrew Ng,         Jonathan Raiman,         Shubho Sengupta,         Mohammad Shoeybi","We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.",http://proceedings.mlr.press/v70/arik17a.html,http://proceedings.mlr.press/v70/arik17a/arik17a.pdf,ICML
1486,2017,Strongly-Typed Agents are Guaranteed to Interact Safely,David Balduzzi,"As artificial agents proliferate, it is becoming increasingly important to ensure that their interactions with one another are well-behaved. In this paper, we formalize a common-sense notion of when algorithms are well-behaved: an algorithm is safe if it does no harm. Motivated by recent progress in deep learning, we focus on the specific case where agents update their actions according to gradient descent. The paper shows that that gradient descent converges to a Nash equilibrium in safe games. The main contribution is to define strongly-typed agents and show they are guaranteed to interact safely, thereby providing sufficient conditions to guarantee safe interactions. A series of examples show that strong-typing generalizes certain key features of convexity, is closely related to blind source separation, and introduces a new perspective on classical multilinear games based on tensor decomposition.",http://proceedings.mlr.press/v70/balduzzi17a.html,http://proceedings.mlr.press/v70/balduzzi17a/balduzzi17a.pdf,ICML
1487,2017,Warped Convolutions: Efficient Invariance to Spatial Transformations,"João F. Henriques,         Andrea Vedaldi","Convolutional Neural Networks (CNNs) are extremely efficient, since they exploit the inherent translation-invariance of natural images. However, translation is just one of a myriad of useful spatial transformations. Can the same efficiency be attained when considering other spatial invariances? Such generalized convolutions have been considered in the past, but at a high computational cost. We present a construction that is simple and exact, yet has the same computational complexity that standard convolutions enjoy. It consists of a constant image warp followed by a simple convolution, which are standard blocks in deep learning toolboxes. With a carefully crafted warp, the resulting architecture can be made equivariant to a wide range of two-parameter spatial transformations. We show encouraging results in realistic scenarios, including the estimation of vehicle poses in the Google Earth dataset (rotation and scale), and face poses in Annotated Facial Landmarks in the Wild (3D rotations under perspective).",http://proceedings.mlr.press/v70/henriques17a.html,http://proceedings.mlr.press/v70/henriques17a/henriques17a.pdf,ICML
1488,2017,Robust Structured Estimation with Single-Index Models,"Sheng Chen,         Arindam Banerjee","In this paper, we investigate general single-index models (SIMs) in high dimensions. Based on U-statistics, we propose two types of robust estimators for the recovery of model parameters, which can be viewed as generalizations of several existing algorithms for one-bit compressed sensing (1-bit CS). With minimal assumption on noise, the statistical guarantees are established for the generalized estimators under suitable conditions, which allow general structures of underlying parameter. Moreover, the proposed estimator is novelly instantiated for SIMs with monotone transfer function, and the obtained estimator can better leverage the monotonicity. Experimental results are provided to support our theoretical analyses.",http://proceedings.mlr.press/v70/chen17a.html,http://proceedings.mlr.press/v70/chen17a/chen17a.pdf,ICML
1489,2017,Deriving Neural Architectures from Sequence and Graph Kernels,"Tao Lei,         Wengong Jin,         Regina Barzilay,         Tommi Jaakkola","The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.",http://proceedings.mlr.press/v70/lei17a.html,http://proceedings.mlr.press/v70/lei17a/lei17a.pdf,ICML
1490,2017,Identifying Best Interventions through Online Importance Sampling,"Rajat Sen,         Karthikeyan Shanmugam,         Alexandros G. Dimakis,         Sanjay Shakkottai","Motivated by applications in computational advertising and systems biology, we consider the problem of identifying the best out of several possible soft interventions at a source node VV in an acyclic causal directed graph, to maximize the expected value of a target node YY (located downstream of VV). Our setting imposes a fixed total budget for sampling under various interventions, along with cost constraints on different types of interventions. We pose this as a best arm identification bandit problem with KK arms, where each arm is a soft intervention at VV and leverage the information leakage among the arms to provide the first gap dependent error and simple regret bounds for this problem. Our results are a significant improvement over the traditional best arm identification results. We empirically show that our algorithms outperform the state of the art in the Flow Cytometry data-set, and also apply our algorithm for model interpretation of the Inception-v3 deep net that classifies images.",http://proceedings.mlr.press/v70/sen17a.html,http://proceedings.mlr.press/v70/sen17a/sen17a.pdf,ICML
1491,2017,Consistency Analysis for Binary Classification Revisited,"Krzysztof Dembczyński,         Wojciech Kotłowski,         Oluwasanmi Koyejo,         Nagarajan Natarajan","Statistical learning theory is at an inflection point enabled by recent advances in understanding and optimizing a wide range of metrics. Of particular interest are non-decomposable metrics such as the F-measure and the Jaccard measure which cannot be represented as a simple average over examples. Non-decomposability is the primary source of difficulty in theoretical analysis, and interestingly has led to two distinct settings and notions of consistency. In this manuscript we analyze both settings, from statistical and algorithmic points of view, to explore the connections and to highlight differences between them for a wide range of metrics. The analysis complements previous results on this topic, clarifies common confusions around both settings, and provides guidance to the theory and practice of binary classification with complex metrics.",http://proceedings.mlr.press/v70/dembczynski17a.html,http://proceedings.mlr.press/v70/dembczynski17a/dembczynski17a.pdf,ICML
1492,2017,The Predictron: End-To-End Learning and Planning,"David Silver,         Hado Hasselt,         Matteo Hessel,         Tom Schaul,         Arthur Guez,         Tim Harley,         Gabriel Dulac-Arnold,         David Reichert,         Neil Rabinowitz,         Andre Barreto,         Thomas Degris","One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple “imagined” planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.",http://proceedings.mlr.press/v70/silver17a.html,http://proceedings.mlr.press/v70/silver17a/silver17a.pdf,ICML
1493,2017,Grammar Variational Autoencoder,"Matt J. Kusner,         Brooks Paige,         José Miguel Hernández-Lobato","Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as natural images, artwork, and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which directly encodes from and decodes to these parse trees, ensuring the generated outputs are always syntactically valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecule generation.",http://proceedings.mlr.press/v70/kusner17a.html,http://proceedings.mlr.press/v70/kusner17a/kusner17a.pdf,ICML
1494,2017,"Collect at Once, Use Effectively: Making Non-interactive Locally Private Learning Possible","Kai Zheng,         Wenlong Mou,         Liwei Wang","Non-interactive Local Differential Privacy (LDP) requires data analysts to collect data from users through noisy channel at once. In this paper, we extend the frontiers of Non-interactive LDP learning and estimation from several aspects. For learning with smooth generalized linear losses, we propose an approximate stochastic gradient oracle estimated from non-interactive LDP channel using Chebyshev expansion, which is combined with inexact gradient methods to obtain an efficient algorithm with quasi-polynomial sample complexity bound. For the high-dimensional world, we discover that under ℓ2ℓ2\ell_2-norm assumption on data points, high-dimensional sparse linear regression and mean estimation can be achieved with logarithmic dependence on dimension, using random projection and approximate recovery. We also extend our methods to Kernel Ridge Regression. Our work is the first one that makes learning and estimation possible for a broad range of learning tasks under non-interactive LDP model.",http://proceedings.mlr.press/v70/zheng17c.html,http://proceedings.mlr.press/v70/zheng17c/zheng17c.pdf,ICML
1495,2017,Dueling Bandits with Weak Regret,"Bangrui Chen,         Peter I. Frazier","We consider online content recommendation with implicit feedback through pairwise comparisons, formalized as the so-called dueling bandit problem. We study the dueling bandit problem in the Condorcet winner setting, and consider two notions of regret: the more well-studied strong regret, which is 0 only when both arms pulled are the Condorcet winner; and the less well-studied weak regret, which is 0 if either arm pulled is the Condorcet winner. We propose a new algorithm for this problem, Winner Stays (WS), with variations for each kind of regret: WS for weak regret (WS-W) has expected cumulative weak regret that is O(N2)O(N2)O(N^2), and O(Nlog(N))O(Nlog⁡(N))O(N\log(N)) if arms have a total order; WS for strong regret (WS-S) has expected cumulative strong regret of O(N2+Nlog(T))O(N2+Nlog⁡(T))O(N^2 + N \log(T)), and O(Nlog(N)+Nlog(T))O(Nlog⁡(N)+Nlog⁡(T))O(N\log(N)+N\log(T)) if arms have a total order. WS-W is the first dueling bandit algorithm with weak regret that is constant in time. WS is simple to compute, even for problems with many arms, and we demonstrate through numerical experiments on simulated and real data that WS has significantly smaller regret than existing algorithms in both the weak- and strong-regret settings.",http://proceedings.mlr.press/v70/chen17c.html,http://proceedings.mlr.press/v70/chen17c/chen17c.pdf,ICML
1496,2017,Faster Greedy MAP Inference for Determinantal Point Processes,"Insu Han,         Prabhanjan Kambadur,         Kyoungsoo Park,         Jinwoo Shin","Determinantal point processes (DPPs) are popular probabilistic models that arise in many machine learning tasks, where distributions of diverse sets are characterized by determinants of their features. In this paper, we develop fast algorithms to find the most likely configuration (MAP) of large-scale DPPs, which is NP-hard in general. Due to the submodular nature of the MAP objective, greedy algorithms have been used with empirical success. Greedy implementations require computation of log-determinants, matrix inverses or solving linear systems at each iteration. We present faster implementations of the greedy algorithms by utilizing the orthogonal benefits of two log-determinant approximation schemes: (a) first-order expansions to the matrix log-determinant function and (b) high-order expansions to the scalar log function with stochastic trace estimators. In our experiments, our algorithms are orders of magnitude faster than their competitors, while sacrificing marginal accuracy.",http://proceedings.mlr.press/v70/han17a.html,http://proceedings.mlr.press/v70/han17a/han17a.pdf,ICML
1497,2017,Active Heteroscedastic Regression,"Kamalika Chaudhuri,         Prateek Jain,         Nagarajan Natarajan","An active learner is given a model class ΘΘ\Theta, a large sample of unlabeled data drawn from an underlying distribution and access to a labeling oracle that can provide a label for any of the unlabeled instances. The goal of the learner is to find a model θ∈Θθ∈Θ\theta \in \Theta that fits the data to a given accuracy while making as few label queries to the oracle as possible. In this work, we consider a theoretical analysis of the label requirement of active learning for regression under a heteroscedastic noise model, where the noise depends on the instance. We provide bounds on the convergence rates of active and passive learning for heteroscedastic regression. Our results illustrate that just like in binary classification, some partial knowledge of the nature of the noise can lead to significant gains in the label requirement of active learning.",http://proceedings.mlr.press/v70/chaudhuri17a.html,http://proceedings.mlr.press/v70/chaudhuri17a/chaudhuri17a.pdf,ICML
1498,2017,Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning,"Yevgen Chebotar,         Karol Hausman,         Marvin Zhang,         Gaurav Sukhatme,         Stefan Schaal,         Sergey Levine","Reinforcement learning algorithms for real-world robotic applications must be able to handle complex, unknown dynamical systems while maintaining data-efficient learning. These requirements are handled well by model-free and model-based RL approaches, respectively. In this work, we aim to combine the advantages of these approaches. By focusing on time-varying linear-Gaussian policies, we enable a model-based algorithm based on the linear-quadratic regulator that can be integrated into the model-free framework of path integral policy improvement. We can further combine our method with guided policy search to train arbitrary parameterized policies such as deep neural networks. Our simulation and real-world experiments demonstrate that this method can solve challenging manipulation tasks with comparable or better performance than model-free methods while maintaining the sample efficiency of model-based methods.",http://proceedings.mlr.press/v70/chebotar17a.html,http://proceedings.mlr.press/v70/chebotar17a/chebotar17a.pdf,ICML
1499,2017,Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks,"Kevin Scaman,         Francis Bach,         Sébastien Bubeck,         Yin Tat Lee,         Laurent Massoulié","In this paper, we determine the optimal convergence rates for strongly convex and smooth distributed optimization in two settings: centralized and decentralized communications over a network. For centralized (i.e. master/slave) algorithms, we show that distributing Nesterov’s accelerated gradient descent is optimal and achieves a precision ε>0ε>0\varepsilon > 0 in time O(κg−−√(1+Δτ)ln(1/ε))O(κg(1+Δτ)ln⁡(1/ε))O(\sqrt{\kappa_g}(1+\Delta\tau)\ln(1/\varepsilon)), where κgκg\kappa_g is the condition number of the (global) function to optimize, ΔΔ\Delta is the diameter of the network, and ττ\tau (resp. 111) is the time needed to communicate values between two neighbors (resp. perform local computations). For decentralized algorithms based on gossip, we provide the first optimal algorithm, called the multi-step dual accelerated (MSDA) method, that achieves a precision ε>0ε>0\varepsilon > 0 in time O(κl−−√(1+τγ√)ln(1/ε))O(κl(1+τγ)ln⁡(1/ε))O(\sqrt{\kappa_l}(1+\frac{\tau}{\sqrt{\gamma}})\ln(1/\varepsilon)), where κlκl\kappa_l is the condition number of the local functions and γγ\gamma is the (normalized) eigengap of the gossip matrix used for communication between nodes. We then verify the efficiency of MSDA against state-of-the-art methods for two problems: least-squares regression and classification by logistic regression.",http://proceedings.mlr.press/v70/scaman17a.html,http://proceedings.mlr.press/v70/scaman17a/scaman17a.pdf,ICML
1500,2017,Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data,"Xixian Chen,         Michael R. Lyu,         Irwin King","Estimating covariance matrices is a fundamental technique in various domains, most notably in machine learning and signal processing. To tackle the challenges of extensive communication costs, large storage capacity requirements, and high processing time complexity when handling massive high-dimensional and distributed data, we propose an efficient and accurate covariance matrix estimation method via data compression. In contrast to previous data-oblivious compression schemes, we leverage a data-aware weighted sampling method to construct low-dimensional data for such estimation. We rigorously prove that our proposed estimator is unbiased and requires smaller data to achieve the same accuracy with specially designed sampling distributions. Besides, we depict that the computational procedures in our algorithm are efficient. All achievements imply an improved tradeoff between the estimation accuracy and computational costs. Finally, the extensive experiments on synthetic and real-world datasets validate the superior property of our method and illustrate that it significantly outperforms the state-of-the-art algorithms.",http://proceedings.mlr.press/v70/chen17g.html,http://proceedings.mlr.press/v70/chen17g/chen17g.pdf,ICML
1501,2017,Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression,"Pengfei Wei,         Ramon Sagarna,         Yiping Ke,         Yew-Soon Ong,         Chi-Keong Goh","A key challenge in multi-source transfer learning is to capture the diverse inter-domain similarities. In this paper, we study different approaches based on Gaussian process models to solve the multi-source transfer regression problem. Precisely, we first investigate the feasibility and performance of a family of transfer covariance functions that represent the pairwise similarity of each source and the target domain. We theoretically show that using such a transfer covariance function for general Gaussian process modelling can only capture the same similarity coefficient for all the sources, and thus may result in unsatisfactory transfer performance. This leads us to propose TCMSMS_{MS}Stack, an integrated strategy incorporating the benefits of the transfer covariance function and stacking. Extensive experiments on one synthetic and two real-world datasets, with learning settings of up to 11 sources for the latter, demonstrate the effectiveness of our proposed TCMSMS_{MS}Stack.",http://proceedings.mlr.press/v70/wei17a.html,http://proceedings.mlr.press/v70/wei17a/wei17a.pdf,ICML
1502,2017,Variational Policy for Guiding Point Processes,"Yichen Wang,         Grady Williams,         Evangelos Theodorou,         Le Song","Temporal point processes have been widely applied to model event sequence data generated by online users. In this paper, we consider the problem of how to design the optimal control policy for point processes, such that the stochastic system driven by the point process is steered to a target state. In particular, we exploit the key insight to view the stochastic optimal control problem from the perspective of optimal measure and variational inference. We further propose a convex optimization framework and an efficient algorithm to update the policy adaptively to the current system state. Experiments on synthetic and real-world data show that our algorithm can steer the user activities much more accurately and efficiently than other stochastic control methods.",http://proceedings.mlr.press/v70/wang17k.html,http://proceedings.mlr.press/v70/wang17k/wang17k.pdf,ICML
1503,2017,Learning Discrete Representations via Information Maximizing Self-Augmented Training,"Weihua Hu,         Takeru Miyato,         Seiya Tokui,         Eiichi Matsumoto,         Masashi Sugiyama","Learning discrete representations of data is a central machine learning task because of the compactness of the representations and ease of interpretation. The task includes clustering and hash learning as special cases. Deep neural networks are promising to be used because they can model the non-linearity of data and scale to large datasets. However, their model complexity is huge, and therefore, we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for applications of interest. To this end, we propose a method called Information Maximizing Self-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose the invariance on discrete representations. More specifically, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion. At the same time, we maximize the information-theoretic dependency between data and their predicted discrete representations. Extensive experiments on benchmark datasets show that IMSAT produces state-of-the-art results for both clustering and unsupervised hash learning.",http://proceedings.mlr.press/v70/hu17b.html,http://proceedings.mlr.press/v70/hu17b/hu17b.pdf,ICML
1504,2017,A Unified Variance Reduction-Based Framework for Nonconvex Low-Rank Matrix Recovery,"Lingxiao Wang,         Xiao Zhang,         Quanquan Gu","We propose a generic framework based on a new stochastic variance-reduced gradient descent algorithm for accelerating nonconvex low-rank matrix recovery. Starting from an appropriate initial estimator, our proposed algorithm performs projected gradient descent based on a novel semi-stochastic gradient specifically designed for low-rank matrix recovery. Based upon the mild restricted strong convexity and smoothness conditions, we derive a projected notion of the restricted Lipschitz continuous gradient property, and prove that our algorithm enjoys linear convergence rate to the unknown low-rank matrix with an improved computational complexity. Moreover, our algorithm can be employed to both noiseless and noisy observations, where the (near) optimal sample complexity and statistical rate can be attained respectively. We further illustrate the superiority of our generic framework through several specific examples, both theoretically and experimentally.",http://proceedings.mlr.press/v70/wang17n.html,http://proceedings.mlr.press/v70/wang17n/wang17n.pdf,ICML
1505,2017,Gradient Boosted Decision Trees for High Dimensional Sparse Output,"Si Si,         Huan Zhang,         S. Sathiya Keerthi,         Dhruv Mahajan,         Inderjit S. Dhillon,         Cho-Jui Hsieh","In this paper, we study the gradient boosted decision trees (GBDT) when the output space is high dimensional and sparse. For example, in multilabel classification, the output space is a LLL-dimensional 0/1 vector, where LLL is number of labels that can grow to millions and beyond in many modern applications. We show that vanilla GBDT can easily run out of memory or encounter near-forever running time in this regime, and propose a new GBDT variant, GBDT-SPARSE, to resolve this problem by employing L0L0L_0 regularization. We then discuss in detail how to utilize this sparsity to conduct GBDT training, including splitting the nodes, computing the sparse residual, and predicting in sublinear time. Finally, we apply our algorithm to extreme multilabel classification problems, and show that the proposed GBDT-SPARSE achieves an order of magnitude improvements in model size and prediction time over existing methods, while yielding similar performance.",http://proceedings.mlr.press/v70/si17a.html,http://proceedings.mlr.press/v70/si17a/si17a.pdf,ICML
1506,2017,Measuring Sample Quality with Kernels,"Jackson Gorham,         Lester Mackey","Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points. We develop a theory of weak convergence for KSDs based on Stein’s method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement.",http://proceedings.mlr.press/v70/gorham17a.html,http://proceedings.mlr.press/v70/gorham17a/gorham17a.pdf,ICML
1507,2017,Rule-Enhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement,"Jonathan Eckstein,         Noam Goldberg,         Ai Kagawa","We describe a learning procedure enhancing L1-penalized regression by adding dynamically generated rules describing multidimensional “box” sets. Our rule-adding procedure is based on the classical column generation method for high-dimensional linear programming. The pricing problem for our column generation procedure reduces to the NP-hard rectangular maximum agreement (RMA) problem of finding a box that best discriminates between two weighted datasets. We solve this problem exactly using a parallel branch-and-bound procedure. The resulting rule-enhanced regression procedure is computation-intensive, but has promising prediction performance.",http://proceedings.mlr.press/v70/eckstein17a.html,http://proceedings.mlr.press/v70/eckstein17a/eckstein17a.pdf,ICML
1508,2017,Beyond Filters: Compact Feature Map for Portable Deep Model,"Yunhe Wang,         Chang Xu,         Chao Xu,         Dacheng Tao","Convolutional neural networks (CNNs) have shown extraordinary performance in a number of applications, but they are usually of heavy design for the accuracy reason. Beyond compressing the filters in CNNs, this paper focuses on the redundancy in the feature maps derived from the large number of filters in a layer. We propose to extract intrinsic representation of the feature maps and preserve the discriminability of the features. Circulant matrix is employed to formulate the feature map transformation, which only requires O(dlog d) computation complexity to embed a d-dimensional feature map. The filter is then re-configured to establish the mapping from original input to the new compact feature map, and the resulting network can preserve intrinsic information of the original network with significantly fewer parameters, which not only decreases the online memory for launching CNN but also accelerates the computation speed. Experiments on benchmark image datasets demonstrate the superiority of the proposed algorithm over state-of-the-art methods.",http://proceedings.mlr.press/v70/wang17m.html,http://proceedings.mlr.press/v70/wang17m/wang17m.pdf,ICML
1509,2017,Latent Feature Lasso,"Ian En-Hsu Yen,         Wei-Cheng Lee,         Sung-En Chang,         Arun Sai Suggala,         Shou-De Lin,         Pradeep Ravikumar","The latent feature model (LFM), proposed in (Griffiths \& Ghahramani, 2005), but possibly with earlier origins, is a generalization of a mixture model, where each instance is generated not from a single latent class but from a combination of latent features. Thus, each instance has an associated latent binary feature incidence vector indicating the presence or absence of a feature. Due to its combinatorial nature, inference of LFMs is considerably intractable, and accordingly, most of the attention has focused on nonparametric LFMs, with priors such as the Indian Buffet Process (IBP) on infinite binary matrices. Recent efforts to tackle this complexity either still have computational complexity that is exponential, or sample complexity that is high-order polynomial w.r.t. the number of latent features. In this paper, we address this outstanding problem of tractable estimation of LFMs via a novel atomic-norm regularization, which gives an algorithm with polynomial run-time and sample complexity without impractical assumptions on the data distribution.",http://proceedings.mlr.press/v70/yen17a.html,http://proceedings.mlr.press/v70/yen17a/yen17a.pdf,ICML
1510,2017,Stochastic Bouncy Particle Sampler,"Ari Pakman,         Dar Gilboa,         David Carlson,         Liam Paninski","We introduce a stochastic version of the non-reversible, rejection-free Bouncy Particle Sampler (BPS), a Markov process whose sample trajectories are piecewise linear, to efficiently sample Bayesian posteriors in big datasets. We prove that in the BPS no bias is introduced by noisy evaluations of the log-likelihood gradient. On the other hand, we argue that efficiency considerations favor a small, controllable bias, in exchange for faster mixing. We introduce a simple method that controls this trade-off. We illustrate these ideas in several examples which outperform previous approaches.",http://proceedings.mlr.press/v70/pakman17a.html,http://proceedings.mlr.press/v70/pakman17a/pakman17a.pdf,ICML
1511,2017,Asynchronous Distributed Variational Gaussian Process for Regression,"Hao Peng,         Shandian Zhe,         Xiao Zhang,         Yuan Qi","Gaussian processes (GPs) are powerful non-parametric function estimators. However, their applications are largely limited by the expensive computational cost of the inference procedures. Existing stochastic or distributed synchronous variational inferences, although have alleviated this issue by scaling up GPs to millions of samples, are still far from satisfactory for real-world large applications, where the data sizes are often orders of magnitudes larger, say, billions. To solve this problem, we propose ADVGP, the first Asynchronous Distributed Variational Gaussian Process inference for regression, on the recent large-scale machine learning platform, PARAMETER SERVER. ADVGP uses a novel, flexible variational framework based on a weight space augmentation, and implements the highly efficient, asynchronous proximal gradient optimization. While maintaining comparable or better predictive performance, ADVGP greatly improves upon the efficiency of the existing variational methods. With ADVGP, we effortlessly scale up GP regression to a real-world application with billions of samples and demonstrate an excellent, superior prediction accuracy to the popular linear models.",http://proceedings.mlr.press/v70/peng17a.html,http://proceedings.mlr.press/v70/peng17a/peng17a.pdf,ICML
1512,2017,Exact MAP Inference by Avoiding Fractional Vertices,"Erik M. Lindgren,         Alexandros G. Dimakis,         Adam Klivans","Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice and it is a major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time—we require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.",http://proceedings.mlr.press/v70/lindgren17a.html,http://proceedings.mlr.press/v70/lindgren17a/lindgren17a.pdf,ICML
1513,2017,Robust Probabilistic Modeling with Bayesian Data Reweighting,"Yixin Wang,         Alp Kucukelbir,         David M. Blei","Probabilistic models analyze data by relying on a set of assumptions. Data that exhibit deviations from these assumptions can undermine inference and prediction quality. Robust models offer protection against mismatch between a model’s assumptions and reality. We propose a way to systematically detect and mitigate mismatch of a large class of probabilistic models. The idea is to raise the likelihood of each observation to a weight and then to infer both the latent variables and the weights from data. Inferring the weights allows a model to identify observations that match its assumptions and down-weight others. This enables robust inference and improves predictive accuracy. We study four different forms of mismatch with reality, ranging from missing latent groups to structure misspecification. A Poisson factorization analysis of the Movielens 1M dataset shows the benefits of this approach in a practical scenario.",http://proceedings.mlr.press/v70/wang17g.html,http://proceedings.mlr.press/v70/wang17g/wang17g.pdf,ICML
1514,2017,Online Learning with Local Permutations and Delayed Feedback,"Ohad Shamir,         Liran Szlak","We propose an Online Learning with Local Permutations (OLLP) setting, in which the learner is allowed to slightly permute the order of the loss functions generated by an adversary. On one hand, this models natural situations where the exact order of the learner’s responses is not crucial, and on the other hand, might allow better learning and regret performance, by mitigating highly adversarial loss sequences. Also, with random permutations, this can be seen as a setting interpolating between adversarial and stochastic losses. In this paper, we consider the applicability of this setting to convex online learning with delayed feedback, in which the feedback on the prediction made in round ttt arrives with some delay ττ\tau. With such delayed feedback, the best possible regret bound is well-known to be O(√τT)O(τT−−−√)O(\sqrt{\tau T}). We prove that by being able to permute losses by a distance of at most MMM (for M≥τM≥τM\geq \tau), the regret can be improved to O(√T(1+√τ2/M))O(T−−√(1+τ2/M−−−−−√))O(\sqrt{T}(1+\sqrt{\tau^2/M})), using a Mirror-Descent based algorithm which can be applied for both Euclidean and non-Euclidean geometries. We also prove a lower bound, showing that for M<τ/3M<τ/3M<\tau/3, it is impossible to improve the standard O(√τT)O(τT−−−√)O(\sqrt{\tau T}) regret bound by more than constant factors. Finally, we provide some experiments validating the performance of our algorithm.",http://proceedings.mlr.press/v70/shamir17a.html,http://proceedings.mlr.press/v70/shamir17a/shamir17a.pdf,ICML
1515,2017,Confident Multiple Choice Learning,"Kimin Lee,         Changho Hwang,         KyoungSoo Park,         Jinwoo Shin","Ensemble methods are arguably the most trustworthy techniques for boosting the performance of machine learning models. Popular independent ensembles (IE) relying on naive averaging/voting scheme have been of typical choice for most applications involving deep neural networks, but they do not consider advanced collaboration among ensemble models. In this paper, we propose new ensemble methods specialized for deep neural networks, called confident multiple choice learning (CMCL): it is a variant of multiple choice learning (MCL) via addressing its overconfidence issue.In particular, the proposed major components of CMCL beyond the original MCL scheme are (i) new loss, i.e., confident oracle loss, (ii) new architecture, i.e., feature sharing and (iii) new training method, i.e., stochastic labeling. We demonstrate the effect of CMCL via experiments on the image classification on CIFAR and SVHN, and the foreground-background segmentation on the iCoseg. In particular, CMCL using 5 residual networks provides 14.05\% and 6.60\% relative reductions in the top-1 error rates from the corresponding IE scheme for the classification task on CIFAR and SVHN, respectively.",http://proceedings.mlr.press/v70/lee17b.html,http://proceedings.mlr.press/v70/lee17b/lee17b.pdf,ICML
1516,2017,Dual Supervised Learning,"Yingce Xia,         Tao Qin,         Wei Chen,         Jiang Bian,         Nenghai Yu,         Tie-Yan Liu","Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recognition vs. text to speech, and image classification vs. image generation. Two dual tasks have intrinsic connections with each other due to the probabilistic correlation between their models. This connection is, however, not effectively utilized today, since people usually train the models of two dual tasks separately and independently. In this work, we propose training the models of two dual tasks simultaneously, and explicitly exploiting the probabilistic correlation between them to regularize the training process. For ease of reference, we call the proposed approach dual supervised learning. We demonstrate that dual supervised learning can improve the practical performances of both tasks, for various applications including machine translation, image processing, and sentiment analysis.",http://proceedings.mlr.press/v70/xia17a.html,http://proceedings.mlr.press/v70/xia17a/xia17a.pdf,ICML
1517,2017,Differentially Private Learning of Undirected Graphical Models Using Collective Graphical Models,"Garrett Bernstein,         Ryan McKenna,         Tao Sun,         Daniel Sheldon,         Michael Hay,         Gerome Miklau","We investigate the problem of learning discrete graphical models in a differentially private way. Approaches to this problem range from privileged algorithms that conduct learning completely behind the privacy barrier to schemes that release private summary statistics paired with algorithms to learn parameters from those statistics. We show that the approach of releasing noisy sufficient statistics using the Laplace mechanism achieves a good trade-off between privacy, utility, and practicality. A naive learning algorithm that uses the noisy sufficient statistics “as is” outperforms general-purpose differentially private learning algorithms. However, it has three limitations: it ignores knowledge about the data generating process, rests on uncertain theoretical foundations, and exhibits certain pathologies. We develop a more principled approach that applies the formalism of collective graphical models to perform inference over the true sufficient statistics within an expectation-maximization framework. We show that this learns better models than competing approaches on both synthetic data and on real human mobility data used as a case study.",http://proceedings.mlr.press/v70/bernstein17a.html,http://proceedings.mlr.press/v70/bernstein17a/bernstein17a.pdf,ICML
1518,2017,Stochastic Gradient Monomial Gamma Sampler,"Yizhe Zhang,         Changyou Chen,         Zhe Gan,         Ricardo Henao,         Lawrence Carin","Scaling Markov Chain Monte Carlo (MCMC) to estimate posterior distributions from large datasets has been made possible as a result of advances in stochastic gradient techniques. Despite their success, mixing performance of existing methods when sampling from multimodal distributions can be less efficient with insufficient Monte Carlo samples; this is evidenced by slow convergence and insufficient exploration of posterior distributions. We propose a generalized framework to improve the sampling efficiency of stochastic gradient MCMC, by leveraging a generalized kinetics that delivers superior stationary mixing, especially in multimodal distributions, and propose several techniques to overcome the practical issues. We show that the proposed approach is better at exploring a complicated multimodal posterior distribution, and demonstrate improvements over other stochastic gradient MCMC methods on various applications.",http://proceedings.mlr.press/v70/zhang17a.html,http://proceedings.mlr.press/v70/zhang17a/zhang17a.pdf,ICML
1519,2017,Oracle Complexity of Second-Order Methods for Finite-Sum Problems,"Yossi Arjevani,         Ohad Shamir","Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in second-order methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer – perhaps surprisingly – is negative, at least in terms of worst-case guarantees. However, we also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result.",http://proceedings.mlr.press/v70/arjevani17a.html,http://proceedings.mlr.press/v70/arjevani17a/arjevani17a.pdf,ICML
1520,2017,Learning Sleep Stages from Radio Signals: A Conditional Adversarial Architecture,"Mingmin Zhao,         Shichao Yue,         Dina Katabi,         Tommi S. Jaakkola,         Matt T. Bianchi","We focus on predicting sleep stages from radio measurements without any attached sensors on subjects. We introduce a new predictive model that combines convolutional and recurrent neural networks to extract sleep-specific subject-invariant features from RF signals and capture the temporal progression of sleep. A key innovation underlying our approach is a modified adversarial training regime that discards extraneous information specific to individuals or measurement conditions, while retaining all information relevant to the predictive task. We analyze our game theoretic setup and empirically demonstrate that our model achieves significant improvements over state-of-the-art solutions.",http://proceedings.mlr.press/v70/zhao17d.html,http://proceedings.mlr.press/v70/zhao17d/zhao17d.pdf,ICML
1521,2017,Model-Independent Online Learning for Influence Maximization,"Sharan Vaswani,         Branislav Kveton,         Zheng Wen,         Mohammad Ghavamzadeh,         Laks V. S. Lakshmanan,         Mark Schmidt","We consider influence maximization (IM) in social networks, which is the problem of maximizing the number of users that become aware of a product by selecting a set of “seed” users to expose the product to. While prior work assumes a known model of information diffusion, we propose a novel parametrization that not only makes our framework agnostic to the underlying diffusion model, but also statistically efficient to learn from data. We give a corresponding monotone, submodular surrogate function, and show that it is a good approximation to the original IM objective. We also consider the case of a new marketer looking to exploit an existing social network, while simultaneously learning the factors governing information propagation. For this, we propose a pairwise-influence semi-bandit feedback model and develop a LinUCB-based bandit algorithm. Our model-independent analysis shows that our regret bound has a better (as compared to previous work) dependence on the size of the network. Experimental evaluation suggests that our framework is robust to the underlying diffusion model and can efficiently learn a near-optimal solution.",http://proceedings.mlr.press/v70/vaswani17a.html,http://proceedings.mlr.press/v70/vaswani17a/vaswani17a.pdf,ICML
1522,2017,Sharp Minima Can Generalize For Deep Nets,"Laurent Dinh,         Razvan Pascanu,         Samy Bengio,         Yoshua Bengio","Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g.\ Hochreiter \& Schmidhuber (1997); Keskar et al.\ (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Or, depending on the definition of flatness, it is the same for any given minimum. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.",http://proceedings.mlr.press/v70/dinh17b.html,http://proceedings.mlr.press/v70/dinh17b/dinh17b.pdf,ICML
1523,2017,Scalable Generative Models for Multi-label Learning with Missing Labels,"Vikas Jain,         Nirbhay Modhe,         Piyush Rai","We present a scalable, generative framework for multi-label learning with missing labels. Our framework consists of a latent factor model for the binary label matrix, which is coupled with an exposure model to account for label missingness (i.e., whether a zero in the label matrix is indeed a zero or denotes a missing observation). The underlying latent factor model also assumes that the low-dimensional embeddings of each label vector are directly conditioned on the respective feature vector of that example. Our generative framework admits a simple inference procedure, such that the parameter estimation reduces to a sequence of simple weighted least-square regression problems, each of which can be solved easily, efficiently, and in parallel. Moreover, inference can also be performed in an online fashion using mini-batches of training examples, which makes our framework scalable for large data sets, even when using moderate computational resources. We report both quantitative and qualitative results for our framework on several benchmark data sets, comparing it with a number of state-of-the-art methods.",http://proceedings.mlr.press/v70/jain17a.html,http://proceedings.mlr.press/v70/jain17a/jain17a.pdf,ICML
1524,2017,Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections,"Zakaria Mhammedi,         Andrew Hellicar,         Ashfaqur Rahman,         James Bailey","The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training which ensures that its norm is equal to one and prevents exploding gradients. These methods either have limited expressiveness or scale poorly with the size of the network when compared with the simple RNN case, especially when using stochastic gradient descent with a small mini-batch size. Our contributions are as follows; we first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the matrix is always orthogonal. Our results show that the orthogonal constraint on the transition matrix applied through our parametrisation gives similar benefits to the unitary constraint, without the time complexity limitations.",http://proceedings.mlr.press/v70/mhammedi17a.html,http://proceedings.mlr.press/v70/mhammedi17a/mhammedi17a.pdf,ICML
1525,2017,Optimal and Adaptive Off-policy Evaluation in Contextual Bandits,"Yu-Xiang Wang,         Alekh Agarwal,         Miroslav Dudı́k","We study the off-policy evaluation problem—estimating the value of a target policy using data collected by another policy—under the contextual bandit model. We consider the general (agnostic) setting without access to a consistent model of rewards and establish a minimax lower bound on the mean squared error (MSE). The bound is matched up to constants by the inverse propensity scoring (IPS) and doubly robust (DR) estimators. This highlights the difficulty of the agnostic contextual setting, in contrast with multi-armed bandits and contextual bandits with access to a consistent reward model, where IPS is suboptimal. We then propose the SWITCH estimator, which can use an existing reward model (not necessarily consistent) to achieve a better bias-variance tradeoff than IPS and DR. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of datasets, often outperforming prior work by orders of magnitude.",http://proceedings.mlr.press/v70/wang17a.html,http://proceedings.mlr.press/v70/wang17a/wang17a.pdf,ICML
1526,2017,Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering,"Bo Yang,         Xiao Fu,         Nicholas D. Sidiropoulos,         Mingyi Hong","Most learning approaches treat dimensionality reduction (DR) and clustering separately (i.e., sequentially), but recent research has shown that optimizing the two tasks jointly can substantially improve the performance of both. The premise behind the latter genre is that the data samples are obtained via linear transformation of latent representations that are easy to cluster; but in practice, the transformation from the latent space to the data can be more complicated. In this work, we assume that this transformation is an unknown and possibly nonlinear function. To recover the `clustering-friendly’ latent representations and to better cluster the data, we propose a joint DR and K-means clustering approach in which DR is accomplished via learning a deep neural network (DNN). The motivation is to keep the advantages of jointly optimizing the two tasks, while exploiting the deep neural network’s ability to approximate any nonlinear function. This way, the proposed approach can work well for a broad class of generative models. Towards this end, we carefully design the DNN structure and the associated joint optimization criterion, and propose an effective and scalable algorithm to handle the formulated optimization problem. Experiments using different real datasets are employed to showcase the effectiveness of the proposed approach.",http://proceedings.mlr.press/v70/yang17b.html,http://proceedings.mlr.press/v70/yang17b/yang17b.pdf,ICML
1527,2017,Algebraic Variety Models for High-Rank Matrix Completion,"Greg Ongie,         Rebecca Willett,         Robert D. Nowak,         Laura Balzano","We consider a non-linear generalization of low-rank matrix completion to the case where the data belongs to an algebraic variety, i.e., each data point is a solution to a system of polynomial equations. In this case the original matrix is possibly high-rank, but it becomes low-rank after mapping each column to a higher dimensional space of monomial features. Algebraic varieties capture a range of well-studied linear models, including affine subspaces and their union, but also quadratic and higher degree curves and surfaces. We study the sampling requirements for a general variety model with a focus on the union of affine subspaces. We propose an efficient matrix completion algorithm that minimizes a convex or non-convex surrogate of the rank of the lifted matrix. Our algorithm uses the well-known “kernel trick” to avoid working directly with the high-dimensional lifted data matrix and scales efficiently with data size. We show the proposed algorithm is able to recover synthetically generated data up to the predicted sampling complexity bounds. The algorithm also outperforms standard techniques in experiments with real data.",http://proceedings.mlr.press/v70/ongie17a.html,http://proceedings.mlr.press/v70/ongie17a/ongie17a.pdf,ICML
1528,2017,StingyCD: Safely Avoiding Wasteful Updates in Coordinate Descent,"Tyler B. Johnson,         Carlos Guestrin","Coordinate descent (CD) is a scalable and simple algorithm for solving many optimization problems in machine learning. Despite this fact, CD can also be very computationally wasteful. Due to sparsity in sparse regression problems, for example, the majority of CD updates often result in no progress toward the solution. To address this inefficiency, we propose a modified CD algorithm named “StingyCD.” By skipping over many updates that are guaranteed to not decrease the objective value, StingyCD significantly reduces convergence times. Since StingyCD only skips updates with this guarantee, however, StingyCD does not fully exploit the problem’s sparsity. For this reason, we also propose StingyCD+, an algorithm that achieves further speed-ups by skipping updates more aggressively. Since StingyCD and StingyCD+ rely on simple modifications to CD, it is also straightforward to use these algorithms with other approaches to scaling optimization. In empirical comparisons, StingyCD and StingyCD+ improve convergence times considerably for several L1-regularized optimization problems.",http://proceedings.mlr.press/v70/johnson17a.html,http://proceedings.mlr.press/v70/johnson17a/johnson17a.pdf,ICML
1529,2017,"An Efficient, Sparsity-Preserving, Online Algorithm for Low-Rank Approximation","David Anderson,         Ming Gu","Low-rank matrix approximation is a fundamental tool in data analysis for processing large datasets, reducing noise, and finding important signals. In this work, we present a novel truncated LU factorization called Spectrum-Revealing LU (SRLU) for effective low-rank matrix approximation, and develop a fast algorithm to compute an SRLU factorization. We provide both matrix and singular value approximation error bounds for the SRLU approximation computed by our algorithm. Our analysis suggests that SRLU is competitive with the best low-rank matrix approximation methods, deterministic or randomized, in both computational complexity and approximation quality. Numeric experiments illustrate that SRLU preserves sparsity, highlights important data features and variables, can be efficiently updated, and calculates data approximations nearly as accurately as the best possible. To the best of our knowledge this is the first practical variant of the LU factorization for effective and efficient low-rank matrix approximation.",http://proceedings.mlr.press/v70/anderson17a.html,http://proceedings.mlr.press/v70/anderson17a/anderson17a.pdf,ICML
1530,2017,Global optimization of Lipschitz functions,"Cédric Malherbe,         Nicolas Vayatis","The goal of the paper is to design sequential strategies which lead to efficient optimization of an unknown function under the only assumption that it has a finite Lipschitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a first algorithm called LIPO which assumes the Lipschitz constant to be known. Consistency, minimax rates for LIPO are proved, as well as fast rates under an additional Hölder like condition. An adaptive version of LIPO is also introduced for the more realistic setup where Lipschitz constant is unknown and has to be estimated along with the optimization. Similar theoretical guarantees are shown to hold for the adaptive LIPO algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with respect to state-of-the-art methods over typical benchmark problems for global optimization.",http://proceedings.mlr.press/v70/malherbe17a.html,http://proceedings.mlr.press/v70/malherbe17a/malherbe17a.pdf,ICML
1531,2017,Active Learning for Top-KKK Rank Aggregation from Noisy Comparisons,"Soheil Mohajer,         Changho Suh,         Adel Elmahdy","We explore an active top-KKK ranking problem based on pairwise comparisons that are collected possibly in a sequential manner as per our design choice. We consider two settings: (1) top-KKK sorting in which the goal is to recover the top-KKK items in order out of nnn items; (2) top-KKK partitioning where only the set of top-KKK items is desired. Under a fairly general model which subsumes as special cases various models (e.g., Strong Stochastic Transitivity model, BTL model and uniform noise model), we characterize upper bounds on the sample size required for top-KKK sorting as well as for top-KKK partitioning. As a consequence, we demonstrate that active ranking can offer significant multiplicative gains in sample complexity over passive ranking. Depending on the underlying stochastic noise model, such gain varies from around lognloglognlognloglogn\frac{\log n}{\log \log n} to n2lognloglognn2lognloglogn\frac{ n^2 \log n }{\log \log n}. We also present an algorithm that is applicable to both settings.",http://proceedings.mlr.press/v70/mohajer17a.html,http://proceedings.mlr.press/v70/mohajer17a/mohajer17a.pdf,ICML
1532,2017,A Laplacian Framework for Option Discovery in Reinforcement Learning,"Marlos C. Machado,         Marc G. Bellemare,         Michael Bowling","Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment’s rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.",http://proceedings.mlr.press/v70/machado17a.html,http://proceedings.mlr.press/v70/machado17a/machado17a.pdf,ICML
1533,2017,End-to-End Differentiable Adversarial Imitation Learning,"Nir Baram,         Oron Anschel,         Itai Caspi,         Shie Mannor","Generative Adversarial Networks (GANs) have been successfully applied to the problem of policy imitation in a model-free setup. However, the computation graph of GANs, that include a stochastic policy as the generative model, is no longer differentiable end-to-end, which requires the use of high-variance gradient estimation. In this paper, we introduce the Model-based Generative Adversarial Imitation Learning (MGAIL) algorithm. We show how to use a forward model to make the computation fully differentiable, which enables training policies using the exact gradient of the discriminator. The resulting algorithm trains competent policies using relatively fewer expert samples and interactions with the environment. We test it on both discrete and continuous action domains and report results that surpass the state-of-the-art.",http://proceedings.mlr.press/v70/baram17a.html,http://proceedings.mlr.press/v70/baram17a/baram17a.pdf,ICML
1534,2017,McGan: Mean and Covariance Feature Matching GAN,"Youssef Mroueh,         Tom Sercu,         Vaibhava Goel","We introduce new families of Integral Probability Metrics (IPM) for training Generative Adversarial Networks (GAN). Our IPMs are based on matching statistics of distributions embedded in a finite dimensional feature space. Mean and covariance feature matching IPMs allow for stable training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions.",http://proceedings.mlr.press/v70/mroueh17a.html,http://proceedings.mlr.press/v70/mroueh17a/mroueh17a.pdf,ICML
1535,2017,Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo,Matthew D. Hoffman,"Deep latent Gaussian models are powerful and popular probabilistic models of high-dimensional data. These models are almost always fit using variational expectation-maximization, an approximation to true maximum-marginal-likelihood estimation. In this paper, we propose a different approach: rather than use a variational approximation (which produces biased gradient signals), we use Markov chain Monte Carlo (MCMC, which allows us to trade bias for computation). We find that our MCMC-based approach has several advantages: it yields higher held-out likelihoods, produces sharper images, and does not suffer from the variational overpruning effect. MCMC’s additional computational overhead proves to be significant, but not prohibitive.",http://proceedings.mlr.press/v70/hoffman17a.html,http://proceedings.mlr.press/v70/hoffman17a/hoffman17a.pdf,ICML
1536,2017,Meritocratic Fairness for Cross-Population Selection,"Michael Kearns,         Aaron Roth,         Zhiwei Steven Wu","We consider the problem of selecting a strong pool of individuals from several populations with incomparable skills (e.g. soccer players, mathematicians, and singers) in a fair manner. The quality of an individual is defined to be their relative rank (by cumulative distribution value) within their own population, which permits cross-population comparisons. We study algorithms which attempt to select the highest quality subset despite the fact that true CDF values are not known, and can only be estimated from the finite pool of candidates. Specifically, we quantify the regret in quality imposed by “meritocratic” notions of fairness, which require that individuals are selected with probability that is monotonically increasing in their true quality. We give algorithms with provable fairness and regret guarantees, as well as lower bounds, and provide empirical results which suggest that our algorithms perform better than the theory suggests. We extend our results to a sequential batch setting, in which an algorithm must repeatedly select subsets of individuals from new pools of applicants, but has the benefit of being able to compare them to the accumulated data from previous rounds.",http://proceedings.mlr.press/v70/kearns17a.html,http://proceedings.mlr.press/v70/kearns17a/kearns17a.pdf,ICML
1537,2017,Multiplicative Normalizing Flows for Variational Bayesian Neural Networks,"Christos Louizos,         Max Welling",We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.,http://proceedings.mlr.press/v70/louizos17a.html,http://proceedings.mlr.press/v70/louizos17a/louizos17a.pdf,ICML
1538,2017,Leveraging Node Attributes for Incomplete Relational Data,"He Zhao,         Lan Du,         Wray Buntine","Relational data are usually highly incomplete in practice, which inspires us to leverage side information to improve the performance of community detection and link prediction. This paper presents a Bayesian probabilistic approach that incorporates various kinds of node attributes encoded in binary form in relational models with Poisson likelihood. Our method works flexibly with both directed and undirected relational networks. The inference can be done by efficient Gibbs sampling which leverages sparsity of both networks and node attributes. Extensive experiments show that our models achieve the state-of-the-art link prediction results, especially with highly incomplete relational data.",http://proceedings.mlr.press/v70/zhao17a.html,http://proceedings.mlr.press/v70/zhao17a/zhao17a.pdf,ICML
1539,2017,FeUdal Networks for Hierarchical Reinforcement Learning,"Alexander Sasha Vezhnevets,         Simon Osindero,         Tom Schaul,         Nicolas Heess,         Max Jaderberg,         David Silver,         Koray Kavukcuoglu","We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels – allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a slower time scale and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits – in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation.",http://proceedings.mlr.press/v70/vezhnevets17a.html,http://proceedings.mlr.press/v70/vezhnevets17a/vezhnevets17a.pdf,ICML
1540,2017,Clustering High Dimensional Dynamic Data Streams,"Vladimir Braverman,         Gereon Frahling,         Harry Lang,         Christian Sohler,         Lin F. Yang","We present data streaming algorithms for the kkk-median problem in high-dimensional dynamic geometric data streams, i.e. streams allowing both insertions and deletions of points from a discrete Euclidean space {1,2,…Δ}d{1,2,…Δ}d\{1, 2, \ldots \Delta\}^d. Our algorithms use kϵ−2poly(dlogΔ)kϵ−2poly(dlog⁡Δ)k \epsilon^{-2} \mathrm{poly}(d \log \Delta) space/time and maintain with high probability a small weighted set of points (a coreset) such that for every set of kkk centers the cost of the coreset (1+ϵ)(1+ϵ)(1+\epsilon)-approximates the cost of the streamed point set. We also provide algorithms that guarantee only positive weights in the coreset with additional logarithmic factors in the space and time complexities. We can use this positively-weighted coreset to compute a (1+ϵ)(1+ϵ)(1+\epsilon)-approximation for the kkk-median problem by any efficient offline kkk-median algorithm. All previous algorithms for computing a (1+ϵ)(1+ϵ)(1+\epsilon)-approximation for the kkk-median problem over dynamic data streams required space and time exponential in ddd. Our algorithms can be generalized to metric spaces of bounded doubling dimension.",http://proceedings.mlr.press/v70/braverman17a.html,http://proceedings.mlr.press/v70/braverman17a/braverman17a.pdf,ICML
1541,2017,SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient,"Lam M. Nguyen,         Jie Liu,         Katya Scheinberg,         Martin Takáč","In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm.",http://proceedings.mlr.press/v70/nguyen17b.html,http://proceedings.mlr.press/v70/nguyen17b/nguyen17b.pdf,ICML
1542,2017,Fairness in Reinforcement Learning,"Shahin Jabbari,         Matthew Joseph,         Michael Kearns,         Jamie Morgenstern,         Aaron Roth","We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states to achieve non-trivial approximation to the optimal policy. We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness.",http://proceedings.mlr.press/v70/jabbari17a.html,http://proceedings.mlr.press/v70/jabbari17a/jabbari17a.pdf,ICML
1543,2017,Variants of RMSProp and Adagrad with Logarithmic Regret Bounds,"Mahesh Chandra Mukkamala,         Matthias Hein","Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show T−−√T\sqrt{T}-type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.",http://proceedings.mlr.press/v70/mukkamala17a.html,http://proceedings.mlr.press/v70/mukkamala17a/mukkamala17a.pdf,ICML
1544,2017,Understanding Synthetic Gradients and Decoupled Neural Interfaces,"Wojciech Marian Czarnecki,         Grzegorz Świrszcz,         Max Jaderberg,         Simon Osindero,         Oriol Vinyals,         Koray Kavukcuoglu","When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking – without waiting for a true error gradient to be backpropagated – resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.",http://proceedings.mlr.press/v70/czarnecki17a.html,http://proceedings.mlr.press/v70/czarnecki17a/czarnecki17a.pdf,ICML
1545,2017,Efficient Distributed Learning with Sparsity,"Jialei Wang,         Mladen Kolar,         Nathan Srebro,         Tong Zhang","We propose a novel, efficient approach for distributed sparse learning with observations randomly partitioned across machines. In each round of the proposed method, worker machines compute the gradient of the loss on local data and the master machine solves a shifted ℓ1ℓ1\ell_1 regularized loss minimization problem. After a number of communication rounds that scales only logarithmically with the number of machines, and independent of other parameters of the problem, the proposed approach provably matches the estimation error bound of centralized methods.",http://proceedings.mlr.press/v70/wang17f.html,http://proceedings.mlr.press/v70/wang17f/wang17f.pdf,ICML
1546,2017,Learning the Structure of Generative Models without Labeled Data,"Stephen H. Bach,         Bryan He,         Alexander Ratner,         Christopher Ré","Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model’s dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the l1-regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100x faster than a maximum likelihood approach and selects 1/4 as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal abstracts.",http://proceedings.mlr.press/v70/bach17a.html,http://proceedings.mlr.press/v70/bach17a/bach17a.pdf,ICML
1547,2017,Learning Stable Stochastic Nonlinear Dynamical Systems,"Jonas Umlauft,         Sandra Hirche","A data-driven identification of dynamical systems requiring only minimal prior knowledge is promising whenever no analytically derived model structure is available, e.g., from first principles in physics. However, meta-knowledge on the system’s behavior is often given and should be exploited: Stability as fundamental property is essential when the model is used for controller design or movement generation. Therefore, this paper proposes a framework for learning stable stochastic systems from data. We focus on identifying a state-dependent coefficient form of the nonlinear stochastic model which is globally asymptotically stable according to probabilistic Lyapunov methods. We compare our approach to other state of the art methods on real-world datasets in terms of flexibility and stability.",http://proceedings.mlr.press/v70/umlauft17a.html,http://proceedings.mlr.press/v70/umlauft17a/umlauft17a.pdf,ICML
1548,2017,Lost Relatives of the Gumbel Trick,"Matej Balog,         Nilesh Tripuraneni,         Zoubin Ghahramani,         Adrian Weller","The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with so-called low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.",http://proceedings.mlr.press/v70/balog17a.html,http://proceedings.mlr.press/v70/balog17a/balog17a.pdf,ICML
1549,2017,A Closer Look at Memorization in Deep Networks,"Devansh Arpit,         Stanisław Jastrzębski,         Nicolas Ballas,         David Krueger,         Emmanuel Bengio,         Maxinder S. Kanwal,         Tegan Maharaj,         Asja Fischer,         Aaron Courville,         Yoshua Bengio,         Simon Lacoste-Julien","We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs.~real data. We also demonstrate that for appropriately tuned explicit regularization (e.g.,~dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.",http://proceedings.mlr.press/v70/arpit17a.html,http://proceedings.mlr.press/v70/arpit17a/arpit17a.pdf,ICML
1550,2017,Estimating the unseen from multiple populations,"Aditi Raghunathan,         Gregory Valiant,         James Zou","Given samples from a distribution, how many new elements should we expect to find if we keep on sampling this distribution? This is an important and actively studied problem, with many applications ranging from species estimation to genomics. We generalize this extrapolation and related unseen estimation problems to the multiple population setting, where population jjj has an unknown distribution DjDjD_j from which we observe njnjn_j samples. We derive an optimal estimator for the total number of elements we expect to find among new samples across the populations. Surprisingly, we prove that our estimator’s accuracy is independent of the number of populations. We also develop an efficient optimization algorithm to solve the more general problem of estimating multi-population frequency distributions. We validate our methods and theory through extensive experiments. Finally, on a real dataset of human genomes across multiple ancestries, we demonstrate how our approach for unseen estimation can enable cohort designs that can discover interesting mutations with greater efficiency.",http://proceedings.mlr.press/v70/raghunathan17a.html,http://proceedings.mlr.press/v70/raghunathan17a/raghunathan17a.pdf,ICML
1551,2017,Online Partial Least Square Optimization: Dropping Convexity for Better Efficiency and Scalability,"Zhehui Chen,         Lin F. Yang,         Chris Junchi Li,         Tuo Zhao","Multiview representation learning is popular for latent factor analysis. Many existing approaches formulate the multiview representation learning as convex optimization problems, where global optima can be obtained by certain algorithms in polynomial time. However, many evidences have corroborated that heuristic nonconvex approaches also have good empirical computational performance and convergence to the global optima, although there is a lack of theoretical justification. Such a gap between theory and practice motivates us to study a nonconvex formulation for multiview representation learning, which can be efficiently solved by a simple stochastic gradient descent method. By analyzing the dynamics of the algorithm based on diffusion processes, we establish a global rate of convergence to the global optima. Numerical experiments are provided to support our theory.",http://proceedings.mlr.press/v70/chen17h.html,http://proceedings.mlr.press/v70/chen17h/chen17h.pdf,ICML
1552,2017,PixelCNN Models with Auxiliary Variables for Natural Image Modeling,"Alexander Kolesnikov,         Christoph H. Lampert","We study probabilistic models of natural images and extend the autoregressive family of PixelCNN models by incorporating auxiliary variables. Subsequently, we describe two new generative image models that exploit different image transformations as auxiliary variables: a quantized grayscale view of the image or a multi-resolution image pyramid. The proposed models tackle two known shortcomings of existing PixelCNN models: 1) their tendency to focus on low-level image details, while largely ignoring high-level image information, such as object shapes, and 2) their computationally costly procedure for image sampling. We experimentally demonstrate benefits of our models, in particular showing that they produce much more realistically looking image samples than previous state-of-the-art probabilistic models.",http://proceedings.mlr.press/v70/kolesnikov17a.html,http://proceedings.mlr.press/v70/kolesnikov17a/kolesnikov17a.pdf,ICML
1553,2017,Automatic Discovery of the Statistical Types of Variables in a Dataset,"Isabel Valera,         Zoubin Ghahramani","A common practice in statistics and machine learning is to assume that the statistical data types (e.g., ordinal, categorical or real-valued) of variables, and usually also the likelihood model, is known. However, as the availability of real-world data increases, this assumption becomes too restrictive. Data are often heterogeneous, complex, and improperly or incompletely documented. Surprisingly, despite their practical importance, there is still a lack of tools to automatically discover the statistical types of, as well as appropriate likelihood (noise) models for, the variables in a dataset. In this paper, we fill this gap by proposing a Bayesian method, which accurately discovers the statistical data types in both synthetic and real data.",http://proceedings.mlr.press/v70/valera17a.html,http://proceedings.mlr.press/v70/valera17a/valera17a.pdf,ICML
1554,2017,Learning in POMDPs with Monte Carlo Tree Search,"Sammie Katt,         Frans A. Oliehoek,         Christopher Amato","The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BA-POMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.",http://proceedings.mlr.press/v70/katt17a.html,http://proceedings.mlr.press/v70/katt17a/katt17a.pdf,ICML
1555,2017,Stochastic Modified Equations and Adaptive Stochastic Gradient Algorithms,"Qianxiao Li,         Cheng Tai,         Weinan E","We develop the method of stochastic modified equations (SME), in which stochastic gradient algorithms are approximated in the weak sense by continuous-time stochastic differential equations. We exploit the continuous formulation together with optimal control theory to derive novel adaptive hyper-parameter adjustment policies. Our algorithms have competitive performance with the added benefit of being robust to varying models and datasets. This provides a general methodology for the analysis and design of stochastic gradient algorithms.",http://proceedings.mlr.press/v70/li17f.html,http://proceedings.mlr.press/v70/li17f/li17f.pdf,ICML
1556,2017,Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data,"Tomoya Sakai,         Marthinus Christoffel Plessis,         Gang Niu,         Masashi Sugiyama","Most of the semi-supervised classification methods developed so far use unlabeled data for regularization purposes under particular distributional assumptions such as the cluster assumption. In contrast, recently developed methods of classification from positive and unlabeled data (PU classification) use unlabeled data for risk evaluation, i.e., label information is directly extracted from unlabeled data. In this paper, we extend PU classification to also incorporate negative data and propose a novel semi-supervised learning approach. We establish generalization error bounds for our novel methods and show that the bounds decrease with respect to the number of unlabeled data without the distributional assumptions that are required in existing semi-supervised learning methods. Through experiments, we demonstrate the usefulness of the proposed methods.",http://proceedings.mlr.press/v70/sakai17a.html,http://proceedings.mlr.press/v70/sakai17a/sakai17a.pdf,ICML
1557,2017,On the Sampling Problem for Kernel Quadrature,"François-Xavier Briol,         Chris J. Oates,         Jon Cockayne,         Wilson Ye Chen,         Mark Girolami","The standard Kernel Quadrature method for numerical integration with random point sets (also called Bayesian Monte Carlo) is known to converge in root mean square error at a rate determined by the ratio s/d, where s and d encode the smoothness and dimension of the integrand. However, an empirical investigation reveals that the rate constant C is highly sensitive to the distribution of the random points. In contrast to standard Monte Carlo integration, for which optimal importance sampling is well-understood, the sampling distribution that minimises C for Kernel Quadrature does not admit a closed form. This paper argues that the practical choice of sampling distribution is an important open problem. One solution is considered; a novel automatic approach based on adaptive tempering and sequential Monte Carlo. Empirical results demonstrate a dramatic reduction in integration error of up to 4 orders of magnitude can be achieved with the proposed method.",http://proceedings.mlr.press/v70/briol17a.html,http://proceedings.mlr.press/v70/briol17a/briol17a.pdf,ICML
1558,2017,Differentially Private Chi-squared Test by Unit Circle Mechanism,"Kazuya Kakizaki,         Kazuto Fukuchi,         Jun Sakuma","This paper develops differentially private mechanisms for χ2χ2\chi^2 test of independence. While existing works put their effort into properly controlling the type-I error, in addition to that, we investigate the type-II error of differentially private mechanisms. Based on the analysis, we present unit circle mechanism: a novel differentially private mechanism based on the geometrical property of the test statistics. Compared to existing output perturbation mechanisms, our mechanism improves the dominated term of the type-II error from O(1)O(1)O(1) to O(exp(−√N))O(exp(−N−−√))O(\exp(-\sqrt{N})) where NNN is the sample size. Furthermore, we introduce novel procedures for multiple χ2χ2\chi^2 tests by incorporating the unit circle mechanism into the sparse vector technique and the exponential mechanism. These procedures can control the family-wise error rate (FWER) properly, which has never been attained by existing mechanisms.",http://proceedings.mlr.press/v70/kakizaki17a.html,http://proceedings.mlr.press/v70/kakizaki17a/kakizaki17a.pdf,ICML
1559,2017,Optimal Densification for Fast and Accurate Minwise Hashing,Anshumali Shrivastava,"Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification (Shrivastava \& Li, 2014) have shown that it is possible to compute kkk minwise hashes, of a vector with ddd nonzeros, in mere (d+k)(d+k)(d + k) computations, a significant improvement over the classical O(dk)O(dk)O(dk). These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.",http://proceedings.mlr.press/v70/shrivastava17a.html,http://proceedings.mlr.press/v70/shrivastava17a/shrivastava17a.pdf,ICML
1560,2017,Boosted Fitted Q-Iteration,"Samuele Tosatto,         Matteo Pirotta,         Carlo D’Eramo,         Marcello Restelli","This paper is about the study of B-FQI, an Approximated Value Iteration (AVI) algorithm that exploits a boosting procedure to estimate the action-value function in reinforcement learning problems. B-FQI is an iterative off-line algorithm that, given a dataset of transitions, builds an approximation of the optimal action-value function by summing the approximations of the Bellman residuals across all iterations. The advantage of such approach w.r.t. to other AVI methods is twofold: (1) while keeping the same function space at each iteration, B-FQI can represent more complex functions by considering an additive model; (2) since the Bellman residual decreases as the optimal value function is approached, regression problems become easier as iterations proceed. We study B-FQI both theoretically, providing also a finite-sample error upper bound for it, and empirically, by comparing its performance to the one of FQI in different domains and using different regression techniques.",http://proceedings.mlr.press/v70/tosatto17a.html,http://proceedings.mlr.press/v70/tosatto17a/tosatto17a.pdf,ICML
1561,2017,Fake News Mitigation via Point Process Based Intervention,"Mehrdad Farajtabar,         Jiachen Yang,         Xiaojing Ye,         Huan Xu,         Rakshit Trivedi,         Elias Khalil,         Shuang Li,         Le Song,         Hongyuan Zha","We propose the first multistage intervention framework that tackles fake news in social networks by combining reinforcement learning with a point process network activity model. The spread of fake news and mitigation events within the network is modeled by a multivariate Hawkes process with additional exogenous control terms. By choosing a feature representation of states, defining mitigation actions and constructing reward functions to measure the effectiveness of mitigation activities, we map the problem of fake news mitigation into the reinforcement learning framework. We develop a policy iteration method unique to the multivariate networked point process, with the goal of optimizing the actions for maximal reward under budget constraints. Our method shows promising performance in real-time intervention experiments on a Twitter network to mitigate a surrogate fake news campaign, and outperforms alternatives on synthetic datasets.",http://proceedings.mlr.press/v70/farajtabar17a.html,http://proceedings.mlr.press/v70/farajtabar17a/farajtabar17a.pdf,ICML
1562,2017,Deep Spectral Clustering Learning,"Marc T. Law,         Raquel Urtasun,         Richard S. Zemel","Clustering is the task of grouping a set of examples so that similar examples are grouped into the same cluster while dissimilar examples are in different clusters. The quality of a clustering depends on two problem-dependent factors which are i) the chosen similarity metric and ii) the data representation. Supervised clustering approaches, which exploit labeled partitioned datasets have thus been proposed, for instance to learn a metric optimized to perform clustering. However, most of these approaches assume that the representation of the data is fixed and then learn an appropriate linear transformation. Some deep supervised clustering learning approaches have also been proposed. However, they rely on iterative methods to compute gradients resulting in high algorithmic complexity. In this paper, we propose a deep supervised clustering metric learning method that formulates a novel loss function. We derive a closed-form expression for the gradient that is efficient to compute: the complexity to compute the gradient is linear in the size of the training mini-batch and quadratic in the representation dimensionality. We further reveal how our approach can be seen as learning spectral clustering. Experiments on standard real-world datasets confirm state-of-the-art Recall@K performance.",http://proceedings.mlr.press/v70/law17a.html,http://proceedings.mlr.press/v70/law17a/law17a.pdf,ICML
1563,2017,Tight Bounds for Approximate Carathéodory and Beyond,"Vahab Mirrokni,         Renato Paes Leme,         Adrian Vladu,         Sam Chiu-wai Wong","We present a deterministic nearly-linear time algorithm for approximating any point inside a convex polytope with a sparse convex combination of the polytope’s vertices. Our result provides a constructive proof for the Approximate Carathéodory Problem, which states that any point inside a polytope contained in the ℓpℓp\ell_p ball of radius DDD can be approximated to within ϵϵ\epsilon in ℓpℓp\ell_p norm by a convex combination of O(D2p/ϵ2)O(D2p/ϵ2)O\left(D^2 p/\epsilon^2\right) vertices of the polytope for p≥2p≥2p \geq 2. While for the particular case of p=2p=2p=2, this can be achieved by the well-known Perceptron algorithm, we follow a more principled approach which generalizes to arbitrary p≥2p≥2p\geq 2; furthermore, this naturally extends to domains with more complicated geometry, as it is the case for providing an approximate Birkhoff-von Neumann decomposition. Secondly, we show that the sparsity bound is tight for ℓpℓp\ell_p norms, using an argument based on anti-concentration for the binomial distribution, thus resolving an open question posed by Barman. Experimentally, we verify that our deterministic optimization-based algorithms achieve in practice much better sparsity than previously known sampling-based algorithms. We also show how to apply our techniques to SVM training and rounding fractional points in matroid and flow polytopes.",http://proceedings.mlr.press/v70/mirrokni17a.html,http://proceedings.mlr.press/v70/mirrokni17a/mirrokni17a.pdf,ICML
1564,2017,Adversarial Feature Matching for Text Generation,"Yizhe Zhang,         Zhe Gan,         Kai Fan,         Zhi Chen,         Ricardo Henao,         Dinghan Shen,         Lawrence Carin","The Generative Adversarial Network (GAN) has achieved great success in generating realistic (real-valued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long short-term memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.",http://proceedings.mlr.press/v70/zhang17b.html,http://proceedings.mlr.press/v70/zhang17b/zhang17b.pdf,ICML
1565,2017,State-Frequency Memory Recurrent Neural Networks,"Hao Hu,         Guo-Jun Qi","Modeling temporal sequences plays a fundamental role in various modern applications and has drawn more and more attentions in the machine learning community. Among those efforts on improving the capability to represent temporal data, the Long Short-Term Memory (LSTM) has achieved great success in many areas. Although the LSTM can capture long-range dependency in the time domain, it does not explicitly model the pattern occurrences in the frequency domain that plays an important role in tracking and predicting data points over various time cycles. We propose the State-Frequency Memory (SFM), a novel recurrent architecture that allows to separate dynamic patterns across different frequency components and their impacts on modeling the temporal contexts of input sequences. By jointly decomposing memorized dynamics into state-frequency components, the SFM is able to offer a fine-grained analysis of temporal sequences by capturing the dependency of uncovered patterns in both time and frequency domains. Evaluations on several temporal modeling tasks demonstrate the SFM can yield competitive performances, in particular as compared with the state-of-the-art LSTM models.",http://proceedings.mlr.press/v70/hu17c.html,http://proceedings.mlr.press/v70/hu17c/hu17c.pdf,ICML
1566,2017,Sparse + Group-Sparse Dirty Models: Statistical Guarantees without Unreasonable Conditions and a Case for Non-Convexity,"Eunho Yang,         Aurélie C. Lozano","Imposing sparse + group-sparse superposition structures in high-dimensional parameter estimation is known to provide flexible regularization that is more realistic for many real-world problems. For example, such a superposition enables partially-shared support sets in multi-task learning, thereby striking the right balance between parameter overlap across tasks and task specificity. Existing theoretical results on estimation consistency, however, are problematic as they require too stringent an assumption: the incoherence between sparse and group-sparse superposed components. In this paper, we fill the gap between the practical success and suboptimal analysis of sparse + group-sparse models, by providing the first consistency results that do not require unrealistic assumptions. We also study non-convex counterparts of sparse + group-sparse models. Interestingly, we show that these are guaranteed to recover the true support set under much milder conditions and with smaller sample size than convex models, which might be critical in practical applications as illustrated by our experiments.",http://proceedings.mlr.press/v70/yang17g.html,http://proceedings.mlr.press/v70/yang17g/yang17g.pdf,ICML
1567,2017,No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis,"Rong Ge,         Chi Jin,         Yi Zheng","In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.",http://proceedings.mlr.press/v70/ge17a.html,http://proceedings.mlr.press/v70/ge17a/ge17a.pdf,ICML
1568,2017,Variational Boosting: Iteratively Refining Posterior Approximations,"Andrew C. Miller,         Nicholas J. Foti,         Ryan P. Adams","We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing a trade-off between computation time and accuracy. We expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture. We apply variational boosting to synthetic and real statistical models, and show that the resulting posterior inferences compare favorably to existing variational algorithms.",http://proceedings.mlr.press/v70/miller17a.html,http://proceedings.mlr.press/v70/miller17a/miller17a.pdf,ICML
1569,2017,Multi-fidelity Bayesian Optimisation with Continuous Approximations,"Kirthevasan Kandasamy,         Gautam Dasarathy,         Jeff Schneider,         Barnabás Póczos","Bandit methods for black-box optimisation, such as Bayesian optimisation, are used in a variety of applications including hyper-parameter tuning and experiment design. Recently, multi-fidelity methods have garnered considerable attention since function evaluations have become increasingly expensive in such applications. Multi-fidelity methods use cheap approximations to the function of interest to speed up the overall optimisation process. However, most multi-fidelity methods assume only a finite number of approximations. On the other hand, in many practical applications, a continuous spectrum of approximations might be available. For instance, when tuning an expensive neural network, one might choose to approximate the cross validation performance using less data NN and/or few training iterations TT. Here, the approximations are best viewed as arising out of a continuous two dimensional space (N,T)(N,T). In this work, we develop a Bayesian optimisation method, BOCA, for this setting. We characterise its theoretical properties and show that it achieves better regret than than strategies which ignore the approximations. BOCA outperforms several other baselines in synthetic and real experiments.",http://proceedings.mlr.press/v70/kandasamy17a.html,http://proceedings.mlr.press/v70/kandasamy17a/kandasamy17a.pdf,ICML
1570,2017,Learning Determinantal Point Processes with Moments and Cycles,"John Urschel,         Victor-Emmanuel Brunel,         Ankur Moitra,         Philippe Rigollet","Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important. While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learning the parameters of a DPP. Our contribution is twofold: (i) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter, which we call the cycle sparsity; (ii) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity. Finally, we give experimental results that confirm our theoretical findings.",http://proceedings.mlr.press/v70/urschel17a.html,http://proceedings.mlr.press/v70/urschel17a/urschel17a.pdf,ICML
1571,2017,DeepBach: a Steerable Model for Bach Chorales Generation,"Gaëtan Hadjeres,         François Pachet,         Frank Nielsen","This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach’s strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.",http://proceedings.mlr.press/v70/hadjeres17a.html,http://proceedings.mlr.press/v70/hadjeres17a/hadjeres17a.pdf,ICML
1572,2017,Deep Generative Models for Relational Data with Side Information,"Changwei Hu,         Piyush Rai,         Lawrence Carin","We present a probabilistic framework for overlapping community discovery and link prediction for relational data, given as a graph. The proposed framework has: (1) a deep architecture which enables us to infer multiple layers of latent features/communities for each node, providing superior link prediction performance on more complex networks and better interpretability of the latent features; and (2) a regression model which allows directly conditioning the node latent features on the side information available in form of node attributes. Our framework handles both (1) and (2) via a clean, unified model, which enjoys full local conjugacy via data augmentation, and facilitates efficient inference via closed form Gibbs sampling. Moreover, inference cost scales in the number of edges which is attractive for massive but sparse networks. Our framework is also easily extendable to model weighted networks with count-valued edges. We compare with various state-of-the-art methods and report results, both quantitative and qualitative, on several benchmark data sets.",http://proceedings.mlr.press/v70/hu17d.html,http://proceedings.mlr.press/v70/hu17d/hu17d.pdf,ICML
1573,2017,Differentiable Programs with Neural Libraries,"Alexander L. Gaunt,         Marc Brockschmidt,         Nate Kushman,         Daniel Tarlow","We develop a framework for combining differentiable programming languages with neural networks. Using this framework we create end-to-end trainable systems that learn to write interpretable algorithms with perceptual components. We explore the benefits of inductive biases for strong generalization and modularity that come from the program-like structure of our models. In particular, modularity allows us to learn a library of (neural) functions which grows and improves as more tasks are solved. Empirically, we show that this leads to lifelong learning systems that transfer knowledge to new tasks more effectively than baselines.",http://proceedings.mlr.press/v70/gaunt17a.html,http://proceedings.mlr.press/v70/gaunt17a/gaunt17a.pdf,ICML
1574,2017,Adaptive Neural Networks for Efficient Inference,"Tolga Bolukbasi,         Joseph Wang,         Ofer Dekel,         Venkatesh Saligrama","We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal (<1%<1%<1\%) loss of top5 accuracy.",http://proceedings.mlr.press/v70/bolukbasi17a.html,http://proceedings.mlr.press/v70/bolukbasi17a/bolukbasi17a.pdf,ICML
1575,2017,Tensor-Train Recurrent Neural Networks for Video Classification,"Yinchong Yang,         Denis Krompass,         Volker Tresp","The Recurrent Neural Networks and their variants have shown promising performances in sequence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs’ large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extractors. To address this challenge, we propose a new, more general and efficient approach by factorizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained simultaneously with the weights themselves. We test our model on classification tasks using multiple real-world video datasets and achieve competitive performances with state-of-the-art models, even though our model architecture is orders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architectures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.",http://proceedings.mlr.press/v70/yang17e.html,http://proceedings.mlr.press/v70/yang17e/yang17e.pdf,ICML
1576,2017,Minimax Regret Bounds for Reinforcement Learning,"Mohammad Gheshlaghi Azar,         Ian Osband,         Rémi Munos","We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of O~(HSAT−−−−−−√+H2S2A+HT−−√)O~(HSAT+H2S2A+HT)\tilde {O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T}) where HHH is the time horizon, SSS the number of states, AAA the number of actions and TTT the number of time-steps. This result improves over the best previous known bound O~(HSAT−−−√)O~(HSAT)\tilde {O}(HS \sqrt{AT}) achieved by the UCRL2 algorithm. The key significance of our new results is that when T≥H3S3AT≥H3S3AT\geq H^3S^3A and SA≥HSA≥HSA\geq H, it leads to a regret of O~(HSAT−−−−−−√)O~(HSAT)\tilde{O}(\sqrt{HSAT}) that matches the established lower bound of Ω(HSAT−−−−−−√)Ω(HSAT)\Omega(\sqrt{HSAT}) up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in SSS), and we define Bernstein-based “exploration bonuses” that use the empirical variance of the estimated values at the next states (to improve scaling in HHH).",http://proceedings.mlr.press/v70/azar17a.html,http://proceedings.mlr.press/v70/azar17a/azar17a.pdf,ICML
1577,2017,"iSurvive: An Interpretable, Event-time Prediction Model for mHealth","Walter H. Dempsey,         Alexander Moreno,         Christy K. Scott,         Michael L. Dennis,         David H. Gustafson,         Susan A. Murphy,         James M. Rehg","An important mobile health (mHealth) task is the use of multimodal data, such as sensor streams and self-report, to construct interpretable time-to-event predictions of, for example, lapse to alcohol or illicit drug use. Interpretability of the prediction model is important for acceptance and adoption by domain scientists, enabling model outputs and parameters to inform theory and guide intervention design. Temporal latent state models are therefore attractive, and so we adopt the continuous time hidden Markov model (CT-HMM) due to its ability to describe irregular arrival times of event data. Standard CT-HMMs, however, are not specialized for predicting the time to a future event, the key variable for mHealth interventions. Also, standard emission models lack a sufficiently rich structure to describe multimodal data and incorporate domain knowledge. We present iSurvive, an extension of classical survival analysis to a CT-HMM. We present a parameter learning method for GLM emissions and survival model fitting, and present promising results on both synthetic data and an mHealth drug use dataset.",http://proceedings.mlr.press/v70/dempsey17a.html,http://proceedings.mlr.press/v70/dempsey17a/dempsey17a.pdf,ICML
1578,2017,Guarantees for Greedy Maximization of Non-submodular Functions with Applications,"Andrew An Bian,         Joachim M. Buhmann,         Andreas Krause,         Sebastian Tschiatschek","We investigate the performance of the standard Greedy algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the performance of Greedy for maximizing submodular functions, there are few guarantees for non-submodular ones. However, Greedy enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design. We prove theoretical guarantees supporting the empirical performance. Our guarantees are characterized by a combination of the (generalized) curvature αα\alpha and the submodularity ratio γγ\gamma. In particular, we prove that Greedy enjoys a tight approximation guarantee of 1α(1−e−γα)1α(1−e−γα)\frac{1}{\alpha}(1- e^{-\gamma\alpha}) for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, including the Bayesian A-optimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for both synthetic and real-world applications.",http://proceedings.mlr.press/v70/bian17a.html,http://proceedings.mlr.press/v70/bian17a/bian17a.pdf,ICML
1579,2017,Forward and Reverse Gradient-Based Hyperparameter Optimization,"Luca Franceschi,         Michele Donini,         Paolo Frasconi,         Massimiliano Pontil","We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters of any iterative learning algorithm such as stochastic gradient descent. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offs in terms of running time and space requirements. Our formulation of the reverse-mode procedure is linked to previous work by Maclaurin et al (2015) but does not require reversible dynamics. Additionally, we explore the use of constraints on the hyperparameters. The forward-mode procedure is suitable for real-time hyperparameter updates, which may significantly speedup hyperparameter optimization on large datasets. We present a series of experiments on image and phone classification tasks. In the second task, previous gradient-based approaches are prohibitive. We show that our real-time algorithm yields state-of-the-art results in affordable time.",http://proceedings.mlr.press/v70/franceschi17a.html,http://proceedings.mlr.press/v70/franceschi17a/franceschi17a.pdf,ICML
1580,2017,Identify the Nash Equilibrium in Static Games with Random Payoffs,"Yichi Zhou,         Jialian Li,         Jun Zhu",We study the problem on how to learn the pure Nash Equilibrium of a two-player zero-sum static game with random payoffs under unknown distributions via efficient payoff queries. We introduce a multi-armed bandit model to this problem due to its ability to find the best arm efficiently among random arms and propose two algorithms for this problem—LUCB-G based on the confidence bounds and a racing algorithm based on successive action elimination. We provide an analysis on the sample complexity lower bound when the Nash Equilibrium exists.,http://proceedings.mlr.press/v70/zhou17b.html,http://proceedings.mlr.press/v70/zhou17b/zhou17b.pdf,ICML
1581,2017,Exact Inference for Integer Latent-Variable Models,"Kevin Winner,         Debora Sujono,         Dan Sheldon","Graphical models with latent count variables arise in a number of areas. However, standard inference algorithms do not apply to these models due to the infinite support of the latent variables. Winner and Sheldon (2016) recently developed a new technique using probability generating functions (PGFs) to perform efficient, exact inference for certain Poisson latent variable models. However, the method relies on symbolic manipulation of PGFs, and it is unclear whether this can be extended to more general models. In this paper we introduce a new approach for inference with PGFs: instead of manipulating PGFs symbolically, we adapt techniques from the autodiff literature to compute the higher-order derivatives necessary for inference. This substantially generalizes the class of models for which efficient, exact inference algorithms are available. Specifically, our results apply to a class of models that includes branching processes, which are widely used in applied mathematics and population ecology, and autoregressive models for integer data. Experiments show that our techniques are more scalable than existing approximate methods and enable new applications.",http://proceedings.mlr.press/v70/winner17a.html,http://proceedings.mlr.press/v70/winner17a/winner17a.pdf,ICML
1582,2017,Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks,"Lars Mescheder,         Sebastian Nowozin,         Andreas Geiger","Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.",http://proceedings.mlr.press/v70/mescheder17a.html,http://proceedings.mlr.press/v70/mescheder17a/mescheder17a.pdf,ICML
1583,2017,Multichannel End-to-end Speech Recognition,"Tsubasa Ochiai,         Shinji Watanabe,         Takaaki Hori,         John R. Hershey","The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.",http://proceedings.mlr.press/v70/ochiai17a.html,http://proceedings.mlr.press/v70/ochiai17a/ochiai17a.pdf,ICML
1584,2017,Developing Bug-Free Machine Learning Systems With Formal Mathematics,"Daniel Selsam,         Percy Liang,         David L. Dill","Noisy data, non-convex objectives, model misspecification, and numerical instability can all cause undesired behaviors in machine learning systems. As a result, detecting actual implementation errors can be extremely difficult. We demonstrate a methodology in which developers use an interactive proof assistant to both implement their system and to state a formal theorem defining what it means for their system to be correct. The process of proving this theorem interactively in the proof assistant exposes all implementation errors since any error in the program would cause the proof to fail. As a case study, we implement a new system, Certigrad, for optimizing over stochastic computation graphs, and we generate a formal (i.e. machine-checkable) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients. We train a variational autoencoder using Certigrad and find the performance comparable to training the same model in TensorFlow.",http://proceedings.mlr.press/v70/selsam17a.html,http://proceedings.mlr.press/v70/selsam17a/selsam17a.pdf,ICML
1585,2017,On the Expressive Power of Deep Neural Networks,"Maithra Raghu,         Ben Poole,         Jon Kleinberg,         Surya Ganguli,         Jascha Sohl-Dickstein","We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings show that: (1) The complexity of the computed function grows exponentially with depth (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights (3) Trajectory regularization is a simpler alternative to batch normalization, with the same performance.",http://proceedings.mlr.press/v70/raghu17a.html,http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf,ICML
1586,2017,On Kernelized Multi-armed Bandits,"Sayak Ray Chowdhury,         Aditya Gopalan","We consider the stochastic bandit problem with a continuous set of arms, with the expected reward function over the arms assumed to be fixed but unknown. We provide two new Gaussian process-based algorithms for continuous bandit optimization – Improved GP-UCB (IGP-UCB) and GP-Thomson sampling (GP-TS), and derive corresponding regret bounds. Specifically, the bounds hold when the expected reward function belongs to the reproducing kernel Hilbert space (RKHS) that naturally corresponds to a Gaussian process kernel used as input by the algorithms. Along the way, we derive a new self-normalized concentration inequality for vector-valued martingales of arbitrary, possibly infinite, dimension. Finally, experimental evaluation and comparisons to existing algorithms on synthetic and real-world environments are carried out that highlight the favourable gains of the proposed strategies in many cases.",http://proceedings.mlr.press/v70/chowdhury17a.html,http://proceedings.mlr.press/v70/chowdhury17a/chowdhury17a.pdf,ICML
1587,2017,Learning to Discover Cross-Domain Relations with Generative Adversarial Networks,"Taeksoo Kim,         Moonsu Cha,         Hyunsoo Kim,         Jung Kwon Lee,         Jiwon Kim","While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on a generative adversarial network that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.",http://proceedings.mlr.press/v70/kim17a.html,http://proceedings.mlr.press/v70/kim17a/kim17a.pdf,ICML
1588,2017,Bottleneck Conditional Density Estimation,"Rui Shu,         Hung H. Bui,         Mohammad Ghavamzadeh","We introduce a new framework for training deep generative models for high-dimensional conditional density estimation. The Bottleneck Conditional Density Estimator (BCDE) is a variant of the conditional variational autoencoder (CVAE) that employs layer(s) of stochastic variables as the bottleneck between the input x and target y, where both are high-dimensional. Crucially, we propose a new hybrid training method that blends the conditional generative model with a joint generative model. Hybrid blending is the key to effective training of the BCDE, which avoids overfitting and provides a novel mechanism for leveraging unlabeled data. We show that our hybrid training procedure enables models to achieve competitive results in the MNIST quadrant prediction task in the fully-supervised setting, and sets new benchmarks in the semi-supervised regime for MNIST, SVHN, and CelebA.",http://proceedings.mlr.press/v70/shu17a.html,http://proceedings.mlr.press/v70/shu17a/shu17a.pdf,ICML
1589,2017,Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization,"Qunwei Li,         Yi Zhou,         Yingbin Liang,         Pramod K. Varshney","In this work, we investigate the accelerated proximal gradient method for nonconvex programming (APGnc). The method compares between a usual proximal gradient step and a linear extrapolation step, and accepts the one that has a lower function value to achieve a monotonic decrease. In specific, under a general nonsmooth and nonconvex setting, we provide a rigorous argument to show that the limit points of the sequence generated by APGnc are critical points of the objective function. Then, by exploiting the Kurdyka-Lojasiewicz (KL) property for a broad class of functions, we establish the linear and sub-linear convergence rates of the function value sequence generated by APGnc. We further propose a stochastic variance reduced APGnc (SVRG-APGnc), and establish its linear convergence under a special case of the KL property. We also extend the analysis to the inexact version of these methods and develop an adaptive momentum strategy that improves the numerical performance.",http://proceedings.mlr.press/v70/li17g.html,http://proceedings.mlr.press/v70/li17g/li17g.pdf,ICML
1590,2017,Efficient Nonmyopic Active Search,"Shali Jiang,         Gustavo Malkomes,         Geoff Converse,         Alyssa Shofner,         Benjamin Moseley,         Roman Garnett","Active search is an active learning setting with the goal of identifying as many members of a given class as possible under a labeling budget. In this work, we first establish a theoretical hardness of active search, proving that no polynomial-time policy can achieve a constant factor approximation ratio with respect to the expected utility of the optimal policy. We also propose a novel, computationally efficient active search policy achieving exceptional performance on several real-world tasks. Our policy is nonmyopic, always considering the entire remaining search budget. It also automatically and dynamically balances exploration and exploitation consistent with the remaining budget, without relying on a parameter to control this tradeoff. We conduct experiments on diverse datasets from several domains: drug discovery, materials science, and a citation network. Our efficient nonmyopic policy recovers significantly more valuable points with the same budget than several alternatives from the literature, including myopic approximations to the optimal policy.",http://proceedings.mlr.press/v70/jiang17d.html,http://proceedings.mlr.press/v70/jiang17d/jiang17d.pdf,ICML
1591,2017,Provably Optimal Algorithms for Generalized Linear Contextual Bandits,"Lihong Li,         Yu Lu,         Dengyong Zhou","Contextual bandits are widely used in Internet services from news recommendation to advertising, and to Web search. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications where rewards are binary. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an ∼O(√dT)∼O(dT−−−√)\sim O(\sqrt{dT}) regret over T rounds with d dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a √dd−−√\sqrt{d} factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximum likelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for certain cases.",http://proceedings.mlr.press/v70/li17c.html,http://proceedings.mlr.press/v70/li17c/li17c.pdf,ICML
1592,2017,Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks,"Itay Safran,         Ohad Shamir","We provide several new depth-based separation results for feed-forward neural networks, proving that various types of simple and natural functions can be better approximated using deeper networks than shallower ones, even if the shallower networks are much larger. This includes indicators of balls and ellipses; non-linear functions which are radial with respect to the L1L1L_1 norm; and smooth non-linear functions. We also show that these gaps can be observed experimentally: Increasing the depth indeed allows better learning than increasing width, when training neural networks to learn an indicator of a unit ball.",http://proceedings.mlr.press/v70/safran17a.html,http://proceedings.mlr.press/v70/safran17a/safran17a.pdf,ICML
1593,2017,Soft-DTW: a Differentiable Loss Function for Time-Series,"Marco Cuturi,         Mathieu Blondel","We propose in this paper a differentiable learning loss between time series, building upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the Euclidean distance, DTW can compare time series of variable size and is robust to shifts or dilatations across the time dimension. To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a differentiable loss function, and that both its value and gradient can be computed with quadratic time/space complexity (DTW has quadratic time but linear space complexity). We show that this regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011). Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense. Source code is available at https://github.com/mblondel/soft-dtw",http://proceedings.mlr.press/v70/cuturi17a.html,http://proceedings.mlr.press/v70/cuturi17a/cuturi17a.pdf,ICML
1594,2017,Co-clustering through Optimal Transport,"Charlotte Laclau,         Ievgen Redko,         Basarab Matei,         Younès Bennani,         Vincent Brault","In this paper, we present a novel method for co-clustering, an unsupervised learning approach that aims at discovering homogeneous groups of data instances and features by grouping them simultaneously. The proposed method uses the entropy regularized optimal transport between empirical measures defined on data instances and features in order to obtain an estimated joint probability density function represented by the optimal coupling matrix. This matrix is further factorized to obtain the induced row and columns partitions using multiscale representations approach. To justify our method theoretically, we show how the solution of the regularized optimal transport can be seen from the variational inference perspective thus motivating its use for co-clustering. The algorithm derived for the proposed method and its kernelized version based on the notion of Gromov-Wasserstein distance are fast, accurate and can determine automatically the number of both row and column clusters. These features are vividly demonstrated through extensive experimental evaluations.",http://proceedings.mlr.press/v70/laclau17a.html,http://proceedings.mlr.press/v70/laclau17a/laclau17a.pdf,ICML
1595,2017,Density Level Set Estimation on Manifolds with DBSCAN,Heinrich Jiang,"We show that DBSCAN can estimate the connected components of the λ\lambda-density level set {x:f(x)≥λ}\{ x : f(x) \ge \lambda\} given nn i.i.d. samples from an unknown density ff. We characterize the regularity of the level set boundaries using parameter β>0\beta > 0 and analyze the estimation error under the Hausdorff metric. When the data lies in RD\mathbb{R}^D we obtain a rate of ˜O(n−1/(2β+D))\widetilde{O}(n^{-1/(2\beta + D)}), which matches known lower bounds up to logarithmic factors. When the data lies on an embedded unknown dd-dimensional manifold in RD\mathbb{R}^D, then we obtain a rate of ˜O(n−1/(2β+d⋅max{1,β}))\widetilde{O}(n^{-1/(2\beta + d\cdot \max\{1, \beta \})}). Finally, we provide adaptive parameter tuning in order to attain these rates with no a priori knowledge of the intrinsic dimension, density, or β\beta.",http://proceedings.mlr.press/v70/jiang17a.html,http://proceedings.mlr.press/v70/jiang17a/jiang17a.pdf,ICML
1596,2017,Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms,"Arturs Backurs,         Christos Tzamos","The classic algorithm of Viterbi computes the most likely path in a Hidden Markov Model (HMM) that results in a given sequence of observations. It runs in time O(Tn2)O(Tn2)O(Tn^2) given a sequence of T observations from a HMM with n states. Despite significant interest in the problem and prolonged effort by different communities, no known algorithm achieves more than a polylogarithmic speedup. In this paper, we explain this difficulty by providing matching conditional lower bounds. Our lower bounds are based on assumptions that the best known algorithms for the All-Pairs Shortest Paths problem (APSP) and for the Max-Weight k-Clique problem in edge-weighted graphs are essentially tight. Finally, using a recent algorithm by Green Larsen and Williams for online Boolean matrix-vector multiplication, we get a 2Ω(√logn)2Ω(logn√)2^{\Omega(\sqrt{\log n})} speedup for the Viterbi algorithm when there are few distinct transition probabilities in the HMM.",http://proceedings.mlr.press/v70/backurs17a.html,http://proceedings.mlr.press/v70/backurs17a/backurs17a.pdf,ICML
1597,2017,World of Bits: An Open-Domain Platform for Web-Based Agents,"Tianlin Shi,         Andrej Karpathy,         Linxi Fan,         Jonathan Hernandez,         Percy Liang","While simulated game environments have greatly accelerated research in reinforcement learning, existing environments lack the open-domain realism of tasks in computer vision or natural language processing, which operate on artifacts created by humans in natural, organic settings. To foster reinforcement learning research in such settings, we introduce the World of Bits (WoB), a platform in which agents complete tasks on the Internet by performing low-level keyboard and mouse actions. The two main challenges are: (i) to curate a large, diverse set of interesting web-based tasks, and (ii) to ensure that these tasks have a well-defined reward structure and are reproducible despite the transience of the web. To do this, we develop a methodology in which crowdworkers create tasks defined by natural language questions and provide demonstrations of how to answer the question on real websites using keyboard and mouse; HTTP traffic is cached to create a reproducible offline approximation of the web site. Finally, we show that agents trained via behavioral cloning and reinforcement learning can successfully complete a range of our web-based tasks.",http://proceedings.mlr.press/v70/shi17a.html,http://proceedings.mlr.press/v70/shi17a/shi17a.pdf,ICML
1598,2017,Input Switched Affine Networks: An RNN Architecture Designed for Interpretability,"Jakob N. Foerster,         Justin Gilmer,         Jascha Sohl-Dickstein,         Jan Chorowski,         David Sussillo","There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations – in other words an RNN without any explicit nonlinearities, but with input-dependent recurrent weights. This simple form allows the RNN to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each input to the model predictions; we can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; we can fully reverse-engineer the architecture’s solution to a simple task. Despite this ease of interpretation, the input switched affine network achieves reasonable performance on a text modeling tasks, and allows greater computational efficiency than networks with standard nonlinearities.",http://proceedings.mlr.press/v70/foerster17a.html,http://proceedings.mlr.press/v70/foerster17a/foerster17a.pdf,ICML
1599,2017,Batched High-dimensional Bayesian Optimization via Structural Kernel Learning,"Zi Wang,         Chengtao Li,         Stefanie Jegelka,         Pushmeet Kohli","Optimization of high-dimensional black-box functions is an extremely challenging problem. While Bayesian optimization has emerged as a popular approach for optimizing black-box functions, its applicability has been limited to low-dimensional problems due to its computational and statistical challenges arising from high-dimensional settings. In this paper, we propose to tackle these challenges by (1) assuming a latent additive structure in the function and inferring it properly for more efficient and effective BO, and (2) performing multiple evaluations in parallel to reduce the number of iterations required by the method. Our novel approach learns the latent structure with Gibbs sampling and constructs batched queries using determinantal point processes. Experimental validations on both synthetic and real-world functions demonstrate that the proposed method outperforms the existing state-of-the-art approaches.",http://proceedings.mlr.press/v70/wang17h.html,http://proceedings.mlr.press/v70/wang17h/wang17h.pdf,ICML
1600,2017,Breaking Locality Accelerates Block Gauss-Seidel,"Stephen Tu,         Shivaram Venkataraman,         Ashia C. Wilson,         Alex Gittens,         Michael I. Jordan,         Benjamin Recht","Recent work by Nesterov and Stich (2016) showed that momentum can be used to accelerate the rate of convergence for block Gauss-Seidel in the setting where a fixed partitioning of the coordinates is chosen ahead of time. We show that this setting is too restrictive, constructing instances where breaking locality by running non-accelerated Gauss-Seidel with randomly sampled coordinates substantially outperforms accelerated Gauss-Seidel with any fixed partitioning. Motivated by this finding, we analyze the accelerated block Gauss-Seidel algorithm in the random coordinate sampling setting. Our analysis captures the benefit of acceleration with a new data-dependent parameter which is well behaved when the matrix sub-blocks are well-conditioned. Empirically, we show that accelerated Gauss-Seidel with random coordinate sampling provides speedups for large scale machine learning tasks when compared to non-accelerated Gauss-Seidel and the classical conjugate-gradient algorithm.",http://proceedings.mlr.press/v70/tu17a.html,http://proceedings.mlr.press/v70/tu17a/tu17a.pdf,ICML
1601,2017,Bayesian Boolean Matrix Factorisation,"Tammo Rukat,         Chris C. Holmes,         Michalis K. Titsias,         Christopher Yau","Boolean matrix factorisation aims to decompose a binary data matrix into an approximate Boolean product of two low rank, binary matrices: one containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for Boolean matrix factorisation and derive a Metropolised Gibbs sampler that facilitates efficient parallel posterior inference. On real world and simulated data, our method outperforms all currently existing approaches for Boolean matrix factorisation and completion. This is the first method to provide full posterior inference for Boolean Matrix factorisation which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering and, crucially, improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11 thousand genes on commodity hardware.",http://proceedings.mlr.press/v70/rukat17a.html,http://proceedings.mlr.press/v70/rukat17a/rukat17a.pdf,ICML
1602,2017,Projection-free Distributed Online Learning in Networks,"Wenpeng Zhang,         Peilin Zhao,         Wenwu Zhu,         Steven C. H. Hoi,         Tong Zhang","The conditional gradient algorithm has regained a surge of research interest in recent years due to its high efficiency in handling large-scale machine learning problems. However, none of existing studies has explored it in the distributed online learning setting, where locally light computation is assumed. In this paper, we fill this gap by proposing the distributed online conditional gradient algorithm, which eschews the expensive projection operation needed in its counterpart algorithms by exploiting much simpler linear optimization steps. We give a regret bound for the proposed algorithm as a function of the network size and topology, which will be smaller on smaller graphs or “well-connected” graphs. Experiments on two large-scale real-world datasets for a multiclass classification task confirm the computational benefit of the proposed algorithm and also verify the theoretical regret bound.",http://proceedings.mlr.press/v70/zhang17g.html,http://proceedings.mlr.press/v70/zhang17g/zhang17g.pdf,ICML
1603,2017,Being Robust (in High Dimensions) Can Be Practical,"Ilias Diakonikolas,         Gautam Kamath,         Daniel M. Kane,         Jerry Li,         Ankur Moitra,         Alistair Stewart","Robust estimation is much more challenging in high-dimensions than it is in one-dimension: Most techniques either lead to intractable optimization problems or estimators that can tolerate only a tiny fraction of errors. Recent work in theoretical computer science has shown that, in appropriate distributional models, it is possible to robustly estimate the mean and covariance with polynomial time algorithms that can tolerate a constant fraction of corruptions, independent of the dimension. However, the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications. In this work, we address both of these issues by establishing sample complexity bounds that are optimal, up to logarithmic factors, as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions. Finally, we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make high-dimensional robust estimation a realistic possibility.",http://proceedings.mlr.press/v70/diakonikolas17a.html,http://proceedings.mlr.press/v70/diakonikolas17a/diakonikolas17a.pdf,ICML
1604,2017,Learning from Clinical Judgments: Semi-Markov-Modulated Marked Hawkes Processes for Risk Prognosis,"Ahmed M. Alaa,         Scott Hu,         Mihaela Schaar","Critically ill patients in regular wards are vulnerable to unanticipated adverse events which require prompt transfer to the intensive care unit (ICU). To allow for accurate prognosis of deteriorating patients, we develop a novel continuous-time probabilistic model for a monitored patient’s temporal sequence of physiological data. Our model captures “informatively sampled” patient episodes: the clinicians’ decisions on when to observe a hospitalized patient’s vital signs and lab tests over time are represented by a marked Hawkes process, with intensity parameters that are modulated by the patient’s latent clinical states, and with observable physiological data (mark process) modeled as a switching multi-task Gaussian process. In addition, our model captures “informatively censored” patient episodes by representing the patient’s latent clinical states as an absorbing semi-Markov jump process. The model parameters are learned from offline patient episodes in the electronic health records via an EM-based algorithm. Experiments conducted on a cohort of patients admitted to a major medical center over a 3-year period show that risk prognosis based on our model significantly outperforms the currently deployed medical risk scores and other baseline machine learning algorithms.",http://proceedings.mlr.press/v70/alaa17a.html,http://proceedings.mlr.press/v70/alaa17a/alaa17a.pdf,ICML
1605,2017,Dynamic Word Embeddings,"Robert Bamler,         Stephan Mandt","We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec [Mikolov et al., 2013]. These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms–skip-gram smoothing and skip-gram filtering–that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.",http://proceedings.mlr.press/v70/bamler17a.html,http://proceedings.mlr.press/v70/bamler17a/bamler17a.pdf,ICML
1606,2017,Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations,"Yuanzhi Li,         Yingyu Liang","Non-negative matrix factorization is a basic tool for decomposing data into the feature and weight matrices under non-negativity constraints, and in practice is often solved in the alternating minimization framework. However, it is unclear whether such algorithms can recover the ground-truth feature matrix when the weights for different features are highly correlated, which is common in applications. This paper proposes a simple and natural alternating gradient descent based algorithm, and shows that with a mild initialization it provably recovers the ground-truth in the presence of strong correlations. In most interesting cases, the correlation can be in the same order as the highest possible. Our analysis also reveals its several favorable features including robustness to noise. We complement our theoretical results with empirical studies on semi-synthetic datasets, demonstrating its advantage over several popular methods in recovering the ground-truth.",http://proceedings.mlr.press/v70/li17b.html,http://proceedings.mlr.press/v70/li17b/li17b.pdf,ICML
1607,2017,Consistent k-Clustering,"Silvio Lattanzi,         Sergei Vassilvitskii","The study of online algorithms and competitive analysis provides a solid foundation for studying the quality of irrevocable decision making when the data arrives in an online manner. While in some scenarios the decisions are indeed irrevocable, there are many practical situations when changing a previous decision is not impossible, but simply expensive. In this work we formalize this notion and introduce the consistent k-clustering problem. With points arriving online, the goal is to maintain a constant approximate solution, while minimizing the number of reclusterings necessary. We prove a lower bound, showing that Ω(klogn)\Omega(k \log n) changes are necessary in the worst case for a wide range of objective functions. On the positive side, we give an algorithm that needs only O(k2log4n)O(k^2 \log^4n) changes to maintain a constant competitive solution, an exponential improvement on the naive solution of reclustering at every time step. Finally, we show experimentally that our approach performs much better than the theoretical bound, with the number of changes growing approximately as O(logn)O(\log n).",http://proceedings.mlr.press/v70/lattanzi17a.html,http://proceedings.mlr.press/v70/lattanzi17a/lattanzi17a.pdf,ICML
1608,2017,High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm,"Rongda Zhu,         Lingxiao Wang,         Chengxiang Zhai,         Quanquan Gu","We propose a generic stochastic expectation-maximization (EM) algorithm for the estimation of high-dimensional latent variable models. At the core of our algorithm is a novel semi-stochastic variance-reduced gradient designed for the QQQ-function in the EM algorithm. Under a mild condition on the initialization, our algorithm is guaranteed to attain a linear convergence rate to the unknown parameter of the latent variable model, and achieve an optimal statistical rate up to a logarithmic factor for parameter estimation. Compared with existing high-dimensional EM algorithms, our algorithm enjoys a better computational complexity and is therefore more efficient. We apply our generic algorithm to two illustrative latent variable models: Gaussian mixture model and mixture of linear regression, and demonstrate the advantages of our algorithm by both theoretical analysis and numerical experiments. We believe that the proposed semi-stochastic gradient is of independent interest for general nonconvex optimization problems with bivariate structures.",http://proceedings.mlr.press/v70/zhu17a.html,http://proceedings.mlr.press/v70/zhu17a/zhu17a.pdf,ICML
1609,2017,Uniform Convergence Rates for Kernel Density Estimation,Heinrich Jiang,"Kernel density estimation (KDE) is a popular nonparametric density estimation method. We (1) derive finite-sample high-probability density estimation bounds for multivariate KDE under mild density assumptions which hold uniformly in x∈Rdx∈Rdx \in \mathbb{R}^d and bandwidth matrices. We apply these results to (2) mode, (3) density level set, and (4) class probability estimation and attain optimal rates up to logarithmic factors. We then (5) provide an extension of our results under the manifold hypothesis. Finally, we (6) give uniform convergence results for local intrinsic dimension estimation.",http://proceedings.mlr.press/v70/jiang17b.html,http://proceedings.mlr.press/v70/jiang17b/jiang17b.pdf,ICML
1610,2017,Forest-type Regression with General Losses and Robust Forest,"Alexander Hanbo Li,         Andrew Martin","This paper introduces a new general framework for forest-type regression which allows the development of robust forest regressors by selecting from a large family of robust loss functions. In particular, when plugged in the squared error and quantile losses, it will recover the classical random forest and quantile random forest. We then use robust loss functions to develop more robust forest-type regression algorithms. In the experiments, we show by simulation and real data that our robust forests are indeed much more insensitive to outliers, and choosing the right number of nearest neighbors can quickly improve the generalization performance of random forest.",http://proceedings.mlr.press/v70/li17e.html,http://proceedings.mlr.press/v70/li17e/li17e.pdf,ICML
1611,2017,Learning Continuous Semantic Representations of Symbolic Expressions,"Miltiadis Allamanis,         Pankajan Chanthirasegaran,         Pushmeet Kohli,         Charles Sutton","Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence network, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.",http://proceedings.mlr.press/v70/allamanis17a.html,http://proceedings.mlr.press/v70/allamanis17a/allamanis17a.pdf,ICML
1612,2017,Unsupervised Learning by Predicting Noise,"Piotr Bojanowski,         Armand Joulin","Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision; this paper introduces a generic framework to train such networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of the features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with the state-of-the-arts among unsupervised methods on ImageNet and Pascal VOC.",http://proceedings.mlr.press/v70/bojanowski17a.html,http://proceedings.mlr.press/v70/bojanowski17a/bojanowski17a.pdf,ICML
1613,2017,Algorithmic Stability and Hypothesis Complexity,"Tongliang Liu,         Gábor Lugosi,         Gergely Neu,         Dacheng Tao",We introduce a notion of algorithmic stability of learning algorithms—that we term hypothesis stability—that captures stability of the hypothesis output by the learning algorithm in the normed space of functions from which hypotheses are selected. The main result of the paper bounds the generalization error of any learning algorithm in terms of its hypothesis stability. The bounds are based on martingale inequalities in the Banach space to which the hypotheses belong. We apply the general bounds to bound the performance of some learning algorithms based on empirical risk minimization and stochastic gradient descent.,http://proceedings.mlr.press/v70/liu17c.html,http://proceedings.mlr.press/v70/liu17c/liu17c.pdf,ICML
1614,2017,Nyström Method with Kernel K-means++ Samples as Landmarks,"Dino Oglic,         Thomas Gärtner","We investigate, theoretically and empirically, the effectiveness of kernel K-means++ samples as landmarks in the Nyström method for low-rank approximation of kernel matrices. Previous empirical studies (Zhang et al., 2008; Kumar et al.,2012) observe that the landmarks obtained using (kernel) K-means clustering define a good low-rank approximation of kernel matrices. However, the existing work does not provide a theoretical guarantee on the approximation error for this approach to landmark selection. We close this gap and provide the first bound on the approximation error of the Nyström method with kernel K-means++ samples as landmarks. Moreover, for the frequently used Gaussian kernel we provide a theoretically sound motivation for performing Lloyd refinements of kernel K-means++ landmarks in the instance space. We substantiate our theoretical results empirically by comparing the approach to several state-of-the-art algorithms.",http://proceedings.mlr.press/v70/oglic17a.html,http://proceedings.mlr.press/v70/oglic17a/oglic17a.pdf,ICML
1615,2017,OptNet: Differentiable Optimization as a Layer in Neural Networks,"Brandon Amos,         J. Zico Kolter","This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures.",http://proceedings.mlr.press/v70/amos17a.html,http://proceedings.mlr.press/v70/amos17a/amos17a.pdf,ICML
1616,2017,Stochastic Variance Reduction Methods for Policy Evaluation,"Simon S. Du,         Jianshu Chen,         Lihong Li,         Lin Xiao,         Dengyong Zhou","Policy evaluation is concerned with estimating the value function that predicts long-term values of states under a given policy. It is a crucial step in many reinforcement-learning algorithms. In this paper, we focus on policy evaluation with linear function approximation over a fixed dataset. We first transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle-point problem, and then present a primal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the problem. These algorithms scale linearly in both sample size and feature dimension. Moreover, they achieve linear convergence even when the saddle-point problem has only strong concavity in the dual variables but no strong convexity in the primal variables. Numerical experiments on benchmark problems demonstrate the effectiveness of our methods.",http://proceedings.mlr.press/v70/du17a.html,http://proceedings.mlr.press/v70/du17a/du17a.pdf,ICML
1617,2017,Probabilistic Submodular Maximization in Sub-Linear Time,"Serban Stan,         Morteza Zadimoghaddam,         Andreas Krause,         Amin Karbasi","In this paper, we consider optimizing submodular functions that are drawn from some unknown distribution. This setting arises, e.g., in recommender systems, where the utility of a subset of items may depend on a user-specific submodular utility function. In modern applications, the ground set of items is often so large that even the widely used (lazy) greedy algorithm is not efficient enough. As a remedy, we introduce the problem of sublinear time probabilistic submodular maximization: Given training examples of functions (e.g., via user feature vectors), we seek to reduce the ground set so that optimizing new functions drawn from the same distribution will provide almost as much value when restricted to the reduced ground set as when using the full set. We cast this problem as a two-stage submodular maximization and develop a novel efficient algorithm for this problem which offers 1/2(1−1/e2)1/2(1−1/e2)1/2(1 - 1/e^2) approximation ratio for general monotone submodular functions and general matroid constraints. We demonstrate the effectiveness of our approach on several real-world applications where running the maximization problem on the reduced ground set leads to two orders of magnitude speed-up while incurring almost no loss.",http://proceedings.mlr.press/v70/stan17a.html,http://proceedings.mlr.press/v70/stan17a/stan17a.pdf,ICML
1618,2017,Random Feature Expansions for Deep Gaussian Processes,"Kurt Cutajar,         Edwin V. Bonilla,         Pietro Michiardi,         Maurizio Filippone","The composition of multiple Gaussian Processes as a Deep Gaussian Process DGP enables a deep probabilistic nonparametric approach to flexibly tackle complex machine learning problems with sound quantification of uncertainty. Existing inference approaches for DGP models have limited scalability and are notoriously cumbersome to construct. In this work we introduce a novel formulation of DGPs based on random feature expansions that we train using stochastic variational inference. This yields a practical learning framework which significantly advances the state-of-the-art in inference for DGPs, and enables accurate quantification of uncertainty. We extensively showcase the scalability and performance of our proposal on several datasets with up to 8 million observations, and various DGP architectures with up to 30 hidden layers.",http://proceedings.mlr.press/v70/cutajar17a.html,http://proceedings.mlr.press/v70/cutajar17a/cutajar17a.pdf,ICML
1619,2017,Learning to Detect Sepsis with a Multitask Gaussian Process RNN Classifier,"Joseph Futoma,         Sanjay Hariharan,         Katherine Heller","We present a scalable end-to-end classifier that uses streaming physiological and medication data to accurately predict the onset of sepsis, a life-threatening complication from infections that has high mortality and morbidity. Our proposed framework models the multivariate trajectories of continuous-valued physiological time series using multitask Gaussian processes, seamlessly accounting for the high uncertainty, frequent missingness, and irregular sampling rates typically associated with real clinical data. The Gaussian process is directly connected to a black-box classifier that predicts whether a patient will become septic, chosen in our case to be a recurrent neural network to account for the extreme variability in the length of patient encounters. We show how to scale the computations associated with the Gaussian process in a manner so that the entire system can be discriminatively trained end-to-end using backpropagation. In a large cohort of heterogeneous inpatient encounters at our university health system we find that it outperforms several baselines at predicting sepsis, and yields 19.4\% and 55.5\% improved areas under the Receiver Operating Characteristic and Precision Recall curves as compared to the NEWS score currently used by our hospital.",http://proceedings.mlr.press/v70/futoma17a.html,http://proceedings.mlr.press/v70/futoma17a/futoma17a.pdf,ICML
1620,2017,Uniform Deviation Bounds for k-Means Clustering,"Olivier Bachem,         Mario Lucic,         S. Hamed Hassani,         Andreas Krause","Uniform deviation bounds limit the difference between a model’s expected loss and its loss on an empirical sample uniformly for all models in a learning problem. In this paper, we provide a novel framework to obtain uniform deviation bounds for loss functions which are unbounded. As a result, we obtain competitive uniform deviation bounds for k-Means clustering under weak assumptions on the underlying distribution. If the fourth moment is bounded, we prove a rate of O(m−1/2)O(m−1/2)O(m^{-1/2}) compared to the previously known O(m−1/4)O(m−1/4)O(m^{-1/4}) rate. Furthermore, we show that the rate also depends on the kurtosis – the normalized fourth moment which measures the “tailedness” of a distribution. We also provide improved rates under progressively stronger assumptions, namely, bounded higher moments, subgaussianity and bounded support of the underlying distribution.",http://proceedings.mlr.press/v70/bachem17a.html,http://proceedings.mlr.press/v70/bachem17a/bachem17a.pdf,ICML
1621,2017,On the Iteration Complexity of Support Recovery via Hard Thresholding Pursuit,"Jie Shen,         Ping Li","Recovering the support of a sparse signal from its compressed samples has been one of the most important problems in high dimensional statistics. In this paper, we present a novel analysis for the hard thresholding pursuit (HTP) algorithm, showing that it exactly recovers the support of an arbitrary s-sparse signal within O(sklogk) iterations via a properly chosen proxy function, where k is the condition number of the problem. In stark contrast to the theoretical results in the literature, the iteration complexity we obtained holds without assuming the restricted isometry property, or relaxing the sparsity, or utilizing the optimality of the underlying signal. We further extend our result to a more challenging scenario, where the subproblem involved in HTP cannot be solved exactly. We prove that even in this setting, support recovery is possible and the computational complexity of HTP is established. Numerical study substantiates our theoretical results.",http://proceedings.mlr.press/v70/shen17a.html,http://proceedings.mlr.press/v70/shen17a/shen17a.pdf,ICML
1622,2017,Discovering Discrete Latent Topics with Neural Variational Inference,"Yishu Miao,         Edward Grefenstette,         Phil Blunsom","Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.",http://proceedings.mlr.press/v70/miao17a.html,http://proceedings.mlr.press/v70/miao17a/miao17a.pdf,ICML
1623,2017,meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting,"Xu Sun,         Xuancheng Ren,         Shuming Ma,         Houfeng Wang","We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-kk elements (in terms of magnitude) are kept. As a result, only kk rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (kk divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1–4\% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given.",http://proceedings.mlr.press/v70/sun17c.html,http://proceedings.mlr.press/v70/sun17c/sun17c.pdf,ICML
1624,2017,Convexified Convolutional Neural Networks,"Yuchen Zhang,         Percy Liang,         Martin J. Wainwright","We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve competitive or better performance than CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.",http://proceedings.mlr.press/v70/zhang17f.html,http://proceedings.mlr.press/v70/zhang17f/zhang17f.pdf,ICML
1625,2017,Understanding Black-box Predictions via Influence Functions,"Pang Wei Koh,         Percy Liang","How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.",http://proceedings.mlr.press/v70/koh17a.html,http://proceedings.mlr.press/v70/koh17a/koh17a.pdf,ICML
1626,2017,Reduced Space and Faster Convergence in Imperfect-Information Games via Pruning,"Noam Brown,         Tuomas Sandholm","Iterative algorithms such as Counterfactual Regret Minimization (CFR) are the most popular way to solve large zero-sum imperfect-information games. In this paper we introduce Best-Response Pruning (BRP), an improvement to iterative algorithms such as CFR that allows poorly-performing actions to be temporarily pruned. We prove that when using CFR in zero-sum games, adding BRP will asymptotically prune any action that is not part of a best response to some Nash equilibrium. This leads to provably faster convergence and lower space requirements. Experiments show that BRP results in a factor of 7 reduction in space, and the reduction factor increases with game size.",http://proceedings.mlr.press/v70/brown17a.html,http://proceedings.mlr.press/v70/brown17a/brown17a.pdf,ICML
1627,2017,Distributed Mean Estimation with Limited Communication,"Ananda Theertha Suresh,         Felix X. Yu,         Sanjiv Kumar,         H. Brendan McMahan","Motivated by the need for distributed learning and optimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation. Unlike previous works, we make no probabilistic assumptions on the data. We first show that for ddd dimensional data with nnn clients, a naive stochastic rounding approach yields a mean squared error (MSE) of Θ(d/n)Θ(d/n)\Theta(d/n) and uses a constant number of bits per dimension per client. We then extend this naive algorithm in two ways: we show that applying a structured random rotation before quantization reduces the error to O((logd)/n)O((logd)/n)\mathcal{O}((\log d)/n) and a better coding strategy further reduces the error to O(1/n)O(1/n)\mathcal{O}(1/n). We also show that the latter coding strategy is optimal up to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost. We finally demonstrate the practicality of our algorithms by applying them to distributed Lloyd’s algorithm for k-means and power iteration for PCA.",http://proceedings.mlr.press/v70/suresh17a.html,http://proceedings.mlr.press/v70/suresh17a/suresh17a.pdf,ICML
1628,2017,Nonparanormal Information Estimation,"Shashank Singh,         Barnabás Póczos","We study the problem of using i.i.d. samples from an unknown multivariate probability distribution p to estimate the mutual information of p. This problem has recently received attention in two settings: (1) where p is assumed to be Gaussian and (2) where p is assumed only to lie in a large nonparametric smoothness class. Estimators proposed for the Gaussian case converge in high dimensions when the Gaussian assumption holds, but are brittle, failing dramatically when p is not Gaussian, while estimators proposed for the nonparametric case fail to converge with realistic sample sizes except in very low dimension. Hence, there is a lack of robust mutual information estimators for many realistic data. To address this, we propose estimators for mutual information when p is assumed to be a nonparanormal (or Gaussian copula) model, a semiparametric compromise between Gaussian and nonparametric extremes. Using theoretical bounds and experiments, we show these estimators strike a practical balance between robustness and scalability.",http://proceedings.mlr.press/v70/singh17a.html,http://proceedings.mlr.press/v70/singh17a/singh17a.pdf,ICML
1629,2017,Stochastic DCA for the Large-sum of Non-convex Functions Problem and its Application to Group Variable Selection in Classification,"Hoai An Le Thi,         Hoai Minh Le,         Duy Nhat Phan,         Bach Tran","In this paper, we present a stochastic version of DCA (Difference of Convex functions Algorithm) to solve a class of optimization problems whose objective function is a large sum of non-convex functions and a regularization term. We consider the ℓ2,0ℓ2,0\ell_{2,0} regularization to deal with the group variables selection. By exploiting the special structure of the problem, we propose an efficient DC decomposition for which the corresponding stochastic DCA scheme is very inexpensive: it only requires the projection of points onto balls that is explicitly computed. As an application, we applied our algorithm for the group variables selection in multiclass logistic regression. Numerical experiments on several benchmark datasets and synthetic datasets illustrate the efficiency of our algorithm and its superiority over well-known methods, with respect to classification accuracy, sparsity of solution as well as running time.",http://proceedings.mlr.press/v70/thi17a.html,http://proceedings.mlr.press/v70/thi17a/thi17a.pdf,ICML
1630,2017,On Approximation Guarantees for Greedy Low Rank Optimization,"Rajiv Khanna,         Ethan R. Elenberg,         Alexandros G. Dimakis,         Joydeep Ghosh,         Sahand Negahban","We provide new approximation guarantees for greedy low rank matrix estimation under standard assumptions of restricted strong convexity and smoothness. Our novel analysis also uncovers previously unknown connections between the low rank estimation and combinatorial optimization, so much so that our bounds are reminiscent of corresponding approximation bounds in submodular maximization. Additionally, we provide also provide statistical recovery guarantees. Finally, we present empirical comparison of greedy estimation with established baselines on two important real-world problems.",http://proceedings.mlr.press/v70/khanna17a.html,http://proceedings.mlr.press/v70/khanna17a/khanna17a.pdf,ICML
1631,2017,Relative Fisher Information and Natural Gradient for Learning Large Modular Models,"Ke Sun,         Frank Nielsen","Fisher information and natural gradient provided deep insights and powerful tools to artificial neural networks. However related analysis becomes more and more difficult as the learner’s structure turns large and complex. This paper makes a preliminary step towards a new direction. We extract a local component from a large neural system, and define its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system. This concept is important because the geometry structure is much simplified and it can be easily applied to guide the learning of neural networks. We provide an analysis on a list of commonly used components, and demonstrate how to use this concept to further improve optimization.",http://proceedings.mlr.press/v70/sun17b.html,http://proceedings.mlr.press/v70/sun17b/sun17b.pdf,ICML
1632,2017,Video Pixel Networks,"Nal Kalchbrenner,         Aäron Oord,         Karen Simonyan,         Ivo Danihelka,         Oriol Vinyals,         Alex Graves,         Koray Kavukcuoglu","We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects.",http://proceedings.mlr.press/v70/kalchbrenner17a.html,http://proceedings.mlr.press/v70/kalchbrenner17a/kalchbrenner17a.pdf,ICML
1633,2017,Convolutional Sequence to Sequence Learning,"Jonas Gehring,         Michael Auli,         David Grangier,         Denis Yarats,         Yann N. Dauphin","The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT’14 English-German and WMT’14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.",http://proceedings.mlr.press/v70/gehring17a.html,http://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf,ICML
1634,2017,Unifying Task Specification in Reinforcement Learning,Martha White,"Reinforcement learning tasks are typically specified as Markov decision processes. This formalism has been highly successful, though specifications often couple the dynamics of the environment and the learning objective. This lack of modularity can complicate generalization of the task specification, as well as obfuscate connections between different task settings, such as episodic and continuing. In this work, we introduce the RL task formalism, that provides a unification through simple constructs including a generalization to transition-based discounting. Through a series of examples, we demonstrate the generality and utility of this formalism. Finally, we extend standard learning constructs, including Bellman operators, and extend some seminal theoretical results, including approximation errors bounds. Overall, we provide a well-understood and sound formalism on which to build theoretical results and simplify algorithm use and development.",http://proceedings.mlr.press/v70/white17a.html,http://proceedings.mlr.press/v70/white17a/white17a.pdf,ICML
1635,2017,A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions,"Jayadev Acharya,         Hirakendu Das,         Alon Orlitsky,         Ananda Theertha Suresh","Symmetric distribution properties such as support size, support coverage, entropy, and proximity to uniformity, arise in many applications. Recently, researchers applied different estimators and analysis tools to derive asymptotically sample-optimal approximations for each of these properties. We show that a single, simple, plug-in estimator—profile maximum likelihood (PML)—is sample competitive for all symmetric properties, and in particular is asymptotically sample-optimal for all the above properties.",http://proceedings.mlr.press/v70/acharya17a.html,http://proceedings.mlr.press/v70/acharya17a/acharya17a.pdf,ICML
1636,2017,Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning,"Jakob Foerster,         Nantas Nardelli,         Gregory Farquhar,         Triantafyllos Afouras,         Philip H. S. Torr,         Pushmeet Kohli,         Shimon Whiteson","Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A major stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep Q-learning relies. This paper proposes two methods that address this problem: 1) using a multi-agent variant of importance sampling to naturally decay obsolete data and 2) conditioning each agent’s value function on a fingerprint that disambiguates the age of the data sampled from the replay memory. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.",http://proceedings.mlr.press/v70/foerster17b.html,http://proceedings.mlr.press/v70/foerster17b/foerster17b.pdf,ICML
1637,2017,Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning,"Junhyuk Oh,         Satinder Singh,         Honglak Lee,         Pushmeet Kohli","As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.",http://proceedings.mlr.press/v70/oh17a.html,http://proceedings.mlr.press/v70/oh17a/oh17a.pdf,ICML
1638,2017,Learning Important Features Through Propagating Activation Differences,"Avanti Shrikumar,         Peyton Greenside,         Anshul Kundaje","The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its `reference activation’ and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL code: http://goo.gl/RM8jvH",http://proceedings.mlr.press/v70/shrikumar17a.html,http://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf,ICML
1639,2017,Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control,"Yunpeng Pan,         Xinyan Yan,         Evangelos A. Theodorou,         Byron Boots","Sparse Spectrum Gaussian Processes (SSGPs) are a powerful tool for scaling Gaussian processes (GPs) to large datasets. Existing SSGP algorithms for regression assume deterministic inputs, precluding their use in many real-world robotics and engineering applications where accounting for input uncertainty is crucial. We address this problem by proposing two analytic moment-based approaches with closed-form expressions for SSGP regression with uncertain inputs. Our methods are more general and scalable than their standard GP counterparts, and are naturally applicable to multi-step prediction or uncertainty propagation. We show that efficient algorithms for Bayesian filtering and stochastic model predictive control can use these methods, and we evaluate our algorithms with comparative analyses and both real-world and simulated experiments.",http://proceedings.mlr.press/v70/pan17a.html,http://proceedings.mlr.press/v70/pan17a/pan17a.pdf,ICML
1640,2017,Priv’IT: Private and Sample Efficient Identity Testing,"Bryan Cai,         Constantinos Daskalakis,         Gautam Kamath","We develop differentially private hypothesis testing methods for the small sample regime. Given a sample DD\mathcal{D} from a categorical distribution ppp over some domain ΣΣ\Sigma, an explicitly described distribution qqq over ΣΣ\Sigma, some privacy parameter ϵϵ\epsilon, accuracy parameter αα\alpha, and requirements βIβI\beta_\mathrm{I} and βIIβII\beta_\mathrm{II} for the type I and type II errors of our test, the goal is to distinguish between p=qp=qp=q and dtv(p,q)≥αdtv(p,q)≥αd_\mathrm{tv}(p,q) \ge \alpha. We provide theoretical bounds for the sample size |D||D||\mathcal{D}| so that our method both satisfies (ϵ,0)(ϵ,0)(\epsilon,0)-differential privacy, and guarantees βIβI\beta_\mathrm{I} and βIIβII\beta_\mathrm{II} type I and type II errors. We show that differential privacy may come for free in some regimes of parameters, and we always beat the sample complexity resulting from running the χ2χ2\chi^2-test with noisy counts, or standard approaches such as repetition for endowing non-private χ2χ2\chi^2-style statistics with differential privacy guarantees. We experimentally compare the sample complexity of our method to that of recently proposed methods for private hypothesis testing.",http://proceedings.mlr.press/v70/cai17a.html,http://proceedings.mlr.press/v70/cai17a/cai17a.pdf,ICML
1641,2017,Continual Learning Through Synaptic Intelligence,"Friedemann Zenke,         Ben Poole,         Surya Ganguli","While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.",http://proceedings.mlr.press/v70/zenke17a.html,http://proceedings.mlr.press/v70/zenke17a/zenke17a.pdf,ICML
1642,2017,"When can Multi-Site Datasets be Pooled for Regression? Hypothesis Tests, ℓ2ℓ2\ell_2-consistency and Neuroscience Applications","Hao Henry Zhou,         Yilin Zhang,         Vamsi K. Ithapu,         Sterling C. Johnson,         Grace Wahba,         Vikas Singh","Many studies in biomedical and health sciences involve small sample sizes due to logistic or financial constraints. Often, identifying weak (but scientifically interesting) associations between a set of predictors and a response necessitates pooling datasets from multiple diverse labs or groups. While there is a rich literature in statistical machine learning to address distributional shifts and inference in multi-site datasets, it is less clear when such pooling is guaranteed to help (and when it does not) – independent of the inference algorithms we use. In this paper, we present a hypothesis test to answer this question, both for classical and high dimensional linear regression. We precisely identify regimes where pooling datasets across multiple sites is sensible, and how such policy decisions can be made via simple checks executable on each site before any data transfer ever happens. With a focus on Alzheimer’s disease studies, we present empirical results showing that in regimes suggested by our analysis, pooling a local dataset with data from an international study improves power.",http://proceedings.mlr.press/v70/zhou17c.html,http://proceedings.mlr.press/v70/zhou17c/zhou17c.pdf,ICML
1643,2017,Differentially Private Clustering in High-Dimensional Euclidean Spaces,"Maria-Florina Balcan,         Travis Dick,         Yingyu Liang,         Wenlong Mou,         Hongyang Zhang","We study the problem of clustering sensitive data while preserving the privacy of individuals represented in the dataset, which has broad applications in practical machine learning and data analysis tasks. Although the problem has been widely studied in the context of low-dimensional, discrete spaces, much remains unknown concerning private clustering in high-dimensional Euclidean spaces RdRd\mathbb{R}^d. In this work, we give differentially private and efficient algorithms achieving strong guarantees for kkk-means and kkk-median clustering when d=Ω(polylog(n))d=Ω(polylog(n))d=\Omega(\mathsf{polylog}(n)). Our algorithm achieves clustering loss at most log3(n)OPT+poly(logn,d,k)log3(n)OPT+poly(logn,d,k)\log^3(n)\mathsf{OPT}+\mathsf{poly}(\log n,d,k), advancing the state-of-the-art result of √dOPT+poly(logn,dd,kd)d−−√OPT+poly(logn,dd,kd)\sqrt{d}\mathsf{OPT}+\mathsf{poly}(\log n,d^d,k^d). We also study the case where the data points are sss-sparse and show that the clustering loss can scale logarithmically with ddd, i.e., log3(n)OPT+poly(logn,logd,k,s)log3(n)OPT+poly(logn,logd,k,s)\log^3(n)\mathsf{OPT}+\mathsf{poly}(\log n,\log d,k,s). Experiments on both synthetic and real datasets verify the effectiveness of the proposed method.",http://proceedings.mlr.press/v70/balcan17a.html,http://proceedings.mlr.press/v70/balcan17a/balcan17a.pdf,ICML
1644,2017,RobustFill: Neural Program Learning under Noisy I/O,"Jacob Devlin,         Jonathan Uesato,         Surya Bhupatiraju,         Rishabh Singh,         Abdel-rahman Mohamed,         Pushmeet Kohli","The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for `automatic program learning’ have received significant attention: (1) `neural program synthesis’, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) `neural program induction’, where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task and we additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs, which achieve 92\% accuracy on a real-world test set, compared to the 34\% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highly-engineered rule-based system fails entirely.",http://proceedings.mlr.press/v70/devlin17a.html,http://proceedings.mlr.press/v70/devlin17a/devlin17a.pdf,ICML
1645,2017,An Adaptive Test of Independence with Analytic Kernel Embeddings,"Wittawat Jitkrittum,         Zoltán Szabó,         Arthur Gretton","A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time HSIC test, and outperform competing O(n) and O(n log n) tests.",http://proceedings.mlr.press/v70/jitkrittum17a.html,http://proceedings.mlr.press/v70/jitkrittum17a/jitkrittum17a.pdf,ICML
1646,2017,Interactive Learning from Policy-Dependent Human Feedback,"James MacGlashan,         Mark K. Ho,         Robert Loftin,         Bei Peng,         Guan Wang,         David L. Roberts,         Matthew E. Taylor,         Michael L. Littman","This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner’s current policy. We present empirical results that show this assumption to be false—whether human trainers give a positive or negative feedback for a decision is influenced by the learner’s current policy. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot.",http://proceedings.mlr.press/v70/macglashan17a.html,http://proceedings.mlr.press/v70/macglashan17a/macglashan17a.pdf,ICML
1647,2017,Parallel Multiscale Autoregressive Density Estimation,"Scott Reed,         Aäron Oord,         Nal Kalchbrenner,         Sergio Gómez Colmenarejo,         Ziyu Wang,         Yutian Chen,         Dan Belov,         Nando Freitas","PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pixels. This can be sped up by caching activations, but still involves generating each pixel sequentially. In this work, we propose a parallelized PixelCNN that allows more efficient inference by modeling certain pixel groups as conditionally independent. Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup – O(log N) sampling instead of O(N) – enabling the practical generation of 512x512 images. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling.",http://proceedings.mlr.press/v70/reed17a.html,http://proceedings.mlr.press/v70/reed17a/reed17a.pdf,ICML
1648,2017,Adapting Kernel Representations Online Using Submodular Maximization,"Matthew Schlegel,         Yangchen Pan,         Jiecao Chen,         Martha White","Kernel representations provide a nonlinear representation, through similarities to prototypes, but require only simple linear learning algorithms given those prototypes. In a continual learning setting, with a constant stream of observations, it is critical to have an efficient mechanism for sub-selecting prototypes amongst observations. In this work, we develop an approximately submodular criterion for this setting, and an efficient online greedy submodular maximization algorithm for optimizing the criterion. We extend streaming submodular maximization algorithms to continual learning, by removing the need for multiple passes—which is infeasible—and instead introducing the idea of coverage time. We propose a general block-diagonal approximation for the greedy update with our criterion, that enables updates linear in the number of prototypes. We empirically demonstrate the effectiveness of this approximation, in terms of approximation quality, significant runtime improvements, and effective prediction performance.",http://proceedings.mlr.press/v70/schlegel17a.html,http://proceedings.mlr.press/v70/schlegel17a/schlegel17a.pdf,ICML
1649,2017,Logarithmic Time One-Against-Some,"Hal Daumé III,         Nikos Karampatziakis,         John Langford,         Paul Mineiro","We create a new online reduction of multiclass classification to binary classification for which training and prediction time scale logarithmically with the number of classes. We show that several simple techniques give rise to an algorithm which is superior to previous logarithmic time classification approaches while competing with one-against-all in space. The core construction is based on using a tree to select a small subset of labels with high recall, which are then scored using a one-against-some structure with high precision.",http://proceedings.mlr.press/v70/daume17a.html,http://proceedings.mlr.press/v70/daume17a/daume17a.pdf,ICML
1650,2017,Analogical Inference for Multi-relational Embeddings,"Hanxiao Liu,         Yuexin Wu,         Yiming Yang","Large-scale multi-relational embedding refers to the task of learning the latent representations for entities and relations in large knowledge graphs. An effective and scalable solution for this problem is crucial for the true success of knowledge-based inference in a broad range of applications. This paper proposes a novel framework for optimizing the latent representations with respect to the analogical properties of the embedded entities and relations. By formulating the objective function in a differentiable fashion, our model enjoys both its theoretical power and computational scalability, and significantly outperformed a large number of representative baseline methods on benchmark datasets. Furthermore, the model offers an elegant unification of several well-known methods in multi-relational embedding, which can be proven to be special instantiations of our framework.",http://proceedings.mlr.press/v70/liu17d.html,http://proceedings.mlr.press/v70/liu17d/liu17d.pdf,ICML
1651,2017,Cost-Optimal Learning of Causal Graphs,"Murat Kocaoglu,         Alex Dimakis,         Sriram Vishwanath","We consider the problem of learning a causal graph over a set of variables with interventions. We study the cost-optimal causal graph learning problem: For a given skeleton (undirected version of the causal graph), design the set of interventions with minimum total cost, that can uniquely identify any causal graph with the given skeleton. We show that this problem is solvable in polynomial time. Later, we consider the case when the number of interventions is limited. For this case, we provide polynomial time algorithms when the skeleton is a tree or a clique tree. For a general chordal skeleton, we develop an efficient greedy algorithm, which can be improved when the causal graph skeleton is an interval graph.",http://proceedings.mlr.press/v70/kocaoglu17a.html,http://proceedings.mlr.press/v70/kocaoglu17a/kocaoglu17a.pdf,ICML
1652,2017,Data-Efficient Policy Evaluation Through Behavior Policy Search,"Josiah P. Hanna,         Philip S. Thomas,         Peter Stone,         Scott Niekum","We consider the task of evaluating a policy for a Markov decision process (MDP). The standard unbiased technique for evaluating a policy is to deploy the policy and observe its performance. We show that the data collected from deploying a different policy, commonly called the behavior policy, can be used to produce unbiased estimates with lower mean squared error than this standard technique. We derive an analytic expression for the optimal behavior policy — the behavior policy that minimizes the mean squared error of the resulting estimates. Because this expression depends on terms that are unknown in practice, we propose a novel policy evaluation sub-problem, behavior policy search: searching for a behavior policy that reduces mean squared error. We present a behavior policy search algorithm and empirically demonstrate its effectiveness in lowering the mean squared error of policy performance estimates.",http://proceedings.mlr.press/v70/hanna17a.html,http://proceedings.mlr.press/v70/hanna17a/hanna17a.pdf,ICML
1653,2017,Compressed Sensing using Generative Models,"Ashish Bora,         Ajil Jalal,         Eric Price,         Alexandros G. Dimakis","The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all. Instead, we suppose that vectors lie near the range of a generative model G:Rk→RnG:Rk→RnG: \mathbb{R}^k \to \mathbb{R}^n. Our main theorem is that, if GGG is LLL-Lipschitz, then roughly O(klogL)O(klog⁡L)\mathcal{O}(k \log L) random Gaussian measurements suffice for an ℓ2/ℓ2ℓ2/ℓ2\ell_2/\ell_2 recovery guarantee. We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks. Our method can use 555-101010x fewer measurements than Lasso for the same accuracy.",http://proceedings.mlr.press/v70/bora17a.html,http://proceedings.mlr.press/v70/bora17a/bora17a.pdf,ICML
1654,2017,Identification and Model Testing in Linear Structural Equation Models using Auxiliary Variables,"Bryant Chen,         Daniel Kumor,         Elias Bareinboim","We developed a novel approach to identification and model testing in linear structural equation models (SEMs) based on auxiliary variables (AVs), which generalizes a widely-used family of methods known as instrumental variables. The identification problem is concerned with the conditions under which causal parameters can be uniquely estimated from an observational, non-causal covariance matrix. In this paper, we provide an algorithm for the identification of causal parameters in linear structural models that subsumes previous state-of-the-art methods. In other words, our algorithm identifies strictly more coefficients and models than methods previously known in the literature. Our algorithm builds on a graph-theoretic characterization of conditional independence relations between auxiliary and model variables, which is developed in this paper. Further, we leverage this new characterization for allowing identification when limited experimental data or new substantive knowledge about the domain is available. Lastly, we develop a new procedure for model testing using AVs.",http://proceedings.mlr.press/v70/chen17f.html,http://proceedings.mlr.press/v70/chen17f/chen17f.pdf,ICML
1655,2017,Hyperplane Clustering via Dual Principal Component Pursuit,"Manolis C. Tsakiris,         René Vidal","State-of-the-art methods for clustering data drawn from a union of subspaces are based on sparse and low-rank representation theory and convex optimization algorithms. Existing results guaranteeing the correctness of such methods require the dimension of the subspaces to be small relative to the dimension of the ambient space. When this assumption is violated, as is, e.g., in the case of hyperplanes, existing methods are either computationally too intensive (e.g., algebraic methods) or lack sufficient theoretical support (e.g., K-Hyperplanes or RANSAC). In this paper we provide theoretical and algorithmic contributions to the problem of clustering data from a union of hyperplanes, by extending a recent subspace learning method called Dual Principal Component Pursuit (DPCP) to the multi-hyperplane case. We give theoretical guarantees under which, the non-convex ℓ1ℓ1\ell_1 problem associated with DPCP admits a unique global minimizer equal to the normal vector of the most dominant hyperplane. Inspired by this insight, we propose sequential (RANSAC-style) and iterative (K-Hyperplanes-style) hyperplane learning DPCP algorithms, which, via experiments on synthetic and real data, are shown to outperform or be competitive to the state-of-the-art.",http://proceedings.mlr.press/v70/tsakiris17a.html,http://proceedings.mlr.press/v70/tsakiris17a/tsakiris17a.pdf,ICML
1656,2017,Learning Deep Architectures via Generalized Whitened Neural Networks,Ping Luo,"Whitened Neural Network (WNN) is a recent advanced deep architecture, which improves convergence and generalization of canonical neural networks by whitening their internal hidden representation. However, the whitening transformation increases computation time. Unlike WNN that reduced runtime by performing whitening every thousand iterations, which degenerates convergence due to the ill conditioning, we present generalized WNN (GWNN), which has three appealing properties. First, GWNN is able to learn compact representation to reduce computations. Second, it enables whitening transformation to be performed in a short period, preserving good conditioning. Third, we propose a data-independent estimation of the covariance matrix to further improve computational efficiency. Extensive experiments on various datasets demonstrate the benefits of GWNN.",http://proceedings.mlr.press/v70/luo17a.html,http://proceedings.mlr.press/v70/luo17a/luo17a.pdf,ICML
1657,2017,Pain-Free Random Differential Privacy with Sensitivity Sampling,"Benjamin I. P. Rubinstein,         Francesco Aldà","Popular approaches to differential privacy, such as the Laplace and exponential mechanisms, calibrate randomised smoothing through global sensitivity of the target non-private function. Bounding such sensitivity is often a prohibitively complex analytic calculation. As an alternative, we propose a straightforward sampler for estimating sensitivity of non-private mechanisms. Since our sensitivity estimates hold with high probability, any mechanism that would be (ϵ,δ)(\epsilon,\delta)-differentially private under bounded global sensitivity automatically achieves (ϵ,δ,γ)(\epsilon,\delta,\gamma)-random differential privacy (Hall et al. 2012), without any target-specific calculations required. We demonstrate on worked example learners how our usable approach adopts a naturally-relaxed privacy guarantee, while achieving more accurate releases even for non-private functions that are black-box computer programs.",http://proceedings.mlr.press/v70/rubinstein17a.html,http://proceedings.mlr.press/v70/rubinstein17a/rubinstein17a.pdf,ICML
1658,2017,"A Semismooth Newton Method for Fast, Generic Convex Programming","Alnur Ali,         Eric Wong,         J. Zico Kolter","We introduce Newton-ADMM, a method for fast conic optimization. The basic idea is to view the residuals of consecutive iterates generated by the alternating direction method of multipliers (ADMM) as a set of fixed point equations, and then use a nonsmooth Newton method to find a solution; we apply the basic idea to the Splitting Cone Solver (SCS), a state-of-the-art method for solving generic conic optimization problems. We demonstrate theoretically, by extending the theory of semismooth operators, that Newton-ADMM converges rapidly (i.e., quadratically) to a solution; empirically, Newton-ADMM is significantly faster than SCS on a number of problems. The method also has essentially no tuning parameters, generates certificates of primal or dual infeasibility, when appropriate, and can be specialized to solve specific convex problems.",http://proceedings.mlr.press/v70/ali17a.html,http://proceedings.mlr.press/v70/ali17a/ali17a.pdf,ICML
1659,2017,Deep IV: A Flexible Approach for Counterfactual Prediction,"Jason Hartford,         Greg Lewis,         Kevin Leyton-Brown,         Matt Taddy",Counterfactual prediction requires understanding causal relationships between so-called treatment and outcome variables. This paper provides a recipe for augmenting deep learning methods to accurately characterize such relationships in the presence of instrument variables (IVs) – sources of treatment randomization that are conditionally independent from the outcomes. Our IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework allows us to take advantage of off-the-shelf supervised learning techniques to estimate causal effects by adapting the loss function. Experiments show that it outperforms existing machine learning approaches.,http://proceedings.mlr.press/v70/hartford17a.html,http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf,ICML
1660,2017,Innovation Pursuit: A New Approach to the Subspace Clustering Problem,"Mostafa Rahmani,         George Atia","This paper presents a new scalable approach, termed Innovation Pursuit (iPursuit), to the problem of subspace clustering. iPursuit rests on a new geometrical idea whereby each subspace is identified based on its novelty with respect to the other subspaces. The subspaces are identified consecutively by solving a series of simple linear optimization problems, each searching for a direction of innovation in the span of the data. A detailed mathematical analysis is provided establishing sufficient conditions for the proposed approach to correctly cluster the data points. Moreover, the proposed direction search approach can be integrated with spectral clustering to yield a new variant of spectral-clustering-based algorithms. Remarkably, the proposed approach can provably yield exact clustering even when the subspaces have significant intersections. The numerical simulations demonstrate that iPursuit can often outperform the state-of-the-art subspace clustering algorithms – more so for subspaces with significant intersections – along with substantial reductions in computational complexity.",http://proceedings.mlr.press/v70/rahmani17b.html,http://proceedings.mlr.press/v70/rahmani17b/rahmani17b.pdf,ICML
1661,2017,Curiosity-driven Exploration by Self-supervised Prediction,"Deepak Pathak,         Pulkit Agrawal,         Alexei A. Efros,         Trevor Darrell","In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.",http://proceedings.mlr.press/v70/pathak17a.html,http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf,ICML
1662,2017,Nonnegative Matrix Factorization for Time Series Recovery From a Few Temporal Aggregates,"Jiali Mei,         Yohann De Castro,         Yannig Goude,         Georges Hébrail","Motivated by electricity consumption reconstitution, we propose a new matrix recovery method using nonnegative matrix factorization (NMF). The task tackled here is to reconstitute electricity consumption time series at a fine temporal scale from measures that are temporal aggregates of individual consumption. Contrary to existing NMF algorithms, the proposed method uses temporal aggregates as input data, instead of matrix entries. Furthermore, the proposed method is extended to take into account individual autocorrelation to provide better estimation, using a recent convex relaxation of quadratically constrained quadratic programs. Extensive experiments on synthetic and real-world electricity consumption datasets illustrate the effectiveness of the proposed method.",http://proceedings.mlr.press/v70/mei17a.html,http://proceedings.mlr.press/v70/mei17a/mei17a.pdf,ICML
1663,2017,The Sample Complexity of Online One-Class Collaborative Filtering,"Reinhard Heckel,         Kannan Ramchandran","We consider the online one-class collaborative filtering (CF) problem that consist of recommending items to users over time in an online fashion based on positive ratings only. This problem arises when users respond only occasionally to a recommendation with a positive rating, and never with a negative one. We study the impact of the probability of a user responding to a recommendation, pfpfp_f, on the sample complexity, and ask whether receiving positive and negative ratings, instead of positive ratings only, improves the sample complexity. Both questions arise in the design of recommender systems. We introduce a simple probabilistic user model, and analyze the performance of an online user-based CF algorithm. We prove that after an initial cold start phase, where recommendations are invested in exploring the user’s preferences, this algorithm makes—up to a fraction of the recommendations required for updating the user’s preferences—perfect recommendations. The number of ratings required for the cold start phase is nearly proportional to 1/pf1/pf1/p_f, and that for updating the user’s preferences is essentially independent of pfpfp_f. As a consequence we find that, receiving positive and negative ratings instead of only positive ones improves the number of ratings required for initial exploration by a factor of 1/pf1/pf1/p_f, which can be significant.",http://proceedings.mlr.press/v70/heckel17a.html,http://proceedings.mlr.press/v70/heckel17a/heckel17a.pdf,ICML
1664,2017,Deep Bayesian Active Learning with Image Data,"Yarin Gal,         Riashat Islam,         Zoubin Ghahramani","Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).",http://proceedings.mlr.press/v70/gal17a.html,http://proceedings.mlr.press/v70/gal17a/gal17a.pdf,ICML
1665,2017,Asymmetric Tri-training for Unsupervised Domain Adaptation,"Kuniaki Saito,         Yoshitaka Ushiku,         Tatsuya Harada","It is important to apply models trained on a large number of labeled samples to different domains because collecting many labeled samples in various domains is expensive. To learn discriminative representations for the target domain, we assume that artificially labeling the target samples can result in a good representation. Tri-training leverages three classifiers equally to provide pseudo-labels to unlabeled samples; however, the method does not assume labeling samples generated from a different domain. In this paper, we propose the use of an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train the neural networks as if they are true labels. In our work, we use three networks asymmetrically, and by asymmetric, we mean that two networks are used to label unlabeled target samples, and one network is trained by the pseudo-labeled samples to obtain target-discriminative representations. Our proposed method was shown to achieve a state-of-the-art performance on the benchmark digit recognition datasets for domain adaptation.",http://proceedings.mlr.press/v70/saito17a.html,http://proceedings.mlr.press/v70/saito17a/saito17a.pdf,ICML
1666,2017,Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction,"Wen Sun,         Arun Venkatraman,         Geoffrey J. Gordon,         Byron Boots,         J. Andrew Bagnell","Recently, researchers have demonstrated state-of-the-art performance on sequential prediction problems using deep neural networks and Reinforcement Learning (RL). For some of these problems, oracles that can demonstrate good performance may be available during training, but are not used by plain RL methods. To take advantage of this extra information, we propose AggreVaTeD, an extension of the Imitation Learning (IL) approach of Ross \& Bagnell (2014). AggreVaTeD allows us to use expressive differentiable policy representations such as deep networks, while leveraging training-time oracles to achieve faster and more accurate solutions with less training data. Specifically, we present two gradient procedures that can learn neural network policies for several problems, including a sequential prediction task and several high-dimensional robotics control problems. We also provide a comprehensive theoretical study of IL that demonstrates that we can expect up to exponentially-lower sample complexity for learning with AggreVaTeD than with plain RL algorithms. Our results and theory indicate that IL (and AggreVaTeD in particular) can be a more effective strategy for sequential prediction than plain RL.",http://proceedings.mlr.press/v70/sun17d.html,http://proceedings.mlr.press/v70/sun17d/sun17d.pdf,ICML
1667,2017,Enumerating Distinct Decision Trees,Salvatore Ruggieri,"The search space for the feature selection problem in decision tree learning is the lattice of subsets of the available features. We provide an exact enumeration procedure of the subsets that lead to all and only the distinct decision trees. The procedure can be adopted to prune the search space of complete and heuristics search methods in wrapper models for feature selection. Based on this, we design a computational optimization of the sequential backward elimination heuristics with a performance improvement of up to 100X.",http://proceedings.mlr.press/v70/ruggieri17a.html,http://proceedings.mlr.press/v70/ruggieri17a/ruggieri17a.pdf,ICML
1668,2017,Scalable Bayesian Rule Lists,"Hongyu Yang,         Cynthia Rudin,         Margo Seltzer","We present an algorithm for building probabilistic rule lists that is two orders of magnitude faster than previous work. Rule list algorithms are competitors for decision tree algorithms. They are associative classifiers, in that they are built from pre-mined association rules. They have a logical structure that is a sequence of IF-THEN rules, identical to a decision list or one-sided decision tree. Instead of using greedy splitting and pruning like decision tree algorithms, we aim to fully optimize over rule lists, striking a practical balance between accuracy, interpretability, and computational speed. The algorithm presented here uses a mixture of theoretical bounds (tight enough to have practical implications as a screening or bounding procedure), computational reuse, and highly tuned language libraries to achieve computational efficiency. Currently, for many practical problems, this method achieves better accuracy and sparsity than decision trees. In many cases, the computational time is practical and often less than that of decision trees.",http://proceedings.mlr.press/v70/yang17h.html,http://proceedings.mlr.press/v70/yang17h/yang17h.pdf,ICML
1669,2017,Count-Based Exploration with Neural Density Models,"Georg Ostrovski,         Marc G. Bellemare,         Aäron Oord,         Rémi Munos","Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma’s Revenge. We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.’s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma’s Revenge.",http://proceedings.mlr.press/v70/ostrovski17a.html,http://proceedings.mlr.press/v70/ostrovski17a/ostrovski17a.pdf,ICML
1670,2017,Leveraging Union of Subspace Structure to Improve Constrained Clustering,"John Lipor,         Laura Balzano","Many clustering problems in computer vision and other contexts are also classification problems, where each cluster shares a meaningful label. Subspace clustering algorithms in particular are often applied to problems that fit this description, for example with face images or handwritten digits. While it is straightforward to request human input on these datasets, our goal is to reduce this input as much as possible. We present a pairwise-constrained clustering algorithm that actively selects queries based on the union-of-subspaces model. The central step of the algorithm is in querying points of minimum margin between estimated subspaces; analogous to classifier margin, these lie near the decision boundary. We prove that points lying near the intersection of subspaces are points with low margin. Our procedure can be used after any subspace clustering algorithm that outputs an affinity matrix. We demonstrate on several datasets that our algorithm drives the clustering error down considerably faster than the state-of-the-art active query algorithms on datasets with subspace structure and is competitive on other datasets.",http://proceedings.mlr.press/v70/lipor17a.html,http://proceedings.mlr.press/v70/lipor17a/lipor17a.pdf,ICML
1671,2017,Uncovering Causality from Multivariate Hawkes Integrated Cumulants,"Massil Achab,         Emmanuel Bacry,         Stéphane Gaı̈ffas,         Iacopo Mastromatteo,         Jean-François Muzy","We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each node of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. A consequence is that it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process. A theoretical analysis allows to prove that this new estimation technique is consistent. Moreover, we show on numerical experiments that our approach is indeed very robust to the shape of the kernels, and gives appealing results on the MemeTracker database and on financial order book data.",http://proceedings.mlr.press/v70/achab17a.html,http://proceedings.mlr.press/v70/achab17a/achab17a.pdf,ICML
1672,2017,Recovery Guarantees for One-hidden-layer Neural Networks,"Kai Zhong,         Zhao Song,         Prateek Jain,         Peter L. Bartlett,         Inderjit S. Dhillon","In this paper, we consider regression problems with one-hidden-layer neural networks (1NNs). We distill some properties of activation functions that lead to   local strong convexity in the neighborhood of the ground-truth parameters for the 1NN squared-loss objective and most popular nonlinear activation functions  satisfy the distilled properties, including rectified linear units (ReLUs), leaky ReLUs, squared ReLUs and sigmoids. For activation functions that are also smooth, we show local linear convergence guarantees of gradient descent under a resampling rule. For homogeneous activations, we show tensor methods are able to initialize the parameters to fall into the local strong convexity region. As a result, tensor initialization followed by gradient descent is guaranteed to recover the ground truth with sample complexity d⋅log(1/ϵ)⋅poly(k,λ)d⋅log(1/ϵ)⋅poly(k,λ) d \cdot \log(1/\epsilon) \cdot \mathrm{poly}(k,\lambda ) and computational complexity n⋅d⋅poly(k,λ)n⋅d⋅poly(k,λ)n\cdot d \cdot \mathrm{poly}(k,\lambda)  for smooth  homogeneous activations with high probability, where ddd is the dimension of the input, kkk (k≤dk≤dk\leq d) is the number of hidden nodes, λλ\lambda is a conditioning  property of the ground-truth parameter matrix between the input layer and the hidden layer, ϵϵ\epsilon is the targeted precision and nnn is the number of samples. To the best of our knowledge, this is the first work that provides recovery guarantees for 1NNs with both sample complexity and computational complexity linear in the input dimension and logarithmic in the precision.",http://proceedings.mlr.press/v70/zhong17a.html,http://proceedings.mlr.press/v70/zhong17a/zhong17a.pdf,ICML
1673,2017,Zonotope Hit-and-run for Efficient Sampling from Projection DPPs,"Guillaume Gautier,         Rémi Bardenet,         Michal Valko","Determinantal point processes (DPPs) are distributions over sets of items that model diversity using kernels. Their applications in machine learning include summary extraction and recommendation systems. Yet, the cost of sampling from a DPP is prohibitive in large-scale applications, which has triggered an effort towards efficient approximate samplers. We build a novel MCMC sampler that combines ideas from combinatorial geometry, linear programming, and Monte Carlo methods to sample from DPPs with a fixed sample cardinality, also called projection DPPs. Our sampler leverages the ability of the hit-and-run MCMC kernel to efficiently move across convex bodies. Previous theoretical results yield a fast mixing time of our chain when targeting a distribution that is close to a projection DPP, but not a DPP in general. Our empirical results demonstrate that this extends to sampling projection DPPs, i.e., our sampler is more sample-efficient than previous approaches which in turn translates to faster convergence when dealing with costly-to-evaluate functions, such as summary extraction in our experiments.",http://proceedings.mlr.press/v70/gautier17a.html,http://proceedings.mlr.press/v70/gautier17a/gautier17a.pdf,ICML
1674,2017,Multilabel Classification with Group Testing and Codes,"Shashanka Ubaru,         Arya Mazumdar","In recent years, the multiclass and mutlilabel classification problems we encounter in many applications have very large (10310310^3–10610610^6) number of classes. However, each instance belongs to only one or few classes, i.e., the label vectors are sparse. In this work, we propose a novel approach based on group testing to solve such large multilabel classification problems with sparse label vectors. We describe various group testing constructions, and advocate the use of concatenated Reed Solomon codes and unbalanced bipartite expander graphs for extreme classification problems. The proposed approach has several advantages theoretically and practically over existing popular methods. Our method operates on the binary alphabet and can utilize the well-established binary classifiers for learning. The error correction capabilities of the codes are leveraged for the first time in the learning problem to correct prediction errors. Even if a linearly growing number of classifiers mis-classify, these errors are fully corrected. We establish Hamming loss error bounds for the approach. More importantly, our method utilizes a simple prediction algorithm and does not require matrix inversion or solving optimization problems making the algorithm very inexpensive. Numerical experiments with various datasets illustrate the superior performance of our method.",http://proceedings.mlr.press/v70/ubaru17a.html,http://proceedings.mlr.press/v70/ubaru17a/ubaru17a.pdf,ICML
1675,2017,Learning to Align the Source Code to the Compiled Object Code,"Dor Levy,         Lior Wolf","We propose a new neural network architecture and use it for the task of statement-by-statement alignment of source code and its compiled object code. Our architecture learns the alignment between the two sequences – one being the translation of the other – by mapping each statement to a context-dependent representation vector and aligning such vectors using a grid of the two sequence domains. Our experiments include short C functions, both artificial and human-written, and show that our neural network architecture is able to predict the alignment with high accuracy, outperforming known baselines. We also demonstrate that our model is general and can learn to solve graph problems such as the Traveling Salesman Problem.",http://proceedings.mlr.press/v70/levy17a.html,http://proceedings.mlr.press/v70/levy17a/levy17a.pdf,ICML
1676,2017,Local-to-Global Bayesian Network Structure Learning,"Tian Gao,         Kshitij Fadnis,         Murray Campbell","We introduce a new local-to-global structure learning algorithm, called graph growing structure learning (GGSL), to learn Bayesian network (BN) structures. GGSL starts at a (random) node and then gradually expands the learned structure through a series of local learning steps. At each local learning step, the proposed algorithm only needs to revisit a subset of the learned nodes, consisting of the local neighborhood of a target, and therefore improves on both memory and time efficiency compared to traditional global structure learning approaches. GGSL also improves on the existing local-to-global learning approaches by removing the need for conflict-resolving AND-rules, and achieves better learning accuracy. We provide theoretical analysis for the local learning step, and show that GGSL outperforms existing algorithms on benchmark datasets. Overall, GGSL demonstrates a novel direction to scale up BN structure learning while limiting accuracy loss.",http://proceedings.mlr.press/v70/gao17a.html,http://proceedings.mlr.press/v70/gao17a/gao17a.pdf,ICML
1677,2017,Constrained Policy Optimization,"Joshua Achiam,         David Held,         Aviv Tamar,         Pieter Abbeel","For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.",http://proceedings.mlr.press/v70/achiam17a.html,http://proceedings.mlr.press/v70/achiam17a/achiam17a.pdf,ICML
1678,2017,Bayesian Optimization with Tree-structured Dependencies,"Rodolphe Jenatton,         Cedric Archambeau,         Javier González,         Matthias Seeger","Bayesian optimization has been successfully used to optimize complex black-box functions whose evaluations are expensive. In many applications, like in deep learning and predictive analytics, the optimization domain is itself complex and structured. In this work, we focus on use cases where this domain exhibits a known dependency structure. The benefit of leveraging this structure is twofold: we explore the search space more efficiently and posterior inference scales more favorably with the number of observations than Gaussian Process-based approaches published in the literature. We introduce a novel surrogate model for Bayesian optimization which combines independent Gaussian Processes with a linear model that encodes a tree-based dependency structure and can transfer information between overlapping decision sequences. We also design a specialized two-step acquisition function that explores the search space more effectively. Our experiments on synthetic tree-structured functions and the tuning of feedforward neural networks trained on a range of binary classification datasets show that our method compares favorably with competing approaches.",http://proceedings.mlr.press/v70/jenatton17a.html,http://proceedings.mlr.press/v70/jenatton17a/jenatton17a.pdf,ICML
1679,2017,Self-Paced Co-training,"Fan Ma,         Deyu Meng,         Qi Xie,         Zina Li,         Xuanyi Dong","Co-training is a well-known semi-supervised learning approach which trains classifiers on two different views and exchanges labels of unlabeled instances in an iterative way. During co-training process, labels of unlabeled instances in the training pool are very likely to be false especially in the initial training rounds, while the standard co-training algorithm utilizes a “draw without replacement” manner and does not remove these false labeled instances from training. This issue not only tends to degenerate its performance but also hampers its fundamental theory. Besides, there is no optimization model to explain what objective a cotraining process optimizes. To these issues, in this study we design a new co-training algorithm named self-paced cotraining (SPaCo) with a “draw with replacement” learning mode. The rationality of SPaCo can be proved under theoretical assumptions utilized in traditional co-training research, and furthermore, the algorithm exactly complies with the alternative optimization process for an optimization model of self-paced curriculum learning, which can be finely explained in robust learning manner. Experimental results substantiate the superiority of the proposed method as compared with current state-of-the-art co-training methods.",http://proceedings.mlr.press/v70/ma17b.html,http://proceedings.mlr.press/v70/ma17b/ma17b.pdf,ICML
1680,2017,A Simple Multi-Class Boosting Framework with Theoretical Guarantees and Empirical Proficiency,"Ron Appel,         Pietro Perona","There is a need for simple yet accurate white-box learning systems that train quickly and with little data. To this end, we showcase REBEL, a multi-class boosting method, and present a novel family of weak learners called localized similarities. Our framework provably minimizes the training error of any dataset at an exponential rate. We carry out experiments on a variety of synthetic and real datasets, demonstrating a consistent tendency to avoid overfitting. We evaluate our method on MNIST and standard UCI datasets against other state-of-the-art methods, showing the empirical proficiency of our method.",http://proceedings.mlr.press/v70/appel17a.html,http://proceedings.mlr.press/v70/appel17a/appel17a.pdf,ICML
1681,2017,Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP,"Satyen Kale,         Zohar Karnin,         Tengyuan Liang,         Dávid Pál","Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.",http://proceedings.mlr.press/v70/kale17a.html,http://proceedings.mlr.press/v70/kale17a/kale17a.pdf,ICML
1682,2017,Local Bayesian Optimization of Motor Skills,"Riad Akrour,         Dmitry Sorokin,         Jan Peters,         Gerhard Neumann","Bayesian optimization is renowned for its sample efficiency but its application to higher dimensional tasks is impeded by its focus on global optimization. To scale to higher dimensional problems, we leverage the sample efficiency of Bayesian optimization in a local context. The optimization of the acquisition function is restricted to the vicinity of a Gaussian search distribution which is moved towards high value areas of the objective. The proposed information-theoretic update of the search distribution results in a Bayesian interpretation of local stochastic search: the search distribution encodes prior knowledge on the optimum’s location and is weighted at each iteration by the likelihood of this location’s optimality. We demonstrate the effectiveness of our algorithm on several benchmark objective functions as well as a continuous robotic task in which an informative prior is obtained by imitation learning.",http://proceedings.mlr.press/v70/akrour17a.html,http://proceedings.mlr.press/v70/akrour17a/akrour17a.pdf,ICML
1683,2017,"Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery","Ashkan Panahi,         Devdatt Dubhashi,         Fredrik D. Johansson,         Chiranjib Bhattacharyya","Standard clustering methods such as K-means, Gaussian mixture models, and hierarchical clustering are beset by local minima, which are sometimes drastically suboptimal. Moreover the number of clusters K must be known in advance. The recently introduced the sum-of-norms (SON) or Clusterpath convex relaxation of k-means and hierarchical clustering shrinks cluster centroids toward one another and ensure a unique global minimizer. We give a scalable stochastic incremental algorithm based on proximal iterations to solve the SON problem with convergence guarantees. We also show that the algorithm recovers clusters under quite general conditions which have a similar form to the unifying proximity condition introduced in the approximation algorithms community (that covers paradigm cases such as Gaussian mixtures and planted partition models). We give experimental results to confirm that our algorithm scales much better than previous methods while producing clusters of comparable quality.",http://proceedings.mlr.press/v70/panahi17a.html,http://proceedings.mlr.press/v70/panahi17a/panahi17a.pdf,ICML
1684,2017,Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study,"Samuel Ritter,         David G. T. Barrett,         Adam Santoro,         Matt M. Botvinick","Deep neural networks (DNNs) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. While past work sought to advance our understanding of these models, none has made use of the rich history of problem descriptions, theories, and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.",http://proceedings.mlr.press/v70/ritter17a.html,http://proceedings.mlr.press/v70/ritter17a/ritter17a.pdf,ICML
1685,2017,Learning Hawkes Processes from Short Doubly-Censored Event Sequences,"Hongteng Xu,         Dixin Luo,         Hongyuan Zha","Many real-world applications require robust algorithms to learn point process models based on a type of incomplete data — the so-called short doubly-censored (SDC) event sequences. In this paper, we study this critical problem of quantitative asynchronous event sequence analysis under the framework of Hawkes processes by leveraging the general idea of data synthesis. In particular, given SDC event sequences observed in a variety of time intervals, we propose a sampling-stitching data synthesis method — sampling predecessor and successor for each SDC event sequence from potential candidates and stitching them together to synthesize long training sequences. The rationality and the feasibility of our method are discussed in terms of arguments based on likelihood. Experiments on both synthetic and real-world data demonstrate that the proposed data synthesis method improves learning results indeed for both time-invariant and time-varying Hawkes processes.",http://proceedings.mlr.press/v70/xu17b.html,http://proceedings.mlr.press/v70/xu17b/xu17b.pdf,ICML
1686,2017,Robust Submodular Maximization: A Non-Uniform Partitioning Approach,"Ilija Bogunovic,         Slobodan Mitrović,         Jonathan Scarlett,         Volkan Cevher","We study the problem of maximizing a monotone submodular function subject to a cardinality constraint kkk, with the added twist that a number of items ττ\tau from the returned set may be removed. We focus on the worst-case setting considered by Orlin et al.\ (2016), in which a constant-factor approximation guarantee was given for τ=o(k−−√)τ=o(k)\tau = o(\sqrt{k}). In this paper, we solve a key open problem raised therein, presenting a new Partitioned Robust (PRo) submodular maximization algorithm that achieves the same guarantee for more general τ=o(k)τ=o(k)\tau = o(k). Our algorithm constructs partitions consisting of buckets with exponentially increasing sizes, and applies standard submodular optimization subroutines on the buckets in order to construct the robust solution. We numerically demonstrate the performance of PRo in data summarization and influence maximization, demonstrating gains over both the greedy algorithm and the algorithm of Orlin et al.\ (2016).",http://proceedings.mlr.press/v70/bogunovic17a.html,http://proceedings.mlr.press/v70/bogunovic17a/bogunovic17a.pdf,ICML
1687,2017,Differentially Private Submodular Maximization: Data Summarization in Disguise,"Marko Mitrovic,         Mark Bun,         Andreas Krause,         Amin Karbasi","Many data summarization applications are captured by the general framework of submodular maximization. As a consequence, a wide range of efficient approximation algorithms have been developed. However, when such applications involve sensitive data about individuals, their privacy concerns are not automatically addressed. To remedy this problem, we propose a general and systematic study of differentially private submodular maximization. We present privacy-preserving algorithms for both monotone and non-monotone submodular maximization under cardinality, matroid, and p-extendible system constraints, with guarantees that are competitive with optimal. Along the way, we analyze a new algorithm for non-monotone submodular maximization, which is the first (even non-privately) to achieve a constant approximation ratio while running in linear time. We additionally provide two concrete experiments to validate the efficacy of these algorithms.",http://proceedings.mlr.press/v70/mitrovic17a.html,http://proceedings.mlr.press/v70/mitrovic17a/mitrovic17a.pdf,ICML
1688,2017,Deciding How to Decide: Dynamic Routing in Artificial Neural Networks,"Mason McGill,         Pietro Perona","We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable statically-routed networks.",http://proceedings.mlr.press/v70/mcgill17a.html,http://proceedings.mlr.press/v70/mcgill17a/mcgill17a.pdf,ICML
1689,2017,Robust Adversarial Reinforcement Learning,"Lerrel Pinto,         James Davidson,         Rahul Sukthankar,         Abhinav Gupta","Deep neural networks coupled with fast simulation and improved computational speeds have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can just be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced – that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper, Walker2d and Ant) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.",http://proceedings.mlr.press/v70/pinto17a.html,http://proceedings.mlr.press/v70/pinto17a/pinto17a.pdf,ICML
1690,2017,Learned Optimizers that Scale and Generalize,"Olga Wichrowska,         Niru Maheswaranathan,         Matthew W. Hoffman,         Sergio Gómez Colmenarejo,         Misha Denil,         Nando Freitas,         Jascha Sohl-Dickstein","Learning to learn has emerged as an important direction for achieving artificial intelligence. Two of the primary barriers to its adoption are an inability to scale to larger problems and a limited ability to generalize to new tasks. We introduce a learned gradient descent optimizer that generalizes well to new tasks, and which has significantly reduced memory and computation overhead. We achieve this by introducing a novel hierarchical RNN architecture, with minimal per-parameter overhead, augmented with additional architectural features that mirror the known structure of optimization tasks. We also develop a meta-training ensemble of small, diverse, optimization tasks capturing common properties of loss landscapes. The optimizer learns to outperform RMSProp/ADAM on problems in this corpus. More importantly, it performs comparably or better when applied to small convolutional neural networks, despite seeing no neural networks in its meta-training set. Finally, it generalizes to train Inception V3 and ResNet V2 architectures on the ImageNet dataset for thousands of steps, optimization problems that are of a vastly different scale than those it was trained on.",http://proceedings.mlr.press/v70/wichrowska17a.html,http://proceedings.mlr.press/v70/wichrowska17a/wichrowska17a.pdf,ICML
1691,2017,Asynchronous Stochastic Gradient Descent with Delay Compensation,"Shuxin Zheng,         Qi Meng,         Taifeng Wang,         Wei Chen,         Nenghai Yu,         Zhi-Ming Ma,         Tie-Yan Liu","With the fast development of deep learning, it has become common to learn big neural networks using massive training data. Asynchronous Stochastic Gradient Descent (ASGD) is widely adopted to fulfill this task for its efficiency, which is, however, known to suffer from the problem of delayed gradients. That is, when a local worker adds its gradient to the global model, the global model may have been updated by other workers and this gradient becomes “delayed”. We propose a novel technology to compensate this delay, so as to make the optimization behavior of ASGD closer to that of sequential SGD. This is achieved by leveraging Taylor expansion of the gradient function and efficient approximators to the Hessian matrix of the loss function. We call the new algorithm Delay Compensated ASGD (DC-ASGD). We evaluated the proposed algorithm on CIFAR-10 and ImageNet datasets, and the experimental results demonstrate that DC-ASGD outperforms both synchronous SGD and asynchronous SGD, and nearly approaches the performance of sequential SGD.",http://proceedings.mlr.press/v70/zheng17b.html,http://proceedings.mlr.press/v70/zheng17b/zheng17b.pdf,ICML
1692,2017,Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability,"Shayegan Omidshafiei,         Jason Pazis,         Christopher Amato,         Jonathan P. How,         John Vian","Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.",http://proceedings.mlr.press/v70/omidshafiei17a.html,http://proceedings.mlr.press/v70/omidshafiei17a/omidshafiei17a.pdf,ICML
1693,2017,Re-revisiting Learning on Hypergraphs: Confidence Interval and Subgradient Method,"Chenzi Zhang,         Shuguang Hu,         Zhihao Gavin Tang,         T-H. Hubert Chan","We revisit semi-supervised learning on hypergraphs. Same as previous approaches, our method uses a convex program whose objective function is not everywhere differentiable. We exploit the non-uniqueness of the optimal solutions, and consider confidence intervals which give the exact ranges that unlabeled vertices take in any optimal solution. Moreover, we give a much simpler approach for solving the convex program based on the subgradient method. Our experiments on real-world datasets confirm that our confidence interval approach on hypergraphs outperforms existing methods, and our sub-gradient method gives faster running times when the number of vertices is much larger than the number of edges.",http://proceedings.mlr.press/v70/zhang17d.html,http://proceedings.mlr.press/v70/zhang17d/zhang17d.pdf,ICML
1694,2017,Sliced Wasserstein Kernel for Persistence Diagrams,"Mathieu Carrière,         Marco Cuturi,         Steve Oudot","Persistence diagrams (PDs) play a key role in topological data analysis (TDA), in which they are routinely used to describe succinctly complex topological properties of complicated shapes. PDs enjoy strong stability properties and have proven their utility in various learning contexts. They do not, however, live in a space naturally endowed with a Hilbert structure and are usually compared with specific distances, such as the bottleneck distance. To incorporate PDs in a learning pipeline, several kernels have been proposed for PDs with a strong emphasis on the stability of the RKHS distance w.r.t. perturbations of the PDs. In this article, we use the Sliced Wasserstein approximation of the Wasserstein distance to define a new kernel for PDs, which is not only provably stable but also provably discriminative w.r.t. the Wasserstein distance W1∞W∞1W^1_\infty between PDs. We also demonstrate its practicality, by developing an approximation technique to reduce kernel computation time, and show that our proposal compares favorably to existing kernels for PDs on several benchmarks.",http://proceedings.mlr.press/v70/carriere17a.html,http://proceedings.mlr.press/v70/carriere17a/carriere17a.pdf,ICML
1695,2017,Evaluating the Variance of Likelihood-Ratio Gradient Estimators,"Seiya Tokui,         Issei Sato","The likelihood-ratio method is often used to estimate gradients of stochastic computations, for which baselines are required to reduce the estimation variance. Many types of baselines have been proposed, although their degree of optimality is not well understood. In this study, we establish a novel framework of gradient estimation that includes most of the common gradient estimators as special cases. The framework gives a natural derivation of the optimal estimator that can be interpreted as a special case of the likelihood-ratio method so that we can evaluate the optimal degree of practical techniques with it. It bridges the likelihood-ratio method and the reparameterization trick while still supporting discrete variables. It is derived from the exchange property of the differentiation and integration. To be more specific, it is derived by the reparameterization trick and local marginalization analogous to the local expectation gradient. We evaluate various baselines and the optimal estimator for variational learning and show that the performance of the modern estimators is close to the optimal estimator.",http://proceedings.mlr.press/v70/tokui17a.html,http://proceedings.mlr.press/v70/tokui17a/tokui17a.pdf,ICML
1696,2017,Contextual Decision Processes with low Bellman rank are PAC-Learnable,"Nan Jiang,         Akshay Krishnamurthy,         Alekh Agarwal,         John Langford,         Robert E. Schapire","This paper studies systematic exploration for reinforcement learning (RL) with rich observations and function approximation. We introduce contextual decision processes (CDPs), that unify most prior RL settings. Our first contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-optimal behavior in CDPs and is naturally small for many well-studied RL models. Our second contribution is a new RL algorithm that does systematic exploration to learn near-optimal behavior in CDPs with low Bellman rank. The algorithm requires a number of samples that is polynomial in all relevant parameters but independent of the number of unique contexts. Our approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for RL with function approximation.",http://proceedings.mlr.press/v70/jiang17c.html,http://proceedings.mlr.press/v70/jiang17c/jiang17c.pdf,ICML
1697,2017,Second-Order Kernel Online Convex Optimization with Adaptive Sketching,"Daniele Calandriello,         Alessandro Lazaric,         Michal Valko","Kernel online convex optimization (KOCO) is a framework combining the expressiveness of non-parametric kernel models with the regret guarantees of online learning. First-order KOCO methods such as functional gradient descent require only O(t)O(t)O(t) time and space per iteration, and, when the only information on the losses is their convexity, achieve a minimax optimal O(T−−√)O(T)O(\sqrt{T}) regret. Nonetheless, many common losses in kernel problems, such as squared loss, logistic loss, and squared hinge loss posses stronger curvature that can be exploited. In this case, second-order KOCO methods achieve O(log(Det(K)))O(log⁡(Det(K)))O(\log(\mathrm{Det}(K))) regret, which we show scales as O(defflogT)O(defflog⁡T)O(deff \log T), where deffdeffdeff is the effective dimension of the problem and is usually much smaller than O(T−−√)O(T)O(\sqrt{T}). The main drawback of second-order methods is their much higher O(t2)O(t2)O(t^2) space and time complexity. In this paper, we introduce kernel online Newton step (KONS), a new second-order KOCO method that also achieves O(defflogT)O(defflog⁡T)O(deff\log T) regret. To address the computational complexity of second-order methods, we introduce a new matrix sketching algorithm for the kernel matrix~KKK, and show that for a chosen parameter γ≤1γ≤1\gamma \leq 1 our Sketched-KONS reduces the space and time complexity by a factor of γ2γ2\gamma^2 to O(t2γ2)O(t2γ2)O(t^2\gamma^2) space and time per iteration, while incurring only 1/γ1/γ1/\gamma times more regret.",http://proceedings.mlr.press/v70/calandriello17a.html,http://proceedings.mlr.press/v70/calandriello17a/calandriello17a.pdf,ICML
1698,2017,Preferential Bayesian Optimization,"Javier González,         Zhenwen Dai,         Andreas Damianou,         Neil D. Lawrence","Bayesian optimization (BO) has emerged during the last few years as an effective approach to optimize black-box functions where direct queries of the objective are expensive. We consider the case where direct access to the function is not possible, but information about user preferences is. Such scenarios arise in problems where human preferences are modeled, such as A/B tests or recommender systems. We present a new framework for this scenario that we call Preferential Bayesian Optimization (PBO) and that allows to find the optimum of a latent function that can only be queried through pairwise comparisons, so-called duels. PBO extend the applicability of standard BO ideas and generalizes previous discrete dueling approaches by modeling the probability of the the winner of each duel by means of Gaussian process model with a Bernoulli likelihood. The latent preference function is used to define a family of acquisition functions that extend usual policies used in BO. We illustrate the benefits of PBO in a variety of experiments in which we show how the way correlations are modeled is the key ingredient to drastically reduce the number of comparisons to find the optimum of the latent function of interest.",http://proceedings.mlr.press/v70/gonzalez17a.html,http://proceedings.mlr.press/v70/gonzalez17a/gonzalez17a.pdf,ICML
1699,2017,Delta Networks for Optimized Recurrent Network Computation,"Daniel Neil,         Jun Haeng Lee,         Tobi Delbruck,         Shih-Chii Liu","Many neural networks exhibit stability in their activation patterns over time in response to inputs from sensors operating under real-world conditions. By capitalizing on this property of natural signals, we propose a Recurrent Neural Network (RNN) architecture called a delta network in which each neuron transmits its value only when the change in its activation exceeds a threshold. The execution of RNNs as delta networks is attractive because their states must be stored and fetched at every timestep, unlike in convolutional neural networks (CNNs). We show that a naive run-time delta network implementation offers modest improvements on the number of memory accesses and computes, but optimized training techniques confer higher accuracy at higher speedup. With these optimizations, we demonstrate a 9X reduction in cost with negligible loss of accuracy for the TIDIGITS audio digit recognition benchmark. Similarly, on the large Wall Street Journal (WSJ) speech recognition benchmark, pretrained networks can also be greatly accelerated as delta networks and trained delta networks show a 5.7x improvement with negligible loss of accuracy. Finally, on an end-to-end CNN-RNN network trained for steering angle prediction in a driving dataset, the RNN cost can be reduced by a substantial 100X.",http://proceedings.mlr.press/v70/neil17a.html,http://proceedings.mlr.press/v70/neil17a/neil17a.pdf,ICML
1700,2017,Stochastic Generative Hashing,"Bo Dai,         Ruiqi Guo,         Sanjiv Kumar,         Niao He,         Le Song","Learning-based binary hashing has become a powerful paradigm for fast search and retrieval in massive databases. However, due to the requirement of discrete outputs for the hash functions, learning such functions is known to be very challenging. In addition, the objective functions adopted by existing hashing techniques are mostly chosen heuristically. In this paper, we propose a novel generative approach to learn hash functions through Minimum Description Length principle such that the learned hash codes maximally compress the dataset and can also be used to regenerate the inputs. We also develop an efficient learning algorithm based on the stochastic distributional gradient, which avoids the notorious difficulty caused by binary output constraints, to jointly optimize the parameters of the hash function and the associated generative model. Extensive experiments on a variety of large-scale datasets show that the proposed method achieves better retrieval results than the existing state-of-the-art methods.",http://proceedings.mlr.press/v70/dai17a.html,http://proceedings.mlr.press/v70/dai17a/dai17a.pdf,ICML
1701,2017,Generalization and Equilibrium in Generative Adversarial Nets (GANs),"Sanjeev Arora,         Rong Ge,         Yingyu Liang,         Tengyu Ma,         Yi Zhang","It is shown that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an approximate pure equilibrium exists in the discriminator/generator game for a natural training objective (Wasserstein) when generator capacity and training set sizes are moderate. This existence of equilibrium inspires MIX+GAN protocol, which can be combined with any existing GAN training, and empirically shown to improve some of them.",http://proceedings.mlr.press/v70/arora17a.html,http://proceedings.mlr.press/v70/arora17a/arora17a.pdf,ICML
1702,2017,Robust Budget Allocation via Continuous Submodular Functions,"Matthew Staib,         Stefanie Jegelka","The optimal allocation of resources for maximizing influence, spread of information or coverage, has gained attention in the past years, in particular in machine learning and data mining. But in applications, the parameters of the problem are rarely known exactly, and using wrong parameters can lead to undesirable outcomes. We hence revisit a continuous version of the Budget Allocation or Bipartite Influence Maximization problem introduced by Alon et al. (2012) from a robust optimization perspective, where an adversary may choose the least favorable parameters within a confidence set. The resulting problem is a nonconvex-concave saddle point problem (or game). We show that this nonconvex problem can be solved exactly by leveraging connections to continuous submodular functions, and by solving a constrained submodular minimization problem. Although constrained submodular minimization is hard in general, here, we establish conditions under which such a problem can be solved to arbitrary precision ϵϵ\epsilon.",http://proceedings.mlr.press/v70/staib17a.html,http://proceedings.mlr.press/v70/staib17a/staib17a.pdf,ICML
1703,2017,SplitNet: Learning to Semantically Split Deep Networks for Parameter Reduction and Model Parallelization,"Juyong Kim,         Yookoon Park,         Gunhee Kim,         Sung Ju Hwang","We propose a novel deep neural network that is both lightweight and effectively structured for model parallelization. Our network, which we name as SplitNet, automatically learns to split the network weights into either a set or a hierarchy of multiple groups that use disjoint sets of features, by learning both the class-to-group and feature-to-group assignment matrices along with the network weights. This produces a tree-structured network that involves no connection between branched subtrees of semantically disparate class groups. SplitNet thus greatly reduces the number of parameters and requires significantly less computations, and is also embarrassingly model parallelizable at test time, since the network evaluation for each subnetwork is completely independent except for the shared lower layer weights that can be duplicated over multiple processors. We validate our method with two deep network models (ResNet and AlexNet) on two different datasets (CIFAR-100 and ILSVRC 2012) for image classification, on which our method obtains networks with significantly reduced number of parameters while achieving comparable or superior classification accuracies over original full deep networks, and accelerated test speed with multiple GPUs.",http://proceedings.mlr.press/v70/kim17b.html,http://proceedings.mlr.press/v70/kim17b/kim17b.pdf,ICML
1704,2017,Globally Induced Forest: A Prepruning Compression Scheme,"Jean-Michel Begon,         Arnaud Joly,         Pierre Geurts","Tree-based ensemble models are heavy memory-wise. An undesired state of affairs considering nowadays datasets, memory-constrained environment and fitting/prediction times. In this paper, we propose the Globally Induced Forest (GIF) to remedy this problem. GIF is a fast prepruning approach to build lightweight ensembles by iteratively deepening the current forest. It mixes local and global optimizations to produce accurate predictions under memory constraints in reasonable time. We show that the proposed method is more than competitive with standard tree-based ensembles under corresponding constraints, and can sometimes even surpass much larger models.",http://proceedings.mlr.press/v70/begon17a.html,http://proceedings.mlr.press/v70/begon17a/begon17a.pdf,ICML
1705,2017,Distributed and Provably Good Seedings for k-Means in Constant Rounds,"Olivier Bachem,         Mario Lucic,         Andreas Krause","The k-Means++ algorithm is the state of the art algorithm to solve k-Means clustering problems as the computed clusterings are O(log k) competitive in expectation. However, its seeding step requires k inherently sequential passes through the full data set making it hard to scale to massive data sets. The standard remedy is to use the k-Means|| algorithm which reduces the number of sequential rounds and is thus suitable for a distributed setting. In this paper, we provide a novel analysis of the k-Means|| algorithm that bounds the expected solution quality for any number of rounds and oversampling factors greater than k, the two parameters one needs to choose in practice. In particular, we show that k-Means|| provides provably good clusterings even for a small, constant number of iterations. This theoretical finding explains the common observation that k-Means|| performs extremely well in practice even if the number of rounds is low. We further provide a hard instance that shows that an additive error term as encountered in our analysis is inevitable if less than k-1 rounds are employed.",http://proceedings.mlr.press/v70/bachem17b.html,http://proceedings.mlr.press/v70/bachem17b/bachem17b.pdf,ICML
1706,2017,Tensor Belief Propagation,"Andrew Wrigley,         Wee Sun Lee,         Nan Ye","We propose a new approximate inference algorithm for graphical models, tensor belief propagation, based on approximating the messages passed in the junction tree algorithm. Our algorithm represents the potential functions of the graphical model and all messages on the junction tree compactly as mixtures of rank-1 tensors. Using this representation, we show how to perform the operations required for inference on the junction tree efficiently: marginalisation can be computed quickly due to the factored form of rank-1 tensors while multiplication can be approximated using sampling. Our analysis gives sufficient conditions for the algorithm to perform well, including for the case of high-treewidth graphs, for which exact inference is intractable. We compare our algorithm experimentally with several approximate inference algorithms and show that it performs well.",http://proceedings.mlr.press/v70/wrigley17a.html,http://proceedings.mlr.press/v70/wrigley17a/wrigley17a.pdf,ICML
1707,2017,Efficient Online Bandit Multiclass Learning with O~(T−−√)O~(T)\tilde{O}(\sqrt{T}) Regret,"Alina Beygelzimer,         Francesco Orabona,         Chicheng Zhang","We present an efficient second-order algorithm with O~(1/ηT−−√)O~(1/ηT)\tilde{O}(1/\eta \sqrt{T}) regret for the bandit online multiclass problem. The regret bound holds simultaneously with respect to a family of loss functions parameterized by ηη\eta, ranging from hinge loss (η=0η=0\eta=0) to squared hinge loss (η=1η=1\eta=1). This provides a solution to the open problem of (Abernethy, J. and Rakhlin, A. An efficient bandit algorithm for T−−√T\sqrt{T}-regret in online multiclass prediction? In COLT, 2009). We test our algorithm experimentally, showing that it performs favorably against earlier algorithms.",http://proceedings.mlr.press/v70/beygelzimer17a.html,http://proceedings.mlr.press/v70/beygelzimer17a/beygelzimer17a.pdf,ICML
1708,2017,Kernelized Support Tensor Machines,"Lifang He,         Chun-Ta Lu,         Guixiang Ma,         Shen Wang,         Linlin Shen,         Philip S. Yu,         Ann B. Ragin","In the context of supervised tensor learning, preserving the structural information and exploiting the discriminative nonlinear relationships of tensor data are crucial for improving the performance of learning tasks. Based on tensor factorization theory and kernel methods, we propose a novel Kernelized Support Tensor Machine (KSTM) which integrates kernelized tensor factorization with maximum-margin criterion. Specifically, the kernelized factorization technique is introduced to approximate the tensor data in kernel space such that the complex nonlinear relationships within tensor data can be explored. Further, dual structural preserving kernels are devised to learn the nonlinear boundary between tensor data. As a result of joint optimization, the kernels obtained in KSTM exhibit better generalization power to discriminative analysis. The experimental results on real-world neuroimaging datasets show the superiority of KSTM over the state-of-the-art techniques.",http://proceedings.mlr.press/v70/he17a.html,http://proceedings.mlr.press/v70/he17a/he17a.pdf,ICML
1709,2017,Toward Controlled Generation of Text,"Zhiting Hu,         Zichao Yang,         Xiaodan Liang,         Ruslan Salakhutdinov,         Eric P. Xing","Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible text sentences, whose attributes are controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders (VAEs) and holistic attribute discriminators for effective imposition of semantic structures. The model can alternatively be seen as enhancing VAEs with the wake-sleep algorithm for leveraging fake samples as extra training data. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns interpretable representations from even only word annotations, and produces short sentences with desired attributes of sentiment and tenses. Quantitative experiments using trained classifiers as evaluators validate the accuracy of sentence and attribute generation.",http://proceedings.mlr.press/v70/hu17e.html,http://proceedings.mlr.press/v70/hu17e/hu17e.pdf,ICML
1710,2017,Why is Posterior Sampling Better than Optimism for Reinforcement Learning?,"Ian Osband,         Benjamin Van Roy","Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an O~(HSAT−−−−√)O~(HSAT)\tilde{O}(H\sqrt{SAT}) Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of O~(HSAT−−−√)O~(HSAT)\tilde{O}(H S \sqrt{AT}) for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation.",http://proceedings.mlr.press/v70/osband17a.html,http://proceedings.mlr.press/v70/osband17a/osband17a.pdf,ICML
1711,2017,Stochastic Adaptive Quasi-Newton Methods for Minimizing Expected Values,"Chaoxu Zhou,         Wenbo Gao,         Donald Goldfarb","We propose a novel class of stochastic, adaptive methods for minimizing self-concordant functions which can be expressed as an expected value. These methods generate an estimate of the true objective function by taking the empirical mean over a sample drawn at each step, making the problem tractable. The use of adaptive step sizes eliminates the need for the user to supply a step size. Methods in this class include extensions of gradient descent (GD) and BFGS. We show that, given a suitable amount of sampling, the stochastic adaptive GD attains linear convergence in expectation, and with further sampling, the stochastic adaptive BFGS attains R-superlinear convergence. We present experiments showing that these methods compare favorably to SGD.",http://proceedings.mlr.press/v70/zhou17a.html,http://proceedings.mlr.press/v70/zhou17a/zhou17a.pdf,ICML
1712,2017,Canopy  Fast Sampling with Cover Trees,"Manzil Zaheer,         Satwik Kottur,         Amr Ahmed,         José Moura,         Alex Smola","Hierarchical Bayesian models often capture distributions over a very large number of distinct atoms. The need for these models arises when organizing huge amount of unsupervised data, for instance, features extracted using deep convnets that can be exploited to organize abundant unlabeled images. Inference for hierarchical Bayesian models in such cases can be rather nontrivial, leading to approximate approaches. In this work, we propose Canopy, a sampler based on Cover Trees that is exact, has guaranteed runtime logarithmic in the number of atoms, and is provably polynomial in the inherent dimensionality of the underlying parameter space. In other words, the algorithm is as fast as search over a hierarchical data structure. We provide theory for Canopy and demonstrate its effectiveness on both synthetic and real datasets, consisting of over 100 million images.",http://proceedings.mlr.press/v70/zaheer17b.html,http://proceedings.mlr.press/v70/zaheer17b/zaheer17b.pdf,ICML
1713,2017,"Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging","Shusen Wang,         Alex Gittens,         Michael W. Mahoney","We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR—namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the “mass” in the responses and the optimal objective value. For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching.",http://proceedings.mlr.press/v70/wang17c.html,http://proceedings.mlr.press/v70/wang17c/wang17c.pdf,ICML
1714,2017,An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis,Yuandong Tian,"In this paper, we explore theoretical properties of training a two-layered ReLU network g(x;w)=∑Kj=1σ(w⊤jx)g(x;w)=∑j=1Kσ(wj⊤x)g(\mathbf{x}; \mathbf{w}) = \sum_{j=1}^K \sigma(\mathbf{w}_j^\top\mathbf{x}) with centered ddd-dimensional spherical Gaussian input xx\mathbf{x} (σσ\sigma=ReLU). We train our network with gradient descent on ww\mathbf{w} to mimic the output of a teacher network with the same architecture and fixed parameters w∗w∗\mathbf{w}^*. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters (“out-of-plane“) are not isolated and form manifolds, and characterize in-plane critical-point-free regions for two-ReLU case. On the other hand, convergence to w∗w∗\mathbf{w}^* for one ReLU node is guaranteed with at least (1−ϵ)/2(1−ϵ)/2(1-\epsilon)/2 probability, if weights are initialized randomly with standard deviation upper-bounded by O(ϵ/d−−√)O(ϵ/d)O(\epsilon/\sqrt{d}), in accordance with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards w∗w∗\mathbf{w}^* (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings.",http://proceedings.mlr.press/v70/tian17a.html,http://proceedings.mlr.press/v70/tian17a/tian17a.pdf,ICML
1715,2017,Neural Networks and Rational Functions,Matus Telgarsky,"Neural networks and rational functions efficiently approximate each other. In more detail, it is shown here that for any ReLU network, there exists a rational function of degree O(polylog(1/ϵ))O(polylog(1/ϵ))O(polylog(1/\epsilon)) which is ϵϵ\epsilon-close, and similarly for any rational function there exists a ReLU network of size O(polylog(1/ϵ))O(polylog(1/ϵ))O(polylog(1/\epsilon)) which is ϵϵ\epsilon-close. By contrast, polynomials need degree Ω(poly(1/ϵ))Ω(poly(1/ϵ))\Omega(poly(1/\epsilon)) to approximate even a single ReLU. When converting a ReLU network to a rational function as above, the hidden constants depend exponentially on the number of layers, which is shown to be tight; in other words, a compositional representation can be beneficial even for rational functions.",http://proceedings.mlr.press/v70/telgarsky17a.html,http://proceedings.mlr.press/v70/telgarsky17a/telgarsky17a.pdf,ICML
1716,2017,Prediction and Control with Temporal Segment Models,"Nikhil Mishra,         Pieter Abbeel,         Igor Mordatch","We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.",http://proceedings.mlr.press/v70/mishra17a.html,http://proceedings.mlr.press/v70/mishra17a/mishra17a.pdf,ICML
1717,2017,Active Learning for Cost-Sensitive Classification,"Akshay Krishnamurthy,         Alekh Agarwal,         Tzu-Kuo Huang,         Hal Daumé III,         John Langford","We design an active learning algorithm for cost-sensitive multiclass classification: problems where different errors have different costs. Our algorithm, COAL, makes predictions by regressing to each label’s cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. Our experiment with COAL show significant improvements in labeling effort and test cost over passive and active baselines.",http://proceedings.mlr.press/v70/krishnamurthy17a.html,http://proceedings.mlr.press/v70/krishnamurthy17a/krishnamurthy17a.pdf,ICML
1718,2017,Bayesian inference on random simple graphs with power law degree distributions,"Juho Lee,         Creighton Heaukulani,         Zoubin Ghahramani,         Lancelot F. James,         Seungjin Choi","We present a model for random simple graphs with power law (i.e., heavy-tailed) degree distributions. To attain this behavior, the edge probabilities in the graph are constructed from Bertoin–Fujita–Roynette–Yor (BFRY) random variables, which have been recently utilized in Bayesian statistics for the construction of power law models in several applications. Our construction readily extends to capture the structure of latent factors, similarly to stochastic block-models, while maintaining its power law degree distribution. The BFRY random variables are well approximated by gamma random variables in a variational Bayesian inference routine, which we apply to several network datasets for which power law degree distributions are a natural assumption. By learning the parameters of the BFRY distribution via probabilistic inference, we are able to automatically select the appropriate power law behavior from the data. In order to further scale our inference procedure, we adopt stochastic gradient ascent routines where the gradients are computed on minibatches (i.e., subsets) of the edges in the graph.",http://proceedings.mlr.press/v70/lee17a.html,http://proceedings.mlr.press/v70/lee17a/lee17a.pdf,ICML
1719,2017,Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to Non-smooth Concave Maximization,"Bo Liu,         Xiao-Tong Yuan,         Lezi Wang,         Qingshan Liu,         Dimitris N. Metaxas","Iterative Hard Thresholding (IHT) is a class of projected gradient descent methods for optimizing sparsity-constrained minimization models, with the best known efficiency and scalability in practice. As far as we know, the existing IHT-style methods are designed for sparse minimization in primal form. It remains open to explore duality theory and algorithms in such a non-convex and NP-hard setting. In this article, we bridge the gap by establishing a duality theory for sparsity-constrained minimization with ℓ2ℓ2\ell_2-regularized objective and proposing an IHT-style algorithm for dual maximization. Our sparse duality theory provides a set of sufficient and necessary conditions under which the original NP-hard/non-convex problem can be equivalently solved in a dual space. The proposed dual IHT algorithm is a super-gradient method for maximizing the non-smooth dual objective. An interesting finding is that the sparse recovery performance of dual IHT is invariant to the Restricted Isometry Property (RIP), which is required by all the existing primal IHT without sparsity relaxation. Moreover, a stochastic variant of dual IHT is proposed for large-scale stochastic optimization. Numerical results demonstrate that dual IHT algorithms can achieve more accurate model estimation given small number of training data and have higher computational efficiency than the state-of-the-art primal IHT-style algorithms.",http://proceedings.mlr.press/v70/liu17e.html,http://proceedings.mlr.press/v70/liu17e/liu17e.pdf,ICML
1720,2017,Regret Minimization in Behaviorally-Constrained Zero-Sum Games,"Gabriele Farina,         Christian Kroer,         Tuomas Sandholm","No-regret learning has emerged as a powerful tool for solving extensive-form games. This was facilitated by the counterfactual-regret minimization (CFR) framework, which relies on the instantiation of regret minimizers for simplexes at each information set of the game. We use an instantiation of the CFR framework to develop algorithms for solving behaviorally-constrained (and, as a special case, perturbed in the Selten sense) extensive-form games, which allows us to compute approximate Nash equilibrium refinements. Nash equilibrium refinements are motivated by a major deficiency in Nash equilibrium: it provides virtually no guarantees on how it will play in parts of the game tree that are reached with zero probability. Refinements can mend this issue, but have not been adopted in practice, mostly due to a lack of scalable algorithms. We show that, compared to standard algorithms, our method finds solutions that have substantially better refinement properties, while enjoying a convergence rate that is comparable to that of state-of-the-art algorithms for Nash equilibrium computation both in theory and practice.",http://proceedings.mlr.press/v70/farina17a.html,http://proceedings.mlr.press/v70/farina17a/farina17a.pdf,ICML
1721,2017,Learning Gradient Descent: Better Generalization and Longer Horizons,"Kaifeng Lv,         Shunhua Jiang,         Jian Li","Training deep neural networks is a highly nontrivial task, involving carefully selecting appropriate training algorithms, scheduling step sizes and tuning other hyperparameters. Trying different combinations can be quite labor-intensive and time consuming. Recently, researchers have tried to use deep learning algorithms to exploit the landscape of the loss function of the training problem of interest, and learn how to optimize over it in an automatic way. In this paper, we propose a new learning-to-learn model and some useful and practical tricks. Our optimizer outperforms generic, hand-crafted optimization algorithms and state-of-the-art learning-to-learn optimizers by DeepMind in many tasks. We demonstrate the effectiveness of our algorithms on a number of tasks, including deep MLPs, CNNs, and simple LSTMs.",http://proceedings.mlr.press/v70/lv17a.html,http://proceedings.mlr.press/v70/lv17a/lv17a.pdf,ICML
1722,2017,Recursive Partitioning for Personalization using Observational Data,Nathan Kallus,"We study the problem of learning to choose from mm discrete treatment options (e.g., news item or medical drug) the one with best causal effect for a particular instance (e.g., user or patient) where the training data consists of passive observations of covariates, treatment, and the outcome of the treatment. The standard approach to this problem is regress and compare: split the training data by treatment, fit a regression model in each split, and, for a new instance, predict all mm outcomes and pick the best. By reformulating the problem as a single learning task rather than mm separate ones, we propose a new approach based on recursively partitioning the data into regimes where different treatments are optimal. We extend this approach to an optimal partitioning approach that finds a globally optimal partition, achieving a compact, interpretable, and impactful personalization model. We develop new tools for validating and evaluating personalization models on observational data and use these to demonstrate the power of our novel approaches in a personalized medicine and a job training application.",http://proceedings.mlr.press/v70/kallus17a.html,http://proceedings.mlr.press/v70/kallus17a/kallus17a.pdf,ICML
1723,2017,Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs,"Alon Brutzkus,         Amir Globerson","Deep learning models are often successfully trained using gradient descent, despite the worst case hardness of the underlying non-convex optimization problem. The key question is then under what conditions can one prove that optimization will succeed. Here we provide a strong result of this kind. We consider a neural net with one hidden layer and a convolutional structure with no overlap and a ReLU activation function. For this architecture we show that learning is NP-complete in the general case, but that when the input distribution is Gaussian, gradient descent converges to the global optimum in polynomial time. To the best of our knowledge, this is the first global optimality guarantee of gradient descent on a convolutional neural network with ReLU activations.",http://proceedings.mlr.press/v70/brutzkus17a.html,http://proceedings.mlr.press/v70/brutzkus17a/brutzkus17a.pdf,ICML
1724,2017,Post-Inference Prior Swapping,"Willie Neiswanger,         Eric Xing","While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice, convenient priors that allow for computationally cheap or tractable inference are commonly used. In this paper, we investigate the following question: for a given model, is it possible to compute an inference result with any convenient false prior, and afterwards, given any target prior of interest, quickly transform this result into the target posterior? A potential solution is to use importance sampling (IS). However, we demonstrate that IS will fail for many choices of the target prior, depending on its parametric form and similarity to the false prior. Instead, we propose prior swapping, a method that leverages the pre-inferred false posterior to efficiently generate accurate posterior samples under arbitrary target priors. Prior swapping lets us apply less-costly inference algorithms to certain models, and incorporate new or updated prior information “post-inference”. We give theoretical guarantees about our method, and demonstrate it empirically on a number of models and priors.",http://proceedings.mlr.press/v70/neiswanger17a.html,http://proceedings.mlr.press/v70/neiswanger17a/neiswanger17a.pdf,ICML
1725,2017,Approximate Steepest Coordinate Descent,"Sebastian U. Stich,         Anant Raj,         Martin Jaggi","We propose a new selection rule for the coordinate selection in coordinate descent methods for huge-scale optimization. The efficiency of this novel scheme is provably better than the efficiency of uniformly random selection, and can reach the efficiency of steepest coordinate descent (SCD), enabling an acceleration of a factor of up to nnn, the number of coordinates. In many practical applications, our scheme can be implemented at no extra cost and computational efficiency very close to the faster uniform selection. Numerical experiments with Lasso and Ridge regression show promising improvements, in line with our theoretical guarantees.",http://proceedings.mlr.press/v70/stich17a.html,http://proceedings.mlr.press/v70/stich17a/stich17a.pdf,ICML
1726,2017,Input Convex Neural Networks,"Brandon Amos,         Lei Xu,         J. Zico Kolter","This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.",http://proceedings.mlr.press/v70/amos17b.html,http://proceedings.mlr.press/v70/amos17b/amos17b.pdf,ICML
1727,2017,Risk Bounds for Transferring Representations With and Without Fine-Tuning,"Daniel McNamara,         Maria-Florina Balcan","A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. We develop sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight transfer, which we validate with experiments.",http://proceedings.mlr.press/v70/mcnamara17a.html,http://proceedings.mlr.press/v70/mcnamara17a/mcnamara17a.pdf,ICML
1728,2017,Learning to Discover Sparse Graphical Models,"Eugene Belilovsky,         Kyle Kastner,         Gael Varoquaux,         Matthew B. Blaschko","We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. Popular methods rely on estimating a penalized maximum likelihood of the precision matrix. However, in these approaches structure recovery is an indirect consequence of the data-fit term, the penalty can be difficult to adapt for domain-specific knowledge, and the inference is computationally demanding. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function, parametrized by a neural network, that maps empirical covariance matrices to estimated graph structures. Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. Applying this framework, we find our learnable graph-discovery method trained on synthetic data generalizes well: identifying relevant edges in both synthetic and real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain performance generally superior to analytical methods.",http://proceedings.mlr.press/v70/belilovsky17a.html,http://proceedings.mlr.press/v70/belilovsky17a/belilovsky17a.pdf,ICML
1729,2017,Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things,"Ashish Kumar,         Saurabh Goyal,         Manik Varma","This paper develops a novel tree-based algorithm, called Bonsai, for efficient prediction on IoT devices – such as those based on the Arduino Uno board having an 8 bit ATmega328P microcontroller operating at 16 MHz with no native floating point support, 2 KB RAM and 32 KB read-only flash. Bonsai maintains prediction accuracy while minimizing model size and prediction costs by: (a) developing a tree model which learns a single, shallow, sparse tree with powerful nodes; (b) sparsely projecting all data into a low-dimensional space in which the tree is learnt; and (c) jointly learning all tree and projection parameters. Experimental results on multiple benchmark datasets demonstrate that Bonsai can make predictions in milliseconds even on slow microcontrollers, can fit in KB of memory, has lower battery consumption than all other algorithms while achieving prediction accuracies that can be as much as 30\% higher than state-of-the-art methods for resource-efficient machine learning. Bonsai is also shown to generalize to other resource constrained settings beyond IoT by generating significantly better search results as compared to Bing’s L3 ranker when the model size is restricted to 300 bytes. Bonsai’s code can be downloaded from (http://www.manikvarma.org/code/Bonsai/download.html).",http://proceedings.mlr.press/v70/kumar17a.html,http://proceedings.mlr.press/v70/kumar17a/kumar17a.pdf,ICML
1730,2017,Dissipativity Theory for Nesterov’s Accelerated Method,"Bin Hu,         Laurent Lessard","In this paper, we adapt the control theoretic concept of dissipativity theory to provide a natural understanding of Nesterov’s accelerated method. Our theory ties rigorous convergence rate analysis to the physically intuitive notion of energy dissipation. Moreover, dissipativity allows one to efficiently construct Lyapunov functions (either numerically or analytically) by solving a small semidefinite program. Using novel supply rate functions, we show how to recover known rate bounds for Nesterov’s method and we generalize the approach to certify both linear and sublinear rates in a variety of settings. Finally, we link the continuous-time version of dissipativity to recent works on algorithm analysis that use discretizations of ordinary differential equations.",http://proceedings.mlr.press/v70/hu17a.html,http://proceedings.mlr.press/v70/hu17a/hu17a.pdf,ICML
1731,2017,Deep Transfer Learning with Joint Adaptation Networks,"Mingsheng Long,         Han Zhu,         Jianmin Wang,         Michael I. Jordan","Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.",http://proceedings.mlr.press/v70/long17a.html,http://proceedings.mlr.press/v70/long17a/long17a.pdf,ICML
1732,2017,Sequence Modeling via Segmentations,"Chong Wang,         Yining Wang,         Po-Sen Huang,         Abdelrahman Mohamed,         Dengyong Zhou,         Li Deng","Segmental structure is a common pattern in many types of sequences such as phrases in human languages. In this paper, we present a probabilistic model for sequences via their segmentations. The probability of a segmented sequence is calculated as the product of the probabilities of all its segments, where each segment is modeled using existing tools such as recurrent neural networks. Since the segmentation of a sequence is usually unknown in advance, we sum over all valid segmentations to obtain the final probability for the sequence. An efficient dynamic programming algorithm is developed for forward and backward computations without resorting to any approximation. We demonstrate our approach on text segmentation and speech recognition tasks. In addition to quantitative results, we also show that our approach can discover meaningful segments in their respective application contexts.",http://proceedings.mlr.press/v70/wang17j.html,http://proceedings.mlr.press/v70/wang17j/wang17j.pdf,ICML
1733,2017,"ZipML: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning","Hantian Zhang,         Jerry Li,         Kaan Kara,         Dan Alistarh,         Ji Liu,         Ce Zhang","Recently there has been significant interest in training machine-learning models at low precision: by reducing precision, one can reduce computation and communication by one order of magnitude. We examine training at reduced precision, both from a theoretical and practical perspective, and ask: is it possible to train models at end-to-end low precision with provable guarantees? Can this lead to consistent order-of-magnitude speedups? We mainly focus on linear models, and the answer is yes for linear models. We develop a simple framework called ZipML based on one simple but novel strategy called double sampling. Our ZipML framework is able to execute training at low precision with no bias, guaranteeing convergence, whereas naive quantization would introduce significant bias. We validate our framework across a range of applications, and show that it enables an FPGA prototype that is up to 6.5×6.5×6.5\times faster than an implementation using full 32-bit precision. We further develop a variance-optimal stochastic quantization strategy and show that it can make a significant difference in a variety of settings. When applied to linear models together with double sampling, we save up to another 1.7×1.7×1.7\times in data movement compared with uniform quantization. When training deep networks with quantized models, we achieve higher accuracy than the state-of-the-art XNOR-Net.",http://proceedings.mlr.press/v70/zhang17e.html,http://proceedings.mlr.press/v70/zhang17e/zhang17e.pdf,ICML
1734,2017,Probabilistic Path Hamiltonian Monte Carlo,"Vu Dinh,         Arman Bilge,         Cheng Zhang,         Frederick A. Matsen IV","Hamiltonian Monte Carlo (HMC) is an efficient and effective means of sampling posterior distributions on Euclidean space, which has been extended to manifolds with boundary. However, some applications require an extension to more general spaces. For example, phylogenetic (evolutionary) trees are defined in terms of both a discrete graph and associated continuous parameters; although one can represent these aspects using a single connected space, this rather complex space is not suitable for existing HMC algorithms. In this paper, we develop Probabilistic Path HMC (PPHMC) as a first step to sampling distributions on spaces with intricate combinatorial structure. We define PPHMC on orthant complexes, show that the resulting Markov chain is ergodic, and provide a promising implementation for the case of phylogenetic trees in open-source software. We also show that a surrogate function to ease the transition across a boundary on which the log-posterior has discontinuous derivatives can greatly improve efficiency.",http://proceedings.mlr.press/v70/dinh17a.html,http://proceedings.mlr.press/v70/dinh17a/dinh17a.pdf,ICML
1735,2017,Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs,"Michael Gygli,         Mohammad Norouzi,         Anelia Angelova","We approach structured output prediction by optimizing a deep value network (DVN) to precisely estimate the task loss on different output configurations for a given input. Once the model is trained, we perform inference by gradient descent on the continuous relaxations of the output variables to find outputs with promising scores from the value network. When applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar estimating the intersection over union between the input and ground truth masks. For multi-label classification, the DVN’s objective is to correctly predict the F1 score for any potential label configuration. The DVN framework achieves the state-of-the-art results on multi-label prediction and image segmentation benchmarks.",http://proceedings.mlr.press/v70/gygli17a.html,http://proceedings.mlr.press/v70/gygli17a/gygli17a.pdf,ICML
1736,2017,A Unified View of Multi-Label Performance Measures,"Xi-Zhu Wu,         Zhi-Hua Zhou","Multi-label classification deals with the problem where each instance is associated with multiple class labels. Because evaluation in multi-label classification is more complicated than single-label setting, a number of performance measures have been proposed. It is noticed that an algorithm usually performs differently on different measures. Therefore, it is important to understand which algorithms perform well on which measure(s) and why. In this paper, we propose a unified margin view to revisit eleven performance measures in multi-label classification. In particular, we define label-wise margin and instance-wise margin, and prove that through maximizing these margins, different corresponding performance measures are to be optimized. Based on the defined margins, a max-margin approach called LIMO is designed and empirical results validate our theoretical findings.",http://proceedings.mlr.press/v70/wu17a.html,http://proceedings.mlr.press/v70/wu17a/wu17a.pdf,ICML
1737,2017,The Loss Surface of Deep and Wide Neural Networks,"Quynh Nguyen,         Matthias Hein","While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.",http://proceedings.mlr.press/v70/nguyen17a.html,http://proceedings.mlr.press/v70/nguyen17a/nguyen17a.pdf,ICML
1738,2017,High-Dimensional Structured Quantile Regression,"Vidyashankar Sivakumar,         Arindam Banerjee","Quantile regression aims at modeling the conditional median and quantiles of a response variable given certain predictor variables. In this work we consider the problem of linear quantile regression in high dimensions where the number of predictor variables is much higher than the number of samples available for parameter estimation. We assume the true parameter to have some structure characterized as having a small value according to some atomic norm R(.) and consider the norm regularized quantile regression estimator. We characterize the sample complexity for consistent recovery and give non-asymptotic bounds on the estimation error. While this problem has been previously considered, our analysis reveals geometric and statistical characteristics of the problem not available in prior literature. We perform experiments on synthetic data which support the theoretical results.",http://proceedings.mlr.press/v70/sivakumar17a.html,http://proceedings.mlr.press/v70/sivakumar17a/sivakumar17a.pdf,ICML
1739,2017,Capacity Releasing Diffusion for Speed and Locality,"Di Wang,         Kimon Fountoulakis,         Monika Henzinger,         Michael W. Mahoney,         Satish Rao","Diffusions and related random walk procedures are of central importance in many areas of machine learning, data analysis, and applied mathematics. Because they spread mass agnostically at each step in an iterative manner, they can sometimes spread mass “too aggressively,” thereby failing to find the “right” clusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process, which is both faster and stays more local than the classical spectral diffusion process. As an application, we use our CRD Process to develop an improved local algorithm for graph clustering. Our local graph clustering method can find local clusters in a model of clustering where one begins the CRD Process in a cluster whose vertices are connected better internally than externally by an O(log2n)O(log2n)O(\log^2 n) factor, where nnn is the number of nodes in the cluster. Thus, our CRD Process is the first local graph clustering algorithm that is not subject to the well-known quadratic Cheeger barrier. Our result requires a certain smoothness condition, which we expect to be an artifact of our analysis. Our empirical evaluation demonstrates improved results, in particular for realistic social graphs where there are moderately good—but not very good—clusters.",http://proceedings.mlr.press/v70/wang17b.html,http://proceedings.mlr.press/v70/wang17b/wang17b.pdf,ICML
1740,2017,Zero-Inflated Exponential Family Embeddings,"Li-Ping Liu,         David M. Blei","Word embeddings are a widely-used tool to analyze language, and exponential family embeddings (Rudolph et al., 2016) generalize the technique to other types of data. One challenge to fitting embedding methods is sparse data, such as a document/term matrix that contains many zeros. To address this issue, practitioners typically downweight or subsample the zeros, thus focusing learning on the non-zero entries. In this paper, we develop zero-inflated embeddings, a new embedding method that is designed to learn from sparse observations. In a zero-inflated embedding (ZIE), a zero in the data can come from an interaction to other data (i.e., an embedding) or from a separate process by which many observations are equal to zero (i.e. a probability mass at zero). Fitting a ZIE naturally downweights the zeros and dampens their influence on the model. Across many types of data—language, movie ratings, shopping histories, and bird watching logs—we found that zero-inflated embeddings provide improved predictive performance over standard approaches and find better vector representation of items.",http://proceedings.mlr.press/v70/liu17a.html,http://proceedings.mlr.press/v70/liu17a/liu17a.pdf,ICML
1741,2017,Neural Episodic Control,"Alexander Pritzel,         Benigno Uria,         Sriram Srinivasan,         Adrià Puigdomènech Badia,         Oriol Vinyals,         Demis Hassabis,         Daan Wierstra,         Charles Blundell","Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.",http://proceedings.mlr.press/v70/pritzel17a.html,http://proceedings.mlr.press/v70/pritzel17a/pritzel17a.pdf,ICML
1742,2017,On orthogonality and learning recurrent networks with long term dependencies,"Eugene Vorontsov,         Chiheb Trabelsi,         Samuel Kadoury,         Chris Pal","It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and may therefore be a desirable property. This paper explores issues with optimization convergence, speed and gradient stability when encouraging or enforcing orthogonality. To perform this analysis, we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation. We find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance.",http://proceedings.mlr.press/v70/vorontsov17a.html,http://proceedings.mlr.press/v70/vorontsov17a/vorontsov17a.pdf,ICML
1743,2017,Learning to Learn without Gradient Descent by Gradient Descent,"Yutian Chen,         Matthew W. Hoffman,         Sergio Gómez Colmenarejo,         Misha Denil,         Timothy P. Lillicrap,         Matt Botvinick,         Nando Freitas","We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.",http://proceedings.mlr.press/v70/chen17e.html,http://proceedings.mlr.press/v70/chen17e/chen17e.pdf,ICML
1744,2017,On The Projection Operator to A Three-view Cardinality Constrained Set,"Haichuan Yang,         Shupeng Gui,         Chuyang Ke,         Daniel Stefankovic,         Ryohei Fujimaki,         Ji Liu","The cardinality constraint is an intrinsic way to restrict the solution structure in many domains, for example, sparse learning, feature selection, and compressed sensing. To solve a cardinality constrained problem, the key challenge is to solve the projection onto the cardinality constraint set, which is NP-hard in general when there exist multiple overlapped cardinality constraints. In this paper, we consider the scenario where the overlapped cardinality constraints satisfy a Three-view Cardinality Structure (TVCS), which reflects the natural restriction in many applications, such as identification of gene regulatory networks and task-worker assignment problem. We cast the projection into a linear programming, and show that for TVCS, the vertex solution of this linear programming is the solution for the original projection problem. We further prove that such solution can be found with the complexity proportional to the number of variables and constraints. We finally use synthetic experiments and two interesting applications in bioinformatics and crowdsourcing to validate the proposed TVCS model and method.",http://proceedings.mlr.press/v70/yang17c.html,http://proceedings.mlr.press/v70/yang17c/yang17c.pdf,ICML
1745,2017,Efficient softmax approximation for GPUs,"Grave,         Armand Joulin,         Moustapha Cissé,         David Grangier,         Hervé Jégou","We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. The code of our method is available at https://github.com/facebookresearch/adaptive-softmax.",http://proceedings.mlr.press/v70/grave17a.html,http://proceedings.mlr.press/v70/grave17a/grave17a.pdf,ICML
1746,2017,Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data,"Xiuyan Ni,         Novi Quadrianto,         Yusu Wang,         Chao Chen","Clustering data with both continuous and discrete attributes is a challenging task. Existing methods lack a principled probabilistic formulation. In this paper, we propose a clustering method based on a tree-structured graphical model to describe the generation process of mixed-type data. Our tree-structured model factorized into a product of pairwise interactions, and thus localizes the interaction between feature variables of different types. To provide a robust clustering method based on the tree-model, we adopt a topographical view and compute peaks of the density function and their attractive basins for clustering. Furthermore, we leverage the theory from topology data analysis to adaptively merge trivial peaks into large ones in order to achieve meaningful clusterings. Our method outperforms state-of-the-art methods on mixed-type data.",http://proceedings.mlr.press/v70/ni17a.html,http://proceedings.mlr.press/v70/ni17a/ni17a.pdf,ICML
1747,2017,ChoiceRank: Identifying Preferences from Node Traffic in Networks,"Lucas Maystre,         Matthias Grossglauser","Understanding how users navigate in a network is of high interest in many applications. We consider a setting where only aggregate node-level traffic is observed and tackle the task of learning edge transition probabilities. We cast it as a preference learning problem, and we study a model where choices follow Luce’s axiom. In this case, the O(n)O(n)O(n) marginal counts of node visits are a sufficient statistic for the O(n2)O(n2)O(n^2) transition probabilities. We show how to make the inference problem well-posed regardless of the network’s structure, and we present ChoiceRank, an iterative algorithm that scales to networks that contains billions of nodes and edges. We apply the model to two clickstream datasets and show that it successfully recovers the transition probabilities using only the network structure and marginal (node-level) traffic data. Finally, we also consider an application to mobility networks and apply the model to one year of rides on New York City’s bicycle-sharing system.",http://proceedings.mlr.press/v70/maystre17b.html,http://proceedings.mlr.press/v70/maystre17b/maystre17b.pdf,ICML
1748,2017,Learning Infinite Layer Networks Without the Kernel Trick,"Roi Livni,         Daniel Carmon,         Amir Globerson","Infinite Layer Networks (ILN) have been proposed as an architecture that mimics neural networks while enjoying some of the advantages of kernel methods. ILN are networks that integrate over infinitely many nodes within a single hidden layer. It has been demonstrated by several authors that the problem of learning ILN can be reduced to the kernel trick, implying that whenever a certain integral can be computed analytically they are efficiently learnable. In this work we give an online algorithm for ILN, which avoids the kernel trick assumption. More generally and of independent interest, we show that kernel methods in general can be exploited even when the kernel cannot be efficiently computed but can only be estimated via sampling. We provide a regret analysis for our algorithm, showing that it matches the sample complexity of methods which have access to kernel values. Thus, our method is the first to demonstrate that the kernel trick is not necessary, as such, and random features suffice to obtain comparable performance.",http://proceedings.mlr.press/v70/livni17a.html,http://proceedings.mlr.press/v70/livni17a/livni17a.pdf,ICML
1749,2017,Deletion-Robust Submodular Maximization: Data Summarization with “the Right to be Forgotten”,"Baharan Mirzasoleiman,         Amin Karbasi,         Andreas Krause","How can we summarize a dynamic data stream when elements selected for the summary can be deleted at any time? This is an important challenge in online services, where the users generating the data may decide to exercise their right to restrict the service provider from using (part of) their data due to privacy concerns. Motivated by this challenge, we introduce the dynamic deletion-robust submodular maximization problem. We develop the first resilient streaming algorithm, called ROBUST-STREAMING, with a constant factor approximation guarantee to the optimum solution. We evaluate the effectiveness of our approach on several real-world applica tions, including summarizing (1) streams of geo-coordinates (2); streams of images; and (3) click-stream log data, consisting of 45 million feature vectors from a news recommendation task.",http://proceedings.mlr.press/v70/mirzasoleiman17a.html,http://proceedings.mlr.press/v70/mirzasoleiman17a/mirzasoleiman17a.pdf,ICML
1750,2017,Fast Bayesian Intensity Estimation for the Permanental Process,"Christian J. Walder,         Adrian N. Bishop","The Cox process is a stochastic process which generalises the Poisson process by letting the underlying intensity function itself be a stochastic process. In this paper we present a fast Bayesian inference scheme for the permanental process, a Cox process under which the square root of the intensity is a Gaussian process. In particular we exploit connections with reproducing kernel Hilbert spaces, to derive efficient approximate Bayesian inference algorithms based on the Laplace approximation to the predictive distribution and marginal likelihood. We obtain a simple algorithm which we apply to toy and real-world problems, obtaining orders of magnitude speed improvements over previous work.",http://proceedings.mlr.press/v70/walder17a.html,http://proceedings.mlr.press/v70/walder17a/walder17a.pdf,ICML
1751,2017,Combined Group and Exclusive Sparsity for Deep Neural Networks,"Jaehong Yoon,         Sung Ju Hwang","The number of parameters in a deep neural network is usually very large, which helps with its learning capacity but also hinders its scalability and practicality due to memory/time inefficiency and overfitting. To resolve this issue, we propose a sparsity regularization method that exploits both positive and negative correlations among the features to enforce the network to be sparse, and at the same time remove any redundancies among the features to fully utilize the capacity of the network. Specifically, we propose to use an exclusive sparsity regularization based on (1,2)-norm, which promotes competition for features between different weights, thus enforcing them to fit to disjoint sets of features. We further combine the exclusive sparsity with the group sparsity based on (2,1)-norm, to promote both sharing and competition for features in training of a deep neural network. We validate our method on multiple public datasets, and the results show that our method can obtain more compact and efficient networks while also improving the performance over the base networks with full weights, as opposed to existing sparsity regularizations that often obtain efficiency at the expense of prediction accuracy.",http://proceedings.mlr.press/v70/yoon17a.html,http://proceedings.mlr.press/v70/yoon17a/yoon17a.pdf,ICML
1752,2017,Bidirectional Learning for Time-series Models with Hidden Units,"Takayuki Osogami,         Hiroshi Kajino,         Taro Sekiyama","Hidden units can play essential roles in modeling time-series having long-term dependency or on-linearity but make it difficult to learn associated parameters. Here we propose a way to learn such a time-series model by training a backward model for the time-reversed time-series, where the backward model has a common set of parameters as the original (forward) model. Our key observation is that only a subset of the parameters is hard to learn, and that subset is complementary between the forward model and the backward model. By training both of the two models, we can effectively learn the values of the parameters that are hard to learn if only either of the two models is trained. We apply bidirectional learning to a dynamic Boltzmann machine extended with hidden units. Numerical experiments with synthetic and real datasets clearly demonstrate advantages of bidirectional learning.",http://proceedings.mlr.press/v70/osogami17a.html,http://proceedings.mlr.press/v70/osogami17a/osogami17a.pdf,ICML
1753,2017,Minimizing Trust Leaks for Robust Sybil Detection,"János Höner,         Shinichi Nakajima,         Alexander Bauer,         Klaus-Robert Müller,         Nico Görnitz","Sybil detection is a crucial task to protect online social networks (OSNs) against intruders who try to manipulate automatic services provided by OSNs to their customers. In this paper, we first discuss the robustness of graph-based Sybil detectors SybilRank and Integro and refine theoretically their security guarantees towards more realistic assumptions. After that, we formally introduce adversarial settings for the graph-based Sybil detection problem and derive a corresponding optimal attacking strategy by exploitation of trust leaks. Based on our analysis, we propose transductive Sybil ranking (TSR), a robust extension to SybilRank and Integro that directly minimizes trust leaks. Our empirical evaluation shows significant advantages of TSR over state-of-the-art competitors on a variety of attacking scenarios on artificially generated data and real-world datasets.",http://proceedings.mlr.press/v70/honer17a.html,http://proceedings.mlr.press/v70/honer17a/honer17a.pdf,ICML
1754,2017,Tensor Decomposition via Simultaneous Power Iteration,"Po-An Wang,         Chi-Jen Lu","Tensor decomposition is an important problem with many applications across several disciplines, and a popular approach for this problem is the tensor power method. However, previous works with theoretical guarantee based on this approach can only find the top eigenvectors one after one, unlike the case for matrices. In this paper, we show how to find the eigenvectors simultaneously with the help of a new initialization procedure. This allows us to achieve a better running time in the batch setting, as well as a lower sample complexity in the streaming setting.",http://proceedings.mlr.press/v70/wang17i.html,http://proceedings.mlr.press/v70/wang17i/wang17i.pdf,ICML
1755,2017,Tensor Decomposition with Smoothness,"Masaaki Imaizumi,         Kohei Hayashi","Real data tensors are usually high dimensional but their intrinsic information is preserved in low-dimensional space, which motivates to use tensor decompositions such as Tucker decomposition. Often, real data tensors are not only low dimensional, but also smooth, meaning that the adjacent elements are similar or continuously changing, which typically appear as spatial or temporal data. To incorporate the smoothness property, we propose the smoothed Tucker decomposition (STD). STD leverages the smoothness by the sum of a few basis functions, which reduces the number of parameters. The objective function is formulated as a convex problem and, to solve that, an algorithm based on the alternating direction method of multipliers is derived. We theoretically show that, under the smoothness assumption, STD achieves a better error bound. The theoretical result and performances of STD are numerically verified.",http://proceedings.mlr.press/v70/imaizumi17a.html,http://proceedings.mlr.press/v70/imaizumi17a/imaizumi17a.pdf,ICML
1756,2017,Spherical Structured Feature Maps for Kernel Approximation,Yueming Lyu,"We propose Spherical Structured Feature (SSF) maps to approximate shift and rotation invariant kernels as well as bthbthb^{th}-order arc-cosine kernels (Cho \& Saul, 2009). We construct SSF maps based on the point set on d−1d−1d-1 dimensional sphere Sd−1\mathbb{S}^{d-1}. We prove that the inner product of SSF maps are unbiased estimates for above kernels if asymptotically uniformly distributed point set on Sd−1\mathbb{S}^{d-1} is given. According to (Brauchart \& Grabner, 2015), optimizing the discrete Riesz s-energy can generate asymptotically uniformly distributed point set on Sd−1\mathbb{S}^{d-1}. Thus, we propose an efficient coordinate decent method to find a local optimum of the discrete Riesz s-energy for SSF maps construction. Theoretically, SSF maps construction achieves linear space complexity and loglinear time complexity. Empirically, SSF maps achieve superior performance compared with other methods.",http://proceedings.mlr.press/v70/lyu17a.html,http://proceedings.mlr.press/v70/lyu17a/lyu17a.pdf,ICML
1757,2017,Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control,"Natasha Jaques,         Shixiang Gu,         Dzmitry Bahdanau,         José Miguel Hernández-Lobato,         Richard E. Turner,         Douglas Eck","This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.",http://proceedings.mlr.press/v70/jaques17a.html,http://proceedings.mlr.press/v70/jaques17a/jaques17a.pdf,ICML
1758,2017,Meta Networks,"Tsendsuren Munkhdalai,         Hong Yu","Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6\% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.",http://proceedings.mlr.press/v70/munkhdalai17a.html,http://proceedings.mlr.press/v70/munkhdalai17a/munkhdalai17a.pdf,ICML
1759,2017,Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling,"Hairong Liu,         Zhenyao Zhu,         Xiangang Li,         Sanjeev Satheesh","Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 111) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 222) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this paper, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of target sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.",http://proceedings.mlr.press/v70/liu17f.html,http://proceedings.mlr.press/v70/liu17f/liu17f.pdf,ICML
1760,2017,Dropout Inference in Bayesian Neural Networks with Alpha-divergences,"Yingzhen Li,         Yarin Gal","To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI’s KL objective, which are able to avoid VI’s uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model’s epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model’s uncertainty.",http://proceedings.mlr.press/v70/li17a.html,http://proceedings.mlr.press/v70/li17a/li17a.pdf,ICML
1761,2017,Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,"Zichao Yang,         Zhiting Hu,         Ruslan Salakhutdinov,         Taylor Berg-Kirkpatrick","Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder’s dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.",http://proceedings.mlr.press/v70/yang17d.html,http://proceedings.mlr.press/v70/yang17d/yang17d.pdf,ICML
1762,2017,Analysis and Optimization of Graph Decompositions by Lifted Multicuts,"Andrea Horňáková,         Jan-Hendrik Lange,         Bjoern Andres","We study the set of all decompositions (clusterings) of a graph through its characterization as a set of lifted multicuts. This leads us to practically relevant insights related to the definition of classes of decompositions by must-join and must-cut constraints and related to the comparison of clusterings by metrics. To find optimal decompositions defined by minimum cost lifted multicuts, we establish some properties of some facets of lifted multicut polytopes, define efficient separation procedures and apply these in a branch-and-cut algorithm.",http://proceedings.mlr.press/v70/hornakova17a.html,http://proceedings.mlr.press/v70/hornakova17a/hornakova17a.pdf,ICML
1763,2017,The Price of Differential Privacy for Online Learning,"Naman Agarwal,         Karan Singh","We design differentially private algorithms for the problem of online linear optimization in the full information and bandit settings with optimal O(T0.5)O(T0.5)O(T^{0.5}) regret bounds. In the full-information setting, our results demonstrate that ϵϵ\epsilon-differential privacy may be ensured for free – in particular, the regret bounds scale as O(T0.5+1/ϵ)O(T0.5+1/ϵ)O(T^{0.5}+1/\epsilon). For bandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of O(T0.5/ϵ)O(T0.5/ϵ)O(T^{0.5}/\epsilon), while the previously best known regret bound was O(T2/3/ϵ)O(T2/3/ϵ)O(T^{2/3}/\epsilon).",http://proceedings.mlr.press/v70/agarwal17a.html,http://proceedings.mlr.press/v70/agarwal17a/agarwal17a.pdf,ICML
1764,2017,Learning Texture Manifolds with the Periodic Spatial GAN,"Urs Bergmann,         Nikolay Jetchev,         Roland Vollgraf","This paper introduces a novel approach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014), and call this technique Periodic Spatial GAN (PSGAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures, periodic or non-periodic, from datasets of one or more complex large images. Second, we show that the image generation with PSGANs has properties of a texture manifold: we can smoothly interpolate between samples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources, and the method is highly scalable and can generate output images of arbitrary large size.",http://proceedings.mlr.press/v70/bergmann17a.html,http://proceedings.mlr.press/v70/bergmann17a/bergmann17a.pdf,ICML
1765,2017,Deep Tensor Convolution on Multicores,"David Budden,         Alexander Matveev,         Shibani Santurkar,         Shraman Ray Chaudhuri,         Nir Shavit","Deep convolutional neural networks (ConvNets) of 3-dimensional kernels allow joint modeling of spatiotemporal features. These networks have improved performance of video and volumetric image analysis, but have been limited in size due to the low memory ceiling of GPU hardware. Existing CPU implementations overcome this constraint but are impractically slow. Here we extend and optimize the faster Winograd-class of convolutional algorithms to the NNN-dimensional case and specifically for CPU hardware. First, we remove the need to manually hand-craft algorithms by exploiting the relaxed constraints and cheap sparse access of CPU memory. Second, we maximize CPU utilization and multicore scalability by transforming data matrices to be cache-aware, integer multiples of AVX vector widths. Treating 2-dimensional ConvNets as a special (and the least beneficial) case of our approach, we demonstrate a 5 to 25-fold improvement in throughput compared to previous state-of-the-art.",http://proceedings.mlr.press/v70/budden17a.html,http://proceedings.mlr.press/v70/budden17a/budden17a.pdf,ICML
1766,2017,Spectral Learning from a Single Trajectory under Finite-State Policies,"Borja Balle,         Odalric-Ambrym Maillard","We present spectral methods of moments for learning sequential models from a single trajectory, in stark contrast with the classical literature that assumes the availability of multiple i.i.d. trajectories. Our approach leverages an efficient SVD-based learning algorithm for weighted automata and provides the first rigorous analysis for learning many important models using dependent data. We state and analyze the algorithm under three increasingly difficult scenarios: probabilistic automata, stochastic weighted automata, and reactive predictive state representations controlled by a finite-state policy. Our proofs include novel tools for studying mixing properties of stochastic weighted automata.",http://proceedings.mlr.press/v70/balle17a.html,http://proceedings.mlr.press/v70/balle17a/balle17a.pdf,ICML
1767,2017,Reinforcement Learning with Deep Energy-Based Policies,"Tuomas Haarnoja,         Haoran Tang,         Pieter Abbeel,         Sergey Levine","We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.",http://proceedings.mlr.press/v70/haarnoja17a.html,http://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf,ICML
1768,2017,A Richer Theory of Convex Constrained Optimization with Reduced Projections and Improved Rates,"Tianbao Yang,         Qihang Lin,         Lijun Zhang","This paper focuses on convex constrained optimization problems, where the solution is subject to a convex inequality constraint. In particular, we aim at challenging problems for which both projection into the constrained domain and a linear optimization under the inequality constraint are time-consuming, which render both projected gradient methods and conditional gradient methods (a.k.a. the Frank-Wolfe algorithm) expensive. In this paper, we develop projection reduced optimization algorithms for both smooth and non-smooth optimization with improved convergence rates under a certain regularity condition of the constraint function. We first present a general theory of optimization with only one projection. Its application to smooth optimization with only one projection yields O(1/ϵ)O(1/ϵ)O(1/\epsilon) iteration complexity, which improves over the O(1/ϵ2)O(1/ϵ2)O(1/\epsilon^2) iteration complexity established before for non-smooth optimization and can be further reduced under strong convexity. Then we introduce a local error bound condition and develop faster algorithms for non-strongly convex optimization at the price of a logarithmic number of projections. In particular, we achieve an iteration complexity of O˜(1/ϵ2(1−θ))O~(1/ϵ2(1−θ))\widetilde O(1/\epsilon^{2(1-\theta)}) for non-smooth optimization and O˜(1/ϵ1−θ)O~(1/ϵ1−θ)\widetilde O(1/\epsilon^{1-\theta}) for smooth optimization, where θ∈(0,1]θ∈(0,1]\theta\in(0,1] appearing the local error bound condition characterizes the functional local growth rate around the optimal solutions. Novel applications in solving the constrained ℓ1ℓ1\ell_1 minimization problem and a positive semi-definite constrained distance metric learning problem demonstrate that the proposed algorithms achieve significant speed-up compared with previous algorithms.",http://proceedings.mlr.press/v70/yang17f.html,http://proceedings.mlr.press/v70/yang17f/yang17f.pdf,ICML
1769,2017,Algorithms for ℓp\ell_p Low-Rank Approximation,"Flavio Chierichetti,         Sreenivas Gollapudi,         Ravi Kumar,         Silvio Lattanzi,         Rina Panigrahy,         David P. Woodruff","We consider the problem of approximating a given matrix by a low-rank matrix so as to minimize the entrywise ℓp\ell_p-approximation error, for any p≥1p \geq 1; the case p=2p = 2 is the classical SVD problem. We obtain the first provably good approximation algorithms for this robust version of low-rank approximation that work for every value of pp. Our algorithms are simple, easy to implement, work well in practice, and illustrate interesting tradeoffs between the approximation quality, the running time, and the rank of the approximating matrix.",http://proceedings.mlr.press/v70/chierichetti17a.html,http://proceedings.mlr.press/v70/chierichetti17a/chierichetti17a.pdf,ICML
1770,2017,Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference,"Aditya Chaudhry,         Pan Xu,         Quanquan Gu","Causal inference among high-dimensional time series data proves an important research problem in many fields. While in the classical regime one often establishes causality among time series via a concept known as “Granger causality,” existing approaches for Granger causal inference in high-dimensional data lack the means to characterize the uncertainty associated with Granger causality estimates (e.g., p-values and confidence intervals). We make two contributions in this work. First, we introduce a novel asymptotically unbiased Granger causality estimator with corresponding test statistics and confidence intervals to allow, for the first time, uncertainty characterization in high-dimensional Granger causal inference. Second, we introduce a novel method for false discovery rate control that achieves higher power in multiple testing than existing techniques and that can cope with dependent test statistics and dependent observations. We corroborate our theoretical results with experiments on both synthetic data and real-world climatological data.",http://proceedings.mlr.press/v70/chaudhry17a.html,http://proceedings.mlr.press/v70/chaudhry17a/chaudhry17a.pdf,ICML
1771,2017,Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms,"Jialei Wang,         Lin Xiao","We consider empirical risk minimization of linear predictors with convex loss functions. Such problems can be reformulated as convex-concave saddle point problems and solved by primal-dual first-order algorithms. However, primal-dual algorithms often require explicit strongly convex regularization in order to obtain fast linear convergence, and the required dual proximal mapping may not admit closed-form or efficient solution. In this paper, we develop both batch and randomized primal-dual algorithms that can exploit strong convexity from data adaptively and are capable of achieving linear convergence even without regularization. We also present dual-free variants of adaptive primal-dual algorithms that do not need the dual proximal mapping, which are especially suitable for logistic regression.",http://proceedings.mlr.press/v70/wang17l.html,http://proceedings.mlr.press/v70/wang17l/wang17l.pdf,ICML
1772,2017,Evaluating Bayesian Models with Posterior Dispersion Indices,"Alp Kucukelbir,         Yixin Wang,         David M. Blei","Probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its performance. Evaluation drives the cycle, as we revise our model based on how it performs. This requires a metric. Traditionally, predictive accuracy prevails. Yet, predictive accuracy does not tell the whole story. We propose to evaluate a model through posterior dispersion. The idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure. This highlights datapoints the model struggles to explain and provides complimentary insight to datapoints with low predictive accuracy. We present a family of posterior dispersion indices (PDI) that capture this idea. We show how a PDI identifies patterns of model mismatch in three real data examples: voting preferences, supermarket shopping, and population genetics.",http://proceedings.mlr.press/v70/kucukelbir17a.html,http://proceedings.mlr.press/v70/kucukelbir17a/kucukelbir17a.pdf,ICML
1773,2017,How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?,Andreas Loukas,"How many samples are sufficient to guarantee that the eigenvectors of the sample covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and sub-gaussian distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance and the number of samples. Our findings imply non-asymptotic concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic analysis of PCA and its applications. For instance, they provide conditions for separating components estimated from O(1)O(1)O(1) samples and show that even few samples can be sufficient to perform dimensionality reduction, especially for low-rank covariances.",http://proceedings.mlr.press/v70/loukas17a.html,http://proceedings.mlr.press/v70/loukas17a/loukas17a.pdf,ICML
1774,2017,Sequence to Better Sequence: Continuous Revision of Combinatorial Structures,"Jonas Mueller,         David Gifford,         Tommi Jaakkola","We present a model that, after learning on observations of (sequence, outcome) pairs, can be efficiently used to revise a new sequence in order to improve its associated outcome. Our framework requires neither example improvements, nor additional evaluation of outcomes for proposed revisions. To avoid combinatorial-search over sequence elements, we specify a generative model with continuous latent factors, which is learned via joint approximate inference using a recurrent variational autoencoder (VAE) and an outcome-predicting neural network module. Under this model, gradient methods can be used to efficiently optimize the continuous latent factors with respect to inferred outcomes. By appropriately constraining this optimization and using the VAE decoder to generate a revised sequence, we ensure the revision is fundamentally similar to the original sequence, is associated with better outcomes, and looks natural. These desiderata are proven to hold with high probability under our approach, which is empirically demonstrated for revising natural language sentences.",http://proceedings.mlr.press/v70/mueller17a.html,http://proceedings.mlr.press/v70/mueller17a/mueller17a.pdf,ICML
1775,2017,Learning Hierarchical Features from Deep Generative Models,"Shengjia Zhao,         Jiaming Song,         Stefano Ermon","Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative architecture that do not suffer from these limitations. Our model is able to learn highly interpretable and disentangled hierarchical features on several natural image datasets with no task specific regularization or prior knowledge.",http://proceedings.mlr.press/v70/zhao17c.html,http://proceedings.mlr.press/v70/zhao17c/zhao17c.pdf,ICML
1776,2017,Emulating the Expert: Inverse Optimization through Online Learning,"Andreas Bärmann,         Sebastian Pokutta,         Oskar Schneider","In this paper, we demonstrate how to learn the objective function of a decision maker while only observing the problem input data and the decision maker’s corresponding decisions over multiple rounds. Our approach is based on online learning techniques and works for linear objectives over arbitrary sets for which we have a linear optimization oracle and as such generalizes previous work based on KKT-system decomposition and dualization approaches. The applicability of our framework for learning linear constraints is also discussed briefly. Our algorithm converges at a rate of O(1/sqrt(T)), and we demonstrate its effectiveness and applications in preliminary computational results.",http://proceedings.mlr.press/v70/barmann17a.html,http://proceedings.mlr.press/v70/barmann17a/barmann17a.pdf,ICML
1777,2017,Unimodal Probability Distributions for Deep Ordinal Classification,"Christopher Beckham,         Christopher Pal","Probability distributions produced by the cross-entropy loss for ordinal classification problems can possess undesired properties. We propose a straightforward technique to constrain discrete ordinal probability distributions to be unimodal via the use of the Poisson and binomial probability distributions. We evaluate this approach in the context of deep learning on two large ordinal image datasets, obtaining promising results.",http://proceedings.mlr.press/v70/beckham17a.html,http://proceedings.mlr.press/v70/beckham17a/beckham17a.pdf,ICML
1778,2017,Image-to-Markup Generation with Coarse-to-Fine Attention,"Yuntian Deng,         Anssi Kanervisto,         Jeffrey Ling,         Alexander M. Rush","We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.",http://proceedings.mlr.press/v70/deng17a.html,http://proceedings.mlr.press/v70/deng17a/deng17a.pdf,ICML
1779,2017,Improving Gibbs Sampler Scan Quality with DoGS,"Ioannis Mitliagkas,         Lester Mackey","The pairwise influence matrix of Dobrushin has long been used as an analytical tool to bound the rate of convergence of Gibbs sampling. In this work, we use Dobrushin influence as the basis of a practical tool to certify and efficiently improve the quality of a Gibbs sampler. Our Dobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection orders for a given sampling budget and variable subset of interest, explicit bounds on total variation distance to stationarity, and certifiable improvements over the standard systematic and uniform random scan Gibbs samplers. In our experiments with image segmentation, Markov chain Monte Carlo maximum likelihood estimation, and Ising model inference, DoGS consistently deliver higher-quality inferences with significantly smaller sampling budgets than standard Gibbs samplers.",http://proceedings.mlr.press/v70/mitliagkas17a.html,http://proceedings.mlr.press/v70/mitliagkas17a/mitliagkas17a.pdf,ICML
1780,2017,Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction,"Weizhong Zhang,         Bin Hong,         Wei Liu,         Jieping Ye,         Deng Cai,         Xiaofei He,         Jie Wang","Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features and identify the support vectors. It has achieved great successes in many real-world applications. However, for large-scale problems involving a huge number of samples and extremely high-dimensional features, solving sparse SVMs remains challenging. By noting that sparse SVMs induce sparsities in both feature and sample spaces, we propose a novel approach, which is based on accurate estimations of the primal and dual optima of sparse SVMs, to simultaneously identify the features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified inactive samples and features from the training phase, leading to substantial savings in both the memory usage and computational cost without sacrificing accuracy. To the best of our knowledge, the proposed method is the first static feature and sample reduction method for sparse SVMs. Experiments on both synthetic and real datasets (e.g., the kddb dataset with about 20 million samples and 30 million features) demonstrate that our approach significantly outperforms state-of-the-art methods and the speedup gained by our approach can be orders of magnitude.",http://proceedings.mlr.press/v70/zhang17c.html,http://proceedings.mlr.press/v70/zhang17c/zhang17c.pdf,ICML
1781,2017,Regularising Non-linear Models Using Feature Side-information,"Amina Mollaysa,         Pablo Strasser,         Alexandros Kalousis","Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vectorial descriptions as feature side-information. In the standard learning scenario, input is represented as a vector of features and the feature side-information is most often ignored or used only for feature selection prior to model fitting. We believe that feature side-information which carries information about features intrinsic property will help improve model prediction if used in a proper way during learning process. In this paper, we propose a framework that allows for the incorporation of the feature side-information during the learning of very general model families to improve the prediction performance. We control the structures of the learned models so that they reflect features’ similarities as these are defined on the basis of the side-information. We perform experiments on a number of benchmark datasets which show significant predictive performance gains, over a number of baselines, as a result of the exploitation of the side-information.",http://proceedings.mlr.press/v70/mollaysa17a.html,http://proceedings.mlr.press/v70/mollaysa17a/mollaysa17a.pdf,ICML
1782,2017,Multiple Clustering Views from Multiple Uncertain Experts,"Yale Chang,         Junxiang Chen,         Michael H. Cho,         Peter J. Castaldi,         Edwin K. Silverman,         Jennifer G. Dy","Expert input can improve clustering performance. In today’s collaborative environment, the availability of crowdsourced multiple expert input is becoming common. Given multiple experts’ inputs, most existing approaches can only discover one clustering structure. However, data is multi-faced by nature and can be clustered in different ways (also known as views). In an exploratory analysis problem where ground truth is not known, different experts may have diverse views on how to cluster data. In this paper, we address the problem on how to automatically discover multiple ways to cluster data given potentially diverse inputs from multiple uncertain experts. We propose a novel Bayesian probabilistic model that automatically learns the multiple expert views and the clustering structure associated with each view. The benefits of learning the experts’ views include 1) enabling the discovery of multiple diverse clustering structures, and 2) improving the quality of clustering solution in each view by assigning higher weights to experts with higher confidence. In our approach, the expert views, multiple clustering structures and expert confidences are jointly learned via variational inference. Experimental results on synthetic datasets, benchmark datasets and a real-world disease subtyping problem show that our proposed approach outperforms competing baselines, including meta clustering, semi-supervised clustering, semi-crowdsourced clustering and consensus clustering.",http://proceedings.mlr.press/v70/chang17a.html,http://proceedings.mlr.press/v70/chang17a/chang17a.pdf,ICML
1783,2017,On Calibration of Modern Neural Networks,"Chuan Guo,         Geoff Pleiss,         Yu Sun,         Kilian Q. Weinberger","Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.",http://proceedings.mlr.press/v70/guo17a.html,http://proceedings.mlr.press/v70/guo17a/guo17a.pdf,ICML
1784,2017,Max-value Entropy Search for Efficient Bayesian Optimization,"Zi Wang,         Stefanie Jegelka","Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the argmaxarg⁡max\arg\max of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.",http://proceedings.mlr.press/v70/wang17e.html,http://proceedings.mlr.press/v70/wang17e/wang17e.pdf,ICML
1785,2017,Adaptive Multiple-Arm Identification,"Jiecao Chen,         Xi Chen,         Qin Zhang,         Yuan Zhou","We study the problem of selecting K arms with the highest expected rewards in a stochastic n-armed bandit game. This problem has a wide range of applications, e.g., A/B testing, crowdsourcing, simulation optimization. Our goal is to develop a PAC algorithm, which, with probability at least 1−δ1−δ1-\delta, identifies a set of K arms with the aggregate regret at most ϵϵ\epsilon. The notion of aggregate regret for multiple-arm identification was first introduced in Zhou et. al. (2014), which is defined as the difference of the averaged expected rewards between the selected set of arms and the best K arms. In contrast to Zhou et. al. (2014) that only provides instance-independent sample complexity, we introduce a new hardness parameter for characterizing the difficulty of any given instance. We further develop two algorithms and establish the corresponding sample complexity in terms of this hardness parameter. The derived sample complexity can be significantly smaller than state-of-the-art results for a large class of instances and matches the instance-independent lower bound up to a log(ϵ−1)log⁡(ϵ−1)\log(\epsilon^{-1}) factor in the worst case. We also prove a lower bound result showing that the extra log(ϵ−1)log⁡(ϵ−1)\log(\epsilon^{-1}) is necessary for instance-dependent algorithms using the introduced hardness parameter.",http://proceedings.mlr.press/v70/chen17b.html,http://proceedings.mlr.press/v70/chen17b/chen17b.pdf,ICML
1786,2017,Neural Message Passing for Quantum Chemistry,"Justin Gilmer,         Samuel S. Schoenholz,         Patrick F. Riley,         Oriol Vinyals,         George E. Dahl","Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.",http://proceedings.mlr.press/v70/gilmer17a.html,http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf,ICML
1787,2017,Understanding the Representation and Computation of Multilayer Perceptrons: A Case Study in Speech Recognition,"Tasha Nagamine,         Nima Mesgarani","Despite the recent success of deep learning, the nature of the transformations they apply to the input features remains poorly understood. This study provides an empirical framework to study the encoding properties of node activations in various layers of the network, and to construct the exact function applied to each data point in the form of a linear transform. These methods are used to discern and quantify properties of feed-forward neural networks trained to map acoustic features to phoneme labels. We show a selective and nonlinear warping of the feature space, achieved by forming prototypical functions to account for the possible variation of each class. This study provides a joint framework where the properties of node activations and the functions implemented by the network can be linked together.",http://proceedings.mlr.press/v70/nagamine17a.html,http://proceedings.mlr.press/v70/nagamine17a/nagamine17a.pdf,ICML
1788,2017,Decoupled Neural Interfaces using Synthetic Gradients,"Max Jaderberg,         Wojciech Marian Czarnecki,         Simon Osindero,         Oriol Vinyals,         Alex Graves,         David Silver,         Koray Kavukcuoglu","Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one’s future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.",http://proceedings.mlr.press/v70/jaderberg17a.html,http://proceedings.mlr.press/v70/jaderberg17a/jaderberg17a.pdf,ICML
1789,2017,Magnetic Hamiltonian Monte Carlo,"Nilesh Tripuraneni,         Mark Rowland,         Zoubin Ghahramani,         Richard Turner","Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct efficient proposals for Markov chain Monte Carlo (MCMC). In this paper, we present a generalization of HMC which exploits non-canonical Hamiltonian dynamics. We refer to this algorithm as magnetic HMC, since in 3 dimensions a subset of the dynamics map onto the mechanics of a charged particle coupled to a magnetic field. We establish a theoretical basis for the use of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic, leapfrog-like integrator allowing for the implementation of magnetic HMC. Finally, we exhibit several examples where these non-canonical dynamics can lead to improved mixing of magnetic HMC relative to ordinary HMC.",http://proceedings.mlr.press/v70/tripuraneni17a.html,http://proceedings.mlr.press/v70/tripuraneni17a/tripuraneni17a.pdf,ICML
1790,2017,Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs,"Li Jing,         Yichen Shen,         Tena Dubcek,         John Peurifoy,         Scott Skirlo,         Yann LeCun,         Max Tegmark,         Marin Soljačić","Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely O(1)O(1)\mathcal{O}(1) per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixel-permuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.",http://proceedings.mlr.press/v70/jing17a.html,http://proceedings.mlr.press/v70/jing17a/jing17a.pdf,ICML
1791,2017,Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders,"Jesse Engel,         Cinjon Resnick,         Adam Roberts,         Sander Dieleman,         Mohammad Norouzi,         Douglas Eck,         Karen Simonyan","Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.",http://proceedings.mlr.press/v70/engel17a.html,http://proceedings.mlr.press/v70/engel17a/engel17a.pdf,ICML
1792,2017,Real-Time Adaptive Image Compression,"Oren Rippel,         Lubomir Bourdev","We present a machine learning-based approach to lossy image compression which outperforms all existing codecs, while running in real-time. Our algorithm typically produces file sizes 3 times smaller than JPEG, 2.5 times smaller than JPEG 2000, and 2.3 times smaller than WebP on datasets of generic images across a spectrum of quality levels. At the same time, our codec is designed to be lightweight and deployable: for example, it can encode or decode the Kodak dataset in less than 10ms per image on GPU. Our architecture is an autoencoder featuring pyramidal analysis, an adaptive coding module, and regularization of the expected codelength. We also supplement our approach with adversarial training specialized towards use in a compression setting: this enables us to produce visually pleasing reconstructions for very low bitrates.",http://proceedings.mlr.press/v70/rippel17a.html,http://proceedings.mlr.press/v70/rippel17a/rippel17a.pdf,ICML
1793,2017,Geometry of Neural Network Loss Surfaces via Random Matrix Theory,"Jeffrey Pennington,         Yasaman Bahri","Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, ϕϕ\phi, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function 1/2(1−ϕ)21/2(1−ϕ)21/2(1-\phi)^2.",http://proceedings.mlr.press/v70/pennington17a.html,http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf,ICML
1794,2017,Bayesian Models of Data Streams with Hierarchical Power Priors,"Andrés Masegosa,         Thomas D. Nielsen,         Helge Langseth,         Darı́o Ramos-López,         Antonio Salmerón,         Anders L. Madsen","Making inferences from data streams is a pervasive problem in many modern data analysis applications. But it requires to address the problem of continuous model updating, and adapt to changes or drifts in the underlying data generating distribution. In this paper, we approach these problems from a Bayesian perspective covering general conjugate exponential models. Our proposal makes use of non-conjugate hierarchical priors to explicitly model temporal changes of the model parameters. We also derive a novel variational inference scheme which overcomes the use of non-conjugate priors while maintaining the computational efficiency of variational methods over conjugate models. The approach is validated on three real data sets over three latent variable models.",http://proceedings.mlr.press/v70/masegosa17a.html,http://proceedings.mlr.press/v70/masegosa17a/masegosa17a.pdf,ICML
1795,2017,Prox-PDA: The Proximal Primal-Dual Algorithm for Fast Distributed Nonconvex Optimization and Learning Over Networks,"Mingyi Hong,         Davood Hajinezhad,         Ming-Min Zhao","In this paper we consider nonconvex optimization and learning over a network of distributed nodes. We develop a Proximal Primal-Dual Algorithm (Prox-PDA), which enables the network nodes to distributedly and collectively compute the set of first-order stationary solutions in a global sublinear manner [with a rate of O(1/r)O(1/r)O(1/r), where rrr is the iteration counter]. To the best of our knowledge, this is the first algorithm that enables distributed nonconvex optimization with global rate guarantees. Our numerical experiments also demonstrate the effectiveness of the proposed algorithm.",http://proceedings.mlr.press/v70/hong17a.html,http://proceedings.mlr.press/v70/hong17a/hong17a.pdf,ICML
1796,2017,On Relaxing Determinism in Arithmetic Circuits,"Arthur Choi,         Adnan Darwiche","The past decade has seen a significant interest in learning tractable probabilistic representations. Arithmetic circuits (ACs) were among the first proposed tractable representations, with some subsequent representations being instances of ACs with weaker or stronger properties. In this paper, we provide a formal basis under which variants on ACs can be compared, and where the precise roles and semantics of their various properties can be made more transparent. This allows us to place some recent developments on ACs in a clearer perspective and to also derive new results for ACs. This includes an exponential separation between ACs with and without determinism; completeness and incompleteness results; and tractability results (or lack thereof) when computing most probable explanations (MPEs).",http://proceedings.mlr.press/v70/choi17a.html,http://proceedings.mlr.press/v70/choi17a/choi17a.pdf,ICML
1797,2017,On Mixed Memberships and Symmetric Nonnegative Matrix Factorizations,"Xueyu Mao,         Purnamrita Sarkar,         Deepayan Chakrabarti","The problem of finding overlapping communities in networks has gained much attention recently. Optimization-based approaches use non-negative matrix factorization (NMF) or variants, but the global optimum cannot be provably attained in general. Model-based approaches, such as the popular mixed-membership stochastic blockmodel or MMSB (Airoldi et al., 2008), use parameters for each node to specify the overlapping communities, but standard inference techniques cannot guarantee consistency. We link the two approaches, by (a) establishing sufficient conditions for the symmetric NMF optimization to have a unique solution under MMSB, and (b) proposing a computationally efficient algorithm called GeoNMF that is provably optimal and hence consistent for a broad parameter regime. We demonstrate its accuracy on both simulated and real-world datasets.",http://proceedings.mlr.press/v70/mao17a.html,http://proceedings.mlr.press/v70/mao17a/mao17a.pdf,ICML
1798,2017,AdaNet: Adaptive Structural Learning of Artificial Neural Networks,"Corinna Cortes,         Xavier Gonzalvo,         Vitaly Kuznetsov,         Mehryar Mohri,         Scott Yang","We present a new framework for analyzing and learning artificial neural networks. Our approach simultaneously and adaptively learns both the structure of the network as well as its weights. The methodology is based upon and accompanied by strong data-dependent theoretical learning guarantees, so that the final network architecture provably adapts to the complexity of any given problem.",http://proceedings.mlr.press/v70/cortes17a.html,http://proceedings.mlr.press/v70/cortes17a/cortes17a.pdf,ICML
1799,2016,Barron and Cover’s Theory in Supervised Learning and its Application to Lasso,"Masanori Kawakita,         Jun’ichi Takeuchi","We study Barron and Cover’s theory (BC theory) in supervised learning. The original BC theory can be applied to supervised learning only approximately and limitedly. Though Barron (2008) and Chatterjee and Barron (2014) succeeded in removing the approximation, their idea cannot be essentially applied to supervised learning in general. By solving this issue, we propose an extension of BC theory to supervised learning. The extended theory has several advantages inherited from the original BC theory. First, it holds for finite sample number n. Second, it requires remarkably few assumptions. Third, it gives a justification of the MDL principle in supervised learning. We also derive new risk and regret bounds of lasso with random design as its application. The derived risk bound hold for any finite n without boundedness of features in contrast to past work. Behavior of the regret bound is investigated by numerical simulations. We believe that this is the first extension of BC theory to general supervised learning without approximation.",http://proceedings.mlr.press/v48/kawakita16.html,http://proceedings.mlr.press/v48/kawakita16.pdf,ICML
1800,2016,Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters,"Jelena Luketina,         Mathias Berglund,         Klaus Greff,         Tapani Raiko","Hyperparameter selection generally relies on running multiple full training trials, with selection based on validation set performance. We propose a gradient-based approach for locally adjusting hyperparameters during training of the model. Hyperparameters are adjusted so as to make the model parameter gradients, and hence updates, more advantageous for the validation cost. We explore the approach for tuning regularization hyperparameters and find that in experiments on MNIST, SVHN and CIFAR-10, the resulting regularization levels are within the optimal regions. The additional computational cost depends on how frequently the hyperparameters are trained, but the tested scheme adds only 30% computational overhead regardless of the model size. Since the method is significantly less computationally demanding compared to similar gradient-based approaches to hyperparameter optimization, and consistently finds good hyperparameter values, it can be a useful tool for training neural network models.",http://proceedings.mlr.press/v48/luketina16.html,http://proceedings.mlr.press/v48/luketina16.pdf,ICML
1801,2016,Gaussian process nonparametric tensor estimator and its minimax optimality,"Heishiro Kanagawa,         Taiji Suzuki,         Hayato Kobayashi,         Nobuyuki Shimizu,         Yukihiro Tagami","We investigate the statistical efficiency of a nonparametric Gaussian process method for a nonlinear tensor estimation problem. Low-rank tensor estimation has been used as a method to learn higher order relations among several data sources in a wide range of applications, such as multi-task learning, recommendation systems, and spatiotemporal analysis. We consider a general setting where a common linear tensor learning is extended to a nonlinear learning problem in reproducing kernel Hilbert space and propose a nonparametric Bayesian method based on the Gaussian process method. We prove its statistical convergence rate without assuming any strong convexity, such as restricted strong convexity. Remarkably, it is shown that our convergence rate achieves the minimax optimal rate. We apply our proposed method to multi-task learning and show that our method significantly outperforms existing methods through numerical experiments on real-world data sets.",http://proceedings.mlr.press/v48/kanagawa16.html,http://proceedings.mlr.press/v48/kanagawa16.pdf,ICML
1802,2016,Expressiveness of Rectifier Networks,"Xingyuan Pan,         Vivek Srikumar","Rectified Linear Units (ReLUs) have been shown to ameliorate the vanishing gradient problem, allow for efficient backpropagation, and empirically promote sparsity in the learned parameters. They have led to state-of-the-art results in a variety of applications. However, unlike threshold and sigmoid networks, ReLU networks are less explored from the perspective of their expressiveness. This paper studies the expressiveness of ReLU networks. We characterize the decision boundary of two-layer ReLU networks by constructing functionally equivalent threshold networks. We show that while the decision boundary of a two-layer ReLU network can be captured by a threshold network, the latter may require an exponentially larger number of hidden units. We also formulate sufficient conditions for a corresponding logarithmic reduction in the number of hidden units to represent a sign network as a ReLU network. Finally, we experimentally compare threshold networks and their much smaller ReLU counterparts with respect to their ability to learn from synthetically generated data.",http://proceedings.mlr.press/v48/panb16.html,http://proceedings.mlr.press/v48/panb16.pdf,ICML
1803,2016,K-Means Clustering with Distributed Dimensions,"Hu Ding,         Yu Liu,         Lingxiao Huang,         Jian Li","Distributed clustering has attracted significant attention in recent years. In this paper, we study the k-means problem in the distributed dimension setting, where the dimensions of the data are partitioned across multiple machines. We provide new approximation algorithms, which incur low communication costs and achieve constant approximation ratios. The communication complexity of our algorithms significantly improve on existing algorithms. We also provide the first communication lower bound, which nearly matches our upper bound in a certain range of parameter setting. Our experimental results show that our algorithms outperform existing algorithms on real data-sets in the distributed dimension setting.",http://proceedings.mlr.press/v48/ding16.html,http://proceedings.mlr.press/v48/ding16.pdf,ICML
1804,2016,Autoencoding beyond pixels using a learned similarity metric,"Anders Boesen Lindbo Larsen,         Søren Kaae Sønderby,         Hugo Larochelle,         Ole Winther","We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GAN) we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.",http://proceedings.mlr.press/v48/larsen16.html,http://proceedings.mlr.press/v48/larsen16.pdf,ICML
1805,2016,Correcting Forecasts with Multifactor Neural Attention,"Matthew Riemer,         Aditya Vempaty,         Flavio Calmon,         Fenno Heath,         Richard Hull,         Elham Khabiri","Automatic forecasting of time series data is a challenging problem in many industries. Current forecast models adopted by businesses do not provide adequate means for including data representing external factors that may have a significant impact on the time series, such as weather, national events, local events, social media trends, promotions, etc. This paper introduces a novel neural network attention mechanism that naturally incorporates data from multiple external sources without the feature engineering needed to get other techniques to work. We demonstrate empirically that the proposed model achieves superior performance for predicting the demand of 20 commodities across 107 stores of one of America’s largest retailers when compared to other baseline models, including neural networks, linear models, certain kernel methods, Bayesian regression, and decision trees. Our method ultimately accounts for a 23.9% relative improvement as a result of the incorporation of external data sources, and provides an unprecedented level of descriptive ability for a neural network forecasting model.",http://proceedings.mlr.press/v48/riemer16.html,http://proceedings.mlr.press/v48/riemer16.pdf,ICML
1806,2016,Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units,"Wenling Shang,         Kihyuk Sohn,         Diogo Almeida,         Honglak Lee","Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CReLU) and theoretically analyze its reconstruction property in CNNs. We integrate CReLU into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.",http://proceedings.mlr.press/v48/shang16.html,http://proceedings.mlr.press/v48/shang16.pdf,ICML
1807,2016,The Label Complexity of Mixed-Initiative Classifier Training,"Jina Suh,         Xiaojin Zhu,         Saleema Amershi","Mixed-initiative classifier training, where the human teacher can choose which items to label or to label items chosen by the computer, has enjoyed empirical success but without a rigorous statistical learning theoretical justification. We analyze the label complexity of a simple mixed-initiative training mechanism using teach- ing dimension and active learning. We show that mixed-initiative training is advantageous com- pared to either computer-initiated (represented by active learning) or human-initiated classifier training. The advantage exists across all human teaching abilities, from optimal to completely unhelpful teachers. We further improve classifier training by educating the human teachers. This is done by showing, or explaining, optimal teaching sets to the human teachers. We conduct Mechanical Turk human experiments on two stylistic classifier training tasks to illustrate our approach.",http://proceedings.mlr.press/v48/suh16.html,http://proceedings.mlr.press/v48/suh16.pdf,ICML
1808,2016,Multi-Bias Non-linear Activation in Deep Neural Networks,"Hongyang Li,         Wanli Ouyang,         Xiaogang Wang","As a widely used non-linear activation, Rectified Linear Unit (ReLU) separates noise and signal in a feature map by learning a threshold or bias. However, we argue that the classification of noise and signal not only depends on the magnitude of responses, but also the context of how the feature responses would be used to detect more abstract patterns in higher layers. In order to output multiple response maps with magnitude in different ranges for a particular visual pattern, existing networks employing ReLU and its variants have to learn a large number of redundant filters. In this paper, we propose a multi-bias non-linear activation (MBA) layer to explore the information hidden in the magnitudes of responses. It is placed after the convolution layer to decouple the responses to a convolution kernel into multiple maps by multi-thresholding magnitudes, thus generating more patterns in the feature space at a low computational cost. It provides great flexibility of selecting responses to different visual patterns in different magnitude ranges to form rich representations in higher layers. Such a simple and yet effective scheme achieves the state-of-the-art performance on several benchmarks.",http://proceedings.mlr.press/v48/lia16.html,http://proceedings.mlr.press/v48/lia16.pdf,ICML
1809,2016,Bayesian Poisson Tucker Decomposition for Learning the Structure of International Relations,"Aaron Schein,         Mingyuan Zhou,         David Blei,         Hanna Wallach","We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling country–country interaction event data. These data consist of interaction events of the form “country i took action a toward country j at time t.” BPTD discovers overlapping country–community memberships, including the number of latent communities. In addition, it discovers directed community–community interaction networks that are specific to “topics” of action types and temporal “regimes.” We show that BPTD yields an efficient MCMC inference algorithm and achieves better predictive performance than related models. We also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations.",http://proceedings.mlr.press/v48/schein16.html,http://proceedings.mlr.press/v48/schein16.pdf,ICML
1810,2016,A Kernelized Stein Discrepancy for Goodness-of-fit Tests,"Qiang Liu,         Jason Lee,         Michael Jordan","We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein’s identity and the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.",http://proceedings.mlr.press/v48/liub16.html,http://proceedings.mlr.press/v48/liub16.pdf,ICML
1811,2016,Conservative Bandits,"Yifan Wu,         Roshan Shariff,         Tor Lattimore,         Csaba Szepesvari","We study a novel multi-armed bandit problem that models the challenge faced by a company wishing to explore new strategies to maximize revenue whilst simultaneously maintaining their revenue above a fixed baseline, uniformly over time. While previous work addressed the problem under the weaker requirement of maintaining the revenue constraint only at a given fixed time in the future, the design of those algorithms makes them unsuitable under the more stringent constraints. We consider both the stochastic and the adversarial settings, where we propose natural yet novel strategies and analyze the price for maintaining the constraints. Amongst other things, we prove both high probability and expectation bounds on the regret, while we also consider both the problem of maintaining the constraints with high probability or expectation. For the adversarial setting the price of maintaining the constraint appears to be higher, at least for the algorithm considered. A lower bound is given showing that the algorithm for the stochastic setting is almost optimal. Empirical results obtained in synthetic environments complement our theoretical findings.",http://proceedings.mlr.press/v48/wu16.html,http://proceedings.mlr.press/v48/wu16.pdf,ICML
1812,2016,Pliable Rejection Sampling,"Akram Erraqabi,         Michal Valko,         Alexandra Carpentier,         Odalric Maillard","Rejection sampling is a technique for sampling from difficult distributions. However, its use is limited due to a high rejection rate. Common adaptive rejection sampling methods either work only for very specific distributions or without performance guarantees. In this paper, we present pliable rejection sampling (PRS), a new approach to rejection sampling, where we learn the sampling proposal using a kernel estimator. Since our method builds on rejection sampling, the samples obtained are with high probability i.i.d. and distributed according to f. Moreover, PRS comes with a guarantee on the number of accepted samples.",http://proceedings.mlr.press/v48/erraqabi16.html,http://proceedings.mlr.press/v48/erraqabi16.pdf,ICML
1813,2016,Power of Ordered Hypothesis Testing,"Lihua Lei,         William Fithian","Ordered testing procedures are multiple testing procedures that exploit a pre-specified ordering of the null hypotheses, from most to least promising. We analyze and compare the power of several recent proposals using the asymptotic framework of Li & Barber (2015). While accumulation tests including ForwardStop can be quite powerful when the ordering is very informative, they are asymptotically powerless when the ordering is weaker. By contrast, Selective SeqStep, proposed by Barber & Candes (2015), is much less sensitive to the quality of the ordering. We compare the power of these procedures in different regimes, concluding that Selective SeqStep dominates accumulation tests if either the ordering is weak or non-null hypotheses are sparse or weak. Motivated by our asymptotic analysis, we derive an improved version of Selective SeqStep which we call Adaptive SeqStep, analogous to Storey’s improvement on the Benjamini-Hochberg procedure. We compare these methods using the GEO-Query data set analyzed by (Li & Barber, 2015) and find Adaptive SeqStep has favorable performance for both good and bad prior orderings.",http://proceedings.mlr.press/v48/lei16.html,http://proceedings.mlr.press/v48/lei16.pdf,ICML
1814,2016,Recycling Randomness with Structure for Sublinear time Kernel Expansions,"Krzysztof Choromanski,         Vikas Sindhwani","We propose a scheme for recycling Gaussian random vectors into structured matrices to ap- proximate various kernel functions in sublin- ear time via random embeddings. Our frame- work includes the Fastfood construction of Le et al. (2013) as a special case, but also ex- tends to Circulant, Toeplitz and Hankel matri- ces, and the broader family of structured matri- ces that are characterized by the concept of low- displacement rank. We introduce notions of co- herence and graph-theoretic structural constants that control the approximation quality, and prove unbiasedness and low-variance properties of ran- dom feature maps that arise within our frame- work. For the case of low-displacement matri- ces, we show how the degree of structure and randomness can be controlled to reduce statis- tical variance at the cost of increased computa- tion and storage requirements. Empirical results strongly support our theory and justify the use of a broader family of structured matrices for scal- ing up kernel methods using random features.",http://proceedings.mlr.press/v48/choromanski16.html,http://proceedings.mlr.press/v48/choromanski16.pdf,ICML
1815,2016,Representational Similarity Learning with Application to Brain Networks,"Urvashi Oswal,         Christopher Cox,         Matthew Lambon-Ralph,         Timothy Rogers,         Robert Nowak","Representational Similarity Learning (RSL) aims to discover features that are important in representing (human-judged) similarities among objects. RSL can be posed as a sparsity-regularized multi-task regression problem. Standard methods, like group lasso, may not select important features if they are strongly correlated with others. To address this shortcoming we present a new regularizer for multitask regression called Group Ordered Weighted \ell_1 (GrOWL). Another key contribution of our paper is a novel application to fMRI brain imaging. Representational Similarity Analysis (RSA) is a tool for testing whether localized brain regions encode perceptual similarities. Using GrOWL, we propose a new approach called Network RSA that can discover arbitrarily structured brain networks (possibly widely distributed and non-local) that encode similarity information. We show, in theory and fMRI experiments, how GrOWL deals with strongly correlated covariates.",http://proceedings.mlr.press/v48/oswal16.html,http://proceedings.mlr.press/v48/oswal16.pdf,ICML
1816,2016,Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling,"Atsushi Shibagaki,         Masayuki Karasuyama,         Kohei Hatano,         Ichiro Takeuchi","The problem of learning a sparse model is conceptually interpreted as the process of identifying active features/samples and then optimizing the model over them. Recently introduced safe screening allows us to identify a part of non-active features/samples. So far, safe screening has been individually studied either for feature screening or for sample screening. In this paper, we introduce a new approach for safely screening features and samples simultaneously by alternatively iterating feature and sample screening steps. A significant advantage of considering them simultaneously rather than individually is that they have a synergy effect in the sense that the results of the previous safe feature screening can be exploited for improving the next safe sample screening performances, and vice-versa. We first theoretically investigate the synergy effect, and then illustrate the practical advantage through intensive numerical experiments for problems with large numbers of features and samples.",http://proceedings.mlr.press/v48/shibagaki16.html,http://proceedings.mlr.press/v48/shibagaki16.pdf,ICML
1817,2016,Low-rank tensor completion: a Riemannian manifold preconditioning approach,"Hiroyuki Kasai,         Bamdev Mishra","We propose a novel Riemannian manifold preconditioning approach for the tensor completion problem with rank constraint. A novel Riemannian metric or inner product is proposed that exploits the least-squares structure of the cost function and takes into account the structured symmetry that exists in Tucker decomposition. The specific metric allows to use the versatile framework of Riemannian optimization on quotient manifolds to develop preconditioned nonlinear conjugate gradient and stochastic gradient descent algorithms in batch and online setups, respectively. Concrete matrix representations of various optimization-related ingredients are listed. Numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets.",http://proceedings.mlr.press/v48/kasai16.html,http://proceedings.mlr.press/v48/kasai16.pdf,ICML
1818,2016,Auxiliary Deep Generative Models,"Lars Maaløe,         Casper Kaae Sønderby,         Søren Kaae Sønderby,         Ole Winther","Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets.",http://proceedings.mlr.press/v48/maaloe16.html,http://proceedings.mlr.press/v48/maaloe16.pdf,ICML
1819,2016,Model-Free Imitation Learning with Policy Optimization,"Jonathan Ho,         Jayesh Gupta,         Stefano Ermon","In imitation learning, an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations. Existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems. Such algorithms are therefore not directly applicable to large, high-dimensional environments, and their performance can significantly degrade if the planning problems are not solved to optimality. Under the apprenticeship learning formalism, we develop alternative model-free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function, based on sample trajectories from the expert. Our approach, based on policy gradients, scales to large continuous environments with guaranteed convergence to local minima.",http://proceedings.mlr.press/v48/ho16.html,http://proceedings.mlr.press/v48/ho16.pdf,ICML
1820,2016,Parallel and Distributed Block-Coordinate Frank-Wolfe Algorithms,"Yu-Xiang Wang,         Veeranjaneyulu Sadhanala,         Wei Dai,         Willie Neiswanger,         Suvrit Sra,         Eric Xing","We study parallel and distributed Frank-Wolfe algorithms; the former on shared memory machines with mini-batching, and the latter in a delayed update framework. In both cases, we perform computations asynchronously whenever possible. We assume block-separable constraints as in Block-Coordinate Frank-Wolfe (BCFW) method (Lacoste et. al., 2013) , but our analysis subsumes BCFW and reveals problem-dependent quantities that govern the speedups of our methods over BCFW. A notable feature of our algorithms is that they do not depend on worst-case bounded delays, but only (mildly) on **expected** delays, making them robust to stragglers and faulty worker threads. We present experiments on structural SVM and Group Fused Lasso, and observe significant speedups over competing state-of-the-art (and synchronous) methods.",http://proceedings.mlr.press/v48/wangd16.html,http://proceedings.mlr.press/v48/wangd16.pdf,ICML
1821,2016,Persistent RNNs: Stashing Recurrent Weights On-Chip,"Greg Diamos,         Shubho Sengupta,         Bryan Catanzaro,         Mike Chrzanowski,         Adam Coates,         Erich Elsen,         Jesse Engel,         Awni Hannun,         Sanjeev Satheesh","This paper introduces a new technique for mapping Deep Recurrent Neural Networks (RNN) efficiently onto GPUs. We show how it is possi- ble to achieve substantially higher computational throughput at low mini-batch sizes than direct implementations of RNNs based on matrix multiplications. The key to our approach is the use of persistent computational kernels that exploit the GPU’s inverted memory hierarchy to reuse network weights over multiple timesteps. Our initial implementation sustains 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU. This provides a 16x reduction in activation memory footprint, enables model training with 12x more parameters on the same hardware, allows us to strongly scale RNN training to 128 GPUs, and allows us to efficiently explore end-to-end speech recognition models with over 100 layers.",http://proceedings.mlr.press/v48/diamos16.html,http://proceedings.mlr.press/v48/diamos16.pdf,ICML
1822,2016,Uprooting and Rerooting Graphical Models,Adrian Weller,"We show how any binary pairwise model may be “uprooted” to a fully symmetric model, wherein original singleton potentials are transformed to potentials on edges to an added variable, and then “rerooted” to a new model on the original number of variables. The new model is essentially equivalent to the original model, with the same partition function and allowing recovery of the original marginals or a MAP configuration, yet may have very different computational properties that allow much more efficient inference. This meta-approach deepens our understanding, may be applied to any existing algorithm to yield improved methods in practice, generalizes earlier theoretical results, and reveals a remarkable interpretation of the triplet-consistent polytope.",http://proceedings.mlr.press/v48/weller16.html,http://proceedings.mlr.press/v48/weller16.pdf,ICML
1823,2016,A New PAC-Bayesian Perspective on Domain Adaptation,"Pascal Germain,         Amaury Habrard,         François Laviolette,         Emilie Morvant","We study the issue of PAC-Bayesian domain adaptation: We want to learn, from a source domain, a majority vote model dedicated to a target one. Our theoretical contribution brings a new perspective by deriving an upper-bound on the target risk where the distributions’ divergence - expressed as a ratio - controls the trade-off between a source error measure and the target voters’ disagreement. Our bound suggests that one has to focus on regions where the source data is informative. From this result, we derive a PAC-Bayesian generalization bound, and specialize it to linear classifiers. Then, we infer a learning algorithm and perform experiments on real data.",http://proceedings.mlr.press/v48/germain16.html,http://proceedings.mlr.press/v48/germain16.pdf,ICML
1824,2016,Tensor Decomposition via Joint Matrix Schur Decomposition,"Nicolo Colombo,         Nikos Vlassis","We describe an approach to tensor decomposition that involves extracting a set of observable matrices from the tensor and applying an approximate joint Schur decomposition on those matrices, and we establish the corresponding first-order perturbation bounds. We develop a novel iterative Gauss-Newton algorithm for joint matrix Schur decomposition, which minimizes a nonconvex objective over the manifold of orthogonal matrices, and which is guaranteed to converge to a global optimum under certain conditions. We empirically demonstrate that our algorithm is faster and at least as accurate and robust than state-of-the-art algorithms for this problem.",http://proceedings.mlr.press/v48/colombo16.html,http://proceedings.mlr.press/v48/colombo16.pdf,ICML
1825,2016,Hierarchical Compound Poisson Factorization,"Mehmet Basbug,         Barbara Engelhardt","Non-negative matrix factorization models based on a hierarchical Gamma-Poisson structure capture user and item behavior effectively in extremely sparse data sets, making them the ideal choice for collaborative filtering applications. Hierarchical Poisson factorization (HPF) in particular has proved successful for scalable recommendation systems with extreme sparsity. HPF, however, suffers from a tight coupling of sparsity model (absence of a rating) and response model (the value of the rating), which limits the expressiveness of the latter. Here, we introduce hierarchical compound Poisson factorization (HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to high-dimensional extremely sparse matrices. More importantly, HCPF decouples the sparsity model from the response model, allowing us to choose the most suitable distribution for the response. HCPF can capture binary, non-negative discrete, non-negative continuous, and zero-inflated continuous responses. We compare HCPF with HPF on nine discrete and three continuous data sets and conclude that HCPF captures the relationship between sparsity and response better than HPF.",http://proceedings.mlr.press/v48/basbug16.html,http://proceedings.mlr.press/v48/basbug16.pdf,ICML
1826,2016,Hyperparameter optimization with approximate gradient,Fabian Pedregosa,"Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods.",http://proceedings.mlr.press/v48/pedregosa16.html,http://proceedings.mlr.press/v48/pedregosa16.pdf,ICML
1827,2016,Stochastic Quasi-Newton Langevin Monte Carlo,"Umut Simsekli,         Roland Badeau,         Taylan Cemgil,         Gaël Richard","Recently, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods have been proposed for scaling up Monte Carlo computations to large data problems. Whilst these approaches have proven useful in many applications, vanilla SG-MCMC might suffer from poor mixing rates when random variables exhibit strong couplings under the target densities or big scale differences. In this study, we propose a novel SG-MCMC method that takes the local geometry into account by using ideas from Quasi-Newton optimization methods. These second order methods directly approximate the inverse Hessian by using a limited history of samples and their gradients. Our method uses dense approximations of the inverse Hessian while keeping the time and memory complexities linear with the dimension of the problem. We provide a formal theoretical analysis where we show that the proposed method is asymptotically unbiased and consistent with the posterior expectations. We illustrate the effectiveness of the approach on both synthetic and real datasets. Our experiments on two challenging applications show that our method achieves fast convergence rates similar to Riemannian approaches while at the same time having low computational requirements similar to diagonal preconditioning approaches.",http://proceedings.mlr.press/v48/simsekli16.html,http://proceedings.mlr.press/v48/simsekli16.pdf,ICML
1828,2016,Sequence to Sequence Training of CTC-RNNs with Partial Windowing,"Kyuyeon Hwang,         Wonyong Sung","Connectionist temporal classification (CTC) based supervised sequence training of recurrent neural networks (RNNs) has shown great success in many machine learning areas including end-to-end speech and handwritten character recognition. For the CTC training, however, it is required to unroll (or unfold) the RNN by the length of an input sequence. This unrolling requires a lot of memory and hinders a small footprint implementation of online learning or adaptation. Furthermore, the length of training sequences is usually not uniform, which makes parallel training with multiple sequences inefficient on shared memory models such as graphics processing units (GPUs). In this work, we introduce an expectation-maximization (EM) based online CTC algorithm that enables unidirectional RNNs to learn sequences that are longer than the amount of unrolling. The RNNs can also be trained to process an infinitely long input sequence without pre-segmentation or external reset. Moreover, the proposed approach allows efficient parallel training on GPUs. Our approach achieves 20.7% phoneme error rate (PER) on the very long input sequence that is generated by concatenating all 192 utterances in the TIMIT core test set. In the end-to-end speech recognition task on the Wall Street Journal corpus, a network can be trained with only 64 times of unrolling with little performance loss.",http://proceedings.mlr.press/v48/hwanga16.html,http://proceedings.mlr.press/v48/hwanga16.pdf,ICML
1829,2016,Provable Algorithms for Inference in Topic Models,"Sanjeev Arora,         Rong Ge,         Frederic Koehler,         Tengyu Ma,         Ankur Moitra","Recently, there has been considerable progress on designing algorithms with provable guarantees —typically using linear algebraic methods—for parameter learning in latent variable models. Designing provable algorithms for inference has proved more difficult. Here we take a first step towards provable inference in topic models. We leverage a property of topic models that enables us to construct simple linear estimators for the unknown topic proportions that have small variance, and consequently can work with short documents. Our estimators also correspond to finding an estimate around which the posterior is well-concentrated. We show lower bounds that for shorter documents it can be information theoretically impossible to find the hidden topics. Finally, we give empirical results that demonstrate that our algorithm works on realistic topic models. It yields good solutions on synthetic data and runs in time comparable to a single iteration of Gibbs sampling.",http://proceedings.mlr.press/v48/arorab16.html,http://proceedings.mlr.press/v48/arorab16.pdf,ICML
1830,2016,Multi-Player Bandits – a Musical Chairs Approach,"Jonathan Rosenski,         Ohad Shamir,         Liran Szlak","We consider a variant of the stochastic multi-armed bandit problem, where multiple players simultaneously choose from the same set of arms and may collide, receiving no reward. This setting has been motivated by problems arising in cognitive radio networks, and is especially challenging under the realistic assumption that communication between players is limited. We provide a communication-free algorithm (Musical Chairs) which attains constant regret with high probability, as well as a sublinear-regret, communication-free algorithm (Dynamic Musical Chairs) for the more difficult setting of players dynamically entering and leaving throughout the game. Moreover, both algorithms do not require prior knowledge of the number of players. To the best of our knowledge, these are the first communication-free algorithms with these types of formal guarantees.",http://proceedings.mlr.press/v48/rosenski16.html,http://proceedings.mlr.press/v48/rosenski16.pdf,ICML
1831,2016,Training Neural Networks Without Gradients: A Scalable ADMM Approach,"Gavin Taylor,         Ryan Burmeister,         Zheng Xu,         Bharat Singh,         Ankit Patel,         Tom Goldstein","With the growing importance of large network models and enormous training datasets, GPUs have become increasingly necessary to train neural networks. This is largely because conventional optimization algorithms rely on stochastic gradient methods that don’t scale well to large numbers of cores in a cluster setting. Furthermore, the convergence of all gradient methods, including batch methods, suffers from common problems like saturation effects, poor conditioning, and saddle points. This paper explores an unconventional training method that uses alternating direction methods and Bregman iteration to train networks without gradient descent steps. The proposed method reduces the network training problem to a sequence of minimization sub-steps that can each be solved globally in closed form. The proposed method is advantageous because it avoids many of the caveats that make gradient methods slow on highly non-convex problems. In addition, the method exhibits strong scaling in the distributed setting, yielding linear speedups even when split over thousands of cores.",http://proceedings.mlr.press/v48/taylor16.html,http://proceedings.mlr.press/v48/taylor16.pdf,ICML
1832,2016,Nonlinear Statistical Learning with Truncated Gaussian Graphical Models,"Qinliang Su,         Xuejun Liao,         Changyou Chen,         Lawrence Carin","We introduce the truncated Gaussian graphical model (TGGM) as a novel framework for designing statistical models for nonlinear learning. A TGGM is a Gaussian graphical model (GGM) with a subset of variables truncated to be nonnegative. The truncated variables are assumed latent and integrated out to induce a marginal model. We show that the variables in the marginal model are non-Gaussian distributed and their expected relations are nonlinear. We use expectation-maximization to break the inference of the nonlinear model into a sequence of TGGM inference problems, each of which is efficiently solved by using the properties and numerical methods of multivariate Gaussian distributions. We use the TGGM to design models for nonlinear regression and classification, with the performances of these models demonstrated on extensive benchmark datasets and compared to state-of-the-art competing results.",http://proceedings.mlr.press/v48/su16.html,http://proceedings.mlr.press/v48/su16.pdf,ICML
1833,2016,Computationally Efficient Nyström Approximation using Fast Transforms,"Si Si,         Cho-Jui Hsieh,         Inderjit Dhillon","Our goal is to improve the \it training and \it prediction time of Nyström method, which is a widely-used technique for generating low-rank kernel matrix approximations. When applying the Nyström approximation for large-scale applications, both training and prediction time is dominated by computing kernel values between a data point and all landmark points. With m landmark points, this computation requires Θ(md) time (flops), where d is the input dimension. In this paper, we propose the use of a family of fast transforms to generate structured landmark points for Nyström approximation. By exploiting fast transforms, e.g., Haar transform and Hadamard transform, our modified Nyström method requires only Θ(m) or Θ(m\log d) time to compute the kernel values between a given data point and m landmark points. This improvement in time complexity can significantly speed up kernel approximation and benefit prediction speed in kernel machines. For instance, on the webspam data (more than 300,000 data points), our proposed algorithm enables kernel SVM prediction to deliver 98% accuracy and the resulting prediction time is 1000 times faster than LIBSVM and only 10 times slower than linear SVM prediction (which yields only 91% accuracy).",http://proceedings.mlr.press/v48/si16.html,http://proceedings.mlr.press/v48/si16.pdf,ICML
1834,2016,Stochastic Variance Reduction for Nonconvex Optimization,"Sashank J. Reddi,         Ahmed Hefny,         Suvrit Sra,         Barnabas Poczos,         Alex Smola","We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (SVRG) methods for them. SVRG and related methods have recently surged into prominence for convex optimization given their edge over stochastic gradient descent (SGD); but their theoretical analysis almost exclusively assumes convexity. In contrast, we prove non-asymptotic rates of convergence (to stationary points) of SVRG for nonconvex optimization, and show that it is provably faster than SGD and gradient descent. We also analyze a subclass of nonconvex problems on which SVRG attains linear convergence to the global optimum. We extend our analysis to mini-batch variants of SVRG, showing (theoretical) linear speedup due to minibatching in parallel settings.",http://proceedings.mlr.press/v48/reddi16.html,http://proceedings.mlr.press/v48/reddi16.pdf,ICML
1835,2016,Contextual Combinatorial Cascading Bandits,"Shuai Li,         Baoxiang Wang,         Shengyu Zhang,         Wei Chen","We propose the contextual combinatorial cascading bandits, a combinatorial online learning game, where at each time step a learning agent is given a set of contextual information, then selects a list of items, and observes stochastic outcomes of a prefix in the selected items by some stopping criterion. In online recommendation, the stopping criterion might be the first item a user selects; in network routing, the stopping criterion might be the first edge blocked in a path. We consider position discounts in the list order, so that the agent’s reward is discounted depending on the position where the stopping criterion is met. We design a UCB-type algorithm, C^3-UCB, for this problem, prove an n-step regret bound \tildeO(\sqrtn) in the general setting, and give finer analysis for two special cases. Our work generalizes existing studies in several directions, including contextual information, position discounts, and a more general cascading bandit model. Experiments on synthetic and real datasets demonstrate the advantage of involving contextual information and position discounts.",http://proceedings.mlr.press/v48/lif16.html,http://proceedings.mlr.press/v48/lif16.pdf,ICML
1836,2016,"Why Most Decisions Are Easy in Tetris—And Perhaps in Other Sequential Decision Problems, As Well","Ozgur Simsek,         Simon Algorta,         Amit Kothiyal","We examined the sequence of decision problems that are encountered in the game of Tetris and found that most of the problems are easy in the following sense: One can choose well among the available actions without knowing an evaluation function that scores well in the game. This is a consequence of three conditions that are prevalent in the game: simple dominance, cumulative dominance, and noncompensation. These conditions can be exploited to develop faster and more effective learning algorithms. In addition, they allow certain types of domain knowledge to be incorporated with ease into a learning algorithm. Among the sequential decision problems we encounter, it is unlikely that Tetris is unique or rare in having these properties.",http://proceedings.mlr.press/v48/simsek16.html,http://proceedings.mlr.press/v48/simsek16.pdf,ICML
1837,2016,A Superlinearly-Convergent Proximal Newton-type Method for the Optimization of Finite Sums,"Anton Rodomanov,         Dmitry Kropotov",We consider the problem of minimizing the strongly convex sum of a finite number of convex functions. Standard algorithms for solving this problem in the class of incremental/stochastic methods have at most a linear convergence rate. We propose a new incremental method whose convergence rate is superlinear – the Newton-type incremental method (NIM). The idea of the method is to introduce a model of the objective with the same sum-of-functions structure and further update a single component of the model per iteration. We prove that NIM has a superlinear local convergence rate and linear global convergence rate. Experiments show that the method is very effective for problems with a large number of functions and a small number of variables.,http://proceedings.mlr.press/v48/rodomanov16.html,http://proceedings.mlr.press/v48/rodomanov16.pdf,ICML
1838,2016,Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning,"Philip Thomas,         Emma Brunskill","In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods—it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang & Li, 2015), and a new way to mix between model based and importance sampling based estimates.",http://proceedings.mlr.press/v48/thomasa16.html,http://proceedings.mlr.press/v48/thomasa16.pdf,ICML
1839,2016,Fast methods for estimating the Numerical rank of large matrices,"Shashanka Ubaru,         Yousef Saad","We present two computationally inexpensive techniques for estimating the numerical rank of a matrix, combining powerful tools from computational linear algebra. These techniques exploit three key ingredients. The first is to approximate the projector on the non-null invariant subspace of the matrix by using a polynomial filter. Two types of filters are discussed, one based on Hermite interpolation and the other based on Chebyshev expansions. The second ingredient employs stochastic trace estimators to compute the rank of this wanted eigen-projector, which yields the desired rank of the matrix. In order to obtain a good filter, it is necessary to detect a gap between the eigenvalues that correspond to noise and the relevant eigenvalues that correspond to the non-null invariant subspace. The third ingredient of the proposed approaches exploits the idea of spectral density, popular in physics, and the Lanczos spectroscopic method to locate this gap.",http://proceedings.mlr.press/v48/ubaru16.html,http://proceedings.mlr.press/v48/ubaru16.pdf,ICML
1840,2016,Fast k-means with accurate bounds,"James Newling,         Francois Fleuret","We propose a novel accelerated exact k-means algorithm, which outperforms the current state-of-the-art low-dimensional algorithm in 18 of 22 experiments, running up to 3 times faster. We also propose a general improvement of existing state-of-the-art accelerated exact k-means algorithms through better estimates of the distance bounds used to reduce the number of distance calculations, obtaining speedups in 36 of 44 experiments, of up to 1.8 times. We have conducted experiments with our own implementations of existing methods to ensure homogeneous evaluation of performance, and we show that our implementations perform as well or better than existing available implementations. Finally, we propose simplified variants of standard approaches and show that they are faster than their fully-fledged counterparts in 59 of 62 experiments.",http://proceedings.mlr.press/v48/newling16.html,http://proceedings.mlr.press/v48/newling16.pdf,ICML
1841,2016,Starting Small - Learning with Adaptive Sample Sizes,"Hadi Daneshmand,         Aurelien Lucchi,         Thomas Hofmann","For many machine learning problems, data is abundant and it may be prohibitive to make multiple passes through the full training set. In this context, we investigate strategies for dynamically increasing the effective sample size, when using iterative methods such as stochastic gradient descent. Our interest is motivated by the rise of variance-reduced methods, which achieve linear convergence rates that scale favorably for smaller sample sizes. Exploiting this feature, we show - theoretically and empirically - how to obtain significant speed-ups with a novel algorithm that reaches statistical accuracy on an n-sample in 2n, instead of n log n steps.",http://proceedings.mlr.press/v48/daneshmand16.html,http://proceedings.mlr.press/v48/daneshmand16.pdf,ICML
1842,2016,Gaussian quadrature for matrix inverse forms with applications,"Chengtao Li,         Suvrit Sra,         Stefanie Jegelka","We present a framework for accelerating a spectrum of machine learning algorithms that require computation of \emphbilinear inverse forms u^T A^-1u, where A is a positive definite matrix and u a given vector. Our framework is built on Gauss-type quadrature and easily scales to large, sparse matrices. Further, it allows retrospective computation of lower and upper bounds on u^T A^-1u, which in turn accelerates several algorithms. We prove that these bounds tighten iteratively and converge at a linear (geometric) rate. To our knowledge, ours is the first work to demonstrate these key properties of Gauss-type quadrature, which is a classical and deeply studied topic. We illustrate empirical consequences of our results by using quadrature to accelerate machine learning tasks involving determinantal point processes and submodular optimization, and observe tremendous speedups in several instances.",http://proceedings.mlr.press/v48/lig16.html,http://proceedings.mlr.press/v48/lig16.pdf,ICML
1843,2016,DCM Bandits: Learning to Rank with Multiple Clicks,"Sumeet Katariya,         Branislav Kveton,         Csaba Szepesvari,         Zheng Wen","A search engine recommends to the user a list of web pages. The user examines this list, from the first page to the last, and clicks on all attractive pages until the user is satisfied. This behavior of the user can be described by the dependent click model (DCM). We propose DCM bandits, an online learning variant of the DCM where the goal is to maximize the probability of recommending satisfactory items, such as web pages. The main challenge of our learning problem is that we do not observe which attractive item is satisfactory. We propose a computationally-efficient learning algorithm for solving our problem, dcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonable assumptions; and also prove a matching lower bound up to logarithmic factors. We evaluate our algorithm on synthetic and real-world problems, and show that it performs well even when our model is misspecified. This work presents the first practical and regret-optimal online algorithm for learning to rank with multiple clicks in a cascade-like click model.",http://proceedings.mlr.press/v48/katariya16.html,http://proceedings.mlr.press/v48/katariya16.pdf,ICML
1844,2016,Unsupervised Deep Embedding for Clustering Analysis,"Junyuan Xie,         Ross Girshick,         Ali Farhadi","Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.",http://proceedings.mlr.press/v48/xieb16.html,http://proceedings.mlr.press/v48/xieb16.pdf,ICML
1845,2016,Estimating Structured Vector Autoregressive Models,"Igor Melnyk,         Arindam Banerjee","While considerable advances have been made in estimating high-dimensional structured models from independent data using Lasso-type models, limited progress has been made for settings when the samples are dependent. We consider estimating structured VAR (vector auto-regressive model), where the structure can be captured by any suitable norm, e.g., Lasso, group Lasso, order weighted Lasso, etc. In VAR setting with correlated noise, although there is strong dependence over time and covariates, we establish bounds on the non-asymptotic estimation error of structured VAR parameters. The estimation error is of the same order as that of the corresponding Lasso-type estimator with independent samples, and the analysis holds for any norm. Our analysis relies on results in generic chaining, sub-exponential martingales, and spectral representation of VAR models. Experimental results on synthetic and real data with a variety of structures are presented, validating theoretical results.",http://proceedings.mlr.press/v48/melnyk16.html,http://proceedings.mlr.press/v48/melnyk16.pdf,ICML
1846,2016,Discrete Distribution Estimation under Local Privacy,"Peter Kairouz,         Keith Bonawitz,         Daniel Ramage","The collection and analysis of user data drives improvements in the app and web ecosystems, but comes with risks to privacy. This paper examines discrete distribution estimation under local privacy, a setting wherein service providers can learn the distribution of a categorical statistic of interest without collecting the underlying data. We present new mechanisms, including hashed k-ary Randomized Response (KRR), that empirically meet or exceed the utility of existing mechanisms at all privacy levels. New theoretical results demonstrate the order-optimality of KRR and the existing RAPPOR mechanism at different privacy regimes.",http://proceedings.mlr.press/v48/kairouz16.html,http://proceedings.mlr.press/v48/kairouz16.pdf,ICML
1847,2016,Hierarchical Decision Making In Electricity Grid Management,"Gal Dalal,         Elad Gilboa,         Shie Mannor","The power grid is a complex and vital system that necessitates careful reliability management. Managing the grid is a difficult problem with multiple time scales of decision making and stochastic behavior due to renewable energy generations, variable demand and unplanned outages. Solving this problem in the face of uncertainty requires a new methodology with tractable algorithms. In this work, we introduce a new model for hierarchical decision making in complex systems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., a level of abstraction, for real-time power grid reliability. We devise an algorithm that alternates between slow time-scale policy improvement, and fast time-scale value function approximation. We compare our results to prevailing heuristics, and show the strength of our method.",http://proceedings.mlr.press/v48/dalal16.html,http://proceedings.mlr.press/v48/dalal16.pdf,ICML
1848,2016,Faster Convex Optimization: Simulated Annealing with an Efficient Universal Barrier,"Jacob Abernethy,         Elad Hazan","This paper explores a surprising equivalence between two seemingly-distinct convex optimization methods. We show that simulated annealing, a well-studied random walk algorithms, is *directly equivalent*, in a certain sense, to the central path interior point algorithm for the the entropic universal barrier function. This connection exhibits several benefits. First, we are able improve the state of the art time complexity for convex optimization under the membership oracle model by devising a new temperature schedule for simulated annealing motivated by central path following interior point methods. Second, we get an efficient randomized interior point method with an efficiently computable universal barrier for any convex set described by a membership oracle. Previously, efficiently computable barriers were known only for particular convex sets.",http://proceedings.mlr.press/v48/abernethy16.html,http://proceedings.mlr.press/v48/abernethy16.pdf,ICML
1849,2016,Graying the black box: Understanding DQNs,"Tom Zahavy,         Nir Ben-Zrihem,         Shie Mannor","In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize of deep neural networks in Reinforcement Learning.",http://proceedings.mlr.press/v48/zahavy16.html,http://proceedings.mlr.press/v48/zahavy16.pdf,ICML
1850,2016,Principal Component Projection Without Principal Component Analysis,"Roy Frostig,         Cameron Musco,         Christopher Musco,         Aaron Sidford","We show how to efficiently project a vector onto the top principal components of a matrix, *without explicitly computing these components*. Specifically, we introduce an iterative algorithm that provably computes the projection using few calls to any black-box routine for ridge regression. By avoiding explicit principal component analysis (PCA), our algorithm is the first with no runtime dependence on the number of top principal components. We show that it can be used to give a fast iterative method for the popular principal component regression problem, giving the first major runtime improvement over the naive method of combining PCA with regression. To achieve our results, we first observe that ridge regression can be used to obtain a ""smooth projection"" onto the top principal components. We then sharpen this approximation to true projection using a low-degree polynomial approximation to the matrix step function. Step function approximation is a topic of long-term interest in scientific computing. We extend prior theory by constructing polynomials with simple iterative structure and rigorously analyzing their behavior under limited precision.",http://proceedings.mlr.press/v48/frostig16.html,http://proceedings.mlr.press/v48/frostig16.pdf,ICML
1851,2016,"Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm","Junpei Komiyama,         Junya Honda,         Hiroshi Nakagawa","We study the K-armed dueling bandit problem, a variation of the standard stochastic bandit problem where the feedback is limited to relative comparisons of a pair of arms. The hardness of recommending Copeland winners, the arms that beat the greatest number of other arms, is characterized by deriving an asymptotic regret bound. We propose Copeland Winners Deterministic Minimum Empirical Divergence (CW-RMED), an algorithm inspired by the DMED algorithm (Honda and Takemura, 2010), and derive an asymptotically optimal regret bound for it. However, it is not known whether the algorithm can be efficiently computed or not. To address this issue, we devise an efficient version (ECW-RMED) and derive its asymptotic regret bound. Experimental comparisons of dueling bandit algorithms show that ECW-RMED significantly outperforms existing ones.",http://proceedings.mlr.press/v48/komiyama16.html,http://proceedings.mlr.press/v48/komiyama16.pdf,ICML
1852,2016,A Convex Atomic-Norm Approach to Multiple Sequence Alignment and Motif Discovery,"Ian En-Hsu Yen,         Xin Lin,         Jiong Zhang,         Pradeep Ravikumar,         Inderjit Dhillon","Multiple Sequence Alignment and Motif Discovery, known as NP-hard problems, are two fundamental tasks in Bioinformatics. Existing approaches to these two problems are based on either local search methods such as Expectation Maximization (EM), Gibbs Sampling or greedy heuristic methods. In this work, we develop a convex relaxation approach to both problems based on the recent concept of atomic norm and develop a new algorithm, termed Greedy Direction Method of Multiplier, for solving the convex relaxation with two convex atomic constraints. Experiments show that our convex relaxation approach produces solutions of higher quality than those standard tools widely-used in Bioinformatics community on the Multiple Sequence Alignment and Motif Discovery problems.",http://proceedings.mlr.press/v48/yena16.html,http://proceedings.mlr.press/v48/yena16.pdf,ICML
1853,2016,"Shifting Regret, Mirror Descent, and Matrices","Andras Gyorgy,         Csaba Szepesvari","We consider the problem of online prediction in changing environments. In this framework the performance of a predictor is evaluated as the loss relative to an arbitrarily changing predictor, whose individual components come from a base class of predictors. Typical results in the literature consider different base classes (experts, linear predictors on the simplex, etc.) separately. Introducing an arbitrary mapping inside the mirror decent algorithm, we provide a framework that unifies and extends existing results. As an example, we prove new shifting regret bounds for matrix prediction problems.",http://proceedings.mlr.press/v48/gyorgy16.html,http://proceedings.mlr.press/v48/gyorgy16.pdf,ICML
1854,2016,Generative Adversarial Text to Image Synthesis,"Scott Reed,         Zeynep Akata,         Xinchen Yan,         Lajanugen Logeswaran,         Bernt Schiele,         Honglak Lee","Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories such as faces, album covers, room interiors and flowers. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.",http://proceedings.mlr.press/v48/reed16.html,http://proceedings.mlr.press/v48/reed16.pdf,ICML
1855,2016,Bidirectional Helmholtz Machines,"Jorg Bornschein,         Samira Shabanian,         Asja Fischer,         Yoshua Bengio","Efficient unsupervised training and inference in deep generative models remains a challenging problem. One basic approach, called Helmholtz machine or Variational Autoencoder, involves training a top-down directed generative model together with a bottom-up auxiliary model used for approximate inference. Recent results indicate that better generative models can be obtained with better approximate inference procedures. Instead of improving the inference procedure, we here propose a new model, the bidirectional Helmholtz machine, which guarantees that the top-down and bottom-up distributions can efficiently invert each other. We achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the model distribution to be the geometric mean of these two. We present a lower-bound for the likelihood of this model and we show that optimizing this bound regularizes the model so that the Bhattacharyya distance between the bottom-up and top-down approximate distributions is minimized. This approach results in state of the art generative models which prefer significantly deeper architectures while it allows for orders of magnitude more efficient likelihood estimation.",http://proceedings.mlr.press/v48/bornschein16.html,http://proceedings.mlr.press/v48/bornschein16.pdf,ICML
1856,2016,Online Stochastic Linear Optimization under One-bit Feedback,"Lijun Zhang,         Tianbao Yang,         Rong Jin,         Yichi Xiao,         Zhi-hua Zhou","In this paper, we study a special bandit setting of online stochastic linear optimization, where only one-bit of information is revealed to the learner at each round. This problem has found many applications including online advertisement and online recommendation. We assume the binary feedback is a random variable generated from the logit model, and aim to minimize the regret defined by the unknown linear function. Although the existing method for generalized linear bandit can be applied to our problem, the high computational cost makes it impractical for real-world applications. To address this challenge, we develop an efficient online learning algorithm by exploiting particular structures of the observation model. Specifically, we adopt online Newton step to estimate the unknown parameter and derive a tight confidence region based on the exponential concavity of the logistic loss. Our analysis shows that the proposed algorithm achieves a regret bound of O(d\sqrtT), which matches the optimal result of stochastic linear bandits.",http://proceedings.mlr.press/v48/zhangb16.html,http://proceedings.mlr.press/v48/zhangb16.pdf,ICML
1857,2016,Automatic Construction of Nonparametric Relational Regression Models for Multiple Time Series,"Yunseong Hwang,         Anh Tong,         Jaesik Choi","Gaussian Processes (GPs) provide a general and analytically tractable way of modeling complex time-varying, nonparametric functions. The Automatic Bayesian Covariance Discovery (ABCD) system constructs natural-language description of time-series data by treating unknown time-series data nonparametrically using GP with a composite covariance kernel function. Unfortunately, learning a composite covariance kernel with a single time-series data set often results in less informative kernel that may not give qualitative, distinctive descriptions of data. We address this challenge by proposing two relational kernel learning methods which can model multiple time-series data sets by finding common, shared causes of changes. We show that the relational kernel learning methods find more accurate models for regression problems on several real-world data sets; US stock data, US house price index data and currency exchange rate data.",http://proceedings.mlr.press/v48/hwangb16.html,http://proceedings.mlr.press/v48/hwangb16.pdf,ICML
1858,2016,Accurate Robust and Efficient Error Estimation for Decision Trees,Lixin Fan,"This paper illustrates a novel approach to the estimation of generalization error of decision tree classifiers. We set out the study of decision tree errors in the context of consistency analysis theory, which proved that the Bayes error can be achieved only if when the number of data samples thrown into each leaf node goes to infinity. For the more challenging and practical case where the sample size is finite or small, a novel sampling error term is introduced in this paper to cope with the small sample problem effectively and efficiently. Extensive experimental results show that the proposed error estimate is superior to the well known K-fold cross validation methods in terms of robustness and accuracy. Moreover it is orders of magnitudes more efficient than cross validation methods.",http://proceedings.mlr.press/v48/fan16.html,http://proceedings.mlr.press/v48/fan16.pdf,ICML
1859,2016,Early and Reliable Event Detection Using Proximity Space Representation,"Maxime Sangnier,         Jerome Gauthier,         Alain Rakotomamonjy","Let us consider a specific action or situation (called event) that takes place within a time series. The objective in early detection is to build a decision function that is able to go off as soon as possible from the onset of an occurrence of this event. This implies making a decision with an incomplete information. This paper proposes a novel framework that i) guarantees that a detection made with a partial observation will also occur at full observation of the time-series; ii) incorporates in a consistent manner the lack of knowledge about the minimal amount of information needed to make a decision. The proposed detector is based on mapping the temporal sequences to a landmarking space thanks to appropriately designed similarity functions. As a by-product, the framework benefits from a scalable training algorithm and a theoretical guarantee concerning its generalization ability. We also discuss an important improvement of our framework in which decision function can still be made reliable while being more expressive. Our experimental studies provide compelling results on toy data, presenting the trade-off that occurs when aiming at accuracy, earliness and reliability. Results on real physiological and video datasets show that our proposed approach is as accurate and early as state-of-the-art algorithm, while ensuring reliability and being far more efficient to learn.",http://proceedings.mlr.press/v48/sangnier16.html,http://proceedings.mlr.press/v48/sangnier16.pdf,ICML
1860,2016,Beyond CCA: Moment Matching for Multi-View Models,"Anastasia Podosinnikova,         Francis Bach,         Simon Lacoste-Julien","We introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees. We consider moment matching techniques for estimation in these models. For that, by drawing explicit links between the new models and a discrete version of independent component analysis (DICA), we first extend the DICA cumulant tensors to the new discrete version of CCA. By further using a close connection with independent component analysis, we introduce generalized covariance matrices, which can replace the cumulant tensors in the moment matching framework, and, therefore, improve sample complexity and simplify derivations and algorithms significantly. As the tensor power method or orthogonal joint diagonalization are not applicable in the new setting, we use non-orthogonal joint diagonalization techniques for matching the cumulants. We demonstrate performance of the proposed models and estimation techniques on experiments with both synthetic and real datasets.",http://proceedings.mlr.press/v48/podosinnikova16.html,http://proceedings.mlr.press/v48/podosinnikova16.pdf,ICML
1861,2016,Energetic Natural Gradient Descent,"Philip Thomas,         Bruno Castro Silva,         Christoph Dann,         Emma Brunskill","We propose a new class of algorithms for minimizing or maximizing functions of parametric probabilistic models. These new algorithms are natural gradient algorithms that leverage more information than prior methods by using a new metric tensor in place of the commonly used Fisher information matrix. This new metric tensor is derived by computing directions of steepest ascent where the distance between distributions is measured using an approximation of energy distance (as opposed to Kullback-Leibler divergence, which produces the Fisher information matrix), and so we refer to our new ascent direction as the energetic natural gradient.",http://proceedings.mlr.press/v48/thomasb16.html,http://proceedings.mlr.press/v48/thomasb16.pdf,ICML
1862,2016,Dirichlet Process Mixture Model for Correcting Technical Variation in Single-Cell Gene Expression Data,"Sandhya Prabhakaran,         Elham Azizi,         Ambrose Carr,         Dana Pe’er","We introduce an iterative normalization and clustering method for single-cell gene expression data. The emerging technology of single-cell RNA-seq gives access to gene expression measurements for thousands of cells, allowing discovery and characterization of cell types. However, the data is confounded by technical variation emanating from experimental errors and cell type-specific biases. Current approaches perform a global normalization prior to analyzing biological signals, which does not resolve missing data or variation dependent on latent cell types. Our model is formulated as a hierarchical Bayesian mixture model with cell-specific scalings that aid the iterative normalization and clustering of cells, teasing apart technical variation from biological signals. We demonstrate that this approach is superior to global normalization followed by clustering. We show identifiability and weak convergence guarantees of our method and present a scalable Gibbs inference algorithm. This method improves cluster inference in both synthetic and real single-cell data compared with previous methods, and allows easy interpretation and recovery of the underlying structure and cell types.",http://proceedings.mlr.press/v48/prabhakaran16.html,http://proceedings.mlr.press/v48/prabhakaran16.pdf,ICML
1863,2016,Learning Simple Algorithms from Examples,"Wojciech Zaremba,         Tomas Mikolov,         Armand Joulin,         Rob Fergus","We present an approach for learning simple algorithms such as copying, multi-digit addition and single digit multiplication directly from examples. Our framework consists of a set of interfaces, accessed by a controller. Typical interfaces are 1-D tapes or 2-D grids that hold the input and output data. For the controller, we explore a range of neural network-based models which vary in their ability to abstract the underlying algorithm from training instances and generalize to test examples with many thousands of digits. The controller is trained using Q-learning with several enhancements and we show that the bottleneck is in the capabilities of the controller rather than in the search incurred by Q-learning.",http://proceedings.mlr.press/v48/zaremba16.html,http://proceedings.mlr.press/v48/zaremba16.pdf,ICML
1864,2016,Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin,"Dario Amodei,         Sundaram Ananthanarayanan,         Rishita Anubhai,         Jingliang Bai,         Eric Battenberg,         Carl Case,         Jared Casper,         Bryan Catanzaro,         Qiang Cheng,         Guoliang Chen,         Jie Chen,         Jingdong Chen,         Zhijie Chen,         Mike Chrzanowski,         Adam Coates,         Greg Diamos,         Ke Ding,         Niandong Du,         Erich Elsen,         Jesse Engel,         Weiwei Fang,         Linxi Fan,         Christopher Fougner,         Liang Gao,         Caixia Gong,         Awni Hannun,         Tony Han,         Lappi Johannes,         Bing Jiang,         Cai Ju,         Billy Jun,         Patrick LeGresley,         Libby Lin,         Junjie Liu,         Yang Liu,         Weigao Li,         Xiangang Li,         Dongpeng Ma,         Sharan Narang,         Andrew Ng,         Sherjil Ozair,         Yiping Peng,         Ryan Prenger,         Sheng Qian,         Zongfeng Quan,         Jonathan Raiman,         Vinay Rao,         Sanjeev Satheesh,         David Seetapun,         Shubho Sengupta,         Kavya Srinet,         Anuroop Sriram,         Haiyuan Tang,         Liliang Tang,         Chong Wang,         Jidong Wang,         Kaifu Wang,         Yi Wang,         Zhijian Wang,         Zhiqian Wang,         Shuang Wu,         Likai Wei,         Bo Xiao,         Wen Xie,         Yan Xie,         Dani Yogatama,         Bin Yuan,         Jun Zhan,         Zhenyao Zhu","We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech–two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",http://proceedings.mlr.press/v48/amodei16.html,http://proceedings.mlr.press/v48/amodei16.pdf,ICML
1865,2016,Faster Eigenvector Computation via Shift-and-Invert Preconditioning,"Dan Garber,         Elad Hazan,         Chi Jin,          Sham,         Cameron Musco,         Praneeth Netrapalli,         Aaron Sidford","We give faster algorithms and improved sample complexities for the fundamental problem of estimating the top eigenvector. Given an explicit matrix A∈Rn×dA∈Rn×dA \in \mathbb{R}^{n \times d}, we show how to compute an ϵϵ\epsilon-approximate top eigenvector of ATAATAA^TA in time O~([nnz(A)+dsr(A)gap2]⋅log1/ϵ)O~([nnz(A)+dsr(A)gap2]⋅log⁡1/ϵ)\tilde O\left( \left[\text{nnz}(A) + \frac{d \text{sr}(A)}{\text{gap}^2} \right] \cdot \log 1/\epsilon\right). Here nnz(A)nnz(A)\text{nnz}(A) is the number of nonzeros in AAA, sr(A)sr(A)\text{sr}(A) is the stable rank, and gap is the relative eigengap. We also consider an online setting in which, given a stream of i.i.d. samples from a distribution D with covariance matrix ΣΣ\Sigma and a vector x0x0x_0 which is an O(gap)O(gap)O(\text{gap}) approximate top eigenvector for ΣΣ\Sigma, we show how to refine x0x0x_0 to an ϵϵ\epsilon approximation using  O(var(D)gap−ϵ)O(var(D)gap−ϵ)O \left( \frac{\text{var}(\mathcal{D})}{\text{gap}-\epsilon}\right) samples from DD\mathcal{D}. Here var(D)var(D)\text{var}(\mathcal{D}) is a natural notion of variance. Combining our algorithm with previous work to initialize x0x0x_0, we obtain improved sample complexities and runtimes under a variety of assumptions on D. We achieve our results via a robust analysis of the classic shift-and-invert preconditioning method. This technique lets us reduce eigenvector computation to approximately solving a series of linear systems with fast stochastic gradient methods.",http://proceedings.mlr.press/v48/garber16.html,http://proceedings.mlr.press/v48/garber16.pdf,ICML
1866,2016,"The Segmented iHMM: A Simple, Efficient Hierarchical Infinite HMM","Ardavan Saeedi,         Matthew Hoffman,         Matthew Johnson,         Ryan Adams","We propose the segmented iHMM (siHMM), a hierarchical infinite hidden Markov model (iHMM) that supports a simple, efficient inference scheme. The siHMM is well suited to segmentation problems, where the goal is to identify points at which a time series transitions from one relatively stable regime to a new regime. Conventional iHMMs often struggle with such problems, since they have no mechanism for distinguishing between high-and low-level dynamics. Hierarchical HMMs (HHMMs) can do better, but they require much more complex and expensive inference algorithms. The siHMM retains the simplicity and efficiency of the iHMM, but outperforms it on a variety of segmentation problems, achieving performance that matches or exceeds that of a more complicated HHMM.",http://proceedings.mlr.press/v48/saeedi16.html,http://proceedings.mlr.press/v48/saeedi16.pdf,ICML
1867,2016,Robust Monte Carlo Sampling using Riemannian Nosé-Poincaré Hamiltonian Dynamics,"Anirban Roychowdhury,         Brian Kulis,         Srinivasan Parthasarathy","We present a Monte Carlo sampler using a modified Nosé-Poincaré Hamiltonian along with Riemannian preconditioning. Hamiltonian Monte Carlo samplers allow better exploration of the state space as opposed to random walk-based methods, but, from a molecular dynamics perspective, may not necessarily provide samples from the canonical ensemble. Nosé-Hoover samplers rectify that shortcoming, but the resultant dynamics are not Hamiltonian. Furthermore, usage of these algorithms on large real-life datasets necessitates the use of stochastic gradients, which acts as another potentially destabilizing source of noise. In this work, we propose dynamics based on a modified Nosé-Poincaré Hamiltonian augmented with Riemannian manifold corrections. The resultant symplectic sampling algorithm samples from the canonical ensemble while using structural cues from the Riemannian preconditioning matrices to efficiently traverse the parameter space. We also propose a stochastic variant using additional terms in the Hamiltonian to correct for the noise from the stochastic gradients. We show strong performance of our algorithms on synthetic datasets and high-dimensional Poisson factor analysis-based topic modeling scenarios.",http://proceedings.mlr.press/v48/roychowdhury16.html,http://proceedings.mlr.press/v48/roychowdhury16.pdf,ICML
1868,2016,Interactive Bayesian Hierarchical Clustering,"Sharad Vikram,         Sanjoy Dasgupta","Clustering is a powerful tool in data analysis, but it is often difficult to find a grouping that aligns with a user’s needs. To address this, several methods incorporate constraints obtained from users into clustering algorithms, but unfortunately do not apply to hierarchical clustering. We design an interactive Bayesian algorithm that incorporates user interaction into hierarchical clustering while still utilizing the geometry of the data by sampling a constrained posterior distribution over hierarchies. We also suggest several ways to intelligently query a user. The algorithm, along with the querying schemes, shows promising results on real data.",http://proceedings.mlr.press/v48/vikram16.html,http://proceedings.mlr.press/v48/vikram16.pdf,ICML
1869,2016,Dealbreaker: A Nonlinear Latent Variable Model for Educational Data,"Andrew Lan,         Tom Goldstein,         Richard Baraniuk,         Christoph Studer","Statistical models of student responses on assessment questions, such as those in homeworks and exams, enable educators and computer-based personalized learning systems to gain insights into students’ knowledge using machine learning. Popular student-response models, including the Rasch model and item response theory models, represent the probability of a student answering a question correctly using an affine function of latent factors. While such models can accurately predict student responses, their ability to interpret the underlying knowledge structure (which is certainly nonlinear) is limited. In response, we develop a new, nonlinear latent variable model that we call the dealbreaker model, in which a student’s success probability is determined by their weakest concept mastery. We develop efficient parameter inference algorithms for this model using novel methods for nonconvex optimization. We show that the dealbreaker model achieves comparable or better prediction performance as compared to affine models with real-world educational datasets. We further demonstrate that the parameters learned by the dealbreaker model are interpretable—they provide key insights into which concepts are critical (i.e., the “dealbreaker”) to answering a question correctly. We conclude by reporting preliminary results for a movie-rating dataset, which illustrate the broader applicability of the dealbreaker model.",http://proceedings.mlr.press/v48/lan16.html,http://proceedings.mlr.press/v48/lan16.pdf,ICML
1870,2016,The Information Sieve,"Greg Ver Steeg,         Aram Galstyan","We introduce a new framework for unsupervised learning of representations based on a novel hierarchical decomposition of information. Intuitively, data is passed through a series of progressively fine-grained sieves. Each layer of the sieve recovers a single latent factor that is maximally informative about multivariate dependence in the data. The data is transformed after each pass so that the remaining unexplained information trickles down to the next layer. Ultimately, we are left with a set of latent factors explaining all the dependence in the original data and remainder information consisting of independent noise. We present a practical implementation of this framework for discrete variables and apply it to a variety of fundamental tasks in unsupervised learning including independent component analysis, lossy and lossless compression, and predicting missing values in data.",http://proceedings.mlr.press/v48/steeg16.html,http://proceedings.mlr.press/v48/steeg16.pdf,ICML
1871,2016,Fast DPP Sampling for Nystrom with Application to Kernel Methods,"Chengtao Li,         Stefanie Jegelka,         Suvrit Sra","The Nystrom method has long been popular for scaling up kernel methods. Its theoretical guarantees and empirical performance rely critically on the quality of the landmarks selected. We study landmark selection for Nystrom using Determinantal Point Processes (DPPs), discrete probability models that allow tractable generation of diverse samples. We prove that landmarks selected via DPPs guarantee bounds on approximation errors; subsequently, we analyze implications for kernel ridge regression. Contrary to prior reservations due to cubic complexity of DPP sampling, we show that (under certain conditions) Markov chain DPP sampling requires only linear time in the size of the data. We present several empirical results that support our theoretical analysis, and demonstrate the superior performance of DPP-based landmark selection compared with existing approaches.",http://proceedings.mlr.press/v48/lih16.html,http://proceedings.mlr.press/v48/lih16.pdf,ICML
1872,2016,An optimal algorithm for the Thresholding Bandit Problem,"Andrea Locatelli,         Maurilio Gutzeit,         Alexandra Carpentier","We study a specific combinatorial pure exploration stochastic bandit problem where the learner aims at finding the set of arms whose means are above a given threshold, up to a given precision, and for a fixed time horizon. We propose a parameter-free algorithm based on an original heuristic, and prove that it is optimal for this problem by deriving matching upper and lower bounds. To the best of our knowledge, this is the first non-trivial pure exploration setting with fixed budget for which provably optimal strategies are constructed.",http://proceedings.mlr.press/v48/locatelli16.html,http://proceedings.mlr.press/v48/locatelli16.pdf,ICML
1873,2016,Train and Test Tightness of LP Relaxations in Structured Prediction,"Ofer Meshi,         Mehrdad Mahdavi,         Adrian Weller,         David Sontag","Structured prediction is used in areas such as computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation to the striking observation that approximations based on linear programming (LP) relaxations are often tight on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that tightness generalizes from train to test data.",http://proceedings.mlr.press/v48/meshi16.html,http://proceedings.mlr.press/v48/meshi16.pdf,ICML
1874,2016,"Loss factorization, weakly supervised learning and label noise robustness","Giorgio Patrini,         Frank Nielsen,         Richard Nock,         Marcello Carioni","We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the same loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator — the focal quantity of this work — which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation. Factorization has a direct application on weakly supervised learning. In particular, we demonstrate that algorithms like SGD and proximal methods can be adapted with minimal effort to handle weak supervision, once the mean operator has been estimated. We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results.",http://proceedings.mlr.press/v48/patrini16.html,http://proceedings.mlr.press/v48/patrini16.pdf,ICML
1875,2016,Neural Variational Inference for Text Processing,"Yishu Miao,         Lei Yu,         Phil Blunsom","Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.",http://proceedings.mlr.press/v48/miao16.html,http://proceedings.mlr.press/v48/miao16.pdf,ICML
1876,2016,Truthful Univariate Estimators,"Ioannis Caragiannis,         Ariel Procaccia,         Nisarg Shah","We revisit the classic problem of estimating the population mean of an unknown single-dimensional distribution from samples, taking a game-theoretic viewpoint. In our setting, samples are supplied by strategic agents, who wish to pull the estimate as close as possible to their own value. In this setting, the sample mean gives rise to manipulation opportunities, whereas the sample median does not. Our key question is whether the sample median is the best (in terms of mean squared error) truthful estimator of the population mean. We show that when the underlying distribution is symmetric, there are truthful estimators that dominate the median. Our main result is a characterization of worst-case optimal truthful estimators, which provably outperform the median, for possibly asymmetric distributions with bounded support.",http://proceedings.mlr.press/v48/caragiannis16.html,http://proceedings.mlr.press/v48/caragiannis16.pdf,ICML
1877,2016,Learning Granger Causality for Hawkes Processes,"Hongteng Xu,         Mehrdad Farajtabar,         Hongyuan Zha","Learning Granger causality for general point processes is a very challenging task. We propose an effective method learning Granger causality for a special but significant type of point processes — Hawkes processes. Focusing on Hawkes processes, we reveal the relationship between Hawkes process’s impact functions and its Granger causality graph. Specifically, our model represents impact functions using a series of basis functions and recovers the Granger causality graph via group sparsity of the impact functions’ coefficients. We propose an effective learning algorithm combining a maximum likelihood estimator (MLE) with a sparse-group-lasso (SGL) regularizer. Additionally, the pairwise similarity between the dimensions of the process is considered when their clustering structure is available. We analyze our learning method and discuss the selection of the basis functions. Experiments on synthetic data and real-world data show that our method can learn the Granger causality graph and the triggering patterns of Hawkes processes simultaneously.",http://proceedings.mlr.press/v48/xuc16.html,http://proceedings.mlr.press/v48/xuc16.pdf,ICML
1878,2016,Solving Ridge Regression using Sketched Preconditioned SVRG,"Alon Gonen,         Francesco Orabona,         Shai Shalev-Shwartz","We develop a novel preconditioning method for ridge regression, based on recent linear sketching methods. By equipping Stochastic Variance Reduced Gradient (SVRG) with this preconditioning process, we obtain a significant speed-up relative to fast stochastic methods such as SVRG, SDCA and SAG.",http://proceedings.mlr.press/v48/gonen16.html,http://proceedings.mlr.press/v48/gonen16.pdf,ICML
1879,2016,Efficient Private Empirical Risk Minimization for High-dimensional Learning,"Shiva Prasad Kasiviswanathan,         Hongxia Jin","Dimensionality reduction is a popular approach for dealing with high dimensional data that leads to substantial computational savings. Random projections are a simple and effective method for universal dimensionality reduction with rigorous theoretical guarantees. In this paper, we theoretically study the problem of differentially private empirical risk minimization in the projected subspace (compressed domain). We ask: is it possible to design differentially private algorithms with small excess risk given access to only projected data? In this paper, we answer this question in affirmative, by showing that for the class of generalized linear functions, given only the projected data and the projection matrix, we can obtain excess risk bounds of O(w(Theta)2/3/n1/3)undereps−differentialprivacy,andO((w(Theta)/n)1/2)O(w(Theta)2/3/n1/3)undereps−differentialprivacy,andO((w(Theta)/n)1/2)O(w(Theta)^2/3/n^1/3) under eps-differential privacy, and O((w(Theta)/n)^1/2) under (eps,delta)-differential privacy, where n is the sample size and w(Theta) is the Gaussian width of the parameter space that we optimize over. A simple consequence of these results is that, for a large class of ERM problems, in the traditional setting (i.e., with access to the original data), under eps-differential privacy, we improve the worst-case risk bounds of Bassily et al. (FOCS 2014).",http://proceedings.mlr.press/v48/kasiviswanathan16.html,http://proceedings.mlr.press/v48/kasiviswanathan16.pdf,ICML
1880,2016,PD-Sparse : A Primal and Dual Sparse Approach to Extreme Multiclass and Multilabel Classification,"Ian En-Hsu Yen,         Xiangru Huang,         Pradeep Ravikumar,         Kai Zhong,         Inderjit Dhillon","We consider Multiclass and Multilabel classification with extremely large number of classes, of which only few are labeled to each instance. In such setting, standard methods that have training, prediction cost linear to the number of classes become intractable. State-of-the-art methods thus aim to reduce the complexity by exploiting correlation between labels under assumption that the similarity between labels can be captured by structures such as low-rank matrix or balanced tree. However, as the diversity of labels increases in the feature space, structural assumption can be easily violated, which leads to degrade in the testing performance. In this work, we show that a margin-maximizing loss with l1 penalty, in case of Extreme Classification, yields extremely sparse solution both in primal and in dual without sacrificing the expressive power of predictor. We thus propose a Fully-Corrective Block-Coordinate Frank-Wolfe (FC-BCFW) algorithm that exploits both primal and dual sparsity to achieve a complexity sublinear to the number of primal and dual variables. A bi-stochastic search method is proposed to further improve the efficiency. In our experiments on both Multiclass and Multilabel problems, the proposed method achieves significant higher accuracy than existing approaches of Extreme Classification with very competitive training and prediction time.",http://proceedings.mlr.press/v48/yenb16.html,http://proceedings.mlr.press/v48/yenb16.pdf,ICML
1881,2016,Deconstructing the Ladder Network Architecture,"Mohammad Pezeshki,         Linxi Fan,         Philemon Brakel,         Aaron Courville,         Yoshua Bengio","The Ladder Network is a recent new approach to semi-supervised learning that turned out to be very successful. While showing impressive performance, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. This paper presents an extensive experimental investigation of variants of the Ladder Network in which we replaced or removed individual components to learn about their relative importance. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connections, followed by the application of noise, and the choice of what we refer to as the ‘combinator function’. As the number of labeled training examples increases, the lateral connections and the reconstruction criterion become less important, with most of the generalization improvement coming from the injection of noise in each layer. Finally, we introduce a combinator function that reduces test error rates on Permutation-Invariant MNIST to 0.57% for the supervised setting, and to 0.97% and 1.0% for semi-supervised settings with 1000 and 100 labeled examples, respectively.",http://proceedings.mlr.press/v48/pezeshki16.html,http://proceedings.mlr.press/v48/pezeshki16.pdf,ICML
1882,2016,Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms,"Mathieu Blondel,         Masakazu Ishihata,         Akinori Fujino,         Naonori Ueda","Polynomial networks and factorization machines are two recently-proposed models that can efficiently use feature interactions in classification and regression tasks. In this paper, we revisit both models from a unified perspective. Based on this new view, we study the properties of both models and propose new efficient training algorithms. Key to our approach is to cast parameter learning as a low-rank symmetric tensor estimation problem, which we solve by multi-convex optimization. We demonstrate our approach on regression and recommender system tasks.",http://proceedings.mlr.press/v48/blondel16.html,http://proceedings.mlr.press/v48/blondel16.pdf,ICML
1883,2016,Fast Constrained Submodular Maximization: Personalized Data Summarization,"Baharan Mirzasoleiman,         Ashwinkumar Badanidiyuru,         Amin Karbasi","Can we summarize multi-category data based on user preferences in a scalable manner? Many utility functions used for data summarization satisfy submodularity, a natural diminishing returns property. We cast personalized data summarization as an instance of a general submodular maximization problem subject to multiple constraints. We develop the first practical and FAst coNsTrained submOdular Maximization algorithm, FANTOM, with strong theoretical guarantees. FANTOM maximizes a submodular function (not necessarily monotone) subject to intersection of a p-system and l knapsacks constrains. It achieves a (1 + ε)(p + 1)(2p + 2l + 1)/p approximation guarantee with only O(nrp log(n)/ε) query complexity (n and r indicate the size of the ground set and the size of the largest feasible solution, respectively). We then show how we can use FANTOM for personalized data summarization. In particular, a p-system can model different aspects of data, such as categories or time stamps, from which the users choose. In addition, knapsacks encode users’ constraints including budget or time. In our set of experiments, we consider several concrete applications: movie recommendation over 11K movies, personalized image summarization with 10K images, and revenue maximization on the YouTube social networks with 5000 communities. We observe that FANTOM constantly provides the highest utility against all the baselines.",http://proceedings.mlr.press/v48/mirzasoleiman16.html,http://proceedings.mlr.press/v48/mirzasoleiman16.pdf,ICML
1884,2016,A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization,Frank Curtis,"An algorithm for stochastic (convex or nonconvex) optimization is presented. The algorithm is variable-metric in the sense that, in each iteration, the step is computed through the product of a symmetric positive definite scaling matrix and a stochastic (mini-batch) gradient of the objective function, where the sequence of scaling matrices is updated dynamically by the algorithm. A key feature of the algorithm is that it does not overly restrict the manner in which the scaling matrices are updated. Rather, the algorithm exploits fundamental self-correcting properties of BFGS-type updating—properties that have been over-looked in other attempts to devise quasi-Newton methods for stochastic optimization. Numerical experiments illustrate that the method and a limited memory variant of it are stable and outperform (mini-batch) stochastic gradient and other quasi-Newton methods when employed to solve a few machine learning problems.",http://proceedings.mlr.press/v48/curtis16.html,http://proceedings.mlr.press/v48/curtis16.pdf,ICML
1885,2016,Efficient Multi-Instance Learning for Activity Recognition from Time Series Data Using an Auto-Regressive Hidden Markov Model,"Xinze Guan,         Raviv Raich,         Weng-Keen Wong","Activity recognition from sensor data has spurred a great deal of interest due to its impact on health care. Prior work on activity recognition from multivariate time series data has mainly applied supervised learning techniques which require a high degree of annotation effort to produce training data with the start and end times of each activity. In order to reduce the annotation effort, we present a weakly supervised approach based on multi-instance learning. We introduce a generative graphical model for multi-instance learning on time series data based on an auto-regressive hidden Markov model. Our model has a number of advantages, including the ability to produce both bag and instance-level predictions as well as an efficient exact inference algorithm based on dynamic programming.",http://proceedings.mlr.press/v48/guan16.html,http://proceedings.mlr.press/v48/guan16.pdf,ICML
1886,2016,Nonparametric Canonical Correlation Analysis,"Tomer Michaeli,         Weiran Wang,         Karen Livescu","Canonical correlation analysis (CCA) is a classical representation learning technique for finding correlated variables in multi-view data. Several nonlinear extensions of the original linear CCA have been proposed, including kernel and deep neural network methods. These approaches seek maximally correlated projections among families of functions, which the user specifies (by choosing a kernel or neural network structure), and are computationally demanding. Interestingly, the theory of nonlinear CCA, without functional restrictions, had been studied in the population setting by Lancaster already in the 1950s, but these results have not inspired practical algorithms. We revisit Lancaster’s theory to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution can be expressed in terms of the singular value decomposition of a certain operator associated with the joint density of the views. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without requiring the inversion of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and perform better than kernel CCA and comparable to deep CCA.",http://proceedings.mlr.press/v48/michaeli16.html,http://proceedings.mlr.press/v48/michaeli16.pdf,ICML
1887,2016,The Variational Nystrom method for large-scale spectral problems,"Max Vladymyrov,         Miguel Carreira-Perpinan","Spectral methods for dimensionality reduction and clustering require solving an eigenproblem defined by a sparse affinity matrix. When this matrix is large, one seeks an approximate solution. The standard way to do this is the Nystrom method, which first solves a small eigenproblem considering only a subset of landmark points, and then applies an out-of-sample formula to extrapolate the solution to the entire dataset. We show that by constraining the original problem to satisfy the Nystrom formula, we obtain an approximation that is computationally simple and efficient, but achieves a lower approximation error using fewer landmarks and less runtime. We also study the role of normalization in the computational cost and quality of the resulting solution.",http://proceedings.mlr.press/v48/vladymyrov16.html,http://proceedings.mlr.press/v48/vladymyrov16.pdf,ICML
1888,2016,Opponent Modeling in Deep Reinforcement Learning,"He He,         Jordan Boyd-Graber,         Kevin Kwok,         Hal Daumé III","Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because of strategies’ complex interaction and the non-stationary nature. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent’s action, we encode observation of the opponents into a deep Q-Network (DQN), while retaining explicit modeling under multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents even without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.",http://proceedings.mlr.press/v48/he16.html,http://proceedings.mlr.press/v48/he16.pdf,ICML
1889,2016,Stochastically Transitive Models for Pairwise Comparisons: Statistical and Computational Issues,"Nihar Shah,         Sivaraman Balakrishnan,         Aditya Guntuboyina,         Martin Wainwright","There are various parametric models for analyzing pairwise comparison data, including the Bradley-Terry-Luce (BTL) and Thurstone models, but their reliance on strong parametric assumptions is limiting. In this work, we study a flexible model for pairwise comparisons, under which the probabilities of outcomes are required only to satisfy a natural form of stochastic transitivity. This class includes parametric models including the BTL and Thurstone models as special cases, but is considerably more general. We provide various examples of models in this broader stochastically transitive class for which classical parametric models provide poor fits. Despite this greater flexibility, we show that the matrix of probabilities can be estimated at the same rate as in standard parametric models. On the other hand, unlike in the BTL and Thurstone models, computing the minimax-optimal estimator in the stochastically transitive model is non-trivial, and we explore various computationally tractable alternatives. We show that a simple singular value thresholding algorithm is statistically consistent but does not achieve the minimax rate. We then propose and study algorithms that achieve the minimax rate over interesting sub-classes of the full stochastically transitive class. We complement our theoretical results with thorough numerical simulations.",http://proceedings.mlr.press/v48/shahb16.html,http://proceedings.mlr.press/v48/shahb16.pdf,ICML
1890,2016,Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,"Yarin Gal,         Zoubin Ghahramani","Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.",http://proceedings.mlr.press/v48/gal16.html,http://proceedings.mlr.press/v48/gal16.pdf,ICML
1891,2016,k-variates++: more pluses in the k-means++,"Richard Nock,         Raphael Canyasse,         Roksana Boreli,         Frank Nielsen","k-means++ seeding has become a de facto standard for hard clustering algorithms. In this paper, our first contribution is a two-way generalisation of this seeding, k-variates++, that includes the sampling of general densities rather than just a discrete set of Dirac densities anchored at the point locations, *and* a generalisation of the well known Arthur-Vassilvitskii (AV) approximation guarantee, in the form of a *bias+variance* approximation bound of the *global* optimum. This approximation exhibits a reduced dependency on the ""noise"" component with respect to the optimal potential — actually approaching the statistical lower bound. We show that k-variates++ *reduces* to efficient (biased seeding) clustering algorithms tailored to specific frameworks; these include distributed, streaming and on-line clustering, with *direct* approximation results for these algorithms. Finally, we present a novel application of k-variates++ to differential privacy. For either the specific frameworks considered here, or for the differential privacy setting, there is little to no prior results on the direct application of k-means++ and its approximation bounds — state of the art contenders appear to be significantly more complex and / or display less favorable (approximation) properties. We stress that our algorithms can still be run in cases where there is *no* closed form solution for the population minimizer. We demonstrate the applicability of our analysis via experimental evaluation on several domains and settings, displaying competitive performances vs state of the art.",http://proceedings.mlr.press/v48/nock16.html,http://proceedings.mlr.press/v48/nock16.pdf,ICML
1892,2016,Epigraph projections for fast general convex programming,"Po-Wei Wang,         Matt Wytock,         Zico Kolter","This paper develops an approach for efficiently solving general convex optimization problems specified as disciplined convex programs (DCP), a common general-purpose modeling framework. Specifically we develop an algorithm based upon fast epigraph projections, projections onto the epigraph of a convex function, an approach closely linked to proximal operator methods. We show that by using these operators, we can solve any disciplined convex program without transforming the problem to a standard cone form, as is done by current DCP libraries. We then develop a large library of efficient epigraph projection operators, mirroring and extending work on fast proximal algorithms, for many common convex functions. Finally, we evaluate the performance of the algorithm, and show it often achieves order of magnitude speedups over existing general-purpose optimization solvers.",http://proceedings.mlr.press/v48/wangh16.html,http://proceedings.mlr.press/v48/wangh16.pdf,ICML
1893,2016,Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient,"Tianbao Yang,         Lijun Zhang,         Rong Jin,         Jinfeng Yi","This work focuses on dynamic regret of online convex optimization that compares the performance of online learning to a clairvoyant who knows the sequence of loss functions in advance and hence selects the minimizer of the loss function at each step. By assuming that the clairvoyant moves slowly (i.e., the minimizers change slowly), we present several improved variation-based upper bounds of the dynamic regret under the true and noisy gradient feedback, which are \it optimal in light of the presented lower bounds. The key to our analysis is to explore a regularity metric that measures the temporal changes in the clairvoyant’s minimizers, to which we refer as path variation. Firstly, we present a general lower bound in terms of the path variation, and then show that under full information or gradient feedback we are able to achieve an optimal dynamic regret. Secondly, we present a lower bound with noisy gradient feedback and then show that we can achieve optimal dynamic regrets under a stochastic gradient feedback and two-point bandit feedback. Moreover, for a sequence of smooth loss functions that admit a small variation in the gradients, our dynamic regret under the two-point bandit feedback matches that is achieved with full information.",http://proceedings.mlr.press/v48/yangb16.html,http://proceedings.mlr.press/v48/yangb16.pdf,ICML
1894,2016,ADIOS: Architectures Deep In Output Space,"Moustapha Cisse,         Maruan Al-Shedivat,         Samy Bengio","Multi-label classification is a generalization of binary classification where the task consists in predicting \emphsets of labels. With the availability of ever larger datasets, the multi-label setting has become a natural one in many applications, and the interest in solving multi-label problems has grown significantly. As expected, deep learning approaches are now yielding state-of-the-art performance for this class of problems. Unfortunately, they usually do not take into account the often unknown but nevertheless rich relationships between labels. In this paper, we propose to make use of this underlying structure by learning to partition the labels into a Markov Blanket Chain and then applying a novel deep architecture that exploits the partition. Experiments on several popular and large multi-label datasets demonstrate that our approach not only yields significant improvements, but also helps to overcome trade-offs specific to the multi-label classification setting.",http://proceedings.mlr.press/v48/cisse16.html,http://proceedings.mlr.press/v48/cisse16.pdf,ICML
1895,2016,Low-rank Solutions of Linear Matrix Equations via Procrustes Flow,"Stephen Tu,         Ross Boczar,         Max Simchowitz,         Mahdi Soltanolkotabi,         Ben Recht","In this paper we study the problem of recovering a low-rank matrix from linear measurements. Our algorithm, which we call Procrustes Flow, starts from an initial estimate obtained by a thresholding scheme followed by gradient descent on a non-convex objective. We show that as long as the measurements obey a standard restricted isometry property, our algorithm converges to the unknown matrix at a geometric rate. In the case of Gaussian measurements, such convergence occurs for a n1 \times n2 matrix of rank r when the number of measurements exceeds a constant times (n1 + n2)r.",http://proceedings.mlr.press/v48/tu16.html,http://proceedings.mlr.press/v48/tu16.pdf,ICML
1896,2016,Stability of Controllers for Gaussian Process Forward Models,"Julia Vinogradska,         Bastian Bischoff,         Duy Nguyen-Tuong,         Anne Romer,         Henner Schmidt,         Jan Peters","Learning control has become an appealing alternative to the derivation of control laws based on classic control theory. However, a major shortcoming of learning control is the lack of performance guarantees which prevents its application in many real-world scenarios. As a step in this direction, we provide a stability analysis tool for controllers acting on dynamics represented by Gaussian processes (GPs). We consider arbitrary Markovian control policies and system dynamics given as (i) the mean of a GP, and (ii) the full GP distribution. For the first case, our tool finds a state space region, where the closed-loop system is provably stable. In the second case, it is well known that infinite horizon stability guarantees cannot exist. Instead, our tool analyzes finite time stability. Empirical evaluations on simulated benchmark problems support our theoretical results.",http://proceedings.mlr.press/v48/vinogradska16.html,http://proceedings.mlr.press/v48/vinogradska16.pdf,ICML
1897,2016,The Arrow of Time in Multivariate Time Series,"Stefan Bauer,         Bernhard Schölkopf,         Jonas Peters","We prove that a time series satisfying a (linear) multivariate autoregressive moving average (VARMA) model satisfies the same model assumption in the reversed time direction, too, if all innovations are normally distributed. This reversibility breaks down if the innovations are non-Gaussian. This means that under the assumption of a VARMA process with non-Gaussian noise, the arrow of time becomes detectable. Our work thereby provides a theoretic justification of an algorithm that has been used for inferring the direction of video snippets. We present a slightly modified practical algorithm that estimates the time direction for a given sample and prove its consistency. We further investigate how the performance of the algorithm depends on sample size, number of dimensions of the time series and the order of the process. An application to real world data from economics shows that considering multivariate processes instead of univariate processes can be beneficial for estimating the time direction. Our result extends earlier work on univariate time series. It relates to the concept of causal inference, where recent methods exploit non-Gaussianity of the error terms for causal structure learning.",http://proceedings.mlr.press/v48/bauer16.html,http://proceedings.mlr.press/v48/bauer16.pdf,ICML
1898,2016,Extreme F-measure Maximization using Sparse Probability Estimates,"Kalina Jasinska,         Krzysztof Dembczynski,         Robert Busa-Fekete,         Karlson Pfannschmidt,         Timo Klerx,         Eyke Hullermeier","We consider the problem of (macro) F-measure maximization in the context of extreme multi-label classification (XMLC), i.e., multi-label classification with extremely large label spaces. We investigate several approaches based on recent results on the maximization of complex performance measures in binary classification. According to these results, the F-measure can be maximized by properly thresholding conditional class probability estimates. We show that a naive adaptation of this approach can be very costly for XMLC and propose to solve the problem by classifiers that efficiently deliver sparse probability estimates (SPEs), that is, probability estimates restricted to the most probable labels. Empirical results provide evidence for the strong practical performance of this approach.",http://proceedings.mlr.press/v48/jasinska16.html,http://proceedings.mlr.press/v48/jasinska16.pdf,ICML
1899,2016,Cross-Graph Learning of Multi-Relational Associations,"Hanxiao Liu,         Yiming Yang","Cross-graph Relational Learning (CGRL) refers to the problem of predicting the strengths or labels of multi-relational tuples of heterogeneous object types, through the joint inference over multiple graphs which specify the internal connections among each type of objects. CGRL is an open challenge in machine learning due to the daunting number of all possible tuples to deal with when the numbers of nodes in multiple graphs are large, and because the labeled training instances are extremely sparse as typical. Existing methods such as tensor factorization or tensor-kernel machines do not work well because of the lack of convex formulation for the optimization of CGRL models, the poor scalability of the algorithms in handling combinatorial numbers of tuples, and/or the non-transductive nature of the learning methods which limits their ability to leverage unlabeled data in training. This paper proposes a novel framework which formulates CGRL as a convex optimization problem, enables transductive learning using both labeled and unlabeled tuples, and offers a scalable algorithm that guarantees the optimal solution and enjoys a constant time complexity with respect to the sizes of input graphs. In our experiments with a subset of DBLP publication records and an Enzyme multi-source dataset, the proposed method successfully scaled to the large cross-graph inference problem, and outperformed other representative approaches significantly.",http://proceedings.mlr.press/v48/liuf16.html,http://proceedings.mlr.press/v48/liuf16.pdf,ICML
1900,2016,Diversity-Promoting Bayesian Learning of Latent Variable Models,"Pengtao Xie,         Jun Zhu,         Eric Xing","In learning latent variable models (LVMs), it is important to effectively capture infrequent patterns and shrink model size without sacrificing modeling power. Various studies have been done to “diversify” a LVM, which aim to learn a diverse set of latent components in LVMs. Most existing studies fall into a frequentist-style regularization framework, where the components are learned via point estimation. In this paper, we investigate how to “diversify” LVMs in the paradigm of Bayesian learning, which has advantages complementary to point estimation, such as alleviating overfitting via model averaging and quantifying uncertainty. We propose two approaches that have complementary advantages. One is to define diversity-promoting mutual angular priors which assign larger density to components with larger mutual angles based on Bayesian network and von Mises-Fisher distribution and use these priors to affect the posterior via Bayes rule. We develop two efficient approximate posterior inference algorithms based on variational inference and Markov chain Monte Carlo sampling. The other approach is to impose diversity-promoting regularization directly over the post-data distribution of components. These two methods are applied to the Bayesian mixture of experts model to encourage the “experts” to be diverse and experimental results demonstrate the effectiveness and efficiency of our methods.",http://proceedings.mlr.press/v48/xiea16.html,http://proceedings.mlr.press/v48/xiea16.pdf,ICML
1901,2016,Learning Mixtures of Plackett-Luce Models,"Zhibing Zhao,         Peter Piech,         Lirong Xia","In this paper we address the identifiability and efficient learning problems of finite mixtures of Plackett-Luce models for rank data. We prove that for any k≥2, the mixture of k Plackett-Luce models for no more than 2k-1 alternatives is non-identifiable and this bound is tight for k=2. For generic identifiability, we prove that the mixture of k Plackett-Luce models over m alternatives is \em generically identifiable if k≤⌊\frac m-2 2⌋!. We also propose an efficient generalized method of moments (GMM) algorithm to learn the mixture of two Plackett-Luce models and show that the algorithm is consistent. Our experiments show that our GMM algorithm is significantly faster than the EMM algorithm by Gormley & Murphy (2008), while achieving competitive statistical efficiency.",http://proceedings.mlr.press/v48/zhaob16.html,http://proceedings.mlr.press/v48/zhaob16.pdf,ICML
1902,2016,Generalization Properties and Implicit Regularization for Multiple Passes SGM,"Junhong Lin,         Raffaello Camoriano,         Lorenzo Rosasco","We study the generalization properties of stochastic gradient methods for learning with convex loss functions and linearly parameterized functions. We show that, in the absence of penalizations or constraints, the stability and approximation properties of the algorithm can be controlled by tuning either the step-size or the number of passes over the data. In this view, these parameters can be seen to control a form of implicit regularization. Numerical results complement the theoretical findings.",http://proceedings.mlr.press/v48/lina16.html,http://proceedings.mlr.press/v48/lina16.pdf,ICML
1903,2016,Dynamic Capacity Networks,"Amjad Almahairi,         Nicolas Ballas,         Tim Cooijmans,         Yin Zheng,         Hugo Larochelle,         Aaron Courville","We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity sub-networks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN’s output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance.",http://proceedings.mlr.press/v48/almahairi16.html,http://proceedings.mlr.press/v48/almahairi16.pdf,ICML
1904,2016,Hawkes Processes with Stochastic Excitations,"Young Lee,         Kar Wai Lim,         Cheng Soon Ong","We propose an extension to Hawkes processes by treating the levels of self-excitation as a stochastic differential equation. Our new point process allows better approximation in application domains where events and intensities accelerate each other with correlated levels of contagion. We generalize a recent algorithm for simulating draws from Hawkes processes whose levels of excitation are stochastic processes, and propose a hybrid Markov chain Monte Carlo approach for model fitting. Our sampling procedure scales linearly with the number of required events and does not require stationarity of the point process. A modular inference procedure consisting of a combination between Gibbs and Metropolis Hastings steps is put forward. We recover expectation maximization as a special case. Our general approach is illustrated for contagion following geometric Brownian motion and exponential Langevin dynamics.",http://proceedings.mlr.press/v48/leea16.html,http://proceedings.mlr.press/v48/leea16.pdf,ICML
1905,2016,Preconditioning Kernel Matrices,"Kurt Cutajar,         Michael Osborne,         John Cunningham,         Maurizio Filippone","The computational and storage complexity of kernel machines presents the primary barrier to their scaling to large, modern, datasets. A common way to tackle the scalability issue is to use the conjugate gradient algorithm, which relieves the constraints on both storage (the kernel matrix need not be stored) and computation (both stochastic gradients and parallelization can be used). Even so, conjugate gradient is not without its own issues: the conditioning of kernel matrices is often such that conjugate gradients will have poor convergence in practice. Preconditioning is a common approach to alleviating this issue. Here we propose preconditioned conjugate gradients for kernel machines, and develop a broad range of preconditioners particularly useful for kernel matrices. We describe a scalable approach to both solving kernel machines and learning their hyperparameters. We show this approach is exact in the limit of iterations and outperforms state-of-the-art approximations for a given computational budget.",http://proceedings.mlr.press/v48/cutajar16.html,http://proceedings.mlr.press/v48/cutajar16.pdf,ICML
1906,2016,Low-Rank Matrix Approximation with Stability,"Dongsheng Li,         Chao Chen,         Qin Lv,         Junchi Yan,         Li Shang,         Stephen Chu","Low-rank matrix approximation has been widely adopted in machine learning applications with sparse data, such as recommender systems. However, the sparsity of the data, incomplete and noisy, introduces challenges to the algorithm stability – small changes in the training data may significantly change the models. As a result, existing low-rank matrix approximation solutions yield low generalization performance, exhibiting high error variance on the training dataset, and minimizing the training error may not guarantee error reduction on the testing dataset. In this paper, we investigate the algorithm stability problem of low-rank matrix approximations. We present a new algorithm design framework, which (1) introduces new optimization objectives to guide stable matrix approximation algorithm design, and (2) solves the optimization problem to obtain stable low-rank approximation solutions with good generalization performance. Experimental results on real-world datasets demonstrate that the proposed work can achieve better prediction accuracy compared with both state-of-the-art low-rank matrix approximation methods and ensemble methods in recommendation task.",http://proceedings.mlr.press/v48/lib16.html,http://proceedings.mlr.press/v48/lib16.pdf,ICML
1907,2016,Group Equivariant Convolutional Networks,"Taco Cohen,         Max Welling","We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.",http://proceedings.mlr.press/v48/cohenc16.html,http://proceedings.mlr.press/v48/cohenc16.pdf,ICML
1908,2016,Benchmarking Deep Reinforcement Learning for Continuous Control,"Yan Duan,         Xi Chen,         Rein Houthooft,         John Schulman,         Pieter Abbeel","Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.",http://proceedings.mlr.press/v48/duan16.html,http://proceedings.mlr.press/v48/duan16.pdf,ICML
1909,2016,Gromov-Wasserstein Averaging of Kernel and Distance Matrices,"Gabriel Peyré,         Marco Cuturi,         Justin Solomon","This paper presents a new technique for computing the barycenter of a set of distance or kernel matrices. These matrices, which define the inter-relationships between points sampled from individual domains, are not required to have the same size or to be in row-by-row correspondence. We compare these matrices using the softassign criterion, which measures the minimum distortion induced by a probabilistic map from the rows of one similarity matrix to the rows of another; this criterion amounts to a regularized version of the Gromov-Wasserstein (GW) distance between metric-measure spaces. The barycenter is then defined as a Fréchet mean of the input matrices with respect to this criterion, minimizing a weighted sum of softassign values. We provide a fast iterative algorithm for the resulting nonconvex optimization problem, built upon state-of- the-art tools for regularized optimal transportation. We demonstrate its application to the computation of shape barycenters and to the prediction of energy levels from molecular configurations in quantum chemistry.",http://proceedings.mlr.press/v48/peyre16.html,http://proceedings.mlr.press/v48/peyre16.pdf,ICML
1910,2016,On Graduated Optimization for Stochastic Non-Convex Problems,"Elad Hazan,         Kfir Yehuda Levy,         Shai Shalev-Shwartz","The graduated optimization approach, also known as the continuation method, is a popular heuristic to solving non-convex problems that has received renewed interest over the last decade.Despite being popular, very little is known in terms of its theoretical convergence analysis. In this paper we describe a new first-order algorithm based on graduated optimization and analyze its performance. We characterize a family of non-convex functions for which this algorithm provably converges to a global optimum. In particular, we prove that the algorithm converges to an ε-approximate solution within O(1 / ε^2) gradient-based steps. We extend our algorithm and analysis to the setting of stochastic non-convex optimization with noisy gradient feedback, attaining the same convergence rate. Additionally, we discuss the setting of “zero-order optimization"", and devise a variant of our algorithm which converges at rate of O(d^2/ ε^4).",http://proceedings.mlr.press/v48/hazanb16.html,http://proceedings.mlr.press/v48/hazanb16.pdf,ICML
1911,2016,Generalized Direct Change Estimation in Ising Model Structure,"Farideh Fazayeli,         Arindam Banerjee","We consider the problem of estimating change in the dependency structure of two p-dimensional Ising models, based on respectively n_1 and n_2 samples drawn from the models. The change is assumed to be structured, e.g., sparse, block sparse, node-perturbed sparse, etc., such that it can be characterized by a suitable (atomic) norm. We present and analyze a norm-regularized estimator for directly estimating the change in structure, without having to estimate the structures of the individual Ising models. The estimator can work with any norm, and can be generalized to other graphical models under mild assumptions. We show that only one set of samples, say n_2, needs to satisfy the sample complexity requirement for the estimator to work, and the estimation error decreases as \fracc\sqrt\min(n_1,n_2), where c depends on the Gaussian width of the unit norm ball. For example, for \ell_1 norm applied to s-sparse change, the change can be accurately estimated with \min(n_1,n_2)=O(s \log p) which is sharper than an existing result n_1= O(s^2 \log p) and n_2 = O(n_1^2). Experimental results illustrating the effectiveness of the proposed estimator are presented.",http://proceedings.mlr.press/v48/fazayeli16.html,http://proceedings.mlr.press/v48/fazayeli16.pdf,ICML
1912,2016,BASC: Applying Bayesian Optimization to the Search for Global Minima on Potential Energy Surfaces,"Shane Carr,         Roman Garnett,         Cynthia Lo","We present a novel application of Bayesian optimization to the field of surface science: rapidly and accurately searching for the global minimum on potential energy surfaces. Controlling molecule-surface interactions is key for applications ranging from environmental catalysis to gas sensing. We present pragmatic techniques, including exploration/exploitation scheduling and a custom covariance kernel that encodes the properties of our objective function. Our method, the Bayesian Active Site Calculator (BASC), outperforms differential evolution and constrained minima hopping – two state-of-the-art approaches – in trial examples of carbon monoxide adsorption on a hematite substrate, both with and without a defect.",http://proceedings.mlr.press/v48/carr16.html,http://proceedings.mlr.press/v48/carr16.pdf,ICML
1913,2016,Learning Sparse Combinatorial Representations via Two-stage Submodular Maximization,"Eric Balkanski,         Baharan Mirzasoleiman,         Andreas Krause,         Yaron Singer","We consider the problem of learning sparse representations of data sets, where the goal is to reduce a data set in manner that optimizes multiple objectives. Motivated by applications of data summarization, we develop a new model which we refer to as the two-stage submodular maximization problem. This task can be viewed as a combinatorial analogue of representation learning problems such as dictionary learning and sparse regression. The two-stage problem strictly generalizes the problem of cardinality constrained submodular maximization, though the objective function is not submodular and the techniques for submodular maximization cannot be applied. We describe a continuous optimization method which achieves an approximation ratio which asymptotically approaches 1-1/e. For instances where the asymptotics do not kick in, we design a local-search algorithm whose approximation ratio is arbitrarily close to 1/2. We empirically demonstrate the effectiveness of our methods on two multi-objective data summarization tasks, where the goal is to construct summaries via sparse representative subsets w.r.t. to predefined objectives.",http://proceedings.mlr.press/v48/balkanski16.html,http://proceedings.mlr.press/v48/balkanski16.pdf,ICML
1914,2016,Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning,"Xingguo Li,         Tuo Zhao,         Raman Arora,         Han Liu,         Jarvis Haupt","We propose a stochastic variance reduced optimization algorithm for solving a class of large-scale nonconvex optimization problems with cardinality constraints, and provide sufficient conditions under which the proposed algorithm enjoys strong linear convergence guarantees and optimal estimation accuracy in high dimensions. Numerical experiments demonstrate the efficiency of our method in terms of both parameter estimation and computational performance.",http://proceedings.mlr.press/v48/lid16.html,http://proceedings.mlr.press/v48/lid16.pdf,ICML
1915,2016,Stochastic Block BFGS: Squeezing More Curvature out of Data,"Robert Gower,         Donald Goldfarb,         Peter Richtarik","We propose a novel limited-memory stochastic block BFGS update for incorporating enriched curvature information in stochastic approximation methods. In our method, the estimate of the inverse Hessian matrix that is maintained by it, is updated at each iteration using a sketch of the Hessian, i.e., a randomly generated compressed form of the Hessian. We propose several sketching strategies, present a new quasi-Newton method that uses stochastic block BFGS updates combined with the variance reduction approach SVRG to compute batch stochastic gradients, and prove linear convergence of the resulting method. Numerical tests on large-scale logistic regression problems reveal that our method is more robust and substantially outperforms current state-of-the-art methods.",http://proceedings.mlr.press/v48/gower16.html,http://proceedings.mlr.press/v48/gower16.pdf,ICML
1916,2016,Structure Learning of Partitioned Markov Networks,"Song Liu,         Taiji Suzuki,         Masashi Sugiyama,         Kenji Fukumizu","We learn the structure of a Markov Network between two groups of random variables from joint observations. Since modelling and learning the full MN structure may be hard, learning the links between two groups directly may be a preferable option. We introduce a novel concept called the \emphpartitioned ratio whose factorization directly associates with the Markovian properties of random variables across two groups. A simple one-shot convex optimization procedure is proposed for learning the \emphsparse factorizations of the partitioned ratio and it is theoretically guaranteed to recover the correct inter-group structure under mild conditions. The performance of the proposed method is experimentally compared with the state of the art MN structure learning methods using ROC curves. Real applications on analyzing bipartisanship in US congress and pairwise DNA/time-series alignments are also reported.",http://proceedings.mlr.press/v48/liuc16.html,http://proceedings.mlr.press/v48/liuc16.pdf,ICML
1917,2016,Parameter Estimation for Generalized Thurstone Choice Models,"Milan Vojnovic,         Seyoung Yun","We consider the maximum likelihood parameter estimation problem for a generalized Thurstone choice model, where choices are from comparison sets of two or more items. We provide tight characterizations of the mean square error, as well as necessary and sufficient conditions for correct classification when each item belongs to one of two classes. These results provide insights into how the estimation accuracy depends on the choice of a generalized Thurstone choice model and the structure of comparison sets. We find that for a priori unbiased structures of comparisons, e.g., when comparison sets are drawn independently and uniformly at random, the number of observations needed to achieve a prescribed estimation accuracy depends on the choice of a generalized Thurstone choice model. For a broad set of generalized Thurstone choice models, which includes all popular instances used in practice, the estimation error is shown to be largely insensitive to the cardinality of comparison sets. On the other hand, we found that there exist generalized Thurstone choice models for which the estimation error decreases much faster with the cardinality of comparison sets.",http://proceedings.mlr.press/v48/vojnovic16.html,http://proceedings.mlr.press/v48/vojnovic16.pdf,ICML
1918,2016,Learning from Multiway Data: Simple and Efficient Tensor Regression,"Rose Yu,         Yan Liu","Tensor regression has shown to be advantageous in learning tasks with multi-directional relatedness. Given massive multiway data, traditional methods are often too slow to operate on or suffer from memory bottleneck. In this paper, we introduce subsampled tensor projected gradient to solve the problem. Our algorithm is impressively simple and efficient. It is built upon projected gradient method with fast tensor power iterations, leveraging randomized sketching for further acceleration. Theoretical analysis shows that our algorithm converges to the correct solution in fixed number of iterations. The memory requirement grows linearly with the size of the problem. We demonstrate superior empirical performance on both multi-linear multi-task learning and spatio-temporal applications.",http://proceedings.mlr.press/v48/yu16.html,http://proceedings.mlr.press/v48/yu16.pdf,ICML
1919,2016,Generalization and Exploration via Randomized Value Functions,"Ian Osband,         Benjamin Van Roy,         Zheng Wen","We propose randomized least-squares value iteration (RLSVI) – a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.",http://proceedings.mlr.press/v48/osband16.html,http://proceedings.mlr.press/v48/osband16.pdf,ICML
1920,2016,Geometric Mean Metric Learning,"Pourya Zadeh,         Reshad Hosseini,         Suvrit Sra","We revisit the task of learning a Euclidean metric from data. We approach this problem from first principles and formulate it as a surprisingly simple optimization problem. Indeed, our formulation even admits a closed form solution. This solution possesses several very attractive properties: (i) an innate geometric appeal through the Riemannian geometry of positive definite matrices; (ii) ease of interpretability; and (iii) computational speed several orders of magnitude faster than the widely used LMNN and ITML methods. Furthermore, on standard benchmark datasets, our closed-form solution consistently attains higher classification accuracy.",http://proceedings.mlr.press/v48/zadeh16.html,http://proceedings.mlr.press/v48/zadeh16.pdf,ICML
1921,2016,Analysis of Variational Bayesian Factorizations for Sparse and Low-Rank Estimation,David Wipf,"Variational Bayesian (VB) approximations anchor a wide variety of probabilistic models, where tractable posterior inference is almost never possible. Typically based on the so-called VB mean-field approximation to the Kullback-Leibler divergence, a posterior distribution is sought that factorizes across groups of latent variables such that, with the distributions of all but one group of variables held fixed, an optimal closed-form distribution can be obtained for the remaining group, with differing algorithms distinguished by how different variables are grouped and ultimately factored. This basic strategy is particularly attractive when estimating structured low-dimensional models of high-dimensional data, exemplified by the search for minimal rank and/or sparse approximations to observed data. To this end, VB models are frequently deployed across applications including multi-task learning, robust PCA, subspace clustering, matrix completion, affine rank minimization, source localization, compressive sensing, and assorted combinations thereof. Perhaps surprisingly however, there exists almost no attendant theoretical explanation for how various VB factorizations operate, and in which situations one may be preferable to another. We address this relative void by comparing arguably two of the most popular factorizations, one built upon Gaussian scale mixture priors, the other bilinear Gaussian priors, both of which can favor minimal rank or sparsity depending on the context. More specifically, by reexpressing the respective VB objective functions, we weigh multiple factors related to local minima avoidance, feature transformation invariance and correlation, and computational complexity to arrive at insightful conclusions useful in explaining performance and deciding which VB flavor is advantageous. We also envision that the principles explored here are quite relevant to other structured inverse problems where VB serves as a viable solution.",http://proceedings.mlr.press/v48/wipf16.html,http://proceedings.mlr.press/v48/wipf16.pdf,ICML
1922,2016,Anytime optimal algorithms in stochastic multi-armed bandits,"Rémy Degenne,         Vianney Perchet","We introduce an anytime algorithm for stochastic multi-armed bandit with optimal distribution free and distribution dependent bounds (for a specific family of parameters). The performances of this algorithm (as well as another one motivated by the conjectured optimal bound) are evaluated empirically. A similar analysis is provided with full information, to serve as a benchmark.",http://proceedings.mlr.press/v48/degenne16.html,http://proceedings.mlr.press/v48/degenne16.pdf,ICML
1923,2016,Associative Long Short-Term Memory,"Ivo Danihelka,         Greg Wayne,         Benigno Uria,         Nal Kalchbrenner,         Alex Graves","We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.",http://proceedings.mlr.press/v48/danihelka16.html,http://proceedings.mlr.press/v48/danihelka16.pdf,ICML
1924,2016,Online Low-Rank Subspace Clustering by Basis Dictionary Pursuit,"Jie Shen,         Ping Li,         Huan Xu","Low-Rank Representation (LRR) has been a significant method for segmenting data that are generated from a union of subspaces. It is also known that solving LRR is challenging in terms of time complexity and memory footprint, in that the size of the nuclear norm regularized matrix is n-by-n (where n is the number of samples). In this paper, we thereby develop a novel online implementation of LRR that reduces the memory cost from O(n^2) to O(pd), with p being the ambient dimension and d being some estimated rank (d < p < n). We also establish the theoretical guarantee that the sequence of solutions produced by our algorithm converges to a stationary point of the expected loss function asymptotically. Extensive experiments on synthetic and realistic datasets further substantiate that our algorithm is fast, robust and memory efficient.",http://proceedings.mlr.press/v48/shen16.html,http://proceedings.mlr.press/v48/shen16.pdf,ICML
1925,2016,Exact Exponent in Optimal Rates for Crowdsourcing,"Chao Gao,         Yu Lu,         Dengyong Zhou","Crowdsourcing has become a popular tool for labeling large datasets. This paper studies the optimal error rate for aggregating crowdsourced labels provided by a collection of amateur workers. Under the Dawid-Skene probabilistic model, we establish matching upper and lower bounds with an exact exponent mI(\pi), where m is the number of workers and I(\pi) is the average Chernoff information that characterizes the workers’ collective ability. Such an exact characterization of the error exponent allows us to state a precise sample size requirement m \ge \frac1I(\pi)\log\frac1ε in order to achieve an εmisclassification error. In addition, our results imply optimality of various forms of EM algorithms given accurate initializers of the model parameters.",http://proceedings.mlr.press/v48/gaoa16.html,http://proceedings.mlr.press/v48/gaoa16.pdf,ICML
1926,2016,The knockoff filter for FDR control in group-sparse and multitask regression,"Ran Dai,         Rina Barber","We propose the group knockoff filter, a method for false discovery rate control in a linear regression setting where the features are grouped, and we would like to select a set of relevant groups which have a nonzero effect on the response. By considering the set of true and false discoveries at the group level, this method gains power relative to sparse regression methods. We also apply our method to the multitask regression problem where multiple response variables share similar sparsity patterns across the set of possible features. Empirically, the group knockoff filter successfully controls false discoveries at the group level in both settings, with substantially more discoveries made by leveraging the group structure.",http://proceedings.mlr.press/v48/daia16.html,http://proceedings.mlr.press/v48/daia16.pdf,ICML
1927,2016,Boolean Matrix Factorization and Noisy Completion via Message Passing,"Siamak Ravanbakhsh,         Barnabas Poczos,         Russell Greiner","Boolean matrix factorization and Boolean matrix completion from noisy observations are desirable unsupervised data-analysis methods due to their interpretability, but hard to perform due to their NP-hardness. We treat these problems as maximum a posteriori inference problems in a graphical model and present a message passing approach that scales linearly with the number of observations and factors. Our empirical study demonstrates that message passing is able to recover low-rank Boolean matrices, in the boundaries of theoretically possible recovery and compares favorably with state-of-the-art in real-world applications, such collaborative filtering with large-scale Boolean data.",http://proceedings.mlr.press/v48/ravanbakhsha16.html,http://proceedings.mlr.press/v48/ravanbakhsha16.pdf,ICML
1928,2016,Structured Prediction Energy Networks,"David Belanger,         Andrew McCallum","We introduce structured prediction energy networks (SPENs), a flexible framework for structured prediction. A deep architecture is used to define an energy function of candidate labels, and then predictions are produced by using back-propagation to iteratively optimize the energy with respect to the labels. This deep architecture captures dependencies between labels that would lead to intractable graphical models, and performs structure learning by automatically learning discriminative features of the structured output. One natural application of our technique is multi-label classification, which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction. We are able to apply SPENs to multi-label problems with substantially larger label sets than previous applications of structured prediction, while modeling high-order interactions using minimal structural assumptions. Overall, deep learning provides remarkable tools for learning features of the inputs to a prediction problem, and this work extends these techniques to learning features of structured outputs. Our experiments provide impressive performance on a variety of benchmark multi-label classification tasks, demonstrate that our technique can be used to provide interpretable structure learning, and illuminate fundamental trade-offs between feed-forward and iterative structured prediction.",http://proceedings.mlr.press/v48/belanger16.html,http://proceedings.mlr.press/v48/belanger16.pdf,ICML
1929,2016,Sparse Parameter Recovery from Aggregated Data,"Avradeep Bhowmik,         Joydeep Ghosh,         Oluwasanmi Koyejo","Data aggregation is becoming an increasingly common technique for sharing sensitive information, and for reducing data size when storage and/or communication costs are high. Aggregate quantities such as group-average are a form of semi-supervision as they do not directly provide information of individual values, but despite their wide-spread use, prior literature on learning individual-level models from aggregated data is extremely limited. This paper investigates the effect of data aggregation on parameter recovery for a sparse linear model, when known results are no longer applicable. In particular, we consider a scenario where the data are collected into groups e.g. aggregated patient records, and first-order empirical moments are available only at the group level. Despite this obfuscation of individual data values, we can show that the true parameter is recoverable with high probability using these aggregates when the collection of true group moments is an incoherent matrix, and the empirical moment estimates have been computed from a sufficiently large number of samples. To the best of our knowledge, ours are the first results on structured parameter recovery using only aggregated data. Experimental results on synthetic data are provided in support of these theoretical claims. We also show that parameter estimation from aggregated data approaches the accuracy of parameter estimation obtainable from non-aggregated or “individual"" samples, when applied to two real world healthcare applications- predictive modeling of CMS Medicare reimbursement claims, and modeling of Texas State healthcare charges.",http://proceedings.mlr.press/v48/bhowmik16.html,http://proceedings.mlr.press/v48/bhowmik16.pdf,ICML
1930,2016,Unitary Evolution Recurrent Neural Networks,"Martin Arjovsky,         Amar Shah,         Yoshua Bengio","Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.",http://proceedings.mlr.press/v48/arjovsky16.html,http://proceedings.mlr.press/v48/arjovsky16.pdf,ICML
1931,2016,Variational Inference for Monte Carlo Objectives,"Andriy Mnih,         Danilo Rezende","Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2016) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multi-sample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator proposed for the single-sample variational objective, and is competitive with the currently used biased estimators.",http://proceedings.mlr.press/v48/mnihb16.html,http://proceedings.mlr.press/v48/mnihb16.pdf,ICML
1932,2016,Near Optimal Behavior via Approximate State Abstraction,"David Abel,         David Hershkowitz,         Michael Littman","The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments.",http://proceedings.mlr.press/v48/abel16.html,http://proceedings.mlr.press/v48/abel16.pdf,ICML
1933,2016,Provable Non-convex Phase Retrieval with Outliers: Median TruncatedWirtinger Flow,"Huishuai Zhang,         Yuejie Chi,         Yingbin Liang","Solving systems of quadratic equations is a central problem in machine learning and signal processing. One important example is phase retrieval, which aims to recover a signal from only magnitudes of its linear measurements. This paper focuses on the situation when the measurements are corrupted by arbitrary outliers, for which the recently developed non-convex gradient descent Wirtinger flow (WF) and truncated Wirtinger flow (TWF) algorithms likely fail. We develop a novel median-TWF algorithm that exploits robustness of sample median to resist arbitrary outliers in the initialization and the gradient update in each iteration. We show that such a non-convex algorithm provably recovers the signal from a near-optimal number of measurements composed of i.i.d. Gaussian entries, up to a logarithmic factor, even when a constant portion of the measurements are corrupted by arbitrary outliers. We further show that median-TWF is also robust when measurements are corrupted by both arbitrary outliers and bounded noise. Our analysis of performance guarantee is accomplished by development of non-trivial concentration measures of median-related quantities, which may be of independent interest. We further provide numerical experiments to demonstrate the effectiveness of the approach.",http://proceedings.mlr.press/v48/zhange16.html,http://proceedings.mlr.press/v48/zhange16.pdf,ICML
1934,2016,Learning Physical Intuition of Block Towers by Example,"Adam Lerer,         Sam Gross,         Rob Fergus","Wooden blocks are a common toy for infants, allowing them to develop motor skills and gain intuition about the physical behavior of the world. In this paper, we explore the ability of deep feed-forward models to learn such intuitive physics. Using a 3D game engine, we create small towers of wooden blocks whose stability is randomized and render them collapsing (or remaining upright). This data allows us to train large convolutional network models which can accurately predict the outcome, as well as estimating the trajectories of the blocks. The models are also able to generalize in two important ways: (i) to new physical scenarios, e.g. towers with an additional block and (ii) to images of real wooden blocks, where it obtains a performance comparable to human subjects.",http://proceedings.mlr.press/v48/lerer16.html,http://proceedings.mlr.press/v48/lerer16.pdf,ICML
1935,2016,PHOG: Probabilistic Model for Code,"Pavol Bielik,         Veselin Raychev,         Martin Vechev","We introduce a new generative model for code called probabilistic higher order grammar (PHOG). PHOG generalizes probabilistic context free grammars (PCFGs) by allowing conditioning of a production rule beyond the parent non-terminal, thus capturing rich contexts relevant to programs. Even though PHOG is more powerful than a PCFG, it can be learned from data just as efficiently. We trained a PHOG model on a large JavaScript code corpus and show that it is more precise than existing models, while similarly fast. As a result, PHOG can immediately benefit existing programming tools based on probabilistic models of code.",http://proceedings.mlr.press/v48/bielik16.html,http://proceedings.mlr.press/v48/bielik16.pdf,ICML
1936,2016,Collapsed Variational Inference for Sum-Product Networks,"Han Zhao,         Tameem Adel,         Geoff Gordon,         Brandon Amos","Sum-Product Networks (SPNs) are probabilistic inference machines that admit exact inference in linear time in the size of the network. Existing parameter learning approaches for SPNs are largely based on the maximum likelihood principle and are subject to overfitting compared to more Bayesian approaches. Exact Bayesian posterior inference for SPNs is computationally intractable. Even approximation techniques such as standard variational inference and posterior sampling for SPNs are computationally infeasible even for networks of moderate size due to the large number of local latent variables per instance. In this work, we propose a novel deterministic collapsed variational inference algorithm for SPNs that is computationally efficient, easy to implement and at the same time allows us to incorporate prior information into the optimization formulation. Extensive experiments show a significant improvement in accuracy compared with a maximum likelihood based approach.",http://proceedings.mlr.press/v48/zhaoa16.html,http://proceedings.mlr.press/v48/zhaoa16.pdf,ICML
1937,2016,Model-Free Trajectory Optimization for Reinforcement Learning,"Riad Akrour,         Gerhard Neumann,         Hany Abdulsamad,         Abbas Abdolmaleki","Many of the recent Trajectory Optimization algorithms alternate between local approximation of the dynamics and conservative policy update. However, linearly approximating the dynamics in order to derive the new policy can bias the update and prevent convergence to the optimal policy. In this article, we propose a new model-free algorithm that backpropagates a local quadratic time-dependent Q-Function, allowing the derivation of the policy update in closed form. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics demonstrating improved performance in comparison to related Trajectory Optimization algorithms linearizing the dynamics.",http://proceedings.mlr.press/v48/akrour16.html,http://proceedings.mlr.press/v48/akrour16.pdf,ICML
1938,2016,Exploiting Cyclic Symmetry in Convolutional Neural Networks,"Sander Dieleman,         Jeffrey De Fauw,         Koray Kavukcuoglu","Many classes of images exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but they are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We evaluate the effect of these architectural modifications on three datasets which exhibit rotational symmetry and demonstrate improved performance with smaller models.",http://proceedings.mlr.press/v48/dieleman16.html,http://proceedings.mlr.press/v48/dieleman16.pdf,ICML
1939,2016,"Conditional Dependence via Shannon Capacity: Axioms, Estimators and Applications","Weihao Gao,         Sreeram Kannan,         Sewoong Oh,         Pramod Viswanath","We consider axiomatically the problem of estimating the strength of a conditional dependence relationship P_Y|X from a random variables X to a random variable Y. This has applications in determining the strength of a known causal relationship, where the strength depends only on the conditional distribution of the effect given the cause (and not on the driving distribution of the cause). Shannon capacity, appropriately regularized, emerges as a natural measure under these axioms. We examine the problem of calculating Shannon capacity from the observed samples and propose a novel fixed-k nearest neighbor estimator, and demonstrate its consistency. Finally, we demonstrate an application to single-cell flow-cytometry, where the proposed estimators significantly reduce sample complexity.",http://proceedings.mlr.press/v48/gaob16.html,http://proceedings.mlr.press/v48/gaob16.pdf,ICML
1940,2016,Actively Learning Hemimetrics with Applications to Eliciting User Preferences,"Adish Singla,         Sebastian Tschiatschek,         Andreas Krause","Motivated by an application of eliciting users’ preferences, we investigate the problem of learning hemimetrics, i.e., pairwise distances among a set of n items that satisfy triangle inequalities and non-negativity constraints. In our application, the (asymmetric) distances quantify private costs a user incurs when substituting one item by another. We aim to learn these distances (costs) by asking the users whether they are willing to switch from one item to another for a given incentive offer. Without exploiting structural constraints of the hemimetric polytope, learning the distances between each pair of items requires Θ(n^2) queries. We propose an active learning algorithm that substantially reduces this sample complexity by exploiting the structural constraints on the version space of hemimetrics. Our proposed algorithm achieves provably-optimal sample complexity for various instances of the task. For example, when the items are embedded into K tight clusters, the sample complexity of our algorithm reduces to O(n K). Extensive experiments on a restaurant recommendation data set support the conclusions of our theoretical analysis.",http://proceedings.mlr.press/v48/singla16.html,http://proceedings.mlr.press/v48/singla16.pdf,ICML
1941,2016,Optimality of Belief Propagation for Crowdsourced Classification,"Jungseul Ok,         Sewoong Oh,         Jinwoo Shin,         Yung Yi","Crowdsourcing systems are popular for solving large-scale labelling tasks with low-paid (or even non-paid) workers. We study the problem of recovering the true labels from noisy crowdsourced labels under the popular Dawid-Skene model. To address this inference problem, several algorithms have recently been proposed, but the best known guarantee is still significantly larger than the fundamental limit. We close this gap under a simple but canonical scenario where each worker is assigned at most two tasks. In particular, we introduce a tighter lower bound on the fundamental limit and prove that Belief Propagation (BP) exactly matches this lower bound. The guaranteed optimality of BP is the strongest in the sense that it is information-theoretically impossible for any other algorithm to correctly la- bel a larger fraction of the tasks. In the general setting, when more than two tasks are assigned to each worker, we establish the dominance result on BP that it outperforms other existing algorithms with known provable guarantees. Experimental results suggest that BP is close to optimal for all regimes considered, while existing state-of-the-art algorithms exhibit suboptimal performances.",http://proceedings.mlr.press/v48/ok16.html,http://proceedings.mlr.press/v48/ok16.pdf,ICML
1942,2016,Anytime Exploration for Multi-armed Bandits using Confidence Information,"Kwang-Sung Jun,         Robert Nowak","We introduce anytime Explore-m, a pure exploration problem for multi-armed bandits (MAB) that requires making a prediction of the top-m arms at every time step. Anytime Explore-m is more practical than fixed budget or fixed confidence formulations of the top-m problem, since many applications involve a finite, but unpredictable, budget. However, the development and analysis of anytime algorithms present many challenges. We propose AT-LUCB (AnyTime Lower and Upper Confidence Bound), the first nontrivial algorithm that provably solves anytime Explore-m. Our analysis shows that the sample complexity of AT-LUCB is competitive to anytime variants of existing algorithms. Moreover, our empirical evaluation on AT-LUCB shows that AT-LUCB performs as well as or better than state-of-the-art baseline methods for anytime Explore-m.",http://proceedings.mlr.press/v48/jun16.html,http://proceedings.mlr.press/v48/jun16.pdf,ICML
1943,2016,Variance-Reduced and Projection-Free Stochastic Optimization,"Elad Hazan,         Haipeng Luo","The Frank-Wolfe optimization algorithm has recently regained popularity for machine learning applications due to its projection-free property and its ability to handle structured constraints. However, in the stochastic learning setting, it is still relatively understudied compared to the gradient descent counterpart. In this work, leveraging a recent variance reduction technique, we propose two stochastic Frank-Wolfe variants which substantially improve previous results in terms of the number of stochastic gradient evaluations needed to achieve 1-εaccuracy. For example, we improve from O(\frac1ε) to O(\ln\frac1ε) if the objective function is smooth and strongly convex, and from O(\frac1ε^2) to O(\frac1ε^1.5) if the objective function is smooth and Lipschitz. The theoretical improvement is also observed in experiments on real-world datasets for a multiclass classification application.",http://proceedings.mlr.press/v48/hazana16.html,http://proceedings.mlr.press/v48/hazana16.pdf,ICML
1944,2016,Learning End-to-end Video Classification with Rank-Pooling,"Basura Fernando,         Stephen Gould","We introduce a new model for representation learning and classification of video sequences. Our model is based on a convolutional neural network coupled with a novel temporal pooling layer. The temporal pooling layer relies on an inner-optimization problem to efficiently encode temporal semantics over arbitrarily long video clips into a fixed-length vector representation. Importantly, the representation and classification parameters of our model can be estimated jointly in an end-to-end manner by formulating learning as a bilevel optimization problem. Furthermore, the model can make use of any existing convolutional neural network architecture (e.g., AlexNet or VGG) without modification or introduction of additional parameters. We demonstrate our approach on action and activity recognition tasks.",http://proceedings.mlr.press/v48/fernando16.html,http://proceedings.mlr.press/v48/fernando16.pdf,ICML
1945,2016,Sparse Nonlinear Regression: Parameter Estimation under Nonconvexity,"Zhuoran Yang,         Zhaoran Wang,         Han Liu,         Yonina Eldar,         Tong Zhang","We study parameter estimation for sparse nonlinear regression. More specifically, we assume the data are given by y = f( \bf x^T \bf β^* ) + ε, where f is nonlinear. To recover \bf βs, we propose an \ell_1-regularized least-squares estimator. Unlike classical linear regression, the corresponding optimization problem is nonconvex because of the nonlinearity of f. In spite of the nonconvexity, we prove that under mild conditions, every stationary point of the objective enjoys an optimal statistical rate of convergence. Detailed numerical results are provided to back up our theory.",http://proceedings.mlr.press/v48/yangc16.html,http://proceedings.mlr.press/v48/yangc16.pdf,ICML
1946,2016,Softened Approximate Policy Iteration for Markov Games,"Julien Pérolat,         Bilal Piot,         Matthieu Geist,         Bruno Scherrer,         Olivier Pietquin","This paper reports theoretical and empirical investigations on the use of quasi-Newton methods to minimize the Optimal Bellman Residual (OBR) of zero-sum two-player Markov Games. First, it reveals that state-of-the-art algorithms can be derived by the direct application of Newton’s method to different norms of the OBR. More precisely, when applied to the norm of the OBR, Newton’s method results in the Bellman Residual Minimization Policy Iteration (BRMPI) and, when applied to the norm of the Projected OBR (POBR), it results into the standard Least Squares Policy Iteration (LSPI) algorithm. Consequently, new algorithms are proposed, making use of quasi-Newton methods to minimize the OBR and the POBR so as to take benefit of enhanced empirical performances at low cost. Indeed, using a quasi-Newton method approach introduces slight modifications in term of coding of LSPI and BRMPI but improves significantly both the stability and the performance of those algorithms. These phenomena are illustrated on an experiment conducted on artificially constructed games called Garnets.",http://proceedings.mlr.press/v48/perolat16.html,http://proceedings.mlr.press/v48/perolat16.pdf,ICML
1947,2016,Correlation Clustering and Biclustering with Locally Bounded Errors,"Gregory Puleo,         Olgica Milenkovic","We consider a generalized version of the correlation clustering problem, defined as follows. Given a complete graph G whose edges are labeled with + or -, we wish to partition the graph into clusters while trying to avoid errors: + edges between clusters or - edges within clusters. Classically, one seeks to minimize the total number of such errors. We introduce a new framework that allows the objective to be a more general function of the number of errors at each vertex (for example, we may wish to minimize the number of errors at the worst vertex) and provide a rounding algorithm which converts “fractional clusterings” into discrete clusterings while causing only a constant-factor blowup in the number of errors at each vertex. This rounding algorithm yields constant-factor approximation algorithms for the discrete problem under a wide variety of objective functions.",http://proceedings.mlr.press/v48/puleo16.html,http://proceedings.mlr.press/v48/puleo16.pdf,ICML
1948,2016,Stochastic Discrete Clenshaw-Curtis Quadrature,"Nico Piatkowski,         Katharina Morik","The partition function is fundamental for probabilistic graphical models—it is required for inference, parameter estimation, and model selection. Evaluating this function corresponds to discrete integration, namely a weighted sum over an exponentially large set. This task quickly becomes intractable as the dimensionality of the problem increases. We propose an approximation scheme that, for any discrete graphical model whose parameter vector has bounded norm, estimates the partition function with arbitrarily small error. Our algorithm relies on a near minimax optimal polynomial approximation to the potential function and a Clenshaw-Curtis style quadrature. Furthermore, we show that this algorithm can be randomized to split the computation into a high-complexity part and a low-complexity part, where the latter may be carried out on small computational devices. Experiments confirm that the new randomized algorithm is highly accurate if the parameter norm is small, and is otherwise comparable to methods with unbounded error.",http://proceedings.mlr.press/v48/piatkowski16.html,http://proceedings.mlr.press/v48/piatkowski16.pdf,ICML
1949,2016,Persistence weighted Gaussian kernel for topological data analysis,"Genki Kusano,         Yasuaki Hiraoka,         Kenji Fukumizu","Topological data analysis (TDA) is an emerging mathematical concept for characterizing shapes in complex data. In TDA, persistence diagrams are widely recognized as a useful descriptor of data, and can distinguish robust and noisy topological properties. This paper proposes a kernel method on persistence diagrams to develop a statistical framework in TDA. The proposed kernel satisfies the stability property and provides explicit control on the effect of persistence. Furthermore, the method allows a fast approximation technique. The method is applied into practical data on proteins and oxide glasses, and the results show the advantage of our method compared to other relevant methods on persistence diagrams.",http://proceedings.mlr.press/v48/kusano16.html,http://proceedings.mlr.press/v48/kusano16.pdf,ICML
1950,2016,PAC Lower Bounds and Efficient Algorithms for The Max K-Armed Bandit Problem,"Yahel David,         Nahum Shimkin","We consider the Max K-Armed Bandit problem, where a learning agent is faced with several stochastic arms, each a source of i.i.d. rewards of unknown distribution. At each time step the agent chooses an arm, and observes the reward of the obtained sample. Each sample is considered here as a separate item with the reward designating its value, and the goal is to find an item with the highest possible value. Our basic assumption is a known lower bound on the \em tail function of the reward distributions. Under the PAC framework, we provide a lower bound on the sample complexity of any (ε,δ)-correct algorithm, and propose an algorithm that attains this bound up to logarithmic factors. We provide an analysis of the robustness of the proposed algorithm to the model assumptions, and further compare its performance to the simple non-adaptive variant, in which the arms are chosen randomly at each stage.",http://proceedings.mlr.press/v48/david16.html,http://proceedings.mlr.press/v48/david16.pdf,ICML
1951,2016,A Random Matrix Approach to Echo-State Neural Networks,"Romain Couillet,         Gilles Wainrib,         Hafiz Tiomoko Ali,         Harry Sevi","Recurrent neural networks, especially in their linear version, have provided many qualitative insights on their performance under different configurations. This article provides, through a novel random matrix framework, the quantitative counterpart of these performance results, specifically in the case of echo-state networks. Beyond mere insights, our approach conveys a deeper understanding on the core mechanism under play for both training and testing.",http://proceedings.mlr.press/v48/couillet16.html,http://proceedings.mlr.press/v48/couillet16.pdf,ICML
1952,2016,Discrete Deep Feature Extraction: A Theory and New Architectures,"Thomas Wiatowski,         Michael Tschannen,         Aleksandar Stanic,         Philipp Grohs,         Helmut Boelcskei","First steps towards a mathematical theory of deep convolutional neural networks for feature extraction were made—for the continuous-time case—in Mallat, 2012, and Wiatowski and Bölcskei, 2015. This paper considers the discrete case, introduces new convolutional neural network architectures, and proposes a mathematical framework for their analysis. Specifically, we establish deformation and translation sensitivity results of local and global nature, and we investigate how certain structural properties of the input signal are reflected in the corresponding feature vectors. Our theory applies to general filters and general Lipschitz-continuous non-linearities and pooling operators. Experiments on handwritten digit classification and facial landmark detection—including feature importance evaluation—complement the theoretical findings.",http://proceedings.mlr.press/v48/wiatowski16.html,http://proceedings.mlr.press/v48/wiatowski16.pdf,ICML
1953,2016,Matrix Eigen-decomposition via Doubly Stochastic Riemannian Optimization,"Zhiqiang Xu,         Peilin Zhao,         Jianneng Cao,         Xiaoli Li","Matrix eigen-decomposition is a classic and long-standing problem that plays a fundamental role in scientific computing and machine learning. Despite some existing algorithms for this inherently non-convex problem, the study remains inadequate for the need of large data nowadays. To address this gap, we propose a Doubly Stochastic Riemannian Gradient EIGenSolver, DSRG-EIGS, where the double stochasticity comes from the generalization of the stochastic Euclidean gradient ascent and the stochastic Euclidean coordinate ascent to Riemannian manifolds. As a result, it induces a greatly reduced complexity per iteration, enables the algorithm to completely avoid the matrix inversion, and consequently makes it well-suited to large-scale applications. We theoretically analyze its convergence properties and empirically validate it on real-world datasets. Encouraging experimental results demonstrate its advantages over the deterministic counterparts.",http://proceedings.mlr.press/v48/xub16.html,http://proceedings.mlr.press/v48/xub16.pdf,ICML
1954,2016,A Simple and Provable Algorithm for Sparse Diagonal CCA,"Megasthenis Asteris,         Anastasios Kyrillidis,         Oluwasanmi Koyejo,         Russell Poldrack","Given two sets of variables, derived from a common set of samples, sparse Canonical Correlation Analysis (CCA) seeks linear combinations of a small number of variables in each set, such that the induced \emphcanonical variables are maximally correlated. Sparse CCA is NP-hard. We propose a novel combinatorial algorithm for sparse diagonal CCA, \textiti.e., sparse CCA under the additional assumption that variables within each set are standardized and uncorrelated. Our algorithm operates on a low rank approximation of the input data and its computational complexity scales linearly with the number of input variables. It is simple to implement, and parallelizable. In contrast to most existing approaches, our algorithm administers precise control on the sparsity of the extracted canonical vectors, and comes with theoretical data-dependent global approximation guarantees, that hinge on the spectrum of the input data. Finally, it can be straightforwardly adapted to other constrained variants of CCA enforcing structure beyond sparsity. We empirically evaluate the proposed scheme and apply it on a real neuroimaging dataset to investigate associations between brain activity and behavior measurements.",http://proceedings.mlr.press/v48/asteris16.html,http://proceedings.mlr.press/v48/asteris16.pdf,ICML
1955,2016,Conditional Bernoulli Mixtures for Multi-label Classification,"Cheng Li,         Bingyu Wang,         Virgil Pavlu,         Javed Aslam","Multi-label classification is an important machine learning task wherein one assigns a subset of candidate labels to an object. In this paper, we propose a new multi-label classification method based on Conditional Bernoulli Mixtures. Our proposed method has several attractive properties: it captures label dependencies; it reduces the multi-label problem to several standard binary and multi-class problems; it subsumes the classic independent binary prediction and power-set subset prediction methods as special cases; and it exhibits accuracy and/or computational complexity advantages over existing approaches. We demonstrate two implementations of our method using logistic regressions and gradient boosted trees, together with a simple training procedure based on Expectation Maximization. We further derive an efficient prediction procedure based on dynamic programming, thus avoiding the cost of examining an exponential number of potential label subsets. Experimental results show the effectiveness of the proposed method against competitive alternatives on benchmark datasets.",http://proceedings.mlr.press/v48/lij16.html,http://proceedings.mlr.press/v48/lij16.pdf,ICML
1956,2016,False Discovery Rate Control and Statistical Quality Assessment of Annotators in Crowdsourced Ranking,"QianQian Xu,         Jiechao Xiong,         Xiaochun Cao,         Yuan Yao","With the rapid growth of crowdsourcing platforms it has become easy and relatively inexpensive to collect a dataset labeled by multiple annotators in a short time. However due to the lack of control over the quality of the annotators, some abnormal annotators may be affected by position bias which can potentially degrade the quality of the final consensus labels. In this paper we introduce a statistical framework to model and detect annotator’s position bias in order to control the false discovery rate (FDR) without a prior knowledge on the amount of biased annotators–the expected fraction of false discoveries among all discoveries being not too high, in order to assure that most of the discoveries are indeed true and replicable. The key technical development relies on some new knockoff filters adapted to our problem and new algorithms based on the Inverse Scale Space dynamics whose discretization is potentially suitable for large scale crowdsourcing data analysis. Our studies are supported by experiments with both simulated examples and real-world data. The proposed framework provides us a useful tool for quantitatively studying annotator’s abnormal behavior in crowdsourcing.",http://proceedings.mlr.press/v48/xua16.html,http://proceedings.mlr.press/v48/xua16.pdf,ICML
1957,2016,Noisy Activation Functions,"Caglar Gulcehre,         Marcin Moczulski,         Misha Denil,         Yoshua Bengio","Common nonlinear activation functions used in neural networks can cause training difficulties due to the saturation behavior of the activation function, which may hide dependencies that are not visible to vanilla-SGD (using first order gradients only). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the activation function would yield zero gradients. Large noise will dominate the noise-free gradient and allow stochastic gradient descent to explore more. By adding noise only to the problematic parts of the activation function, we allow the optimization procedure to explore the boundary between the degenerate saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by noisy variants helps optimization in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results.",http://proceedings.mlr.press/v48/gulcehre16.html,http://proceedings.mlr.press/v48/gulcehre16.pdf,ICML
1958,2016,On the Consistency of Feature Selection With Lasso for Non-linear Targets,"Yue Zhang,         Weihong Guo,         Soumya Ray","An important question in feature selection is whether a selection strategy recovers the “true” set of features, given enough data. We study this question in the context of the popular Least Absolute Shrinkage and Selection Operator (Lasso) feature selection strategy. In particular, we consider the scenario when the model is misspecified so that the learned model is linear while the underlying real target is nonlinear. Surprisingly, we prove that under certain conditions, Lasso is still able to recover the correct features in this case. We also carry out numerical studies to empirically verify the theoretical results and explore the necessity of the conditions under which the proof holds.",http://proceedings.mlr.press/v48/zhanga16.html,http://proceedings.mlr.press/v48/zhanga16.pdf,ICML
1959,2016,Training Deep Neural Networks via Direct Loss Minimization,"Yang Song,         Alexander Schwing,          Richard,         Raquel Urtasun","Supervised training of deep neural nets typically relies on minimizing cross-entropy. However, in many domains, we are interested in performing well on metrics specific to the application. In this paper we propose a direct loss minimization approach to train deep neural networks, which provably minimizes the application-specific loss function. This is often non-trivial, since these functions are neither smooth nor decomposable and thus are not amenable to optimization with standard gradient-based methods. We demonstrate the effectiveness of our approach in the context of maximizing average precision for ranking problems. Towards this goal, we develop a novel dynamic programming algorithm that can efficiently compute the weight updates. Our approach proves superior to a variety of baselines in the context of action classification and object detection, especially in the presence of label noise.",http://proceedings.mlr.press/v48/songb16.html,http://proceedings.mlr.press/v48/songb16.pdf,ICML
1960,2016,Fast Algorithms for Segmented Regression,"Jayadev Acharya,         Ilias Diakonikolas,         Jerry Li,         Ludwig Schmidt","We study the fixed design segmented regression problem: Given noisy samples from a piecewise linear function f, we want to recover f up to a desired accuracy in mean-squared error. Previous rigorous approaches for this problem rely on dynamic programming (DP) and, while sample efficient, have running time quadratic in the sample size. As our main contribution, we provide new sample near-linear time algorithms for the problem that - while not being minimax optimal - achieve a significantly better sample-time tradeoff on large datasets compared to the DP approach. Our experimental evaluation shows that, compared with the DP approach, our algorithms provide a convergence rate that is only off by a factor of 2 to 4, while achieving speedups of three orders of magnitude.",http://proceedings.mlr.press/v48/acharya16.html,http://proceedings.mlr.press/v48/acharya16.pdf,ICML
1961,2016,Smooth Imitation Learning for Online Sequence Prediction,"Hoang Le,         Andrew Kang,         Yisong Yue,         Peter Carr","We study the problem of smooth imitation learning for online sequence prediction, where the goal is to train a policy that can smoothly imitate demonstrated behavior in a dynamic and continuous environment in response to online, sequential context input. Since the mapping from context to behavior is often complex, we take a learning reduction approach to reduce smooth imitation learning to a regression problem using complex function classes that are regularized to ensure smoothness. We present a learning meta-algorithm that achieves fast and stable convergence to a good policy. Our approach enjoys several attractive properties, including being fully deterministic, employing an adaptive learning rate that can provably yield larger policy improvements compared to previous approaches, and the ability to ensure stable convergence. Our empirical results demonstrate significant performance gains over previous approaches.",http://proceedings.mlr.press/v48/le16.html,http://proceedings.mlr.press/v48/le16.pdf,ICML
1962,2016,A Theory of Generative ConvNet,"Jianwen Xie,         Yang Lu,         Song-Chun Zhu,         Yingnian Wu","We show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the category is a base category generated by a reference distribution. If we further assume that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then we obtain a generative ConvNet model that is unique among energy-based models: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. The Langevin dynamics for sampling the generative ConvNet is driven by the reconstruction error of this auto-encoder. The contrastive divergence learning of the generative ConvNet reconstructs the training images by the auto-encoder. The maximum likelihood learning algorithm can synthesize realistic natural image patterns.",http://proceedings.mlr.press/v48/xiec16.html,http://proceedings.mlr.press/v48/xiec16.pdf,ICML
1963,2016,Adaptive Algorithms for Online Convex Optimization with Long-term Constraints,"Rodolphe Jenatton,         Jim Huang,         Cedric Archambeau","We present an adaptive online gradient descent algorithm to solve online convex optimization problems with long-term constraints, which are constraints that need to be satisfied when accumulated over a finite number of rounds T, but can be violated in intermediate rounds. For some user-defined trade-off parameter βin (0, 1), the proposed algorithm achieves cumulative regret bounds of O(T^maxβ,1_β) and O(T^1_β/2), respectively for the loss and the constraint violations. Our results hold for convex losses, can handle arbitrary convex constraints and rely on a single computationally efficient algorithm. Our contributions improve over the best known cumulative regret bounds of Mahdavi et al. (2012), which are respectively O(T^1/2) and O(T^3/4) for general convex domains, and respectively O(T^2/3) and O(T^2/3) when the domain is further restricted to be a polyhedral set. We supplement the analysis with experiments validating the performance of our algorithm in practice.",http://proceedings.mlr.press/v48/jenatton16.html,http://proceedings.mlr.press/v48/jenatton16.pdf,ICML
1964,2016,A Distributed Variational Inference Framework for Unifying Parallel Sparse Gaussian Process Regression Models,"Trong Nghia Hoang,         Quang Minh Hoang,         Bryan Kian Hsiang Low","This paper presents a novel distributed variational inference framework that unifies many parallel sparse Gaussian process regression (SGPR) models for scalable hyperparameter learning with big data. To achieve this, our framework exploits a structure of correlated noise process model that represents the observation noises as a finite realization of a high-order Gaussian Markov random process. By varying the Markov order and covariance function for the noise process model, different variational SGPR models result. This consequently allows the correlation structure of the noise process model to be characterized for which a particular variational SGPR model is optimal. We empirically evaluate the predictive performance and scalability of the distributed variational SGPR models unified by our framework on two real-world datasets.",http://proceedings.mlr.press/v48/hoang16.html,http://proceedings.mlr.press/v48/hoang16.pdf,ICML
1965,2016,Fixed Point Quantization of Deep Convolutional Networks,"Darryl Lin,         Sachin Talathi,         Sreekanth Annapureddy","In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in computation and model storage resources. Fixed point implementation of DCNs has the potential to alleviate some of these complexities and facilitate potential deployment on embedded hardware. In this paper, we propose a quantizer design for fixed point implementation of DCNs. We formulate and solve an optimization problem to identify optimal fixed point bit-width allocation across DCN layers. Our experiments show that in comparison to equal bit-width settings, the fixed point DCNs with optimized bit width allocation offer >20% reduction in the model size without any loss in accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark.",http://proceedings.mlr.press/v48/linb16.html,http://proceedings.mlr.press/v48/linb16.pdf,ICML
1966,2016,Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings,"Rie Johnson,         Tong Zhang","One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson & Zhang, 2015). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of ‘text region embedding + pooling’. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets.",http://proceedings.mlr.press/v48/johnson16.html,http://proceedings.mlr.press/v48/johnson16.pdf,ICML
1967,2016,Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and Convexity,Ohad Shamir,"We study the convergence properties of the VR-PCA algorithm introduced by (Shamir, 2015) for fast computation of leading singular vectors. We prove several new results, including a formal analysis of a block version of the algorithm, and convergence from random initialization. We also make a few observations of independent interest, such as how pre-initializing with just a single exact power iteration can significantly improve the analysis, and what are the convexity and non-convexity properties of the underlying optimization problem.",http://proceedings.mlr.press/v48/shamira16.html,http://proceedings.mlr.press/v48/shamira16.pdf,ICML
1968,2016,Towards Faster Rates and Oracle Property for Low-Rank Matrix Estimation,"Huan Gui,         Jiawei Han,         Quanquan Gu","We present a unified framework for low-rank matrix estimation with a nonconvex penalty. A proximal gradient homotopy algorithm is proposed to solve the proposed optimization problem. Theoretically, we first prove that the proposed estimator attains a faster statistical rate than the traditional low-rank matrix estimator with nuclear norm penalty. Moreover, we rigorously show that under a certain condition on the magnitude of the nonzero singular values, the proposed estimator enjoys oracle property (i.e., exactly recovers the true rank of the matrix), besides attaining a faster rate. Extensive numerical experiments on both synthetic and real world datasets corroborate our theoretical findings.",http://proceedings.mlr.press/v48/gui16.html,http://proceedings.mlr.press/v48/gui16.pdf,ICML
1969,2016,Black-box Optimization with a Politician,"Sebastien Bubeck,         Yin Tat Lee","We propose a new framework for black-box convex optimization which is well-suited for situations where gradient computations are expensive. We derive a new method for this framework which leverages several concepts from convex optimization, from standard first-order methods (e.g. gradient descent or quasi-Newton methods) to analytical centers (i.e. minimizers of self-concordant barriers). We demonstrate empirically that our new technique compares favorably with state of the art algorithms (such as BFGS).",http://proceedings.mlr.press/v48/bubeck16.html,http://proceedings.mlr.press/v48/bubeck16.pdf,ICML
1970,2016,Large-Margin Softmax Loss for Convolutional Neural Networks,"Weiyang Liu,         Yandong Wen,         Zhiding Yu,         Meng Yang","Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks.",http://proceedings.mlr.press/v48/liud16.html,http://proceedings.mlr.press/v48/liud16.pdf,ICML
1971,2016,Speeding up k-means by approximating Euclidean distances via block vectors,"Thomas Bottesch,         Thomas Bühler,         Markus Kächele","This paper introduces a new method to approximate Euclidean distances between points using block vectors in combination with the Hölder inequality. By defining lower bounds based on the proposed approximation, cluster algorithms can be considerably accelerated without loss of quality. In extensive experiments, we show a considerable reduction in terms of computational time in comparison to standard methods and the recently proposed Yinyang k-means. Additionally we show that the memory consumption of the presented clustering algorithm does not depend on the number of clusters, which makes the approach suitable for large scale problems.",http://proceedings.mlr.press/v48/bottesch16.html,http://proceedings.mlr.press/v48/bottesch16.pdf,ICML
1972,2016,Deep Structured Energy Based Models for Anomaly Detection,"Shuangfei Zhai,         Yu Cheng,         Weining Lu,         Zhongfei Zhang","In this paper, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. We hence propose deep structured energy based models (DSEBMs), where the energy function is the output of a deterministic deep neural network with structure. We develop novel model architectures to integrate EBMs with different types of data such as static data, sequential data, and spatial data, and apply appropriate model architectures to adapt to the data structure. Our training algorithm is built upon the recent development of score matching (Hyvarinen, 2005), which connects an EBM with a regularized autoencoder, eliminating the need for complicated sampling method. Statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution. We investigate two decision criteria for performing anomaly detection: the energy score and the reconstruction error. Extensive empirical studies on benchmark anomaly detection tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods.",http://proceedings.mlr.press/v48/zhai16.html,http://proceedings.mlr.press/v48/zhai16.pdf,ICML
1973,2016,SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization,"Zheng Qu,         Peter Richtarik,         Martin Takac,         Olivier Fercoq","We propose a new algorithm for minimizing regularized empirical loss: Stochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each iteration we update a random subset of the dual variables. However, unlike existing methods such as stochastic dual coordinate ascent, SDNA is capable of utilizing all local curvature information contained in the examples, which leads to striking improvements in both theory and practice – sometimes by orders of magnitude. In the special case when an L2-regularizer is used in the primal, the dual problem is a concave quadratic maximization problem plus a separable term. In this regime, SDNA in each step solves a proximal subproblem involving a random principal submatrix of the Hessian of the quadratic function; whence the name of the method.",http://proceedings.mlr.press/v48/qub16.html,http://proceedings.mlr.press/v48/qub16.pdf,ICML
1974,2016,Importance Sampling Tree for Large-scale Empirical Expectation,"Olivier Canevet,         Cijo Jose,         Francois Fleuret","We propose a tree-based procedure inspired by the Monte-Carlo Tree Search that dynamically modulates an importance-based sampling to prioritize computation, while getting unbiased estimates of weighted sums. We apply this generic method to learning on very large training sets, and to the evaluation of large-scale SVMs. The core idea is to reformulate the estimation of a score - whether a loss or a prediction estimate - as an empirical expectation, and to use such a tree whose leaves carry the samples to focus efforts over the problematic ""heavy weight"" ones. We illustrate the potential of this approach on three problems: to improve Adaboost and a multi-layer perceptron on 2D synthetic tasks with several million points, to train a large-scale convolution network on several millions deformations of the CIFAR data-set, and to compute the response of a SVM with several hundreds of thousands of support vectors. In each case, we show how it either cuts down computation by more than one order of magnitude and/or allows to get better loss estimates.",http://proceedings.mlr.press/v48/canevet16.html,http://proceedings.mlr.press/v48/canevet16.pdf,ICML
1975,2016,Linking losses for density ratio and class-probability estimation,"Aditya Menon,         Cheng Soon Ong","Given samples from two densities p and q, density ratio estimation (DRE) is the problem of estimating the ratio p/q. Two popular discriminative approaches to DRE are KL importance estimation (KLIEP), and least squares importance fitting (LSIF). In this paper, we show that KLIEP and LSIF both employ class-probability estimation (CPE) losses. Motivated by this, we formally relate DRE and CPE, and demonstrate the viability of using existing losses from one problem for the other. For the DRE problem, we show that essentially any CPE loss (eg logistic, exponential) can be used, as this equivalently minimises a Bregman divergence to the true density ratio. We show how different losses focus on accurately modelling different ranges of the density ratio, and use this to design new CPE losses for DRE. For the CPE problem, we argue that the LSIF loss is useful in the regime where one wishes to rank instances with maximal accuracy at the head of the ranking. In the course of our analysis, we establish a Bregman divergence identity that may be of independent interest.",http://proceedings.mlr.press/v48/menon16.html,http://proceedings.mlr.press/v48/menon16.pdf,ICML
1976,2016,Texture Networks: Feed-forward Synthesis of Textures and Stylized Images,"Dmitry Ulyanov,         Vadim Lebedev,          Andrea,         Victor Lempitsky","Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods requires a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys et al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.",http://proceedings.mlr.press/v48/ulyanov16.html,http://proceedings.mlr.press/v48/ulyanov16.pdf,ICML
1977,2016,Distributed Clustering of Linear Bandits in Peer to Peer Networks,"Nathan Korda,         Balazs Szorenyi,         Shuai Li","We provide two distributed confidence ball algorithms for solving linear bandit problems in peer to peer networks with limited communication capabilities. For the first, we assume that all the peers are solving the same linear bandit problem, and prove that our algorithm achieves the optimal asymptotic regret rate of any centralised algorithm that can instantly communicate information between the peers. For the second, we assume that there are clusters of peers solving the same bandit problem within each cluster, and we prove that our algorithm discovers these clusters, while achieving the optimal asymptotic regret rate within each one. Through experiments on several real-world datasets, we demonstrate the performance of proposed algorithms compared to the state-of-the-art.",http://proceedings.mlr.press/v48/korda16.html,http://proceedings.mlr.press/v48/korda16.pdf,ICML
1978,2016,Binary embeddings with structured hashed projections,"Anna Choromanska,         Krzysztof Choromanski,         Mariusz Bojarski,         Tony Jebara,         Sanjiv Kumar,         Yann LeCun","We consider the hashing mechanism for constructing binary embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudo-random projection is described by a matrix, where not all entries are independent random variables but instead a fixed “budget of randomness” is distributed across the matrix. Such matrices can be edfficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (i.e. number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input high-dimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier.",http://proceedings.mlr.press/v48/choromanska16.html,http://proceedings.mlr.press/v48/choromanska16.pdf,ICML
1979,2016,Learning Convolutional Neural Networks for Graphs,"Mathias Niepert,         Mohamed Ahmed,         Konstantin Kutzkov","Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.",http://proceedings.mlr.press/v48/niepert16.html,http://proceedings.mlr.press/v48/niepert16.pdf,ICML
1980,2016,One-Shot Generalization in Deep Generative Models,"Danilo Rezende,          Shakir,         Ivo Danihelka,         Karol Gregor,         Daan Wierstra","Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples—having seen new examples just once—providing an important class of general-purpose models for one-shot machine learning.",http://proceedings.mlr.press/v48/rezende16.html,http://proceedings.mlr.press/v48/rezende16.pdf,ICML
1981,2016,Variable Elimination in the Fourier Domain,"Yexiang Xue,         Stefano Ermon,         Ronan Le Bras,          Carla,         Bart Selman","The ability to represent complex high dimensional probability distributions in a compact form is one of the key insights in the field of graphical models. Factored representations are ubiquitous in machine learning and lead to major computational advantages. We explore a different type of compact representation based on discrete Fourier representations, complementing the classical approach based on conditional independencies. We show that a large class of probabilistic graphical models have a compact Fourier representation. This theoretical result opens up an entirely new way of approximating a probability distribution. We demonstrate the significance of this approach by applying it to the variable elimination algorithm. Compared with the traditional bucket representation and other approximate inference algorithms, we obtain significant improvements.",http://proceedings.mlr.press/v48/xue16.html,http://proceedings.mlr.press/v48/xue16.pdf,ICML
1982,2016,Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors,"Christos Louizos,         Max Welling","We introduce a variational Bayesian neural network where the parameters are governed via a probability distribution on random matrices. Specifically, we employ a matrix variate Gaussian (Gupta & Nagar ’99) parameter posterior distribution where we explicitly model the covariance among the input and output dimensions of each layer. Furthermore, with approximate covariance matrices we can achieve a more efficient way to represent those correlations that is also cheaper than fully factorized parameter posteriors. We further show that with the “local reprarametrization trick"" (Kingma & Welling ’15) on this posterior distribution we arrive at a Gaussian Process (Rasmussen ’06) interpretation of the hidden units in each layer and we, similarly with (Gal & Ghahramani ’15), provide connections with deep Gaussian processes. We continue in taking advantage of this duality and incorporate “pseudo-data” (Snelson & Ghahramani ’05) in our model, which in turn allows for more efficient posterior sampling while maintaining the properties of the original model. The validity of the proposed approach is verified through extensive experiments.",http://proceedings.mlr.press/v48/louizos16.html,http://proceedings.mlr.press/v48/louizos16.pdf,ICML
1983,2016,Community Recovery in Graphs with Locality,"Yuxin Chen,         Govinda Kamath,         Changho Suh,         David Tse","Motivated by applications in domains such as social networks and computational biology, we study the problem of community recovery in graphs with locality. In this problem, pairwise noisy measurements of whether two nodes are in the same community or different communities come mainly or exclusively from nearby nodes rather than uniformly sampled between all node pairs, as in most existing models. We present two algorithms that run nearly linearly in the number of measurements and which achieve the information limits for exact recovery.",http://proceedings.mlr.press/v48/chena16.html,http://proceedings.mlr.press/v48/chena16.pdf,ICML
1984,2016,Rich Component Analysis,"Rong Ge,         James Zou","In many settings, we have multiple data sets (also called views) that capture different and overlapping aspects of the same phenomenon. We are often interested in finding patterns that are unique to one or to a subset of the views. For example, we might have one set of molecular observations and one set of physiological observations on the same group of individuals, and we want to quantify molecular patterns that are uncorrelated with physiology. Despite being a common problem, this is highly challenging when the correlations come from complex distributions. In this paper, we develop the general framework of Rich Component Analysis (RCA) to model settings where the observations from different views are driven by different sets of latent components, and each component can be a complex, high-dimensional distribution. We introduce algorithms based on cumulant extraction that provably learn each of the components without having to model the other components. We show how to integrate RCA with stochastic gradient descent into a meta-algorithm for learning general models, and demonstrate substantial improvement in accuracy on several synthetic and real datasets in both supervised and unsupervised tasks. Our method makes it possible to learn latent variable models when we don’t have samples from the true model but only samples after complex perturbations.",http://proceedings.mlr.press/v48/gea16.html,http://proceedings.mlr.press/v48/gea16.pdf,ICML
1985,2016,Learning to Filter with Predictive State Inference Machines,"Wen Sun,         Arun Venkatraman,         Byron Boots,         J.Andrew Bagnell","Latent state space models are a fundamental and widely used tool for modeling dynamical systems. However, they are difficult to learn from data and learned models often lack performance guarantees on inference tasks such as filtering and prediction. In this work, we present the PREDICTIVE STATE INFERENCE MACHINE (PSIM), a data-driven method that considers the inference procedure on a dynamical system as a composition of predictors. The key idea is that rather than first learning a latent state space model, and then using the learned model for inference, PSIM directly learns predictors for inference in predictive state space. We provide theoretical guarantees for inference, in both realizable and agnostic settings, and showcase practical performance on a variety of simulated and real world robotics benchmarks.",http://proceedings.mlr.press/v48/sun16.html,http://proceedings.mlr.press/v48/sun16.pdf,ICML
1986,2016,Compressive Spectral Clustering,"Nicolas Tremblay,         Gilles Puy,         Remi Gribonval,         Pierre Vandergheynst","Spectral clustering has become a popular technique due to its high performance in many contexts. It comprises three main steps: create a similarity graph between N objects to cluster, compute the first k eigenvectors of its Laplacian matrix to define a feature vector for each object, and run k-means on these features to separate objects into k classes. Each of these three steps becomes computationally intensive for large N and/or k. We propose to speed up the last two steps based on recent results in the emerging field of graph signal processing: graph filtering of random signals, and random sampling of bandlimited graph signals. We prove that our method, with a gain in computation time that can reach several orders of magnitude, is in fact an approximation of spectral clustering, for which we are able to control the error. We test the performance of our method on artificial and real-world network data.",http://proceedings.mlr.press/v48/tremblay16.html,http://proceedings.mlr.press/v48/tremblay16.pdf,ICML
1987,2016,Adaptive Sampling for SGD by Exploiting Side Information,Siddharth Gopal,"This paper proposes a new mechanism for sampling training instances for stochastic gradient descent (SGD) methods by exploiting any side-information associated with the instances (for e.g. class-labels) to improve convergence. Previous methods have either relied on sampling from a distribution defined over training instances or from a static distribution that fixed before training. This results in two problems a) any distribution that is set apriori is independent of how the optimization progresses and b) maintaining a distribution over individual instances could be infeasible in large-scale scenarios. In this paper, we exploit the side information associated with the instances to tackle both problems. More specifically, we maintain a distribution over classes (instead of individual instances) that is adaptively estimated during the course of optimization to give the maximum reduction in the variance of the gradient. Intuitively, we sample more from those regions in space that have a \textitlarger gradient contribution. Our experiments on highly multiclass datasets show that our proposal converge significantly faster than existing techniques.",http://proceedings.mlr.press/v48/gopal16.html,http://proceedings.mlr.press/v48/gopal16.pdf,ICML
1988,2016,Robust Principal Component Analysis with Side Information,"Kai-Yang Chiang,         Cho-Jui Hsieh,         Inderjit Dhillon","The robust principal component analysis (robust PCA) problem has been considered in many machine learning applications, where the goal is to decompose the data matrix as a low rank part plus a sparse residual. While current approaches are developed by only considering the low rank plus sparse structure, in many applications, side information of row and/or column entities may also be given, and it is still unclear to what extent could such information help robust PCA. Thus, in this paper, we study the problem of robust PCA with side information, where both prior structure and features of entities are exploited for recovery. We propose a convex problem to incorporate side information in robust PCA and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions. In particular, our guarantee suggests that a substantial amount of low rank matrices, which cannot be recovered by standard robust PCA, become recoverable by our proposed method. The result theoretically justifies the effectiveness of features in robust PCA. In addition, we conduct synthetic experiments as well as a real application on noisy image classification to show that our method also improves the performance in practice by exploiting side information.",http://proceedings.mlr.press/v48/chiang16.html,http://proceedings.mlr.press/v48/chiang16.pdf,ICML
1989,2016,"No Oops, You Won’t Do It Again: Mechanisms for Self-correction in Crowdsourcing","Nihar Shah,         Dengyong Zhou","Crowdsourcing is a very popular means of obtaining the large amounts of labeled data that modern machine learning methods require. Although cheap and fast to obtain, crowdsourced labels suffer from significant amounts of error, thereby degrading the performance of downstream machine learning tasks. With the goal of improving the quality of the labeled data, we seek to mitigate the many errors that occur due to silly mistakes or inadvertent errors by crowdsourcing workers. We propose a two-stage setting for crowdsourcing where the worker first answers the questions, and is then allowed to change her answers after looking at a (noisy) reference answer. We mathematically formulate this process and develop mechanisms to incentivize workers to act appropriately. Our mathematical guarantees show that our mechanism incentivizes the workers to answer honestly in both stages, and refrain from answering randomly in the first stage or simply copying in the second. Numerical experiments reveal a significant boost in performance that such ""self-correction"" can provide when using crowdsourcing to train machine learning algorithms.",http://proceedings.mlr.press/v48/shaha16.html,http://proceedings.mlr.press/v48/shaha16.pdf,ICML
1990,2016,Doubly Decomposing Nonparametric Tensor Regression,"Masaaki Imaizumi,         Kohei Hayashi","Nonparametric extension of tensor regression is proposed. Nonlinearity in a high-dimensional tensor space is broken into simple local functions by incorporating low-rank tensor decomposition. Compared to naive nonparametric approaches, our formulation considerably improves the convergence rate of estimation while maintaining consistency with the same function class under specific conditions. To estimate local functions, we develop a Bayesian estimator with the Gaussian process prior. Experimental results show its theoretical properties and high performance in terms of predicting a summary statistic of a real complex network.",http://proceedings.mlr.press/v48/imaizumi16.html,http://proceedings.mlr.press/v48/imaizumi16.pdf,ICML
1991,2016,Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks,"Devansh Arpit,         Yingbo Zhou,         Bhargava Kota,         Venu Govindaraju","While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks– \textitInternal Covariate Shift– the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size  1  during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \textitNormalization Propagation. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers.",http://proceedings.mlr.press/v48/arpitb16.html,http://proceedings.mlr.press/v48/arpitb16.pdf,ICML
1992,2016,Hierarchical Span-Based Conditional Random Fields for Labeling and Segmenting Events in Wearable Sensor Data Streams,"Roy Adams,         Nazir Saleheen,         Edison Thomaz,         Abhinav Parate,         Santosh Kumar,         Benjamin Marlin","The field of mobile health (mHealth) has the potential to yield new insights into health and behavior through the analysis of continuously recorded data from wearable health and activity sensors. In this paper, we present a hierarchical span-based conditional random field model for the key problem of jointly detecting discrete events in such sensor data streams and segmenting these events into high-level activity sessions. Our model includes higher-order cardinality factors and inter-event duration factors to capture domain-specific structure in the label space. We show that our model supports exact MAP inference in quadratic time via dynamic programming, which we leverage to perform learning in the structured support vector machine framework. We apply the model to the problems of smoking and eating detection using four real data sets. Our results show statistically significant improvements in segmentation performance relative to a hierarchical pairwise CRF.",http://proceedings.mlr.press/v48/adams16.html,http://proceedings.mlr.press/v48/adams16.pdf,ICML
1993,2016,Deep Gaussian Processes for Regression using Approximate Expectation Propagation,"Thang Bui,         Daniel Hernandez-Lobato,         Jose Hernandez-Lobato,         Yingzhen Li,         Richard Turner","Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks.",http://proceedings.mlr.press/v48/bui16.html,http://proceedings.mlr.press/v48/bui16.pdf,ICML
1994,2016,Scalable Discrete Sampling as a Multi-Armed Bandit Problem,"Yutian Chen,         Zoubin Ghahramani","Drawing a sample from a discrete distribution is one of the building components for Monte Carlo methods. Like other sampling algorithms, discrete sampling suffers from the high computational burden in large-scale inference problems. We study the problem of sampling a discrete random variable with a high degree of dependency that is typical in large-scale Bayesian inference and graphical models, and propose an efficient approximate solution with a subsampling approach. We make a novel connection between the discrete sampling and Multi-Armed Bandits problems with a finite reward population and provide three algorithms with theoretical guarantees. Empirical evaluations show the robustness and efficiency of the approximate algorithms in both synthetic and real-world large-scale problems.",http://proceedings.mlr.press/v48/chenb16.html,http://proceedings.mlr.press/v48/chenb16.pdf,ICML
1995,2016,A Kronecker-factored approximate Fisher matrix for convolution layers,"Roger Grosse,         James Martens","Second-order optimization methods such as natural gradient descent have the potential to speed up training of neural networks by correcting for the curvature of the loss function. Unfortunately, the exact natural gradient is impractical to compute for large models, and most approximations either require an expensive iterative procedure or make crude approximations to the curvature. We present Kronecker Factors for Convolution (KFC), a tractable approximation to the Fisher matrix for convolutional networks based on a structured probabilistic model for the distribution over backpropagated derivatives. Similarly to the recently proposed Kronecker-Factored Approximate Curvature (K-FAC), each block of the approximate Fisher matrix decomposes as the Kronecker product of small matrices, allowing for efficient inversion. KFC captures important curvature information while still yielding comparably efficient updates to stochastic gradient descent (SGD). We show that the updates are invariant to commonly used reparameterizations, such as centering of the activations. In our experiments, approximate natural gradient descent with KFC was able to train convolutional networks several times faster than carefully tuned SGD. Furthermore, it was able to train the networks in 10-20 times fewer iterations than SGD, suggesting its potential applicability in a distributed setting.",http://proceedings.mlr.press/v48/grosse16.html,http://proceedings.mlr.press/v48/grosse16.pdf,ICML
1996,2016,"Control of Memory, Active Perception, and Action in Minecraft","Junhyuk Oh,         Valliappa Chockalingam,          Satinder,         Honglak Lee","In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.",http://proceedings.mlr.press/v48/oh16.html,http://proceedings.mlr.press/v48/oh16.pdf,ICML
1997,2016,Metadata-conscious anonymous messaging,"Giulia Fanti,         Peter Kairouz,         Sewoong Oh,         Kannan Ramchandran,         Pramod Viswanath","Anonymous messaging platforms like Whisper and Yik Yak allow users to spread messages over a network (e.g., a social network) without revealing message authorship to other users. The spread of messages on these platforms can be modeled by a diffusion process over a graph. Recent advances in network analysis have revealed that such diffusion processes are vulnerable to author deanonymization by adversaries with access to metadata, such as timing information. In this work, we ask the fundamental question of how to propagate anonymous messages over a graph to make it difficult for adversaries to infer the source. In particular, we study the performance of a message propagation protocol called adaptive diffusion introduced in (Fanti et al., 2015). We prove that when the adversary has access to metadata at a fraction of corrupted graph nodes, adaptive diffusion achieves asymptotically optimal source-hiding and significantly outperforms standard diffusion. We further demonstrate empirically that adaptive diffusion hides the source effectively on real social networks.",http://proceedings.mlr.press/v48/fanti16.html,http://proceedings.mlr.press/v48/fanti16.pdf,ICML
1998,2016,Analysis of Deep Neural Networks with Extended Data Jacobian Matrix,"Shengjie Wang,         Abdel-rahman Mohamed,         Rich Caruana,         Jeff Bilmes,         Matthai Plilipose,         Matthew Richardson,         Krzysztof Geras,         Gregor Urban,         Ozlem Aslan","Deep neural networks have achieved great successes on various machine learning tasks, however, there are many open fundamental questions to be answered. In this paper, we tackle the problem of quantifying the quality of learned wights of different networks with possibly different architectures, going beyond considering the final classification error as the only metric. We introduce \emphExtended Data Jacobian Matrix to help analyze properties of networks of various structures, finding that, the spectrum of the extended data jacobian matrix is a strong discriminating factor for networks of different structures and performance. Based on such observation, we propose a novel regularization method, which manages to improve the network performance comparably to dropout, which in turn verifies the observation.",http://proceedings.mlr.press/v48/wanga16.html,http://proceedings.mlr.press/v48/wanga16.pdf,ICML
1999,2016,Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization,"Chelsea Finn,         Sergey Levine,         Pieter Abbeel","Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.",http://proceedings.mlr.press/v48/finn16.html,http://proceedings.mlr.press/v48/finn16.pdf,ICML
2000,2016,Square Root Graphical Models: Multivariate Generalizations of Univariate Exponential Families that Permit Positive Dependencies,"David Inouye,         Pradeep Ravikumar,         Inderjit Dhillon","We develop Square Root Graphical Models (SQR), a novel class of parametric graphical models that provides multivariate generalizations of univariate exponential family distributions. Previous multivariate graphical models [Yang et al. 2015] did not allow positive dependencies for the exponential and Poisson generalizations. However, in many real-world datasets, variables clearly have positive dependencies. For example, the airport delay time in New York—modeled as an exponential distribution—is positively related to the delay time in Boston. With this motivation, we give an example of our model class derived from the univariate exponential distribution that allows for almost arbitrary positive and negative dependencies with only a mild condition on the parameter matrix—a condition akin to the positive definiteness of the Gaussian covariance matrix. Our Poisson generalization allows for both positive and negative dependencies without any constraints on the parameter values. We also develop parameter estimation methods using node-wise regressions with \ell_1 regularization and likelihood approximation methods using sampling. Finally, we demonstrate our exponential generalization on a synthetic dataset and a real-world dataset of airport delay times.",http://proceedings.mlr.press/v48/inouye16.html,http://proceedings.mlr.press/v48/inouye16.pdf,ICML
2001,2016,Why Regularized Auto-Encoders learn Sparse Representation?,"Devansh Arpit,         Yingbo Zhou,         Hung Ngo,         Venu Govindaraju","Sparse distributed representation is the key to learning useful features in deep learning algorithms, because not only it is an efficient mode of data representation, but also – more importantly – it captures the generation process of most real world data. While a number of regularized auto-encoders (AE) enforce sparsity explicitly in their learned representation and others don’t, there has been little formal analysis on what encourages sparsity in these models in general. Our objective is to formally study this general problem for regularized auto-encoders. We provide sufficient conditions on both regularization and activation functions that encourage sparsity. We show that multiple popular models (de-noising and contractive auto encoders, e.g.) and activations (rectified linear and sigmoid, e.g.) satisfy these conditions; thus, our conditions help explain sparsity in their learned representation. Thus our theoretical and empirical analysis together shed light on the properties of regularization/activation that are conductive to sparsity and unify a number of existing auto-encoder models and activation functions under the same analytical framework.",http://proceedings.mlr.press/v48/arpita16.html,http://proceedings.mlr.press/v48/arpita16.pdf,ICML
2002,2016,Markov-modulated Marked Poisson Processes for Check-in Data,"Jiangwei Pan,         Vinayak Rao,         Pankaj Agarwal,         Alan Gelfand","We develop continuous-time probabilistic models to study trajectory data consisting of times and locations of user “check-ins”. We model the data as realizations of a marked point process, with intensity and mark-distribution modulated by a latent Markov jump process (MJP). We also include user-heterogeneity in our model by assigning each user a vector of “preferred locations”. Our model extends latent Dirichlet allocation by dropping the bag-of-words assumption and operating in continuous time. We show how an appropriate choice of priors allows efficient posterior inference. Our experiments demonstrate the usefulness of our approach by comparing with various baselines on a variety of tasks.",http://proceedings.mlr.press/v48/pana16.html,http://proceedings.mlr.press/v48/pana16.pdf,ICML
2003,2016,Robust Random Cut Forest Based Anomaly Detection on Streams,"Sudipto Guha,         Nina Mishra,         Gourav Roy,         Okke Schrijvers","In this paper we focus on the anomaly detection problem for dynamic data streams through the lens of random cut forests. We investigate a robust random cut data structure that can be used as a sketch or synopsis of the input stream. We provide a plausible definition of non-parametric anomalies based on the influence of an unseen point on the remainder of the data, i.e., the externality imposed by that point. We show how the sketch can be efficiently updated in a dynamic data stream. We demonstrate the viability of the algorithm on publicly available real data.",http://proceedings.mlr.press/v48/guha16.html,http://proceedings.mlr.press/v48/guha16.pdf,ICML
2004,2016,Dueling Network Architectures for Deep Reinforcement Learning,"Ziyu Wang,         Tom Schaul,         Matteo Hessel,         Hado Hasselt,         Marc Lanctot,         Nando Freitas","In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.",http://proceedings.mlr.press/v48/wangf16.html,http://proceedings.mlr.press/v48/wangf16.pdf,ICML
2005,2016,Continuous Deep Q-Learning with Model-based Acceleration,"Shixiang Gu,         Timothy Lillicrap,         Ilya Sutskever,         Sergey Levine","Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized advantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.",http://proceedings.mlr.press/v48/gu16.html,http://proceedings.mlr.press/v48/gu16.pdf,ICML
2006,2016,The Knowledge Gradient for Sequential Decision Making with Stochastic Binary Feedbacks,"Yingfei Wang,         Chu Wang,         Warren Powell","We consider the problem of sequentially making decisions that are rewarded by “successes” and “failures” which can be predicted through an unknown relationship that depends on a partially controllable vector of attributes for each instance. The learner takes an active role in selecting samples from the instance pool. The goal is to maximize the probability of success, either after the offline training phase or minimizing regret in online learning. Our problem is motivated by real-world applications where observations are time consuming and/or expensive. With the adaptation of an online Bayesian linear classifier, we develop a knowledge-gradient type policy to guide the experiment by maximizing the expected value of information of labeling each alternative, in order to reduce the number of expensive physical experiments. We provide a finite-time analysis of the estimated error and demonstrate the performance of the proposed algorithm on both synthetic problems and benchmark UCI datasets.",http://proceedings.mlr.press/v48/wangb16.html,http://proceedings.mlr.press/v48/wangb16.pdf,ICML
2007,2016,From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification,"Andre Martins,         Ramon Astudillo","We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.",http://proceedings.mlr.press/v48/martins16.html,http://proceedings.mlr.press/v48/martins16.pdf,ICML
2008,2016,A Deep Learning Approach to Unsupervised Ensemble Learning,"Uri Shaham,         Xiuyuan Cheng,         Omer Dror,         Ariel Jaffe,         Boaz Nadler,         Joseph Chang,         Yuval Kluger","We show how deep learning methods can be applied in the context of crowdsourcing and unsupervised ensemble learning. First, we prove that the popular model of Dawid and Skene, which assumes that all classifiers are conditionally independent, is \em equivalent to a Restricted Boltzmann Machine (RBM) with a single hidden node. Hence, under this model, the posterior probabilities of the true labels can be instead estimated via a trained RBM. Next, to address the more general case, where classifiers may strongly violate the conditional independence assumption, we propose to apply RBM-based Deep Neural Net (DNN). Experimental results on various simulated and real-world datasets demonstrate that our proposed DNN approach outperforms other state-of-the-art methods, in particular when the data violates the conditional independence assumption.",http://proceedings.mlr.press/v48/shaham16.html,http://proceedings.mlr.press/v48/shaham16.pdf,ICML
2009,2016,Doubly Robust Off-policy Value Evaluation for Reinforcement Learning,"Nan Jiang,         Lihong Li","We study the problem of off-policy value evaluation in reinforcement learning (RL), where one aims to estimate the value of a new policy based on data collected by a different policy. This problem is often a critical step when applying RL to real-world problems. Despite its importance, existing general methods either have uncontrolled bias or suffer high variance. In this work, we extend the doubly robust estimator for bandits to sequential decision-making problems, which gets the best of both worlds: it is guaranteed to be unbiased and can have a much lower variance than the popular importance sampling estimators. We demonstrate the estimator’s accuracy in several benchmark problems, and illustrate its use as a subroutine in safe policy improvement. We also provide theoretical results on the inherent hardness of the problem, and show that our estimator can match the lower bound in certain scenarios.",http://proceedings.mlr.press/v48/jiang16.html,http://proceedings.mlr.press/v48/jiang16.pdf,ICML
2010,2016,Meta–Gradient Boosted Decision Tree Model for Weight and Target Learning,"Yury Ustinovskiy,         Valentina Fedorova,         Gleb Gusev,         Pavel Serdyukov","Labeled training data is an essential part of any supervised machine learning framework. In practice, there is a trade-off between the quality of a label and its cost. In this paper, we consider a problem of learning to rank on a large-scale dataset with low-quality relevance labels aiming at maximizing the quality of a trained ranker on a small validation dataset with high-quality ground truth relevance labels. Motivated by the classical Gauss-Markov theorem for the linear regression problem, we formulate the problems of (1) reweighting training instances and (2) remapping learning targets. We propose meta–gradient decision tree learning framework for optimizing weight and target functions by applying gradient-based hyperparameter optimization. Experiments on a large-scale real-world dataset demonstrate that we can significantly improve state-of-the-art machine-learning algorithms by incorporating our framework.",http://proceedings.mlr.press/v48/ustinovskiy16.html,http://proceedings.mlr.press/v48/ustinovskiy16.pdf,ICML
2011,2016,"Train faster, generalize better: Stability of stochastic gradient descent","Moritz Hardt,         Ben Recht,         Yoram Singer","We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit.",http://proceedings.mlr.press/v48/hardt16.html,http://proceedings.mlr.press/v48/hardt16.pdf,ICML
2012,2016,On the Analysis of Complex Backup Strategies in Monte Carlo Tree Search,"Piyush Khandelwal,         Elad Liebman,         Scott Niekum,         Peter Stone","Over the past decade, Monte Carlo Tree Search (MCTS) and specifically Upper Confidence Bound in Trees (UCT) have proven to be quite effective in large probabilistic planning domains. In this paper, we focus on how values are backpropagated in the MCTS tree, and apply complex return strategies from the Reinforcement Learning (RL) literature to MCTS, producing 4 new MCTS variants. We demonstrate that in some probabilistic planning benchmarks from the International Planning Competition (IPC), selecting a MCTS variant with a backup strategy different from Monte Carlo averaging can lead to substantially better results. We also propose a hypothesis for why different backup strategies lead to different performance in particular environments, and manipulate a carefully structured grid-world domain to provide empirical evidence supporting our hypothesis.",http://proceedings.mlr.press/v48/khandelwal16.html,http://proceedings.mlr.press/v48/khandelwal16.pdf,ICML
2013,2016,A Subspace Learning Approach for High Dimensional Matrix Decomposition with Efficient Column/Row Sampling,"Mostafa Rahmani,         Geroge Atia","This paper presents a new randomized approach to high-dimensional low rank (LR) plus sparse matrix decomposition. For a data matrix D ∈R^N_1 \times N_2, the complexity of conventional decomposition methods is O(N_1 N_2 r), which limits their usefulness in big data settings (r is the rank of the LR component). In addition, the existing randomized approaches rely for the most part on uniform random sampling, which may be inefficient for many real world data matrices. The proposed subspace learning based approach recovers the LR component using only a small subset of the columns/rows of data and reduces complexity to O(\max(N_1,N_2) r^2). Even when the columns/rows are sampled uniformly at random, the sufficient number of sampled columns/rows is shown to be roughly O(r μ), where μis the coherency parameter of the LR component. In addition, efficient sampling algorithms are proposed to address the problem of column/row sampling from structured data.",http://proceedings.mlr.press/v48/rahmani16.html,http://proceedings.mlr.press/v48/rahmani16.pdf,ICML
2014,2016,PAC learning of Probabilistic Automaton based on the Method of Moments,"Hadrien Glaude,         Olivier Pietquin","Probabilitic Finite Automata (PFA) are generative graphical models that define distributions with latent variables over finite sequences of symbols, a.k.a. stochastic languages. Traditionally, unsupervised learning of PFA is performed through algorithms that iteratively improves the likelihood like the Expectation-Maximization (EM) algorithm. Recently, learning algorithms based on the so-called Method of Moments (MoM) have been proposed as a much faster alternative that comes with PAC-style guarantees. However, these algorithms do not ensure the learnt automata to model a proper distribution, limiting their applicability and preventing them to serve as an initialization to iterative algorithms. In this paper, we propose a new MoM-based algorithm with PAC-style guarantees that learns automata defining proper distributions. We assess its performances on synthetic problems from the PAutomaC challenge and real datasets extracted from Wikipedia against previous MoM-based algorithms and EM algorithm.",http://proceedings.mlr.press/v48/glaude16.html,http://proceedings.mlr.press/v48/glaude16.pdf,ICML
2015,2016,Minimum Regret Search for Single- and Multi-Task Optimization,Jan Hendrik Metzen,"We propose minimum regret search (MRS), a novel acquisition function for Bayesian optimization. MRS bears similarities with information-theoretic approaches such as entropy search (ES). However, while ES aims in each query at maximizing the information gain with respect to the global maximum, MRS aims at minimizing the expected simple regret of its ultimate recommendation for the optimum. While empirically ES and MRS perform similar in most of the cases, MRS produces fewer outliers with high simple regret than ES. We provide empirical results both for a synthetic single-task optimization problem as well as for a simulated multi-task robotic control problem.",http://proceedings.mlr.press/v48/metzen16.html,http://proceedings.mlr.press/v48/metzen16.pdf,ICML
2016,2016,Online Learning with Feedback Graphs Without the Graphs,"Alon Cohen,         Tamir Hazan,         Tomer Koren","We study an online learning framework introduced by Mannor and Shamir (2011) in which the feedback is specified by a graph, in a setting where the graph may vary from round to round and is \emphnever fully revealed to the learner. We show a large gap between the adversarial and the stochastic cases. In the adversarial case, we prove that even for dense feedback graphs, the learner cannot improve upon a trivial regret bound obtained by ignoring any additional feedback besides her own loss. In contrast, in the stochastic case we give an algorithm that achieves \widetildeΘ(\sqrtαT) regret over T rounds, provided that the independence numbers of the hidden feedback graphs are at most α. We also extend our results to a more general feedback model, in which the learner does not necessarily observe her own loss, and show that, even in simple cases, concealing the feedback graphs might render the problem unlearnable.",http://proceedings.mlr.press/v48/cohena16.html,http://proceedings.mlr.press/v48/cohena16.pdf,ICML
2017,2016,Learning Representations for Counterfactual Inference,"Fredrik Johansson,         Uri Shalit,         David Sontag","Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, “Would this patient have lower blood sugar had she received a different medication?"". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.",http://proceedings.mlr.press/v48/johansson16.html,http://proceedings.mlr.press/v48/johansson16.pdf,ICML
2018,2016,Learning privately from multiparty data,"Jihun Hamm,         Yingjun Cao,         Mikhail Belkin","Learning a classifier from private data distributed across multiple parties is an important problem that has many potential applications. How can we build an accurate and differentially private global classifier by combining locally-trained classifiers from different parties, without access to any party’s private data? We propose to transfer the “knowledge” of the local classifier ensemble by first creating labeled data from auxiliary unlabeled data, and then train a global differentially private classifier. We show that majority voting is too sensitive and therefore propose a new risk weighted by class probabilities estimated from the ensemble. Relative to a non-private solution, our private solution has a generalization error bounded by O(ε^-2 M^-2). This allows strong privacy without performance loss when the number of participating parties M is large, such as in crowdsensing applications. We demonstrate the performance of our framework with realistic tasks of activity recognition, network intrusion detection, and malicious URL detection.",http://proceedings.mlr.press/v48/hamm16.html,http://proceedings.mlr.press/v48/hamm16.pdf,ICML
2019,2016,Differentially Private Policy Evaluation,"Borja Balle,         Maziar Gomrokchi,         Doina Precup","We present the first differentially private algorithms for reinforcement learning, which apply to the task of evaluating a fixed policy. We establish two approaches for achieving differential privacy, provide a theoretical analysis of the privacy and utility of the two algorithms, and show promising results on simple empirical examples.",http://proceedings.mlr.press/v48/balle16.html,http://proceedings.mlr.press/v48/balle16.pdf,ICML
2020,2016,Non-negative Matrix Factorization under Heavy Noise,"Chiranjib Bhattacharya,         Navin Goyal,         Ravindran Kannan,         Jagdeep Pani","The Noisy Non-negative Matrix factorization (NMF) is: given a data matrix A (d x n), find non-negative matrices B;C (d x k, k x n respy.) so that A = BC +N, where N is a noise matrix. Existing polynomial time algorithms with proven error guarantees require EACH column N_⋅j to have l1 norm much smaller than ||(BC)_⋅j ||_1, which could be very restrictive. In important applications of NMF such as Topic Modeling as well as theoretical noise models (e.g. Gaussian with high sigma), almost EVERY column of N_.j violates this condition. We introduce the heavy noise model which only requires the average noise over large subsets of columns to be small. We initiate a study of Noisy NMF under the heavy noise model. We show that our noise model subsumes noise models of theoretical and practical interest (for e.g. Gaussian noise of maximum possible sigma). We then devise an algorithm TSVDNMF which under certain assumptions on B,C, solves the problem under heavy noise. Our error guarantees match those of previous algorithms. Our running time of O(k.(d+n)^2) is substantially better than the O(d.n^3) for the previous best. Our assumption on B is weaker than the “Separability” assumption made by all previous results. We provide empirical justification for our assumptions on C. We also provide the first proof of identifiability (uniqueness of B) for noisy NMF which is not based on separability and does not use hard to check geometric conditions. Our algorithm outperforms earlier polynomial time algorithms both in time and error, particularly in the presence of high noise.",http://proceedings.mlr.press/v48/bhattacharya16.html,http://proceedings.mlr.press/v48/bhattacharya16.pdf,ICML
2021,2016,Isotonic Hawkes Processes,"Yichen Wang,         Bo Xie,         Nan Du,         Le Song","Hawkes processes are powerful tools for modeling the mutual-excitation phenomena commonly observed in event data from a variety of domains, such as social networks, quantitative finance and healthcare records. The intensity function of a Hawkes process is typically assumed to be linear in the sum of triggering kernels, rendering it inadequate to capture nonlinear effects present in real-world data. To address this shortcoming, we propose an Isotonic-Hawkes process whose intensity function is modulated by an additional nonlinear link function. We also developed a novel iterative algorithm which learns both the nonlinear link function and other parameters provably. We showed that Isotonic-Hawkes processes can fit a variety of nonlinear patterns which cannot be captured by conventional Hawkes processes, and achieve superior empirical performance in real world applications.",http://proceedings.mlr.press/v48/wangg16.html,http://proceedings.mlr.press/v48/wangg16.pdf,ICML
2022,2016,On the Quality of the Initial Basin in Overspecified Neural Networks,"Itay Safran,         Ohad Shamir","Deep learning, in the form of artificial neural networks, has achieved remarkable practical success in recent years, for a variety of difficult machine learning applications. However, a theoretical explanation for this remains a major open problem, since training neural networks involves optimizing a highly non-convex objective function, and is known to be computationally hard in the worst case. In this work, we study the \emphgeometric structure of the associated non-convex objective function, in the context of ReLU networks and starting from a random initialization of the network parameters. We identify some conditions under which it becomes more favorable to optimization, in the sense of (i) High probability of initializing at a point from which there is a monotonically decreasing path to a global minimum; and (ii) High probability of initializing at a basin (suitably defined) with a small minimal objective value. A common theme in our results is that such properties are more likely to hold for larger (“overspecified”) networks, which accords with some recent empirical and theoretical observations.",http://proceedings.mlr.press/v48/safran16.html,http://proceedings.mlr.press/v48/safran16.pdf,ICML
2023,2016,Stochastic Optimization for Multiview Representation Learning using Partial Least Squares,"Raman Arora,         Poorya Mianjy,         Teodor Marinov","Partial Least Squares (PLS) is a ubiquitous statistical technique for bilinear factor analysis. It is used in many data analysis, machine learning, and information retrieval applications to model the covariance structure between a pair of data matrices. In this paper, we consider PLS for representation learning in a multiview setting where we have more than one view in data at training time. Furthermore, instead of framing PLS as a problem about a fixed given data set, we argue that PLS should be studied as a stochastic optimization problem, especially in a ""big data"" setting, with the goal of optimizing a population objective based on sample. This view suggests using Stochastic Approximation (SA) approaches, such as Stochastic Gradient Descent (SGD) and enables a rigorous analysis of their benefits. In this paper, we develop SA approaches to PLS and provide iteration complexity bounds for the proposed algorithms.",http://proceedings.mlr.press/v48/aroraa16.html,http://proceedings.mlr.press/v48/aroraa16.pdf,ICML
2024,2016,Differential Geometric Regularization for Supervised Learning of Classifiers,"Qinxun Bai,         Steven Rosenberg,         Zheng Wu,         Stan Sclaroff","We study the problem of supervised learning for both binary and multiclass classification from a unified geometric perspective. In particular, we propose a geometric regularization technique to find the submanifold corresponding to an estimator of the class probability P(y|\vec x). The regularization term measures the volume of this submanifold, based on the intuition that overfitting produces rapid local oscillations and hence large volume of the estimator. This technique can be applied to regularize any classification function that satisfies two requirements: firstly, an estimator of the class probability can be obtained; secondly, first and second derivatives of the class probability estimator can be calculated. In experiments, we apply our regularization technique to standard loss functions for classification, our RBF-based implementation compares favorably to widely used regularization methods for both binary and multiclass classification.",http://proceedings.mlr.press/v48/baia16.html,http://proceedings.mlr.press/v48/baia16.pdf,ICML
2025,2016,Markov Latent Feature Models,"Aonan Zhang,         John Paisley","We introduce Markov latent feature models (MLFM), a sparse latent feature model that arises naturally from a simple sequential construction. The key idea is to interpret each state of a sequential process as corresponding to a latent feature, and the set of states visited between two null-state visits as picking out features for an observation. We show that, given some natural constraints, we can represent this stochastic process as a mixture of recurrent Markov chains. In this way we can perform correlated latent feature modeling for the sparse coding problem. We demonstrate two cases in which we define finite and infinite latent feature models constructed from first-order Markov chains, and derive their associated scalable inference algorithms. We show empirical results on a genome analysis task and an image denoising task.",http://proceedings.mlr.press/v48/zhangf16.html,http://proceedings.mlr.press/v48/zhangf16.pdf,ICML
2026,2016,Factored Temporal Sigmoid Belief Networks for Sequence Learning,"Jiaming Song,         Zhe Gan,         Lawrence Carin","Deep conditional generative models are developed to simultaneously learn the temporal dependencies of multiple sequences. The model is designed by introducing a three-way weight tensor to capture the multiplicative interactions between side information and sequences. The proposed model builds on the Temporal Sigmoid Belief Network (TSBN), a sequential stack of Sigmoid Belief Networks (SBNs). The transition matrices are further factored to reduce the number of parameters and improve generalization. When side information is not available, a general framework for semi-supervised learning based on the proposed model is constituted, allowing robust sequence classification. Experimental results show that the proposed approach achieves state-of-the-art predictive and classification performance on sequential data, and has the capacity to synthesize sequences, with controlled style transitioning and blending.",http://proceedings.mlr.press/v48/songa16.html,http://proceedings.mlr.press/v48/songa16.pdf,ICML
2027,2016,Beyond Parity Constraints: Fourier Analysis of Hash Functions for Inference,"Tudor Achim,         Ashish Sabharwal,         Stefano Ermon","Random projections have played an important role in scaling up machine learning and data mining algorithms. Recently they have also been applied to probabilistic inference to estimate properties of high-dimensional distributions; however, they all rely on the same class of projections based on universal hashing. We provide a general framework to analyze random projections which relates their statistical properties to their Fourier spectrum, which is a well-studied area of theoretical computer science. Using this framework we introduce two new classes of hash functions for probabilistic inference and model counting that show promising performance on synthetic and real-world benchmarks.",http://proceedings.mlr.press/v48/achim16.html,http://proceedings.mlr.press/v48/achim16.pdf,ICML
2028,2016,Primal-Dual Rates and Certificates,"Celestine Dünner,         Simone Forte,         Martin Takac,         Martin Jaggi","We propose an algorithm-independent framework to equip existing optimization methods with primal-dual certificates. Such certificates and corresponding rate of convergence guarantees are important for practitioners to diagnose progress, in particular in machine learning applications. We obtain new primal-dual convergence rates, e.g., for the Lasso as well as many L1, Elastic Net, group Lasso and TV-regularized problems. The theory applies to any norm-regularized generalized linear model. Our approach provides efficiently computable duality gaps which are globally defined, without modifying the original problems in the region of interest.",http://proceedings.mlr.press/v48/dunner16.html,http://proceedings.mlr.press/v48/dunner16.pdf,ICML
2029,2016,No-Regret Algorithms for Heavy-Tailed Linear Bandits,"Andres Munoz Medina,         Scott Yang","We analyze the problem of linear bandits under heavy tailed noise. Most of of the work on linear bandits has been based on the assumption of bounded or sub-Gaussian noise. However, this assumption is often violated in common scenarios such as financial markets. We present two algorithms to tackle this problem: one based on dynamic truncation and one based on a median of means estimator. We show that, when the noise admits admits only a 1 + εmoment, these algorithms are still able to achieve regret in \widetildeO(T^\frac2 + ε2(1 + ε)) and \widetildeO(T^\frac1+ 2ε1 + 3 ε) respectively. In particular, they guarantee sublinear regret as long as the noise has finite variance. We also present empirical results showing that our algorithms achieve a better performance than the current state of the art for bounded noise when the L_∞bound on the noise is large yet the 1 + εmoment of the noise is small.",http://proceedings.mlr.press/v48/medina16.html,http://proceedings.mlr.press/v48/medina16.pdf,ICML
2030,2016,Asynchronous Methods for Deep Reinforcement Learning,"Volodymyr Mnih,         Adria Puigdomenech Badia,         Mehdi Mirza,         Alex Graves,         Timothy Lillicrap,         Tim Harley,         David Silver,         Koray Kavukcuoglu","We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",http://proceedings.mlr.press/v48/mniha16.html,http://proceedings.mlr.press/v48/mniha16.pdf,ICML
2031,2016,Meta-Learning with Memory-Augmented Neural Networks,"Adam Santoro,         Sergey Bartunov,         Matthew Botvinick,         Daan Wierstra,         Timothy Lillicrap","Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of ""one-shot learning."" Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.",http://proceedings.mlr.press/v48/santoro16.html,http://proceedings.mlr.press/v48/santoro16.pdf,ICML
2032,2016,How to Fake Multiply by a Gaussian Matrix,"Michael Kapralov,         Vamsi Potluru,         David Woodruff","Have you ever wanted to multiply an n \times d matrix X, with n ≫d, on the left by an m \times n matrix \tilde G of i.i.d. Gaussian random variables, but could not afford to do it because it was too slow? In this work we propose a new randomized m \times n matrix T, for which one can compute T ⋅X in only O(nnz(X)) + \tilde O(m^1.5 ⋅d^3) time, for which the total variation distance between the distributions T ⋅X and \tilde G ⋅X is as small as desired, i.e., less than any positive constant. Here nnz(X) denotes the number of non-zero entries of X. Assuming nnz(X) ≫m^1.5 ⋅d^3, this is a significant savings over the naïve O(nnz(X) m) time to compute \tilde G ⋅X. Moreover, since the total variation distance is small, we can provably use T ⋅X in place of \tilde G ⋅X in any application and have the same guarantees as if we were using \tilde G ⋅X, up to a small positive constant in error probability. We apply this transform to nonnegative matrix factorization (NMF) and support vector machines (SVM).",http://proceedings.mlr.press/v48/kapralov16.html,http://proceedings.mlr.press/v48/kapralov16.pdf,ICML
2033,2016,A Simple and Strongly-Local Flow-Based Method for Cut Improvement,"Nate Veldt,         David Gleich,         Michael Mahoney","Many graph-based learning problems can be cast as finding a good set of vertices nearby a seed set, and a powerful methodology for these problems is based on minimum cuts and maximum flows. We introduce and analyze a new method for locally-biased graph-based learning called SimpleLocal, which finds good conductance cuts near a set of seed vertices. An important feature of our algorithm is that it is strongly-local, meaning it does not need to explore the entire graph to find cuts that are locally optimal. This method is related to other strongly-local flow-based methods, but it enables a simple implementation. We also show how it achieves localization through an implicit l1-norm penalty term. As a flow-based method, our algorithm exhibits several advantages in terms of cut optimality and accurate identification of target regions in a graph. We demonstrate the power of SimpleLocal solving segmentation problems on a 467 million edge graph based on an MRI scan.",http://proceedings.mlr.press/v48/veldt16.html,http://proceedings.mlr.press/v48/veldt16.pdf,ICML
2034,2016,Optimal Classification with Multivariate Losses,"Nagarajan Natarajan,         Oluwasanmi Koyejo,         Pradeep Ravikumar,         Inderjit Dhillon","Multivariate loss functions are extensively employed in several prediction tasks arising in Information Retrieval. Often, the goal in the tasks is to minimize expected loss when retrieving relevant items from a presented set of items, where the expectation is with respect to the joint distribution over item sets. Our key result is that for most multivariate losses, the expected loss is provably optimized by sorting the items by the conditional probability of label being positive and then selecting top k items. Such a result was previously known only for the F-measure. Leveraging on the optimality characterization, we give an algorithm for estimating optimal predictions in practice with runtime quadratic in size of item sets for many losses. We provide empirical results on benchmark datasets, comparing the proposed algorithm to state-of-the-art methods for optimizing multivariate losses.",http://proceedings.mlr.press/v48/natarajan16.html,http://proceedings.mlr.press/v48/natarajan16.pdf,ICML
2035,2016,No penalty no tears: Least squares in high-dimensional linear models,"Xiangyu Wang,         David Dunson,         Chenlei Leng","Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms.",http://proceedings.mlr.press/v48/wange16.html,http://proceedings.mlr.press/v48/wange16.pdf,ICML
2036,2016,Extended and Unscented Kitchen Sinks,"Edwin Bonilla,         Daniel Steinberg,         Alistair Reid","We propose a scalable multiple-output generalization of unscented and extended Gaussian processes. These algorithms have been designed to handle general likelihood models by linearizing them using a Taylor series or the Unscented Transform in a variational inference framework. We build upon random feature approximations of Gaussian process covariance functions and show that, on small-scale single-task problems, our methods can attain similar performance as the original algorithms while having less computational cost. We also evaluate our methods at a larger scale on MNIST and on a seismic inversion which is inherently a multi-task problem.",http://proceedings.mlr.press/v48/bonilla16.html,http://proceedings.mlr.press/v48/bonilla16.pdf,ICML
2037,2016,On the Iteration Complexity of Oblivious First-Order Optimization Algorithms,"Yossi Arjevani,         Ohad Shamir","We consider a broad class of first-order optimization algorithms which are \emphoblivious, in the sense that their step sizes are scheduled regardless of the function under consideration, except for limited side-information such as smoothness or strong convexity parameters. With the knowledge of these two parameters, we show that any such algorithm attains an iteration complexity lower bound of Ω(\sqrtL/ε) for L-smooth convex functions, and \tildeΩ(\sqrtL/μ\ln(1/ε)) for L-smooth μ-strongly convex functions. These lower bounds are stronger than those in the traditional oracle model, as they hold independently of the dimension. To attain these, we abandon the oracle model in favor of a structure-based approach which builds upon a framework recently proposed in Arjevani et al. (2015). We further show that without knowing the strong convexity parameter, it is impossible to attain an iteration complexity better than \tildeΩ\sqrt(L/μ)\ln(1/ε). This result is then used to formalize an observation regarding L-smooth convex functions, namely, that the iteration complexity of algorithms employing time-invariant step sizes must be at least Ω(L/ε).",http://proceedings.mlr.press/v48/arjevani16.html,http://proceedings.mlr.press/v48/arjevani16.pdf,ICML
2038,2016,On the Power and Limits of Distance-Based Learning,"Periklis Papakonstantinou,         Jia Xu,         Guang Yang","We initiate the study of low-distortion finite metric embeddings in multi-class (and multi-label) classification where (i) both the space of input instances and the space of output classes have combinatorial metric structure and (ii) the concepts we wish to learn are low-distortion embeddings. We develop new geometric techniques and prove strong learning lower bounds. These provable limits hold even when we allow learners and classifiers to get advice by one or more experts. Our study overwhelmingly indicates that post-geometry assumptions are necessary in multi-class classification, as in natural language processing (NLP). Technically, the mathematical tools we developed in this work could be of independent interest to NLP. To the best of our knowledge, this is the first work which formally studies classification problems in combinatorial spaces. and where the concepts are low-distortion embeddings.",http://proceedings.mlr.press/v48/papakonstantinou16.html,http://proceedings.mlr.press/v48/papakonstantinou16.pdf,ICML
2039,2016,Efficient Algorithms for Adversarial Contextual Learning,"Vasilis Syrgkanis,         Akshay Krishnamurthy,         Robert Schapire","We provide the first oracle efficient sublinear regret algorithms for adversarial versions of the contextual bandit problem. In this problem, the learner repeatedly makes an action on the basis of a context and receives reward for the chosen action, with the goal of achieving reward competitive with a large class of policies. We analyze two settings: i) in the transductive setting the learner knows the set of contexts a priori, ii) in the small separator setting, there exists a small set of contexts such that any two policies behave differently on one of the contexts in the set. Our algorithms fall into the Follow-The-Perturbed-Leader family (Kalai and Vempala, 2005) and achieve regret O(T^3/4\sqrtK\log(N)) in the transductive setting and O(T^2/3 d^3/4 K\sqrt\log(N)) in the separator setting, where T is the number of rounds, K is the number of actions, N is the number of baseline policies, and d is the size of the separator. We actually solve the more general adversarial contextual semi-bandit linear optimization problem, whilst in the full information setting we address the even more general contextual combinatorial optimization. We provide several extensions and implications of our algorithms, such as switching regret and efficient learning with predictable sequences.",http://proceedings.mlr.press/v48/syrgkanis16.html,http://proceedings.mlr.press/v48/syrgkanis16.pdf,ICML
2040,2016,Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification,"Yuting Zhang,         Kibok Lee,         Honglak Lee","Unsupervised learning and supervised learning are key research topics in deep learning. However, as high-capacity supervised neural networks trained with a large amount of labels have achieved remarkable success in many computer vision tasks, the availability of large-scale labeled images reduced the significance of unsupervised learning. Inspired by the recent trend toward revisiting the importance of unsupervised learning, we investigate joint supervised and unsupervised learning in a large-scale setting by augmenting existing neural networks with decoding pathways for reconstruction. First, we demonstrate that the intermediate activations of pretrained large-scale classification networks preserve almost all the information of input images except a portion of local spatial details. Then, by end-to-end training of the entire augmented architecture with the reconstructive objective, we show improvement of the network performance for supervised tasks. We evaluate several variants of autoencoders, including the recently proposed “what-where"" autoencoder that uses the encoder pooling switches, to study the importance of the architecture design. Taking the 16-layer VGGNet trained under the ImageNet ILSVRC 2012 protocol as a strong baseline for image classification, our methods improve the validation-set accuracy by a noticeable margin.",http://proceedings.mlr.press/v48/zhangc16.html,http://proceedings.mlr.press/v48/zhangc16.pdf,ICML
2041,2016,Controlling the distance to a Kemeny consensus without computing it,"Yunlong Jiao,         Anna Korba,         Eric Sibony","Due to its numerous applications, rank aggregation has become a problem of major interest across many fields of the computer science literature. In the vast majority of situations, Kemeny consensus(es) are considered as the ideal solutions. It is however well known that their computation is NP-hard. Many contributions have thus established various results to apprehend this complexity. In this paper we introduce a practical method to predict, for a ranking and a dataset, how close the Kemeny consensus(es) are to this ranking. A major strength of this method is its generality: it does not require any assumption on the dataset nor the ranking. Furthermore, it relies on a new geometric interpretation of Kemeny aggregation that, we believe, could lead to many other results.",http://proceedings.mlr.press/v48/korba16.html,http://proceedings.mlr.press/v48/korba16.pdf,ICML
2042,2016,ForecastICU: A Prognostic Decision Support System for Timely Prediction of Intensive Care Unit Admission,"Jinsung Yoon,         Ahmed Alaa,         Scott Hu,         Mihaela Schaar","We develop ForecastICU: a prognostic decision support system that monitors hospitalized patients and prompts alarms for intensive care unit (ICU) admissions. ForecastICU is first trained in an offline stage by constructing a Bayesian belief system that corresponds to its belief about how trajectories of physiological data streams of the patient map to a clinical status. After that, ForecastICU monitors a new patient in real-time by observing her physiological data stream, updating its belief about her status over time, and prompting an alarm whenever its belief process hits a predefined threshold (confidence). Using a real-world dataset obtained from UCLA Ronald Reagan Medical Center, we show that ForecastICU can predict ICU admissions 9 hours before a physician’s decision (for a sensitivity of 40% and a precision of 50%). Also, ForecastICU performs consistently better than other state-of-the-art machine learning algorithms in terms of sensitivity, precision, and timeliness: it can predict ICU admissions 3 hours earlier, and offers a 7.8% gain in sensitivity and a 5.1% gain in precision compared to the best state-of-the-art algorithm. Moreover, ForecastICU offers an area under curve (AUC) gain of 22.3% compared to the Rothman index, which is the currently deployed technology in most hospital wards.",http://proceedings.mlr.press/v48/yoon16.html,http://proceedings.mlr.press/v48/yoon16.pdf,ICML
2043,2016,Partition Functions from Rao-Blackwellized Tempered Sampling,"David Carlson,         Patrick Stinson,         Ari Pakman,         Liam Paninski","Partition functions of probability distributions are important quantities for model evaluation and comparisons. We present a new method to compute partition functions of complex and multimodal distributions. Such distributions are often sampled using simulated tempering, which augments the target space with an auxiliary inverse temperature variable. Our method exploits the multinomial probability law of the inverse temperatures, and provides estimates of the partition function in terms of a simple quotient of Rao-Blackwellized marginal inverse temperature probability estimates, which are updated while sampling. We show that the method has interesting connections with several alternative popular methods, and offers some significant advantages. In particular, we empirically find that the new method provides more accurate estimates than Annealed Importance Sampling when calculating partition functions of large Restricted Boltzmann Machines (RBM); moreover, the method is sufficiently accurate to track training and validation log-likelihoods during learning of RBMs, at minimal computational cost.",http://proceedings.mlr.press/v48/carlson16.html,http://proceedings.mlr.press/v48/carlson16.pdf,ICML
2044,2016,Inference Networks for Sequential Monte Carlo in Graphical Models,"Brooks Paige,         Frank Wood","We introduce a new approach for amortizing inference in directed graphical models by learning heuristic approximations to stochastic inverses, designed specifically for use as proposal distributions in sequential Monte Carlo methods. We describe a procedure for constructing and learning a structured neural network which represents an inverse factorization of the graphical model, resulting in a conditional density estimator that takes as input particular values of the observed random variables, and returns an approximation to the distribution of the latent variables. This recognition model can be learned offline, independent from any particular dataset, prior to performing inference. The output of these networks can be used as automatically-learned high-quality proposal distributions to accelerate sequential Monte Carlo across a diverse range of problem settings.",http://proceedings.mlr.press/v48/paige16.html,http://proceedings.mlr.press/v48/paige16.pdf,ICML
2045,2016,Recurrent Orthogonal Networks and Long-Memory Tasks,"Mikael Henaff,         Arthur Szlam,         Yann LeCun","Although RNNs have been shown to be power- ful tools for processing sequential data, finding architectures or optimization strategies that al- low them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets orig- inally outlined in (Hochreiter & Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illumi- nate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions fur- thermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.",http://proceedings.mlr.press/v48/henaff16.html,http://proceedings.mlr.press/v48/henaff16.pdf,ICML
2046,2016,Greedy Column Subset Selection: New Bounds and Distributed Algorithms,"Jason Altschuler,         Aditya Bhaskara,         Gang Fu,         Vahab Mirrokni,         Afshin Rostamizadeh,         Morteza Zadimoghaddam","The problem of column subset selection has recently attracted a large body of research, with feature selection serving as one obvious and important application. Among the techniques that have been applied to solve this problem, the greedy algorithm has been shown to be quite effective in practice. However, theoretical guarantees on its performance have not been explored thoroughly, especially in a distributed setting. In this paper, we study the greedy algorithm for the column subset selection problem from a theoretical and empirical perspective and show its effectiveness in a distributed setting. In particular, we provide an improved approximation guarantee for the greedy algorithm which we show is tight up to a constant factor, and present the first distributed implementation with provable approximation factors. We use the idea of randomized composable core-sets, developed recently in the context of submodular maximization. Finally, we validate the effectiveness of this distributed algorithm via an empirical study.",http://proceedings.mlr.press/v48/altschuler16.html,http://proceedings.mlr.press/v48/altschuler16.pdf,ICML
2047,2016,Pixel Recurrent Neural Networks,"Aaron Van Oord,         Nal Kalchbrenner,         Koray Kavukcuoglu","Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.",http://proceedings.mlr.press/v48/oord16.html,http://proceedings.mlr.press/v48/oord16.pdf,ICML
2048,2016,Dynamic Memory Networks for Visual and Textual Question Answering,"Caiming Xiong,         Stephen Merity,         Richard Socher","Neural network architectures with memory and attention mechanisms exhibit certain reason- ing capabilities required for question answering. One such architecture, the dynamic memory net- work (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision.",http://proceedings.mlr.press/v48/xiong16.html,http://proceedings.mlr.press/v48/xiong16.pdf,ICML
2049,2016,A Variational Analysis of Stochastic Gradient Algorithms,"Stephan Mandt,         Matthew Hoffman,         David Blei","Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models.",http://proceedings.mlr.press/v48/mandt16.html,http://proceedings.mlr.press/v48/mandt16.pdf,ICML
2050,2016,Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs,"Anton Osokin,         Jean-Baptiste Alayrac,         Isabella Lukasewitz,         Puneet Dokania,         Simon Lacoste-Julien","In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine (SSVM) objective in the context of structured prediction, though it has wider applications. The key intuition behind our improvements is that the estimates of block gaps maintained by BCFW reveal the block suboptimality that can be used as an *adaptive* criterion. First, we sample objects at each iteration of BCFW in an adaptive non-uniform way via gap-based sampling. Second, we incorporate pairwise and away-step variants of Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls with a cache-hit criterion based on the block gaps. Fourth, we provide the first method to compute an approximate regularization path for SSVM. Finally, we provide an exhaustive empirical evaluation of all our methods on four structured prediction datasets.",http://proceedings.mlr.press/v48/osokin16.html,http://proceedings.mlr.press/v48/osokin16.pdf,ICML
2051,2016,Strongly-Typed Recurrent Neural Networks,"David Balduzzi,         Muhammad Ghifary","Recurrent neural networks are increasing popular models for sequential learning. Unfortunately, although the most effective RNN architectures are perhaps excessively complicated, extensive searches have not found simpler alternatives. This paper imports ideas from physics and functional programming into RNN design to provide guiding principles. From physics, we introduce type constraints, analogous to the constraints that forbids adding meters to seconds. From functional programming, we require that strongly-typed architectures factorize into stateless learnware and state-dependent firmware, reducing the impact of side-effects. The features learned by strongly-typed nets have a simple semantic interpretation via dynamic average-pooling on one-dimensional convolutions. We also show that strongly-typed gradients are better behaved than in classical architectures, and characterize the representational power of strongly-typed nets. Finally, experiments show that, despite being more constrained, strongly-typed architectures achieve lower training and comparable generalization error to classical architectures.",http://proceedings.mlr.press/v48/balduzzi16.html,http://proceedings.mlr.press/v48/balduzzi16.pdf,ICML
2052,2016,A Convolutional Attention Network for Extreme Summarization of Source Code,"Miltiadis Allamanis,         Hao Peng,         Charles Sutton","Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model’s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.",http://proceedings.mlr.press/v48/allamanis16.html,http://proceedings.mlr.press/v48/allamanis16.pdf,ICML
2053,2016,Fast Rate Analysis of Some Stochastic Optimization Algorithms,"Chao Qu,         Huan Xu,         Chong Ong","In this paper, we revisit three fundamental and popular stochastic optimization algorithms (namely, Online Proximal Gradient, Regularized Dual Averaging method and ADMM with online proximal gradient) and analyze their convergence speed under conditions weaker than those in literature. In particular, previous works showed that these algorithms converge at a rate of O (\ln T/T) when the loss function is strongly convex, and O (1 /\sqrtT) in the weakly convex case. In contrast, we relax the strong convexity assumption of the loss function, and show that the algorithms converge at a rate O (\ln T/T) if the \em expectation of the loss function is \em locally strongly convex. This is a much weaker assumption and is satisfied by many practical formulations including Lasso and Logistic Regression. Our analysis thus extends the applicability of these three methods, as well as provides a general recipe for improving analysis of convergence rate for stochastic and online optimization algorithms.",http://proceedings.mlr.press/v48/qua16.html,http://proceedings.mlr.press/v48/qua16.pdf,ICML
2054,2016,Learning to Generate with Memory,"Chongxuan Li,         Jun Zhu,         Bo Zhang","Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data. This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs on various tasks, including density estimation, image generation, and missing value imputation, and DGMs with memory can achieve state-of-the-art quantitative results.",http://proceedings.mlr.press/v48/lie16.html,http://proceedings.mlr.press/v48/lie16.pdf,ICML
2055,2016,Asymmetric Multi-task Learning Based on Task Relatedness and Loss,"Giwoong Lee,         Eunho Yang,         Sung Hwang","We propose a novel multi-task learning method that can minimize the effect of negative transfer by allowing asymmetric transfer between the tasks based on task relatedness as well as the amount of individual task losses, which we refer to as Asymmetric Multi-task Learning (AMTL). To tackle this problem, we couple multiple tasks via a sparse, directed regularization graph, that enforces each task parameter to be reconstructed as a sparse combination of other tasks, which are selected based on the task-wise loss. We present two different algorithms to solve this joint learning of the task predictors and the regularization graph. The first algorithm solves for the original learning objective using alternative optimization, and the second algorithm solves an approximation of it using curriculum learning strategy, that learns one task at a time. We perform experiments on multiple datasets for classification and regression, on which we obtain significant improvements in performance over the single task learning and symmetric multitask learning baselines.",http://proceedings.mlr.press/v48/leeb16.html,http://proceedings.mlr.press/v48/leeb16.pdf,ICML
2056,2016,Discriminative Embeddings of Latent Variable Models for Structured Data,"Hanjun Dai,         Bo Dai,         Le Song","Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. We propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are 10,000 times smaller, while at the same time achieving the state-of-the-art predictive performance.",http://proceedings.mlr.press/v48/daib16.html,http://proceedings.mlr.press/v48/daib16.pdf,ICML
2057,2016,A Box-Constrained Approach for Hard Permutation Problems,"Cong Han Lim,         Steve Wright","We describe the use of sorting networks to form relaxations of problems involving permutations of n objects. This approach is an alternative to relaxations based on the Birkhoff polytope (the set of n \times n doubly stochastic matrices), providing a more compact formulation in which the only constraints are box constraints. Using this approach, we form a variant of the relaxation of the quadratic assignment problem recently studied in Vogelstein et al. (2015), and show that the continuation method applied to this formulation can be quite effective. We develop a coordinate descent algorithm that achieves a per-cycle complexity of O(n^2 \log^2 n). We compare this method with Fast Approximate QAP (FAQ) algorithm introduced in Vogelstein et al. (2015), which uses a conditional-gradient method whose per-iteration complexity is O(n^3). We demonstrate that for most problems in QAPLIB and for a class of synthetic QAP problems, the sorting-network formulation returns solutions that are competitive with the FAQ algorithm, often in significantly less computing time.",http://proceedings.mlr.press/v48/lim16.html,http://proceedings.mlr.press/v48/lim16.pdf,ICML
2058,2016,L1-regularized Neural Networks are Improperly Learnable in Polynomial Time,"Yuchen Zhang,         Jason D. Lee,         Michael I. Jordan","We study the improper learning of multi-layer neural networks. Suppose that the neural network to be learned has k hidden layers and that the \ell_1-norm of the incoming weights of any neuron is bounded by L. We present a kernel-based method, such that with probability at least 1 - δ, it learns a predictor whose generalization error is at most εworse than that of the neural network. The sample complexity and the time complexity of the presented method are polynomial in the input dimension and in (1/ε,\log(1/δ),F(k,L)), where F(k,L) is a function depending on (k,L) and on the activation function, independent of the number of neurons. The algorithm applies to both sigmoid-like activation functions and ReLU-like activation functions. It implies that any sufficiently sparse neural network is learnable in polynomial time.",http://proceedings.mlr.press/v48/zhangd16.html,http://proceedings.mlr.press/v48/zhangd16.pdf,ICML
2059,2016,Mixture Proportion Estimation via Kernel Embeddings of Distributions,"Harish Ramaswamy,         Clayton Scott,         Ambuj Tewari","Mixture proportion estimation (MPE) is the problem of estimating the weight of a component distribution in a mixture, given samples from the mixture and component. This problem constitutes a key part in many ""weakly supervised learning"" problems like learning with positive and unlabelled samples, learning with label noise, anomaly detection and crowdsourcing. While there have been several methods proposed to solve this problem, to the best of our knowledge no efficient algorithm with a proven convergence rate towards the true proportion exists for this problem. We fill this gap by constructing a provably correct algorithm for MPE, and derive convergence rates under certain assumptions on the distribution. Our method is based on embedding distributions onto an RKHS, and implementing it only requires solving a simple convex quadratic programming problem a few times. We run our algorithm on several standard classification datasets, and demonstrate that it performs comparably to or better than other algorithms on most datasets.",http://proceedings.mlr.press/v48/ramaswamy16.html,http://proceedings.mlr.press/v48/ramaswamy16.pdf,ICML
2060,2016,Horizontally Scalable Submodular Maximization,"Mario Lucic,         Olivier Bachem,         Morteza Zadimoghaddam,         Andreas Krause","A variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization. Existing approaches for distributed submodular maximization have a critical drawback: The capacity - number of instances that can fit in memory - must grow with the data set size. In practice, while one can provision many machines, the capacity of each machine is limited by physical constraints. We propose a truly scalable approach for distributed submodular maximization under fixed capacity. The proposed framework applies to a broad class of algorithms and constraints and provides theoretical guarantees on the approximation factor for any available capacity. We empirically evaluate the proposed algorithm on a variety of data sets and demonstrate that it achieves performance competitive with the centralized greedy solution.",http://proceedings.mlr.press/v48/lucic16.html,http://proceedings.mlr.press/v48/lucic16.pdf,ICML
2061,2016,Pareto Frontier Learning with Expensive Correlated Objectives,"Amar Shah,         Zoubin Ghahramani","There has been a surge of research interest in developing tools and analysis for Bayesian optimization, the task of finding the global maximizer of an unknown, expensive function through sequential evaluation using Bayesian decision theory. However, many interesting problems involve optimizing multiple, expensive to evaluate objectives simultaneously, and relatively little research has addressed this setting from a Bayesian theoretic standpoint. A prevailing choice when tackling this problem, is to model the multiple objectives as being independent, typically for ease of computation. In practice, objectives are correlated to some extent. In this work, we incorporate the modelling of inter-task correlations, developing an approximation to overcome intractable integrals. We illustrate the power of modelling dependencies between objectives on a range of synthetic and real world multi-objective optimization problems.",http://proceedings.mlr.press/v48/shahc16.html,http://proceedings.mlr.press/v48/shahc16.pdf,ICML
2062,2016,Hierarchical Variational Models,"Rajesh Ranganath,         Dustin Tran,         David Blei","Black box variational inference allows researchers to easily prototype and evaluate an array of models. Recent advances allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an expressive variational distribution that maintains efficient computation? To address this, we develop hierarchical variational models (HVMs). HVMs augment a variational approximation with a prior on its parameters, which allows it to capture complex structure for both discrete and continuous latent variables. The algorithm we develop is black box, can be used for any HVM, and has the same computational efficiency as the original approximation. We study HVMs on a variety of deep discrete latent variable models. HVMs generalize other expressive variational distributions and maintains higher fidelity to the posterior.",http://proceedings.mlr.press/v48/ranganath16.html,http://proceedings.mlr.press/v48/ranganath16.pdf,ICML
2063,2016,Convolutional Rectifier Networks as Generalized Tensor Decompositions,"Nadav Cohen,         Amnon Shashua","Convolutional rectifier networks, i.e. convolutional neural networks with rectified linear activation and max or average pooling, are the cornerstone of modern deep learning. However, despite their wide use and success, our theoretical understanding of the expressive properties that drive these networks is partial at best. On the other hand, we have a much firmer grasp of these issues in the world of arithmetic circuits. Specifically, it is known that convolutional arithmetic circuits possess the property of ""complete depth efficiency"", meaning that besides a negligible set, all functions realizable by a deep network of polynomial size, require exponential size in order to be realized (or approximated) by a shallow network. In this paper we describe a construction based on generalized tensor decompositions, that transforms convolutional arithmetic circuits into convolutional rectifier networks. We then use mathematical tools available from the world of arithmetic circuits to prove new results. First, we show that convolutional rectifier networks are universal with max pooling but not with average pooling. Second, and more importantly, we show that depth efficiency is weaker with convolutional rectifier networks than it is with convolutional arithmetic circuits. This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners.",http://proceedings.mlr.press/v48/cohenb16.html,http://proceedings.mlr.press/v48/cohenb16.pdf,ICML
2064,2016,Interacting Particle Markov Chain Monte Carlo,"Tom Rainforth,         Christian Naesseth,         Fredrik Lindsten,         Brooks Paige,         Jan-Willem Vandemeent,         Arnaud Doucet,         Frank Wood","We introduce interacting particle Markov chain Monte Carlo (iPMCMC), a PMCMC method based on an interacting pool of standard and conditional sequential Monte Carlo samplers. Like related methods, iPMCMC is a Markov chain Monte Carlo sampler on an extended space. We present empirical results that show significant improvements in mixing rates relative to both non-interacting PMCMC samplers and a single PMCMC sampler with an equivalent memory and computational budget. An additional advantage of the iPMCMC method is that it is suitable for distributed and multi-core architectures.",http://proceedings.mlr.press/v48/rainforth16.html,http://proceedings.mlr.press/v48/rainforth16.pdf,ICML
2065,2016,Experimental Design on a Budget for Sparse Linear Models and Applications,"Sathya Narayanan Ravi,         Vamsi Ithapu,         Sterling Johnson,         Vikas Singh","Budget constrained optimal design of experiments is a classical problem in statistics. Although the optimal design literature is very mature, few efficient strategies are available when these design problems appear in the context of sparse linear models commonly encountered in high dimensional machine learning and statistics. In this work, we study experimental design for the setting where the underlying regression model is characterized by a \ell_1-regularized linear function. We propose two novel strategies: the first is motivated geometrically whereas the second is algebraic in nature. We obtain tractable algorithms for this problem and also hold for a more general class of sparse linear models. We perform an extensive set of experiments, on benchmarks and a large multi-site neuroscience study, showing that the proposed models are effective in practice. The latter experiment suggests that these ideas may play a small role in informing enrollment strategies for similar scientific studies in the short-to-medium term future.",http://proceedings.mlr.press/v48/ravi16.html,http://proceedings.mlr.press/v48/ravi16.pdf,ICML
2066,2016,Bounded Off-Policy Evaluation with Missing Data for Course Recommendation and Curriculum Design,"William Hoiles,         Mihaela Schaar","Successfully recommending personalized course schedules is a difficult problem given the diversity of students knowledge, learning behaviour, and goals. This paper presents personalized course recommendation and curriculum design algorithms that exploit logged student data. The algorithms are based on the regression estimator for contextual multi-armed bandits with a penalized variance term. Guarantees on the predictive performance of the algorithms are provided using empirical Bernstein bounds. We also provide guidelines for including expert domain knowledge into the recommendations. Using undergraduate engineering logged data from a post-secondary institution we illustrate the performance of these algorithms.",http://proceedings.mlr.press/v48/hoiles16.html,http://proceedings.mlr.press/v48/hoiles16.pdf,ICML
2067,2016,Mixing Rates for the Alternating Gibbs Sampler over Restricted Boltzmann Machines and Friends,Christopher Tosh,"Alternating Gibbs sampling is a modification of classical Gibbs sampling where several variables are simultaneously sampled from their joint conditional distribution. In this work, we investigate the mixing rate of alternating Gibbs sampling with a particular emphasis on Restricted Boltzmann Machines (RBMs) and variants.",http://proceedings.mlr.press/v48/tosh16.html,http://proceedings.mlr.press/v48/tosh16.pdf,ICML
2068,2016,Recovery guarantee of weighted low-rank approximation via alternating minimization,"Yuanzhi Li,         Yingyu Liang,         Andrej Risteski","Many applications require recovering a ground truth low-rank matrix from noisy observations of the entries, which in practice is typically formulated as a weighted low-rank approximation problem and solved by non-convex optimization heuristics such as alternating minimization. In this paper, we provide provable recovery guarantee of weighted low-rank via a simple alternating minimization algorithm. In particular, for a natural class of matrices and weights and without any assumption on the noise, we bound the spectral norm of the difference between the recovered matrix and the ground truth, by the spectral norm of the weighted noise plus an additive error term that decreases exponentially with the number of rounds of alternating minimization, from either initialization by SVD or, more importantly, random initialization. These provide the first theoretical results for weighted low-rank approximation via alternating minimization with non-binary deterministic weights, significantly generalizing those for matrix completion, the special case with binary weights, since our assumptions are similar or weaker than those made in existing works. Furthermore, this is achieved by a very simple algorithm that improves the vanilla alternating minimization with a simple clipping step.",http://proceedings.mlr.press/v48/lii16.html,http://proceedings.mlr.press/v48/lii16.pdf,ICML
2069,2016,Stratified Sampling Meets Machine Learning,"Edo Liberty,         Kevin Lang,         Konstantin Shmakov","This paper solves a specialized regression problem to obtain sampling probabilities for records in databases. The goal is to sample a small set of records over which evaluating aggregate queries can be done both efficiently and accurately. We provide a principled and provable solution for this problem; it is parameterless and requires no data insights. Unlike standard regression problems, the loss is inversely proportional to the regressed-to values. Moreover, a cost zero solution always exists and can only be excluded by hard budget constraints. A unique form of regularization is also needed. We provide an efficient and simple regularized Empirical Risk Minimization (ERM) algorithm along with a theoretical generalization result. Our extensive experimental results significantly improve over both uniform sampling and standard stratified sampling which are de-facto the industry standards.",http://proceedings.mlr.press/v48/liberty16.html,http://proceedings.mlr.press/v48/liberty16.pdf,ICML
2070,2016,Evasion and Hardening of Tree Ensemble Classifiers,"Alex Kantchelian,         J. D. Tygar,         Anthony Joseph","Classifier evasion consists in finding for a given instance x the “nearest” instance x’ such that the classifier predictions of x and x’ are different. We present two novel algorithms for systematically computing evasions for tree ensembles such as boosted trees and random forests. Our first algorithm uses a Mixed Integer Linear Program solver and finds the optimal evading instance under an expressive set of constraints. Our second algorithm trades off optimality for speed by using symbolic prediction, a novel algorithm for fast finite differences on tree ensembles. On a digit recognition task, we demonstrate that both gradient boosted trees and random forests are extremely susceptible to evasions. Finally, we harden a boosted tree model without loss of predictive accuracy by augmenting the training set of each boosting round with evading instances, a technique we call adversarial boosting.",http://proceedings.mlr.press/v48/kantchelian16.html,http://proceedings.mlr.press/v48/kantchelian16.pdf,ICML
2071,2016,Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity,"Quanming Yao,         James Kwok","The use of convex regularizers allow for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, for a large class of nonconvex regularizers, we propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex regularizer, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the standard proximal algorithm and Frank-Wolfe algorithm). Moreover, it can be shown that critical points of the transformed problem are also critical points of the original problem. Extensive experiments on a number of nonconvex regularization problems show that the proposed procedure is much faster than the state-of-the-art nonconvex solvers.",http://proceedings.mlr.press/v48/yao16.html,http://proceedings.mlr.press/v48/yao16.pdf,ICML
2072,2016,Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control,"Prashanth L.A.,         Cheng Jie,         Michael Fu,         Steve Marcus,         Csaba Szepesvari","Cumulative prospect theory (CPT) is known to model human decisions well, with substantial empirical evidence supporting this claim. CPT works by distorting probabilities and is more general than the classic expected utility and coherent risk measures. We bring this idea to a risk-sensitive reinforcement learning (RL) setting and design algorithms for both estimation and control. The RL setting presents two particular challenges when CPT is applied: estimating the CPT objective requires estimations of the entire distribution of the value function and finding a randomized optimal policy. The estimation scheme that we propose uses the empirical distribution to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of a CPT-value optimization procedure that is based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation (SPSA). We provide theoretical convergence guarantees for all the proposed algorithms and also empirically demonstrate the usefulness of our algorithms.",http://proceedings.mlr.press/v48/la16.html,http://proceedings.mlr.press/v48/la16.pdf,ICML
2073,2016,Quadratic Optimization with Orthogonality Constraints: Explicit Lojasiewicz Exponent and Linear Convergence of Line-Search Methods,"Huikang Liu,         Weijie Wu,         Anthony Man-Cho So","A fundamental class of matrix optimization problems that arise in many areas of science and engineering is that of quadratic optimization with orthogonality constraints. Such problems can be solved using line-search methods on the Stiefel manifold, which are known to converge globally under mild conditions. To determine the convergence rates of these methods, we give an explicit estimate of the exponent in a Lojasiewicz inequality for the (non-convex) set of critical points of the aforementioned class of problems. This not only allows us to establish the linear convergence of a large class of line-search methods but also answers an important and intriguing problem in mathematical analysis and numerical optimization. A key step in our proof is to establish a local error bound for the set of critical points, which may be of independent interest.",http://proceedings.mlr.press/v48/liue16.html,http://proceedings.mlr.press/v48/liue16.pdf,ICML
2074,2016,DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression,"Jovana Mitrovic,         Dino Sejdinovic,         Yee-Whye Teh","Performing exact posterior inference in complex generative models is often difficult or impossible due to an expensive to evaluate or intractable likelihood function. Approximate Bayesian computation (ABC) is an inference framework that constructs an approximation to the true likelihood based on the similarity between the observed and simulated data as measured by a predefined set of summary statistics. Although the choice of informative problem-specific summary statistics crucially influences the quality of the likelihood approximation and hence also the quality of the posterior sample in ABC, there are only few principled general-purpose approaches to the selection or construction of such summary statistics. In this paper, we develop a novel framework for solving this problem. We model the functional relationship between the data and the optimal choice (with respect to a loss function) of summary statistics using kernel-based distribution regression. Furthermore, we extend our approach to incorporate kernel-based regression from conditional distributions, thus appropriately taking into account the specific structure of the posited generative model. We show that our approach can be implemented in a computationally and statistically efficient way using the random Fourier features framework for large-scale kernel learning. In addition to that, our framework outperforms related methods by a large margin on toy and real-world data, including hierarchical and time series models.",http://proceedings.mlr.press/v48/mitrovic16.html,http://proceedings.mlr.press/v48/mitrovic16.pdf,ICML
2075,2016,Learning Population-Level Diffusions with Generative RNNs,"Tatsunori Hashimoto,         David Gifford,         Tommi Jaakkola","We estimate stochastic processes that govern the dynamics of evolving populations such as cell differentiation. The problem is challenging since longitudinal trajectory measurements of individuals in a population are rarely available due to experimental cost and/or privacy. We show that cross-sectional samples from an evolving population suffice for recovery within a class of processes even if samples are available only at a few distinct time points. We provide a stratified analysis of recoverability conditions, and establish that reversibility is sufficient for recoverability. For estimation, we derive a natural loss and regularization, and parameterize the processes as diffusive recurrent neural networks. We demonstrate the approach in the context of uncovering complex cellular dynamics known as the ‘epigenetic landscape’ from existing biological assays.",http://proceedings.mlr.press/v48/hashimoto16.html,http://proceedings.mlr.press/v48/hashimoto16.pdf,ICML
2076,2016,Network Morphism,"Tao Wei,         Changhu Wang,         Yong Rui,         Chang Wen Chen","We present a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.",http://proceedings.mlr.press/v48/wei16.html,http://proceedings.mlr.press/v48/wei16.pdf,ICML
2077,2016,Domain Adaptation with Conditional Transferable Components,"Mingming Gong,         Kun Zhang,         Tongliang Liu,         Dacheng Tao,         Clark Glymour,         Bernhard Schölkopf","Domain adaptation arises in supervised learning when the training (source domain) and test (target domain) data have different distributions. Let X and Y denote the features and target, respectively, previous work on domain adaptation considers the covariate shift situation where the distribution of the features P(X) changes across domains while the conditional distribution P(Y|X) stays the same. To reduce domain discrepancy, recent methods try to find invariant components \mathcalT(X) that have similar P(\mathcalT(X)) by explicitly minimizing a distribution discrepancy measure. However, it is not clear if P(Y|\mathcalT(X)) in different domains is also similar when P(Y|X) changes. Furthermore, transferable components do not necessarily have to be invariant. If the change in some components is identifiable, we can make use of such components for prediction in the target domain. In this paper, we focus on the case where P(X|Y) and P(Y) both change in a causal system in which Y is the cause for X. Under appropriate assumptions, we aim to extract conditional transferable components whose conditional distribution P(\mathcalT(X)|Y) is invariant after proper location-scale (LS) transformations, and identify how P(Y) changes between domains simultaneously. We provide theoretical analysis and empirical evaluation on both synthetic and real-world data to show the effectiveness of our method.",http://proceedings.mlr.press/v48/gong16.html,http://proceedings.mlr.press/v48/gong16.pdf,ICML
2078,2016,Estimating Accuracy from Unlabeled Data: A Bayesian Approach,"Emmanouil Antonios Platanios,         Avinava Dubey,         Tom Mitchell","We consider the question of how unlabeled data can be used to estimate the true accuracy of learned classifiers, and the related question of how outputs from several classifiers performing the same task can be combined based on their estimated accuracies. To answer these questions, we first present a simple graphical model that performs well in practice. We then provide two nonparametric extensions to it that improve its performance. Experiments on two real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both estimating accuracies, and combining multiple classifier outputs.",http://proceedings.mlr.press/v48/platanios16.html,http://proceedings.mlr.press/v48/platanios16.pdf,ICML
2079,2016,Pricing a Low-regret Seller,"Hoda Heidari,         Mohammad Mahdian,         Umar Syed,         Sergei Vassilvitskii,         Sadra Yazdanbod","As the number of ad exchanges has grown, publishers have turned to low regret learning algorithms to decide which exchange offers the best price for their inventory. This in turn opens the following question for the exchange: how to set prices to attract as many sellers as possible and maximize revenue. In this work we formulate this precisely as a learning problem, and present algorithms showing that by simply knowing that the counterparty is using a low regret algorithm is enough for the exchange to have its own low regret learning algorithm to find the optimal price.",http://proceedings.mlr.press/v48/heidari16.html,http://proceedings.mlr.press/v48/heidari16.pdf,ICML
2080,2016,Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing,"Ke Li,         Jitendra Malik","Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by most existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in data density, supports dynamic updates to the dataset and is easy-to-implement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality, speed and space efficiency.",http://proceedings.mlr.press/v48/lic16.html,http://proceedings.mlr.press/v48/lic16.pdf,ICML
2081,2016,The Sum-Product Theorem: A Foundation for Learning Tractable Models,"Abram Friesen,         Pedro Domingos","Inference in expressive probabilistic models is generally intractable, which makes them difficult to learn and limits their applicability. Sum-product networks are a class of deep models where, surprisingly, inference remains tractable even when an arbitrary number of hidden layers are present. In this paper, we generalize this result to a much broader set of learning problems: all those where inference consists of summing a function over a semiring. This includes satisfiability, constraint satisfaction, optimization, integration, and others. In any semiring, for summation to be tractable it suffices that the factors of every product have disjoint scopes. This unifies and extends many previous results in the literature. Enforcing this condition at learning time thus ensures that the learned models are tractable. We illustrate the power and generality of this approach by applying it to a new type of structured prediction problem: learning a nonconvex function that can be globally optimized in polynomial time. We show empirically that this greatly outperforms the standard approach of learning without regard to the cost of optimization.",http://proceedings.mlr.press/v48/friesen16.html,http://proceedings.mlr.press/v48/friesen16.pdf,ICML
2082,2016,A ranking approach to global optimization,"Cedric Malherbe,         Emile Contal,         Nicolas Vayatis","We consider the problem of maximizing an unknown function f over a compact and convex set using as few observations f(x) as possible. We observe that the optimization of the function f essentially relies on learning the induced bipartite ranking rule of f. Based on this idea, we relate global optimization to bipartite ranking which allows to address problems with high dimensional input space, as well as cases of functions with weak regularity properties. The paper introduces novel meta-algorithms for global optimization which rely on the choice of any bipartite ranking method. Theoretical properties are provided as well as convergence guarantees and equivalences between various optimization methods are obtained as a by-product. Eventually, numerical evidence is given to show that the main algorithm of the paper which adapts empirically to the underlying ranking structure essentially outperforms existing state-of-the-art global optimization algorithms in typical benchmarks.",http://proceedings.mlr.press/v48/malherbe16.html,http://proceedings.mlr.press/v48/malherbe16.pdf,ICML
2083,2016,Estimating Maximum Expected Value through Gaussian Approximation,"Carlo D’Eramo,         Marcello Restelli,         Alessandro Nuara","This paper is about the estimation of the maximum expected value of a set of independent random variables. The performance of several learning algorithms (e.g., Q-learning) is affected by the accuracy of such estimation. Unfortunately, no unbiased estimator exists. The usual approach of taking the maximum of the sample means leads to large overestimates that may significantly harm the performance of the learning algorithm. Recent works have shown that the cross validation estimator—which is negatively biased—outperforms the maximum estimator in many sequential decision-making scenarios. On the other hand, the relative performance of the two estimators is highly problem-dependent. In this paper, we propose a new estimator for the maximum expected value, based on a weighted average of the sample means, where the weights are computed using Gaussian approximations for the distributions of the sample means. We compare the proposed estimator with the other state-of-the-art methods both theoretically, by deriving upper bounds to the bias and the variance of the estimator, and empirically, by testing the performance on different sequential learning problems.",http://proceedings.mlr.press/v48/deramo16.html,http://proceedings.mlr.press/v48/deramo16.pdf,ICML
2084,2016,Complex Embeddings for Simple Link Prediction,"Théo Trouillon,         Johannes Welbl,         Sebastian Riedel,         Eric Gaussier,         Guillaume Bouchard","In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.",http://proceedings.mlr.press/v48/trouillon16.html,http://proceedings.mlr.press/v48/trouillon16.pdf,ICML
2085,2016,Learning and Inference via Maximum Inner Product Search,"Stephen Mussmann,         Stefano Ermon","A large class of commonly used probabilistic models known as log-linear models are defined up to a normalization constant.Typical learning algorithms for such models require solving a sequence of probabilistic inference queries. These inferences are typically intractable, and are a major bottleneck for learning models with large output spaces. In this paper, we provide a new approach for amortizing the cost of a sequence of related inference queries, such as the ones arising during learning. Our technique relies on a surprising connection with algorithms developed in the past two decades for similarity search in large data bases. Our approach achieves improved running times with provable approximation guarantees. We show that it performs well both on synthetic data and neural language models with large output spaces.",http://proceedings.mlr.press/v48/mussmann16.html,http://proceedings.mlr.press/v48/mussmann16.pdf,ICML
2086,2016,Estimation from Indirect Supervision with Linear Moments,"Aditi Raghunathan,         Roy Frostig,         John Duchi,         Percy Liang","In structured prediction problems where we have indirect supervision of the output, maximum marginal likelihood faces two computational obstacles: non-convexity of the objective and intractability of even a single gradient computation. In this paper, we bypass both obstacles for a class of what we call linear indirectly-supervised problems. Our approach is simple: we solve a linear system to estimate sufficient statistics of the model, which we then use to estimate parameters via convex optimization. We analyze the statistical properties of our approach and show empirically that it is effective in two settings: learning with local privacy constraints and learning from low-cost count-based annotations.",http://proceedings.mlr.press/v48/raghunathan16.html,http://proceedings.mlr.press/v48/raghunathan16.pdf,ICML
2087,2016,The Teaching Dimension of Linear Learners,"Ji Liu,         Xiaojin Zhu,         Hrag Ohannessian","Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners.",http://proceedings.mlr.press/v48/liua16.html,http://proceedings.mlr.press/v48/liua16.pdf,ICML
2088,2016,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,"Ankit Kumar,         Ozan Irsoy,         Peter Ondruska,         Mohit Iyyer,         James Bradbury,         Ishaan Gulrajani,         Victor Zhong,         Romain Paulus,         Richard Socher","Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook’s bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.",http://proceedings.mlr.press/v48/kumar16.html,http://proceedings.mlr.press/v48/kumar16.pdf,ICML
2089,2016,Revisiting Semi-Supervised Learning with Graph Embeddings,"Zhilin Yang,         William Cohen,         Ruslan Salakhudinov","We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.",http://proceedings.mlr.press/v48/yanga16.html,http://proceedings.mlr.press/v48/yanga16.pdf,ICML
2090,2016,Additive Approximations in High Dimensional Nonparametric Regression via the SALSA,"Kirthevasan Kandasamy,         Yaoliang Yu","High dimensional nonparametric regression is an inherently difficult problem with known lower bounds depending exponentially in dimension. A popular strategy to alleviate this curse of dimensionality has been to use additive models of \emphfirst order, which model the regression function as a sum of independent functions on each dimension. Though useful in controlling the variance of the estimate, such models are often too restrictive in practical settings. Between non-additive models which often have large variance and first order additive models which have large bias, there has been little work to exploit the trade-off in the middle via additive models of intermediate order. In this work, we propose salsa, which bridges this gap by allowing interactions between variables, but controls model capacity by limiting the order of interactions. salsas minimises the residual sum of squares with squared RKHS norm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression with an additive kernel. When the regression function is additive, the excess risk is only polynomial in dimension. Using the Girard-Newton formulae, we efficiently sum over a combinatorial number of terms in the additive expansion. Via a comparison on 15 real datasets, we show that our method is competitive against 21 other alternatives.",http://proceedings.mlr.press/v48/kandasamy16.html,http://proceedings.mlr.press/v48/kandasamy16.pdf,ICML
2091,2016,Differentially Private Chi-Squared Hypothesis Testing: Goodness of Fit and Independence Testing,"Marco Gaboardi,         Hyun Lim,         Ryan Rogers,         Salil Vadhan","Hypothesis testing is a useful statistical tool in determining whether a given model should be rejected based on a sample from the population. Sample data may contain sensitive information about individuals, such as medical information. Thus it is important to design statistical tests that guarantee the privacy of subjects in the data. In this work, we study hypothesis testing subject to differential privacy, specifically chi-squared tests for goodness of fit for multinomial data and independence between two categorical variables.",http://proceedings.mlr.press/v48/rogers16.html,http://proceedings.mlr.press/v48/rogers16.pdf,ICML
2092,2016,A Neural Autoregressive Approach to Collaborative Filtering,"Yin Zheng,         Bangsheng Tang,         Wenkui Ding,         Hanning Zhou","This paper proposes CF-NADE, a neural autoregressive architecture for collaborative filtering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing parameters between different ratings. A factored version of CF-NADE is also proposed for better scalability. Furthermore, we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize CF-NADE, which shows superior performance. Finally, CF-NADE can be extended to a deep model, with only moderately increased computational complexity. Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance.",http://proceedings.mlr.press/v48/zheng16.html,http://proceedings.mlr.press/v48/zheng16.pdf,ICML
2093,2016,Recommendations as Treatments: Debiasing Learning and Evaluation,"Tobias Schnabel,         Adith Swaminathan,         Ashudeep Singh,         Navin Chandak,         Thorsten Joachims","Most data for evaluating and training recommender systems is subject to selection biases, either through self-selection by the users or through the actions of the recommendation system itself. In this paper, we provide a principled approach to handle selection biases by adapting models and estimation techniques from causal inference. The approach leads to unbiased performance estimators despite biased data, and to a matrix factorization method that provides substantially improved prediction performance on real-world data. We theoretically and empirically characterize the robustness of the approach, and find that it is highly practical and scalable.",http://proceedings.mlr.press/v48/schnabel16.html,http://proceedings.mlr.press/v48/schnabel16.pdf,ICML
2094,2016,BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits,"Alexander Rakhlin,         Karthik Sridharan","We present efficient algorithms for the problem of contextual bandits with i.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of policies. Our algorithm BISTRO requires d calls to the empirical risk minimization (ERM) oracle per round, where d is the number of actions. The method uses unlabeled data to make the problem computationally simple. When the ERM problem itself is computationally hard, we extend the approach by employing multiplicative approximation algorithms for the ERM. The integrality gap of the relaxation only enters in the regret bound rather than the benchmark. Finally, we show that the adversarial version of the contextual bandit problem is learnable (and efficient) whenever the full-information supervised online learning problem has a non-trivial regret bound (and efficient).",http://proceedings.mlr.press/v48/rakhlin16.html,http://proceedings.mlr.press/v48/rakhlin16.pdf,ICML
2095,2016,Convergence of Stochastic Gradient Descent for PCA,Ohad Shamir,"We consider the problem of principal component analysis (PCA) in a streaming stochastic setting, where our goal is to find a direction of approximate maximal variance, based on a stream of i.i.d. data points in R^d. A simple and computationally cheap algorithm for this is stochastic gradient descent (SGD), which incrementally updates its estimate based on each new data point. However, due to the non-convex nature of the problem, analyzing its performance has been a challenge. In particular, existing guarantees rely on a non-trivial eigengap assumption on the covariance matrix, which is intuitively unnecessary. In this paper, we provide (to the best of our knowledge) the first eigengap-free convergence guarantees for SGD in the context of PCA. This also partially resolves an open problem posed in (Hardt & Price, 2014). Moreover, under an eigengap assumption, we show that the same techniques lead to new SGD convergence guarantees with better dependence on the eigengap.",http://proceedings.mlr.press/v48/shamirb16.html,http://proceedings.mlr.press/v48/shamirb16.pdf,ICML
2096,2016,Dictionary Learning for Massive Matrix Factorization,"Arthur Mensch,         Julien Mairal,         Bertrand Thirion,         Gael Varoquaux","Sparse matrix factorization is a popular tool to obtain interpretable data decompositions, which are also effective to perform data completion or denoising. Its applicability to large datasets has been addressed with online and randomized methods, that reduce the complexity in one of the matrix dimension, but not in both of them. In this paper, we tackle very large matrices in both dimensions. We propose a new factorization method that scales gracefully to terabyte-scale datasets. Those could not be processed by previous algorithms in a reasonable amount of time. We demonstrate the efficiency of our approach on massive functional Magnetic Resonance Imaging (fMRI) data, and on matrix completion problems for recommender systems, where we obtain significant speed-ups compared to state-of-the art coordinate descent methods.",http://proceedings.mlr.press/v48/mensch16.html,http://proceedings.mlr.press/v48/mensch16.pdf,ICML
2097,2016,Estimating Cosmological Parameters from the Dark Matter Distribution,"Siamak Ravanbakhsh,         Junier Oliva,         Sebastian Fromenteau,         Layne Price,         Shirley Ho,         Jeff Schneider,         Barnabas Poczos","A grand challenge of the 21st century cosmology is to accurately estimate the cosmological parameters of our Universe. A major approach in estimating the cosmological parameters is to use the large scale matter distribution of the Universe. Galaxy surveys provide the means to map out cosmic large-scale structure in three dimensions. Information about galaxy locations is typically summarized in a ""single"" function of scale, such as the galaxy correlation function or power-spectrum. We show that it is possible to estimate these cosmological parameters directly from the distribution of matter. This paper presents the application of deep 3D convolutional networks to volumetric representation of dark matter simulations as well as the results obtained using a recently proposed distribution regression framework, showing that machine learning techniques are comparable to, and can sometimes outperform, maximum-likelihood point estimates using ""cosmological models"". This opens the way to estimating the parameters of our Universe with higher accuracy.",http://proceedings.mlr.press/v48/ravanbakhshb16.html,http://proceedings.mlr.press/v48/ravanbakhshb16.pdf,ICML
2098,2016,Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling,"Christopher De Sa,         Chris Re,         Kunle Olukotun","Gibbs sampling is a Markov chain Monte Carlo technique commonly used for estimating marginal distributions. To speed up Gibbs sampling, there has recently been interest in parallelizing it by executing asynchronously. While empirical results suggest that many models can be efficiently sampled asynchronously, traditional Markov chain analysis does not apply to the asynchronous case, and thus asynchronous Gibbs sampling is poorly understood. In this paper, we derive a better understanding of the two main challenges of asynchronous Gibbs: bias and mixing time. We show experimentally that our theoretical results match practical outcomes.",http://proceedings.mlr.press/v48/sa16.html,http://proceedings.mlr.press/v48/sa16.pdf,ICML
2099,2016,Dropout distillation,"Samuel Rota Bulò,         Lorenzo Porzi,         Peter Kontschieder","Dropout is a popular stochastic regularization technique for deep neural networks that works by randomly dropping (i.e. zeroing) units from the network during training. This randomization process allows to implicitly train an ensemble of exponentially many networks sharing the same parametrization, which should be averaged at test time to deliver the final prediction. A typical workaround for this intractable averaging operation consists in scaling the layers undergoing dropout randomization. This simple rule called ’standard dropout’ is efficient, but might degrade the accuracy of the prediction. In this work we introduce a novel approach, coined ’dropout distillation’, that allows us to train a predictor in a way to better approximate the intractable, but preferable, averaging process, while keeping under control its computational efficiency. We are thus able to construct models that are as efficient as standard dropout, or even more efficient, while being more accurate. Experiments on standard benchmark datasets demonstrate the validity of our method, yielding consistent improvements over conventional dropout.",http://proceedings.mlr.press/v48/bulo16.html,http://proceedings.mlr.press/v48/bulo16.pdf,ICML
2100,2016,Gossip Dual Averaging for Decentralized Optimization of Pairwise Functions,"Igor Colin,         Aurelien Bellet,         Joseph Salmon,         Stéphan Clémençon","In decentralized networks (of sensors, connected objects, etc.), there is an important need for efficient algorithms to optimize a global cost function, for instance to learn a global model from the local data collected by each computing unit. In this paper, we address the problem of decentralized minimization of pairwise functions of the data points, where these points are distributed over the nodes of a graph defining the communication topology of the network. This general problem finds applications in ranking, distance metric learning and graph inference, among others. We propose new gossip algorithms based on dual averaging which aims at solving such problems both in synchronous and asynchronous settings. The proposed framework is flexible enough to deal with constrained and regularized variants of the optimization problem. Our theoretical analysis reveals that the proposed algorithms preserve the convergence rate of centralized dual averaging up to an additive bias term. We present numerical simulations on Area Under the ROC Curve (AUC) maximization and metric learning problems which illustrate the practical interest of our approach.",http://proceedings.mlr.press/v48/colin16.html,http://proceedings.mlr.press/v48/colin16.pdf,ICML
2101,2016,On the Statistical Limits of Convex Relaxations,"Zhaoran Wang,         Quanquan Gu,         Han Liu","Many high dimensional sparse learning problems are formulated as nonconvex optimization. A popular approach to solve these nonconvex optimization problems is through convex relaxations such as linear and semidefinite programming. In this paper, we study the statistical limits of convex relaxations. Particularly, we consider two problems: Mean estimation for sparse principal submatrix and edge probability estimation for stochastic block model. We exploit the sum-of-squares relaxation hierarchy to sharply characterize the limits of a broad class of convex relaxations. Our result shows statistical optimality needs to be compromised for achieving computational tractability using convex relaxations. Compared with existing results on computational lower bounds for statistical problems, which consider general polynomial-time algorithms and rely on computational hardness hypotheses on problems like planted clique detection, our theory focuses on a broad class of convex relaxations and does not rely on unproven hypotheses.",http://proceedings.mlr.press/v48/wangc16.html,http://proceedings.mlr.press/v48/wangc16.pdf,ICML
2102,2016,Clustering High Dimensional Categorical Data via Topographical Features,"Chao Chen,         Novi Quadrianto","Analysis of categorical data is a challenging task. In this paper, we propose to compute topographical features of high-dimensional categorical data. We propose an efficient algorithm to extract modes of the underlying distribution and their attractive basins. These topographical features provide a geometric view of the data and can be applied to visualization and clustering of real world challenging datasets. Experiments show that our principled method outperforms state-of-the-art clustering methods while also admits an embarrassingly parallel property.",http://proceedings.mlr.press/v48/chenc16.html,http://proceedings.mlr.press/v48/chenc16.pdf,ICML
2103,2016,A Comparative Analysis and Study of Multiview CNN Models for Joint Object Categorization and Pose Estimation,"Mohamed Elhoseiny,         Tarek El-Gaaly,         Amr Bakry,         Ahmed Elgammal","In the Object Recognition task, there exists a dichotomy between the categorization of objects and estimating object pose, where the former necessitates a view-invariant representation, while the latter requires a representation capable of capturing pose information over different categories of objects. With the rise of deep architectures, the prime focus has been on object category recognition. Deep learning methods have achieved wide success in this task. In contrast, object pose estimation using these approaches has received relatively less attention. In this work, we study how Convolutional Neural Networks (CNN) architectures can be adapted to the task of simultaneous object recognition and pose estimation. We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representations. We extensively experiment on two recent large and challenging multi-view datasets and we achieve better than the state-of-the-art.",http://proceedings.mlr.press/v48/elhoseiny16.html,http://proceedings.mlr.press/v48/elhoseiny16.pdf,ICML
2104,2016,Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis,"Rong Ge,         Chi Jin,          Sham,         Praneeth Netrapalli,         Aaron Sidford","This paper considers the problem of canonical-correlation analysis (CCA) and, more broadly, the generalized eigenvector problem for a pair of symmetric matrices. These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics. We provide simple iterative algorithms, with improved runtimes, for solving these problems that are globally linearly convergent with moderate dependencies on the condition numbers and eigenvalue gaps of the matrices involved. We obtain our results by reducing CCA to the top-k generalized eigenvector problem. We solve this problem through a general framework that simply requires black box access to an approximate linear system solver. Instantiating this framework with accelerated gradient descent we obtain a running time of \order\fracz k \sqrtκρ \log(1/ε) \log \left(kκ/ρ\right) where z is the total number of nonzero entries, κis the condition number and ρis the relative eigenvalue gap of the appropriate matrices. Our algorithm is linear in the input size and the number of components k up to a \log(k) factor. This is essential for handling large-scale matrices that appear in practice. To the best of our knowledge this is the first such algorithm with global linear convergence. We hope that our results prompt further research and ultimately improve the practical running time for performing these important data analysis procedures on large data sets.",http://proceedings.mlr.press/v48/geb16.html,http://proceedings.mlr.press/v48/geb16.pdf,ICML
2105,2016,Heteroscedastic Sequences: Beyond Gaussianity,"Oren Anava,         Shie Mannor","We address the problem of sequential prediction in the heteroscedastic setting, when both the signal and its variance are assumed to depend on explanatory variables. By applying regret minimization techniques, we devise an efficient online learning algorithm for the problem, without assuming that the error terms comply with a specific distribution. We show that our algorithm can be adjusted to provide confidence bounds for its predictions, and provide an application to ARCH models. The theoretic results are corroborated by an empirical study.",http://proceedings.mlr.press/v48/anava16.html,http://proceedings.mlr.press/v48/anava16.pdf,ICML
2106,2016,Algorithms for Optimizing the Ratio of Submodular Functions,"Wenruo Bai,         Rishabh Iyer,         Kai Wei,         Jeff Bilmes","We investigate a new optimization problem involving minimizing the Ratio of Submodular (RS) functions. We argue that this problem occurs naturally in several real world applications. We then show the connection between this problem and several related problems, including minimizing the difference of submodular functions, and to submodular optimization subject to submodular constraints. We show RS that optimization can be solved within bounded approximation factors. We also provide a hardness bound and show that our tightest algorithm matches the lower bound up to a \log factor. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.",http://proceedings.mlr.press/v48/baib16.html,http://proceedings.mlr.press/v48/baib16.pdf,ICML
2107,2016,A Kernel Test of Goodness of Fit,"Kacper Chwialkowski,         Heiko Strathmann,         Arthur Gretton","We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein’s method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.",http://proceedings.mlr.press/v48/chwialkowski16.html,http://proceedings.mlr.press/v48/chwialkowski16.pdf,ICML
2108,2016,Fast Parameter Inference in Nonlinear Dynamical Systems using Iterative Gradient Matching,"Mu Niu,         Simon Rogers,         Maurizio Filippone,         Dirk Husmeier","Parameter inference in mechanistic models of coupled differential equations is a topical and challenging problem. We propose a new method based on kernel ridge regression and gradient matching, and an objective function that simultaneously encourages goodness of fit and penalises inconsistencies with the differential equations. Fast minimisation is achieved by exploiting partial convexity inherent in this function, and setting up an iterative algorithm in the vein of the EM algorithm. An evaluation of the proposed method on various benchmark data suggests that it compares favourably with state-of-the-art alternatives.",http://proceedings.mlr.press/v48/niu16.html,http://proceedings.mlr.press/v48/niu16.pdf,ICML
2109,2016,On collapsed representation of hierarchical Completely Random Measures,"Gaurav Pandey,         Ambedkar Dukkipati","The aim of the paper is to provide an exact approach for generating a Poisson process sampled from a hierarchical CRM, without having to instantiate the infinitely many atoms of the random measures. We use completely random measures (CRM) and hierarchical CRM to define a prior for Poisson processes. We derive the marginal distribution of the resultant point process, when the underlying CRM is marginalized out. Using well known properties unique to Poisson processes, we were able to derive an exact approach for instantiating a Poisson process with a hierarchical CRM prior. Furthermore, we derive Gibbs sampling strategies for hierarchical CRM models based on Chinese restaurant franchise sampling scheme. As an example, we present the sum of generalized gamma process (SGGP), and show its application in topic-modelling. We show that one can determine the power-law behaviour of the topics and words in a Bayesian fashion, by defining a prior on the parameters of SGGP.",http://proceedings.mlr.press/v48/pandey16.html,http://proceedings.mlr.press/v48/pandey16.pdf,ICML
2110,2016,Data-driven Rank Breaking for Efficient Rank Aggregation,"Ashish Khetan,         Sewoong Oh","Rank aggregation systems collect ordinal preferences from individuals to produce a global ranking that represents the social preference. To reduce the computational complexity of learning the global ranking, a common practice is to use rank-breaking. Individuals’ preferences are broken into pairwise comparisons and then applied to efficient algorithms tailored for independent pairwise comparisons. However, due to the ignored dependencies, naive rank-breaking approaches can result in inconsistent estimates. The key idea to produce unbiased and accurate estimates is to treat the paired comparisons outcomes unequally, depending on the topology of the collected data. In this paper, we provide the optimal rank-breaking estimator, which not only achieves consistency but also achieves the best error bound. This allows us to characterize the fundamental tradeoff between accuracy and complexity in some canonical scenarios. Further, we identify how the accuracy depends on the spectral gap of a corresponding comparison graph.",http://proceedings.mlr.press/v48/khetan16.html,http://proceedings.mlr.press/v48/khetan16.pdf,ICML
2111,2015,Deep Learning with Limited Numerical Precision,"Suyog Gupta,         Ankur Agrawal,         Kailash Gopalakrishnan,         Pritish Narayanan","Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network’s behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding",http://proceedings.mlr.press/v37/gupta15.html,http://proceedings.mlr.press/v37/gupta15.pdf,ICML
2112,2015,CUR Algorithm for Partially Observed Matrices,"Miao Xu,         Rong Jin,         Zhi-Hua Zhou","CUR matrix decomposition computes the low rank approximation of a given matrix by using the actual rows and columns of the matrix. It has been a very useful tool for handling large matrices. One limitation with the existing algorithms for CUR matrix decomposition is that they cannot deal with entries in a \it partially observed matrix, while incomplete matrices are found in many real world applications. In this work, we alleviate this limitation by developing a CUR decomposition algorithm for partially observed matrices. In particular, the proposed algorithm computes the low rank approximation of the target matrix based on (i) the randomly sampled rows and columns, and (ii) a subset of observed entries that are randomly sampled from the matrix. Our analysis shows the relative error bound, measured by spectral norm, for the proposed algorithm when the target matrix is of full rank. We also show that only O(n r\ln r) observed entries are needed by the proposed algorithm to perfectly recover a rank r matrix of size n\times n, which improves the sample complexity of the existing algorithms for matrix completion. Empirical studies on both synthetic and real-world datasets verify our theoretical claims and demonstrate the effectiveness of the proposed algorithm.",http://proceedings.mlr.press/v37/xua15.html,http://proceedings.mlr.press/v37/xua15.pdf,ICML
2113,2015,Swept Approximate Message Passing for Sparse Estimation,"Andre Manoel,         Florent Krzakala,         Eric Tramel,         Lenka Zdeborovà","Approximate Message Passing (AMP) has been shown to be a superior method for inference problems, such as the recovery of signals from sets of noisy, lower-dimensionality measurements, both in terms of reconstruction accuracy and in computational efficiency. However, AMP suffers from serious convergence issues in contexts that do not exactly match its assumptions. We propose a new approach to stabilizing AMP in these contexts by applying AMP updates to individual coefficients rather than in parallel. Our results show that this change to the AMP iteration can provide theoretically expected, but hitherto unobtainable, performance for problems on which the standard AMP iteration diverges. Additionally, we find that the computational costs of this swept coefficient update scheme is not unduly burdensome, allowing it to be applied efficiently to signals of large dimensionality.",http://proceedings.mlr.press/v37/manoel15.html,http://proceedings.mlr.press/v37/manoel15.pdf,ICML
2114,2015,The Benefits of Learning with Strongly Convex Approximate Inference,"Ben London,         Bert Huang,         Lise Getoor","We explore the benefits of strongly convex free energies in variational inference, providing both theoretical motivation and a new meta-algorithm. Using the duality between strong convexity and stability, we prove a high-probability bound on the error of learned marginals that is inversely proportional to the modulus of convexity of the free energy, thereby motivating free energies whose moduli are constant with respect to the size of the graph. We identify sufficient conditions for Ω(1)-strong convexity in two popular variational techniques: tree-reweighted and counting number entropies. Our insights for the latter suggest a novel counting number optimization framework, which guarantees strong convexity for any given modulus. Our experiments demonstrate that learning with a strongly convex free energy, using our optimization framework to guarantee a given modulus, results in substantially more accurate marginal probabilities, thereby validating our theoretical claims and the effectiveness of our framework.",http://proceedings.mlr.press/v37/london15.html,http://proceedings.mlr.press/v37/london15.pdf,ICML
2115,2015,Multi-Task Learning for Subspace Segmentation,"Yu Wang,         David Wipf,         Qing Ling,         Wei Chen,         Ian Wassell","Subspace segmentation is the process of clustering a set of data points that are assumed to lie on the union of multiple linear or affine subspaces, and is increasingly being recognized as a fundamental tool for data analysis in high dimensional settings. Arguably one of the most successful approaches is based on the observation that the sparsest representation of a given point with respect to a dictionary formed by the others involves nonzero coefficients associated with points originating in the same subspace. Such sparse representations are computed independently for each data point via \ell_1-norm minimization and then combined into an affinity matrix for use by a final spectral clustering step. The downside of this procedure is two-fold. First, unlike canonical compressive sensing scenarios with ideally-randomized dictionaries, the data-dependent dictionaries here are unavoidably highly structured, disrupting many of the favorable properties of the \ell_1 norm. Secondly, by treating each data point independently, we ignore useful relationships between points that can be leveraged for jointly computing such sparse representations. Consequently, we motivate a multi-task learning-based framework for learning coupled sparse representations leading to a segmentation pipeline that is both robust against correlation structure and tailored to generate an optimal affinity matrix. Theoretical analysis and empirical tests are provided to support these claims.",http://proceedings.mlr.press/v37/wangc15.html,http://proceedings.mlr.press/v37/wangc15.pdf,ICML
2116,2015,Streaming Sparse Principal Component Analysis,"Wenzhuo Yang,         Huan Xu","This paper considers estimating the leading k principal components with at most s non-zero attributes from p-dimensional samples collected sequentially in memory limited environments. We develop and analyze two memory and computational efficient algorithms called streaming sparse PCA and streaming sparse ECA for analyzing data generated according to the spike model and the elliptical model respectively. In particular, the proposed algorithms have memory complexity O(pk), computational complexity O(pk mink,slogp) and sample complexity Θ(s \log p). We provide their finite sample performance guarantees, which implies statistical consistency in the high dimensional regime. Numerical experiments on synthetic and real-world datasets demonstrate good empirical performance of the proposed algorithms.",http://proceedings.mlr.press/v37/yangd15.html,http://proceedings.mlr.press/v37/yangd15.pdf,ICML
2117,2015,Learning Transferable Features with Deep Adaptation Networks,"Mingsheng Long,         Yue Cao,         Jianmin Wang,         Michael Jordan","Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.",http://proceedings.mlr.press/v37/long15.html,http://proceedings.mlr.press/v37/long15.pdf,ICML
2118,2015,HawkesTopic: A Joint Model for Network Inference and Topic Modeling from Text-Based Cascades,"Xinran He,         Theodoros Rekatsinas,         James Foulds,         Lise Getoor,         Yan Liu","Understanding the diffusion of information in social network and social media requires modeling the text diffusion process. In this work, we develop the HawkesTopic model (HTM) for analyzing text-based cascades, such as ""retweeting a post"" or ""publishing a follow-up blog post"". HTM combines Hawkes processes and topic modeling to simultaneously reason about the information diffusion pathways and the topics characterizing the observed textual information. We show how to jointly infer them with a mean-field variational inference algorithm and validate our approach on both synthetic and real-world data sets, including a news media dataset for modeling information diffusion, and an ArXiv publication dataset for modeling scientific influence. The results show that HTM is significantly more accurate than several baselines for both tasks.",http://proceedings.mlr.press/v37/he15.html,http://proceedings.mlr.press/v37/he15.pdf,ICML
2119,2015,Submodularity in Data Subset Selection and Active Learning,"Kai Wei,         Rishabh Iyer,         Jeff Bilmes","We study the problem of selecting a subset of big data to train a classifier while incurring minimal performance loss. We show the connection of submodularity to the data likelihood functions for Naive Bayes (NB) and Nearest Neighbor (NN) classifiers, and formulate the data subset selection problems for these classifiers as constrained submodular maximization. Furthermore, we apply this framework to active learning and propose a novel scheme filtering active submodular selection (FASS), where we combine the uncertainty sampling method with a submodular data subset selection framework. We extensively evaluate the proposed framework on text categorization and handwritten digit recognition tasks with four different classifiers, including Deep Neural Network (DNN) based classifiers. Empirical results indicate that the proposed framework yields significant improvement over the state-of-the-art algorithms on all classifiers.",http://proceedings.mlr.press/v37/wei15.html,http://proceedings.mlr.press/v37/wei15.pdf,ICML
2120,2015,Following the Perturbed Leader for Online Structured Learning,"Alon Cohen,         Tamir Hazan","We investigate a new Follow the Perturbed Leader (FTPL) algorithm for online structured prediction problems. We show a regret bound which is comparable to the state of the art of FTPL algorithms and is comparable with the best possible regret in some cases. To better understand FTPL algorithms for online structured learning, we present a lower bound on the regret for a large and natural class of FTPL algorithms that use logconcave perturbations. We complete our investigation with an online shortest path experiment and empirically show that our algorithm is both statistically and computationally efficient.",http://proceedings.mlr.press/v37/cohena15.html,http://proceedings.mlr.press/v37/cohena15.pdf,ICML
2121,2015,Support Matrix Machines,"Luo Luo,         Yubo Xie,         Zhihua Zhang,         Wu-Jun Li","In many classification problems such as electroencephalogram (EEG) classification and image classification, the input features are naturally represented as matrices rather than vectors or scalars. In general, the structure information of the original feature matrix is useful and informative for data analysis tasks such as classification. One typical structure information is the correlation between columns or rows in the feature matrix. To leverage this kind of structure information, we propose a new classification method that we call support matrix machine (SMM). Specifically, SMM is defined as a hinge loss plus a so-called spectral elastic net penalty which is a spectral extension of the conventional elastic net over a matrix. The spectral elastic net enjoys a property of grouping effect, i.e., strongly correlated columns or rows tend to be selected altogether or not. Since the optimization problem for SMM is convex, this encourages us to devise an alternating direction method of multipliers algorithm for solving the problem. Experimental results on EEG and face image classification data show that our model is more robust and efficient than the state-of-the-art methods.",http://proceedings.mlr.press/v37/luo15.html,http://proceedings.mlr.press/v37/luo15.pdf,ICML
2122,2015,PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent,"Cho-Jui Hsieh,         Hsiang-Fu Yu,         Inderjit Dhillon","Stochastic Dual Coordinate Descent (DCD) is one of the most efficient ways to solve the family of L2-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of DCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the DCD algorithms in LIBLINEAR. In recent research, several synchronized parallel DCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of parallel asynchronous stochastic dual coordinate descent algorithms (PASSCoDe). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties of DCD when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present a novel error analysis for PASSCoDe under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with a perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers.",http://proceedings.mlr.press/v37/hsieha15.html,http://proceedings.mlr.press/v37/hsieha15.pdf,ICML
2123,2015,Algorithms for the Hard Pre-Image Problem of String Kernels and the General Problem of String Prediction,"Sébastien Giguère,         Amélie Rolland,         Francois Laviolette,         Mario Marchand","We address the pre-image problem encountered in structured output prediction and the one of finding a string maximizing the prediction function of various kernel-based classifiers and regressors. We demonstrate that these problems reduce to a common combinatorial problem valid for many string kernels. For this problem, we propose an upper bound on the prediction function which has low computational complexity and which can be used in a branch and bound search algorithm to obtain optimal solutions. We also show that for many string kernels, the complexity of the problem increases significantly when the kernel is normalized. On the optical word recognition task, the exact solution of the pre-image problem is shown to significantly improve the prediction accuracy in comparison with an approximation found by the best known heuristic. On the task of finding a string maximizing the prediction function of kernel-based classifiers and regressors, we highlight that existing methods can be biased toward long strings that contain many repeated symbols. We demonstrate that this bias is removed when using normalized kernels. Finally, we present results for the discovery of lead compounds in drug discovery. The source code can be found at https://github.com/a-ro/preimage",http://proceedings.mlr.press/v37/giguere15.html,http://proceedings.mlr.press/v37/giguere15.pdf,ICML
2124,2015,Learning to Search Better than Your Teacher,"Kai-Wei Chang,         Akshay Krishnamurthy,         Alekh Agarwal,         Hal Daume,         John Langford","Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently, LOLS can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications.",http://proceedings.mlr.press/v37/changb15.html,http://proceedings.mlr.press/v37/changb15.pdf,ICML
2125,2015,Learning Fast-Mixing Models for Structured Prediction,"Jacob Steinhardt,         Percy Liang","Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate inference inside learning, but their slow mixing can be difficult to diagnose and the resulting approximate gradients can seriously degrade learning. To alleviate these issues, we define a new model family using strong Doeblin Markov chains, whose mixing times can be precisely controlled by a parameter. We also develop an algorithm to learn such models, which involves maximizing the data likelihood under the induced stationary distribution of these chains. We show empirical improvements on two challenging inference tasks.",http://proceedings.mlr.press/v37/steinhardtb15.html,http://proceedings.mlr.press/v37/steinhardtb15.pdf,ICML
2126,2015,Binary Embedding: Fundamental Limits and Fast Algorithm,"Xinyang Yi,         Constantine Caramanis,         Eric Price","Binary embedding is a nonlinear dimension reduction methodology where high dimensional data are embedded into the Hamming cube while preserving the structure of the original space. Specifically, for an arbitrary N distinct points in \mathbbS^p-1, our goal is to encode each point using m-dimensional binary strings such that we can reconstruct their geodesic distance up to δuniform distortion. Existing binary embedding algorithms either lack theoretical guarantees or suffer from running time O(mp). We make three contributions: (1) we establish a lower bound that shows any binary embedding oblivious to the set of points requires m =Ω(\frac1δ^2\logN) bits and a similar lower bound for non-oblivious embeddings into Hamming distance; (2) we propose a novel fast binary embedding algorithm with provably optimal bit complexity m = O(\frac1 δ^2\logN) and near linear running time O(p \log p) whenever \log N ≪δ\sqrtp, with a slightly worse running time for larger \log N; (3) we also provide an analytic result about embedding a general set of points K ⊆\mathbbS^p-1 with even infinite size. Our theoretical findings are supported through experiments on both synthetic and real data sets.",http://proceedings.mlr.press/v37/yi15.html,http://proceedings.mlr.press/v37/yi15.pdf,ICML
2127,2015,Coresets for Nonparametric Estimation - the Case of DP-Means,"Olivier Bachem,         Mario Lucic,         Andreas Krause","Scalable training of Bayesian nonparametric models is a notoriously difficult challenge. We explore the use of coresets - a data summarization technique originating from computational geometry - for this task. Coresets are weighted subsets of the data such that models trained on these coresets are provably competitive with models trained on the full dataset. Coresets sublinear in the dataset size allow for fast approximate inference with provable guarantees. Existing constructions, however, are limited to parametric problems. Using novel techniques in coreset construction we show the existence of coresets for DP-Means - a prototypical nonparametric clustering problem - and provide a practical construction algorithm. We empirically demonstrate that our algorithm allows us to efficiently trade off computation time and approximation error and thus scale DP-Means to large datasets. For instance, with coresets we can obtain a computational speedup of 45x at an approximation error of only 2.4% compared to solving on the full data set. In contrast, for the same subsample size, the “naive” approach of uniformly subsampling the data incurs an approximation error of 22.5%.",http://proceedings.mlr.press/v37/bachem15.html,http://proceedings.mlr.press/v37/bachem15.pdf,ICML
2128,2015,Vector-Space Markov Random Fields via Exponential Families,"Wesley Tansey,         Oscar Hernan Madrid Padilla,         Arun Sai Suggala,         Pradeep Ravikumar","We present Vector-Space Markov Random Fields (VS-MRFs), a novel class of undirected graphical models where each variable can belong to an arbitrary vector space. VS-MRFs generalize a recent line of work on scalar-valued, uni-parameter exponential family and mixed graphical models, thereby greatly broadening the class of exponential families available (e.g., allowing multinomial and Dirichlet distributions). Specifically, VS-MRFs are the joint graphical model distributions where the node-conditional distributions belong to generic exponential families with general vector space domains. We also present a sparsistent M-estimator for learning our class of MRFs that recovers the correct set of edges with high probability. We validate our approach via a set of synthetic data experiments as well as a real-world case study of over four million foods from the popular diet tracking app MyFitnessPal. Our results demonstrate that our algorithm performs well empirically and that VS-MRFs are capable of capturing and highlighting interesting structure in complex, real-world data. All code for our algorithm is open source and publicly available.",http://proceedings.mlr.press/v37/tansey15.html,http://proceedings.mlr.press/v37/tansey15.pdf,ICML
2129,2015,Community Detection Using Time-Dependent Personalized PageRank,"Haim Avron,         Lior Horesh","Local graph diffusions have proven to be valuable tools for solving various graph clustering problems. As such, there has been much interest recently in efficient local algorithms for computing them. We present an efficient local algorithm for approximating a graph diffusion that generalizes both the celebrated personalized PageRank and its recent competitor/companion - the heat kernel. Our algorithm is based on writing the diffusion vector as the solution of an initial value problem, and then using a waveform relaxation approach to approximate the solution. Our experimental results suggest that it produces rankings that are distinct and competitive with the ones produced by high quality implementations of personalized PageRank and localized heat kernel, and that our algorithm is a useful addition to the toolset of localized graph diffusions.",http://proceedings.mlr.press/v37/avron15.html,http://proceedings.mlr.press/v37/avron15.pdf,ICML
2130,2015,A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits,"Pratik Gajane,         Tanguy Urvoy,         Fabrice Clérot","We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner receives only relative feedback about the selected pairs of arms. We propose a new algorithm called Relative Exponential-weight algorithm for Exploration and Exploitation (REX3) to handle the adversarial utility-based formulation of this problem. This algorithm is a non-trivial extension of the Exponential-weight algorithm for Exploration and Exploitation (EXP3) algorithm. We prove a finite time expected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and a general lower bound of order omega(sqrt(KT)). At the end, we provide experimental results using real data from information retrieval applications.",http://proceedings.mlr.press/v37/gajane15.html,http://proceedings.mlr.press/v37/gajane15.pdf,ICML
2131,2015,MRA-based Statistical Learning from Incomplete Rankings,"Eric Sibony,         Stéphan Clemençon,         Jérémie Jakubowicz","Statistical analysis of rank data describing preferences over small and variable subsets of a potentially large ensemble of items 1, ..., n is a very challenging problem. It is motivated by a wide variety of modern applications, such as recommender systems or search engines. However, very few inference methods have been documented in the literature to learn a ranking model from such incomplete rank data. The goal of this paper is twofold: it develops a rigorous mathematical framework for the problem of learning a ranking model from incomplete rankings and introduces a novel general statistical method to address it. Based on an original concept of multi-resolution analysis (MRA) of incomplete rankings, it finely adapts to any observation setting, leading to a statistical accuracy and an algorithmic complexity that depend directly on the complexity of the observed data. Beyond theoretical guarantees, we also provide experimental results that show its statistical performance.",http://proceedings.mlr.press/v37/sibony15.html,http://proceedings.mlr.press/v37/sibony15.pdf,ICML
2132,2015,Markov Chain Monte Carlo and Variational Inference: Bridging the Gap,"Tim Salimans,         Diederik Kingma,         Max Welling","Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.",http://proceedings.mlr.press/v37/salimans15.html,http://proceedings.mlr.press/v37/salimans15.pdf,ICML
2133,2015,Efficient Training of LDA on a GPU by Mean-for-Mode Estimation,"Jean-Baptiste Tristan,         Joseph Tassarotti,         Guy Steele","We introduce Mean-for-Mode estimation, a variant of an uncollapsed Gibbs sampler that we use to train LDA on a GPU. The algorithm combines benefits of both uncollapsed and collapsed Gibbs samplers. Like a collapsed Gibbs sampler — and unlike an uncollapsed Gibbs sampler — it has good statistical performance, and can use sampling complexity reduction techniques such as sparsity. Meanwhile, like an uncollapsed Gibbs sampler — and unlike a collapsed Gibbs sampler — it is embarrassingly parallel, and can use approximate counters.",http://proceedings.mlr.press/v37/tristan15.html,http://proceedings.mlr.press/v37/tristan15.pdf,ICML
2134,2015,Bayesian Multiple Target Localization,"Purnima Rajan,         Weidong Han,         Raphael Sznitman,         Peter Frazier,         Bruno Jedynak","We consider the problem of quickly localizing multiple targets by asking questions of the form “How many targets are within this set"" while obtaining noisy answers. This setting is a generalization to multiple targets of the game of 20 questions in which only a single target is queried. We assume that the targets are points on the real line, or in a two dimensional plane for the experiments, drawn independently from a known distribution. We evaluate the performance of a policy using the expected entropy of the posterior distribution after a fixed number of questions with noisy answers. We derive a lower bound for the value of this problem and study a specific policy, named the dyadic policy. We show that this policy achieves a value which is no more than twice this lower bound when answers are noise-free, and show a more general constant factor approximation guarantee for the noisy setting. We present an empirical evaluation of this policy on simulated data for the problem of detecting multiple instances of the same object in an image. Finally, we present experiments on localizing multiple faces simultaneously on real images.",http://proceedings.mlr.press/v37/rajan15.html,http://proceedings.mlr.press/v37/rajan15.pdf,ICML
2135,2015,Optimizing Neural Networks with Kronecker-factored Approximate Curvature,"James Martens,         Roger Grosse","We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network’s Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC’s approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.",http://proceedings.mlr.press/v37/martens15.html,http://proceedings.mlr.press/v37/martens15.pdf,ICML
2136,2015,A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data,"Yining Wang,         Yu-Xiang Wang,         Aarti Singh","Subspace clustering groups data into several lowrank subspaces. In this paper, we propose a theoretical framework to analyze a popular optimization-based algorithm, Sparse Subspace Clustering (SSC), when the data dimension is compressed via some random projection algorithms. We show SSC provably succeeds if the random projection is a subspace embedding, which includes random Gaussian projection, uniform row sampling, FJLT, sketching, etc. Our analysis applies to the most general deterministic setting and is able to handle both adversarial and stochastic noise. It also results in the first algorithm for privacy-preserved subspace clustering.",http://proceedings.mlr.press/v37/wange15.html,http://proceedings.mlr.press/v37/wange15.pdf,ICML
2137,2015,Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization,"Yuchen Zhang,         Xiao Lin","We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate method, which alternates between maximizing over one (or more) randomly chosen dual variable and minimizing over the primal variable. We also develop an extension to non-smooth and non-strongly convex loss functions, and an extension with better convergence rate on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods.",http://proceedings.mlr.press/v37/zhanga15.html,http://proceedings.mlr.press/v37/zhanga15.pdf,ICML
2138,2015,The Ladder: A Reliable Leaderboard for Machine Learning Competitions,"Avrim Blum,         Moritz Hardt","The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from a Kaggle competition. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.",http://proceedings.mlr.press/v37/blum15.html,http://proceedings.mlr.press/v37/blum15.pdf,ICML
2139,2015,"Distributional Rank Aggregation, and an Axiomatic Analysis","Adarsh Prasad,         Harsh Pareek,         Pradeep Ravikumar","The rank aggregation problem has been studied with varying desiderata in varied communities such as Theoretical Computer Science, Statistics, Information Retrieval and Social Welfare Theory. We introduce a variant of this problem we call distributional rank aggregation, where the ranking data is only available via the induced distribution over the set of all permutations. We provide a novel translation of the usual social welfare theory axioms to this setting. As we show this allows for a more quantitative characterization of these axioms: which then are not only less prone to misinterpretation, but also allow simpler proofs for some key impossibility theorems. Most importantly, these quantitative characterizations lead to natural and novel relaxations of these axioms, which as we show, allow us to get around celebrated impossibility results in social choice theory. We are able to completely characterize the class of positional scoring rules with respect to our axioms and show that Borda Count is optimal in a certain sense.",http://proceedings.mlr.press/v37/prasad15.html,http://proceedings.mlr.press/v37/prasad15.pdf,ICML
2140,2015,Training Deep Convolutional Neural Networks to Play Go,"Christopher Clark,         Amos Storkey","Mastering the game of Go has remained a long-standing challenge to the field of AI. Modern computer Go programs rely on processing millions of possible future positions to play well, but intuitively a stronger and more ’humanlike’ way to play the game would be to rely on pattern recognition rather than brute force computation. Following this sentiment, we train deep convolutional neural networks to play Go by training them to predict the moves made by expert Go players. To solve this problem we introduce a number of novel techniques, including a method of tying weights in the network to ’hard code’ symmetries that are expected to exist in the target function, and demonstrate in an ablation study they considerably improve performance. Our final networks are able to achieve move prediction accuracies of 41.1% and 44.4% on two different Go datasets, surpassing previous state of the art on this task by significant margins. Additionally, while previous move prediction systems have not yielded strong Go playing programs, we show that the networks trained in this work acquired high levels of skill. Our convolutional neural networks can consistently defeat the well known Go program GNU Go and win some games against state of the art Go playing program Fuego while using a fraction of the play time.",http://proceedings.mlr.press/v37/clark15.html,http://proceedings.mlr.press/v37/clark15.pdf,ICML
2141,2015,Threshold Influence Model for Allocating Advertising Budgets,"Atsushi Miyauchi,         Yuni Iwamasa,         Takuro Fukunaga,         Naonori Kakimura","We propose a new influence model for allocating budgets to advertising channels. Our model captures customer’s sensitivity to advertisements as a threshold behavior; a customer is expected to be influenced if the influence he receives exceeds his threshold. Over the threshold model, we discuss two optimization problems. The first one is the budget-constrained influence maximization. We propose two greedy algorithms based on different strategies, and analyze the performance when the influence is submodular. We then introduce a new characteristic to measure the cost-effectiveness of a marketing campaign, that is, the proportion of the resulting influence to the cost spent. We design an almost linear-time approximation algorithm to maximize the cost-effectiveness. Furthermore, we design a better-approximation algorithm based on linear programming for a special case. We conduct thorough experiments to confirm that our algorithms outperform baseline algorithms.",http://proceedings.mlr.press/v37/miyauchi15.html,http://proceedings.mlr.press/v37/miyauchi15.pdf,ICML
2142,2015,A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning,"Debarghya Ghoshdastidar,         Ambedkar Dukkipati","Matrix spectral methods play an important role in statistics and machine learning, and most often the word ‘matrix’ is dropped as, by default, one assumes that similarities or affinities are measured between two points, thereby resulting in similarity matrices. However, recent challenges in computer vision and text mining have necessitated the use of multi-way affinities in the learning methods, and this has led to a considerable interest in hypergraph partitioning methods in machine learning community. A plethora of “higher-order” algorithms have been proposed in the past decade, but their theoretical guarantees are not well-studied. In this paper, we develop a unified approach for partitioning uniform hypergraphs by means of a tensor trace optimization problem involving the affinity tensor, and a number of existing higher-order methods turn out to be special cases of the proposed formulation. We further propose an algorithm to solve the proposed trace optimization problem, and prove that it is consistent under a planted hypergraph model. We also provide experimental results to validate our theoretical findings.",http://proceedings.mlr.press/v37/ghoshdastidar15.html,http://proceedings.mlr.press/v37/ghoshdastidar15.pdf,ICML
2143,2015,Scalable Deep Poisson Factor Analysis for Topic Modeling,"Zhe Gan,         Changyou Chen,         Ricardo Henao,         David Carlson,         Lawrence Carin","A new framework for topic modeling is developed, based on deep graphical models, where interactions between topics are inferred through deep latent binary hierarchies. The proposed multi-layer model employs a deep sigmoid belief network or restricted Boltzmann machine, the bottom binary layer of which selects topics for use in a Poisson factor analysis model. Under this setting, topics live on the bottom layer of the model, while the deep specification serves as a flexible prior for revealing topic structure. Scalable inference algorithms are derived by applying Bayesian conditional density filtering algorithm, in addition to extending recently proposed work on stochastic gradient thermostats. Experimental results on several corpora show that the proposed approach readily handles very large collections of text documents, infers structured topic representations, and obtains superior test perplexities when compared with related models.",http://proceedings.mlr.press/v37/gan15.html,http://proceedings.mlr.press/v37/gan15.pdf,ICML
2144,2015,Telling cause from effect in deterministic linear dynamical systems,"Naji Shajarisales,         Dominik Janzing,         Bernhard Schoelkopf,         Michel Besserve","Telling a cause from its effect using observed time series data is a major challenge in natural and social sciences. Assuming the effect is generated by the cause through a linear system, we propose a new approach based on the hypothesis that nature chooses the “cause” and the “mechanism generating the effect from the cause” independently of each other. Specifically we postulate that the power spectrum of the “cause” time series is uncorrelated with the square of the frequency response of the linear filter (system) generating the effect. While most causal discovery methods for time series mainly rely on the noise, our method relies on asymmetries of the power spectral density properties that exist even in deterministic systems. We describe mathematical assumptions in a deterministic model under which the causal direction is identifiable. In particular, we show a scenario where the method works but Granger causality fails. Experiments show encouraging results on synthetic as well as real-world data. Overall, this suggests that the postulate of Independence of Cause and Mechanism is a promising principle for causal inference on observed time series.",http://proceedings.mlr.press/v37/shajarisales15.html,http://proceedings.mlr.press/v37/shajarisales15.pdf,ICML
2145,2015,Sparse Subspace Clustering with Missing Entries,"Congyuan Yang,         Daniel Robinson,         Rene Vidal","We consider the problem of clustering incomplete data drawn from a union of subspaces. Classical subspace clustering methods are not applicable to this problem because the data are incomplete, while classical low-rank matrix completion methods may not be applicable because data in multiple subspaces may not be low rank. This paper proposes and evaluates two new approaches for subspace clustering and completion. The first one generalizes the sparse subspace clustering algorithm so that it can obtain a sparse representation of the data using only the observed entries. The second one estimates a suitable kernel matrix by assuming a random model for the missing entries and obtains the sparse representation from this kernel. Experiments on synthetic and real data show the advantages and disadvantages of the proposed methods, which all outperform the natural approach (low-rank matrix completion followed by sparse subspace clustering) when the data matrix is high-rank or the percentage of missing entries is large.",http://proceedings.mlr.press/v37/yangf15.html,http://proceedings.mlr.press/v37/yangf15.pdf,ICML
2146,2015,Generalization error bounds for learning to rank: Does the length of document lists matter?,"Ambuj Tewari,         Sougata Chaudhuri","We consider the generalization ability of algorithms for learning to rank at a query level, a problem also called subset ranking. Existing generalization error bounds necessarily degrade as the size of the document list associated with a query increases. We show that such a degradation is not intrinsic to the problem. For several loss functions, including the cross-entropy loss used in the well known ListNet method, there is no degradation in generalization ability as document lists become longer. We also provide novel generalization error bounds under \ell_1 regularization and faster convergence rates if the loss function is smooth.",http://proceedings.mlr.press/v37/tewari15.html,http://proceedings.mlr.press/v37/tewari15.pdf,ICML
2147,2015,Faster cover trees,"Mike Izbicki,         Christian Shelton","The cover tree data structure speeds up exact nearest neighbor queries over arbitrary metric spaces. This paper makes cover trees even faster. In particular, we provide (1) a simpler definition of the cover tree that reduces the number of nodes from O(n) to exactly n, (2) an additional invariant that makes queries faster in practice, (3) algorithms for constructing and querying the tree in parallel on multiprocessor systems, and (4) a more cache efficient memory layout. On standard benchmark datasets, we reduce the number of distance computations by 10–50%. On a large-scale bioinformatics dataset, we reduce the number of distance computations by 71%. On a large-scale image dataset, our parallel algorithm with 16 cores reduces tree construction time from 3.5 hours to 12 minutes.",http://proceedings.mlr.press/v37/izbicki15.html,http://proceedings.mlr.press/v37/izbicki15.pdf,ICML
2148,2015,Stochastic Optimization with Importance Sampling for Regularized Loss Minimization,"Peilin Zhao,         Tong Zhang","Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization, including prox-SMD and prox-SDCA, with importance sampling, which improves the convergence rate by reducing the stochastic variance. We theoretically analyze the algorithms and empirically validate their effectiveness.",http://proceedings.mlr.press/v37/zhaoa15.html,http://proceedings.mlr.press/v37/zhaoa15.pdf,ICML
2149,2015,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,"Sergey Ioffe,         Christian Szegedy","Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.",http://proceedings.mlr.press/v37/ioffe15.html,http://proceedings.mlr.press/v37/ioffe15.pdf,ICML
2150,2015,Subsampling Methods for Persistent Homology,"Frederic Chazal,         Brittany Fasy,         Fabrizio Lecci,         Bertrand Michel,         Alessandro Rinaldo,         Larry Wasserman","Persistent homology is a multiscale method for analyzing the shape of sets and functions from point cloud data arising from an unknown distribution supported on those sets. When the size of the sample is large, direct computation of the persistent homology is prohibitive due to the combinatorial nature of the existing algorithms. We propose to compute the persistent homology of several subsamples of the data and then combine the resulting estimates. We study the risk of two estimators and we prove that the subsampling approach carries stable topological information while achieving a great reduction in computational complexity.",http://proceedings.mlr.press/v37/chazal15.html,http://proceedings.mlr.press/v37/chazal15.pdf,ICML
2151,2015,Scalable Model Selection for Large-Scale Factorial Relational Models,"Chunchen Liu,         Lu Feng,         Ryohei Fujimaki,         Yusuke Muraoka","With a growing need to understand large-scale networks, factorial relational models, such as binary matrix factorization models (BMFs), have become important in many applications. Although BMFs have a natural capability to uncover overlapping group structures behind network data, existing inference techniques have issues of either high computational cost or lack of model selection capability, and this limits their applicability. For scalable model selection of BMFs, this paper proposes stochastic factorized asymptotic Bayesian (sFAB) inference that combines concepts in two recently-developed techniques: stochastic variational inference (SVI) and FAB inference. sFAB is a highly-efficient algorithm, having both scalability and an inherent model selection capability in a single inference framework. Empirical results show the superiority of sFAB/BMF in both accuracy and scalability over state-of-the-art inference methods for overlapping relational models.",http://proceedings.mlr.press/v37/liub15.html,http://proceedings.mlr.press/v37/liub15.pdf,ICML
2152,2015,Theory of Dual-sparse Regularized Randomized Reduction,"Tianbao Yang,         Lijun Zhang,         Rong Jin,         Shenghuo Zhu","In this paper, we study randomized reduction methods, which reduce high-dimensional features into low-dimensional space by randomized methods (e.g., random projection, random hashing), for large-scale high-dimensional classification. Previous theoretical results on randomized reduction methods hinge on strong assumptions about the data, e.g., low rank of the data matrix or a large separable margin of classification, which hinder their in broad domains. To address these limitations, we propose dual-sparse regularized randomized reduction methods that introduce a sparse regularizer into the reduced dual problem. Under a mild condition that the original dual solution is a (nearly) sparse vector, we show that the resulting dual solution is close to the original dual solution and concentrates on its support set. In numerical experiments, we present an empirical study to support the analysis and we also present a novel application of the dual-sparse randomized reduction methods to reducing the communication cost of distributed learning from large-scale high-dimensional data.",http://proceedings.mlr.press/v37/yangb15.html,http://proceedings.mlr.press/v37/yangb15.pdf,ICML
2153,2015,Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis,"Zhuang Ma,         Yichao Lu,         Dean Foster","Canonical Correlation Analysis (CCA) is a widely used spectral technique for finding correlation structures in multi-view datasets. In this paper, we tackle the problem of large scale CCA, where classical algorithms, usually requiring computing the product of two huge matrices and huge matrix decomposition, are computationally and storage expensive. We recast CCA from a novel perspective and propose a scalable and memory efficient \textitAugmented Approximate Gradient (AppGrad) scheme for finding top k dimensional canonical subspace which only involves large matrix multiplying a thin matrix of width k and small matrix decomposition of dimension k\times k. Further, \textitAppGrad achieves optimal storage complexity O(k(p_1+p_2)), compared with classical algorithms which usually require O(p_1^2+p_2^2) space to store two dense whitening matrices. The proposed scheme naturally generalizes to stochastic optimization regime, especially efficient for huge datasets where batch algorithms are prohibitive. The online property of stochastic \textitAppGrad is also well suited to the streaming scenario, where data comes sequentially. To the best of our knowledge, it is the first stochastic algorithm for CCA. Experiments on four real data sets are provided to show the effectiveness of the proposed methods.",http://proceedings.mlr.press/v37/maa15.html,http://proceedings.mlr.press/v37/maa15.pdf,ICML
2154,2015,Is Feature Selection Secure against Training Data Poisoning?,"Huang Xiao,         Battista Biggio,         Gavin Brown,         Giorgio Fumera,         Claudia Eckert,         Fabio Roli","Learning in adversarial settings is becoming an important task for application domains where attackers may inject malicious data into the training set to subvert normal operation of data-driven technologies. Feature selection has been widely used in machine learning for security applications to improve generalization and computational efficiency, although it is not clear whether its use may be beneficial or even counterproductive when training data are poisoned by intelligent attackers. In this work, we shed light on this issue by providing a framework to investigate the robustness of popular feature selection methods, including LASSO, ridge regression and the elastic net. Our results on malware detection show that feature selection methods can be significantly compromised under attack (we can reduce LASSO to almost random choices of feature sets by careful insertion of less than 5% poisoned training samples), highlighting the need for specific countermeasures.",http://proceedings.mlr.press/v37/xiao15.html,http://proceedings.mlr.press/v37/xiao15.pdf,ICML
2155,2015,K-hyperplane Hinge-Minimax Classifier,"Margarita Osadchy,         Tamir Hazan,         Daniel Keren","We explore a novel approach to upper bound the misclassification error for problems with data comprising a small number of positive samples and a large number of negative samples. We assign the hinge-loss to upper bound the misclassification error of the positive examples and use the minimax risk to upper bound the misclassification error with respect to the worst case distribution that generates the negative examples. This approach is computationally appealing since the majority of training examples (belonging to the negative class) are represented by the statistics of their distribution, in contrast to kernel SVM which produces a very large number of support vectors in such settings. We derive empirical risk bounds for linear and non-linear classification and show that they are dimensionally independent and decay as 1/\sqrtm for m samples. We propose an efficient algorithm for training an intersection of finite number of hyperplane and demonstrate its effectiveness on real data, including letter and scene recognition.",http://proceedings.mlr.press/v37/osadchy15.html,http://proceedings.mlr.press/v37/osadchy15.pdf,ICML
2156,2015,A Hybrid Approach for Probabilistic Inference using Random Projections,"Michael Zhu,         Stefano Ermon","We introduce a new meta-algorithm for probabilistic inference in graphical models based on random projections. The key idea is to use approximate inference algorithms for an (exponentially) large number of samples, obtained by randomly projecting the original statistical model using universal hash functions. In the case where the approximate inference algorithm is a variational approximation, this approach can be viewed as interpolating between sampling-based and variational techniques. The number of samples used controls the trade-off between the accuracy of the approximate inference algorithm and the variance of the estimator. We show empirically that by using random projections, we can improve the accuracy of common approximate inference algorithms.",http://proceedings.mlr.press/v37/zhuc15.html,http://proceedings.mlr.press/v37/zhuc15.pdf,ICML
2157,2015,Classification with Low Rank and Missing Data,"Elad Hazan,         Roi Livni,         Yishay Mansour","We consider classification and regression tasks where we have missing data and assume that the (clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper formulation we give an efficient agnostic algorithm that classifies as good as the best linear classifier coupled with the best low-dimensional subspace in which the data resides. A direct implication is that our algorithm can linearly (and non-linearly through kernels) classify provably as well as the best classifier that has access to the full data.",http://proceedings.mlr.press/v37/hazan15.html,http://proceedings.mlr.press/v37/hazan15.pdf,ICML
2158,2015,Entropic Graph-based Posterior Regularization,"Maxwell Libbrecht,         Michael Hoffman,         Jeff Bilmes,         William Noble","Graph smoothness objectives have achieved great success in semi-supervised learning but have not yet been applied extensively to unsupervised generative models. We define a new class of entropic graph-based posterior regularizers that augment a probabilistic model by encouraging pairs of nearby variables in a regularization graph to have similar posterior distributions. We present a three-way alternating optimization algorithm with closed-form updates for performing inference on this joint model and learning its parameters. This method admits updates linear in the degree of the regularization graph, exhibits monotone convergence and is easily parallelizable. We are motivated by applications in computational biology in which temporal models such as hidden Markov models are used to learn a human-interpretable representation of genomic data. On a synthetic problem, we show that our method outperforms existing methods for graph-based regularization and a comparable strategy for incorporating long-range interactions using existing methods for approximate inference. Using genome-scale functional genomics data, we integrate genome 3D interaction data into existing models for genome annotation and demonstrate significant improvements in predicting genomic activity.",http://proceedings.mlr.press/v37/libbrecht15.html,http://proceedings.mlr.press/v37/libbrecht15.pdf,ICML
2159,2015,Learning Program Embeddings to Propagate Feedback on Student Code,"Chris Piech,         Jonathan Huang,         Andy Nguyen,         Mike Phulsuksombati,         Mehran Sahami,         Leonidas Guibas","Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University’s CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions.",http://proceedings.mlr.press/v37/piech15.html,http://proceedings.mlr.press/v37/piech15.pdf,ICML
2160,2015,Unsupervised Riemannian Metric Learning for Histograms Using Aitchison Transformations,"Tam Le,         Marco Cuturi","Many applications in machine learning handle bags of features or histograms rather than simple vectors. In that context, defining a proper geometry to compare histograms can be crucial for many machine learning algorithms. While one might be tempted to use a default metric such as the Euclidean metric, empirical evidence shows this may not be the best choice when dealing with observations that lie in the probability simplex. Additionally, it might be desirable to choose a metric adaptively based on data. We consider in this paper the problem of learning a Riemannian metric on the simplex given unlabeled histogram data. We follow the approach of Lebanon(2006), who proposed to estimate such a metric within a parametric family by maximizing the inverse volume of a given data set of points under that metric. The metrics we consider on the multinomial simplex are pull-back metrics of the Fisher information parameterized by operations within the simplex known as Aitchison(1982) transformations. We propose an algorithmic approach to maximize inverse volumes using sampling and contrastive divergences. We provide experimental evidence that the metric obtained under our proposal outperforms alternative approaches.",http://proceedings.mlr.press/v37/le15.html,http://proceedings.mlr.press/v37/le15.pdf,ICML
2161,2015,On Identifying Good Options under Combinatorially Structured Feedback in Finite Noisy Environments,"Yifan Wu,         Andras Gyorgy,         Csaba Szepesvari","We consider the problem of identifying a good option out of finite set of options under combinatorially structured, noisy feedback about the quality of the options in a sequential process: In each round, a subset of the options, from an available set of subsets, can be selected to receive noisy information about the quality of the options in the chosen subset. The goal is to identify the highest quality option, or a group of options of the highest quality, with a small error probability, while using the smallest number of measurements. The problem generalizes best-arm identification problems. By extending previous work, we design new algorithms that are shown to be able to exploit the combinatorial structure of the problem in a nontrivial fashion, while being unimprovable in special cases. The algorithms call a set multi-covering oracle, hence their performance and efficiency is strongly tied to whether the associated set multi-covering problem can be efficiently solved.",http://proceedings.mlr.press/v37/wub15.html,http://proceedings.mlr.press/v37/wub15.pdf,ICML
2162,2015,Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons,"Yuxin Chen,         Changho Suh","This paper explores the preference-based top-K rank aggregation problem. Suppose that a collection of items is repeatedly compared in pairs, and one wishes to recover a consistent ordering that emphasizes the top-K ranked items, based on partially revealed preferences. We focus on the Bradley-Terry-Luce (BTL) model that postulates a set of latent preference scores underlying all items, where the odds of paired comparisons depend only on the relative scores of the items involved. We characterize the minimax limits on identifiability of top-K ranked items, in the presence of random and non-adaptive sampling. Our results highlight a separation measure that quantifies the gap of preference scores between the K-th and (K+1)-th ranked items. The minimum sample complexity required for reliable top-K ranking scales inversely with the separation measure irrespective of other preference distribution metrics. To approach this minimax limit, we propose a nearly linear-time ranking scheme, called Spectral MLE, that returns the indices of the top-K items in accordance to a careful score estimate. In a nutshell, Spectral MLE starts with an initial score estimate with minimal squared loss (obtained via a spectral method), and then successively refines each component with the assistance of coordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-K item identification under minimal sample complexity. The practical applicability of Spectral MLE is further corroborated by numerical experiments.",http://proceedings.mlr.press/v37/chena15.html,http://proceedings.mlr.press/v37/chena15.pdf,ICML
2163,2015,Nested Sequential Monte Carlo Methods,"Christian Naesseth,         Fredrik Lindsten,         Thomas Schon","We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences of probability distributions, even where the random variables are high-dimensional. NSMC generalises the SMC framework by requiring only approximate, properly weighted, samples from the SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC can in itself be used to produce such properly weighted samples. Consequently, one NSMC sampler can be used to construct an efficient high-dimensional proposal distribution for another NSMC sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to consider complex and high-dimensional models using SMC. We show results that motivate the efficacy of our approach on several filtering problems with dimensions in the order of 100 to 1000.",http://proceedings.mlr.press/v37/naesseth15.html,http://proceedings.mlr.press/v37/naesseth15.pdf,ICML
2164,2015,Entropy-Based Concentration Inequalities for Dependent Variables,"Liva Ralaivola,         Massih-Reza Amini","We provide new concentration inequalities for functions of dependent variables. The work extends that of Janson (2004), which proposes concentration inequalities using a combination of the Laplace transform and the idea of fractional graph coloring, as well as many works that derive concentration inequalities using the entropy method (see, e.g., (Boucheron et al., 2003)). We give inequalities for fractionally sub-additive and fractionally self-bounding functions. In the way, we prove a new Talagrand concentration inequality for fractionally sub-additive functions of dependent variables. The results allow us to envision the derivation of generalization bounds for various applications where dependent variables naturally appear, such as in bipartite ranking.",http://proceedings.mlr.press/v37/ralaivola15.html,http://proceedings.mlr.press/v37/ralaivola15.pdf,ICML
2165,2015,Fictitious Self-Play in Extensive-Form Games,"Johannes Heinrich,         Marc Lanctot,         David Silver","Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of fictitious play that are implemented in behavioural strategies of an extensive-form game. The first variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements fictitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria.",http://proceedings.mlr.press/v37/heinrich15.html,http://proceedings.mlr.press/v37/heinrich15.pdf,ICML
2166,2015,Optimal and Adaptive Algorithms for Online Boosting,"Alina Beygelzimer,         Satyen Kale,         Haipeng Luo","We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. The second algorithm is adaptive and parameter-free, albeit not optimal.",http://proceedings.mlr.press/v37/beygelzimer15.html,http://proceedings.mlr.press/v37/beygelzimer15.pdf,ICML
2167,2015,Generative Moment Matching Networks,"Yujia Li,         Kevin Swersky,         Rich Zemel","We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.",http://proceedings.mlr.press/v37/li15.html,http://proceedings.mlr.press/v37/li15.pdf,ICML
2168,2015,Modeling Order in Neural Word Embeddings at Scale,"Andrew Trask,         David Gilmore,         Matthew Russell","Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships. The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks. We propose a new neural language model incorporating both word order and character order in its embedding. The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin. Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network.",http://proceedings.mlr.press/v37/trask15.html,http://proceedings.mlr.press/v37/trask15.pdf,ICML
2169,2015,Fast Kronecker Inference in Gaussian Processes with non-Gaussian Likelihoods,"Seth Flaxman,         Andrew Wilson,         Daniel Neill,         Hannes Nickisch,         Alex Smola","Gaussian processes (GPs) are a flexible class of methods with state of the art performance on spatial statistics applications. However, GPs require O(n^3) computations and O(n^2) storage, and popular GP kernels are typically limited to smoothing and interpolation. To address these difficulties, Kronecker methods have been used to exploit structure in the GP covariance matrix for scalability, while allowing for expressive kernel learning (Wilson et al., 2014). However, fast Kronecker methods have been confined to Gaussian likelihoods. We propose new scalable Kronecker methods for Gaussian processes with non-Gaussian likelihoods, using a Laplace approximation which involves linear conjugate gradients for inference, and a lower bound on the GP marginal likelihood for kernel learning. Our approach has near linear scaling, requiring O(D n^(D+1)/D) operations and O(D n^2/D) storage, for n training data-points on a dense D > 1 dimensional grid. Moreover, we introduce a log Gaussian Cox process, with highly expressive kernels, for modelling spatiotemporal count processes, and apply it to a point pattern (n = 233,088) of a decade of crime events in Chicago. Using our model, we discover spatially varying multiscale seasonal trends and produce highly accurate long-range local area forecasts.",http://proceedings.mlr.press/v37/flaxman15.html,http://proceedings.mlr.press/v37/flaxman15.pdf,ICML
2170,2015,Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing,"Rongda Zhu,         Quanquan Gu","In this paper, we propose a novel algorithm based on nonconvex sparsity-inducing penalty for one-bit compressed sensing. We prove that our algorithm has a sample complexity of O(s/ε^2) for strong signals, and O(s\log d/ε^2) for weak signals, where s is the number of nonzero entries in the signal vector, d is the signal dimension and εis the recovery error. For general signals, the sample complexity of our algorithm lies between O(s/ε^2) and O(s\log d/ε^2). This is a remarkable improvement over the existing best sample complexity O(s\log d/ε^2). Furthermore, we show that our algorithm achieves exact support recovery with high probability for strong signals. Our theory is verified by extensive numerical experiments, which clearly illustrate the superiority of our algorithm for both approximate signal and support recovery in the noisy setting.",http://proceedings.mlr.press/v37/zhua15.html,http://proceedings.mlr.press/v37/zhua15.pdf,ICML
2171,2015,Geometric Conditions for Subspace-Sparse Recovery,"Chong You,         Rene Vidal","Given a dictionary \Pi and a signal ξ= \Pi \mathbf x generated by a few \textitlinearly independent columns of \Pi, classical sparse recovery theory deals with the problem of uniquely recovering the sparse representation \mathbf x of ξ. In this work, we consider the more general case where ξlies in a low-dimensional subspace spanned by a few columns of \Pi, which are possibly \textitlinearly dependent. In this case, \mathbf x may not unique, and the goal is to recover any subset of the columns of \Pi that spans the subspace containing ξ. We call such a representation \mathbf x \textitsubspace-sparse. We study conditions under which existing pursuit methods recover a subspace-sparse representation. Such conditions reveal important geometric insights and have implications for the theory of classical sparse recovery as well as subspace clustering.",http://proceedings.mlr.press/v37/you15.html,http://proceedings.mlr.press/v37/you15.pdf,ICML
2172,2015,Preference Completion: Large-scale Collaborative Ranking from Pairwise Comparisons,"Dohyung Park,         Joe Neeman,         Jin Zhang,         Sujay Sanghavi,         Inderjit Dhillon","In this paper we consider the collaborative ranking setting: a pool of users each provides a set of pairwise preferences over a small subset of the set of d possible items; from these we need to predict each user’s preferences for items s/he has not yet seen. We do so via fitting a rank r score matrix to the pairwise data, and provide two main contributions: (a) We show that an algorithm based on convex optimization provides good generalization guarantees once each user provides as few as O(r \log^2 d) pairwise comparisons — essentially matching the sample complexity required in the related matrix completion setting (which uses actual numerical as opposed to pairwise information), and also matching a lower bound we establish here. (b) We develop a large-scale non-convex implementation, which we call AltSVM, which trains a factored form of the matrix via alternating minimization (which we show reduces to alternating SVM problems), and scales and parallelizes very well to large problem settings. It also outperforms common baselines on many moderately large popular collaborative filtering datasets in both NDCG and other measures of ranking performance.",http://proceedings.mlr.press/v37/park15.html,http://proceedings.mlr.press/v37/park15.pdf,ICML
2173,2015,Cheap Bandits,"Manjesh Hanawal,         Venkatesh Saligrama,         Michal Valko,         Remi Munos","We consider stochastic sequential learning problems where the learner can observe the average reward of several actions. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually cheaper to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is smooth over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while minimizing the sensing cost. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a Ω(\sqrt(dT)) lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension d.",http://proceedings.mlr.press/v37/hanawal15.html,http://proceedings.mlr.press/v37/hanawal15.pdf,ICML
2174,2015,Convex Calibrated Surrogates for Hierarchical Classification,"Harish Ramaswamy,         Ambuj Tewari,         Shivani Agarwal","Hierarchical classification problems are multiclass supervised learning problems with a pre-defined hierarchy over the set of class labels. In this work, we study the consistency of hierarchical classification algorithms with respect to a natural loss, namely the tree distance metric on the hierarchy tree of class labels, via the usage of calibrated surrogates. We first show that the Bayes optimal classifier for this loss classifies an instance according to the deepest node in the hierarchy such that the total conditional probability of the subtree rooted at the node is greater than \frac12. We exploit this insight to develop new consistent algorithm for hierarchical classification, that makes use of an algorithm known to be consistent for the “multiclass classification with reject option (MCRO)” problem as a sub-routine. Our experiments on a number of benchmark datasets show that the resulting algorithm, which we term OvA-Cascade, gives improved performance over other state-of-the-art hierarchical classification algorithms.",http://proceedings.mlr.press/v37/ramaswamy15.html,http://proceedings.mlr.press/v37/ramaswamy15.pdf,ICML
2175,2015,Consistent Multiclass Algorithms for Complex Performance Measures,"Harikrishna Narasimhan,         Harish Ramaswamy,         Aadirupa Saha,         Shivani Agarwal","This paper presents new consistent algorithms for multiclass learning with complex performance measures, defined by arbitrary functions of the confusion matrix. This setting includes as a special case all loss-based performance measures, which are simply linear functions of the confusion matrix, but also includes more complex performance measures such as the multiclass G-mean and micro F_1 measures. We give a general framework for designing consistent algorithms for such performance measures by viewing the learning problem as an optimization problem over the set of feasible confusion matrices, and give two specific instantiations based on the Frank-Wolfe method for concave performance measures and on the bisection method for ratio-of-linear performance measures. The resulting algorithms are provably consistent and outperform a multiclass version of the state-of-the-art SVMperf method in experiments; for large multiclass problems, the algorithms are also orders of magnitude faster than SVMperf.",http://proceedings.mlr.press/v37/narasimhanb15.html,http://proceedings.mlr.press/v37/narasimhanb15.pdf,ICML
2176,2015,Feature-Budgeted Random Forest,"Feng Nan,         Joseph Wang,         Venkatesh Saligrama","We seek decision rules for \it prediction-time cost reduction, where complete data is available for training, but during prediction-time, each feature can only be acquired for an additional cost. We propose a novel random forest algorithm to minimize prediction error for a user-specified \it average feature acquisition budget. While random forests yield strong generalization performance, they do not explicitly account for feature costs and furthermore require low correlation among trees, which amplifies costs. Our random forest grows trees with low acquisition cost and high strength based on greedy minimax cost-weighted-impurity splits. Theoretically, we establish near-optimal acquisition cost guarantees for our algorithm. Empirically, on a number of benchmark datasets we demonstrate competitive accuracy-cost curves against state-of-the-art prediction-time algorithms.",http://proceedings.mlr.press/v37/nan15.html,http://proceedings.mlr.press/v37/nan15.pdf,ICML
2177,2015,Online Time Series Prediction with Missing Data,"Oren Anava,         Elad Hazan,         Assaf Zeevi","We consider the problem of time series prediction in the presence of missing data. We cast the problem as an online learning problem in which the goal of the learner is to minimize prediction error. We then devise an efficient algorithm for the problem, which is based on autoregressive model, and does not assume any structure on the missing data nor on the mechanism that generates the time series. We show that our algorithm’s performance asymptotically approaches the performance of the best AR predictor in hindsight, and corroborate the theoretic results with an empirical study on synthetic and real-world data.",http://proceedings.mlr.press/v37/anava15.html,http://proceedings.mlr.press/v37/anava15.pdf,ICML
2178,2015,A Unifying Framework of Anytime Sparse Gaussian Process Regression Models with Stochastic Variational Inference for Big Data,"Trong Nghia Hoang,         Quang Minh Hoang,         Bryan Kian Hsiang Low","This paper presents a novel unifying framework of anytime sparse Gaussian process regression (SGPR) models that can produce good predictive performance fast and improve their predictive performance over time. Our proposed unifying framework reverses the variational inference procedure to theoretically construct a non-trivial, concave functional that is maximized at the predictive distribution of any SGPR model of our choice. As a result, a stochastic natural gradient ascent method can be derived that involves iteratively following the stochastic natural gradient of the functional to improve its estimate of the predictive distribution of the chosen SGPR model and is guaranteed to achieve asymptotic convergence to it. Interestingly, we show that if the predictive distribution of the chosen SGPR model satisfies certain decomposability conditions, then the stochastic natural gradient is an unbiased estimator of the exact natural gradient and can be computed in constant time (i.e., independent of data size) at each iteration. We empirically evaluate the trade-off between the predictive performance vs. time efficiency of the anytime SGPR models on two real-world million-sized datasets.",http://proceedings.mlr.press/v37/hoang15.html,http://proceedings.mlr.press/v37/hoang15.pdf,ICML
2179,2015,Scalable Bayesian Optimization Using Deep Neural Networks,"Jasper Snoek,         Oren Rippel,         Kevin Swersky,         Ryan Kiros,         Nadathur Satish,         Narayanan Sundaram,         Mostofa Patwary,         Mr Prabhat,         Ryan Adams","Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.",http://proceedings.mlr.press/v37/snoek15.html,http://proceedings.mlr.press/v37/snoek15.pdf,ICML
2180,2015,Convergence rate of Bayesian tensor estimator and its minimax optimality,Taiji Suzuki,"We investigate the statistical convergence rate of a Bayesian low-rank tensor estimator, and derive the minimax optimal rate for learning a low-rank tensor. Our problem setting is the regression problem where the regression coefficient forms a tensor structure. This problem setting occurs in many practical applications, such as collaborative filtering, multi-task learning, and spatio-temporal data analysis. The convergence rate of the Bayes tensor estimator is analyzed in terms of both in-sample and out-of-sample predictive accuracies. It is shown that a fast learning rate is achieved without any strong convexity of the observation. Moreover, we show that the method has adaptivity to the unknown rank of the true tensor, that is, the near optimal rate depending on the true rank is achieved even if it is not known a priori. Finally, we show the minimax optimal learning rate for the tensor estimation problem, and thus show that the derived bound of the Bayes estimator is tight and actually near minimax optimal.",http://proceedings.mlr.press/v37/suzuki15.html,http://proceedings.mlr.press/v37/suzuki15.pdf,ICML
2181,2015,Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs,"Yarin Gal,         Richard Turner","Standard sparse pseudo-input approximations to the Gaussian process (GP) cannot handle complex functions well. Sparse spectrum alternatives attempt to answer this but are known to over-fit. We suggest the use of variational inference for the sparse spectrum approximation to avoid both issues. We model the covariance function with a finite Fourier series approximation and treat it as a random variable. The random covariance function has a posterior, on which a variational distribution is placed. The variational distribution transforms the random covariance function to fit the data. We study the properties of our approximate inference, compare it to alternative ones, and extend it to the distributed and stochastic domains. Our approximation captures complex functions better than standard approaches and avoids over-fitting.",http://proceedings.mlr.press/v37/galb15.html,http://proceedings.mlr.press/v37/galb15.pdf,ICML
2182,2015,Abstraction Selection in Model-based Reinforcement Learning,"Nan Jiang,         Alex Kulesza,         Satinder Singh","State abstractions are often used to reduce the complexity of model-based reinforcement learning when only limited quantities of data are available. However, choosing the appropriate level of abstraction is an important problem in practice. Existing approaches have theoretical guarantees only under strong assumptions on the domain or asymptotically large amounts of data, but in this paper we propose a simple algorithm based on statistical hypothesis testing that comes with a finite-sample guarantee under assumptions on candidate abstractions. Our algorithm trades off the low approximation error of finer abstractions against the low estimation error of coarser abstractions, resulting in a loss bound that depends only on the quality of the best available abstraction and is polynomial in planning horizon.",http://proceedings.mlr.press/v37/jiang15.html,http://proceedings.mlr.press/v37/jiang15.pdf,ICML
2183,2015,Unsupervised Learning of Video Representations using LSTMs,"Nitish Srivastava,         Elman Mansimov,         Ruslan Salakhudinov","We use Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences – patches of image pixels and high-level representations (“percepts"") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem – human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.",http://proceedings.mlr.press/v37/srivastava15.html,http://proceedings.mlr.press/v37/srivastava15.pdf,ICML
2184,2015,Information Geometry and Minimum Description Length Networks,"Ke Sun,         Jun Wang,         Alexandros Kalousis,         Stephan Marchand-Maillet","We study parametric unsupervised mixture learning. We measure the loss of intrinsic information from the observations to complex mixture models, and then to simple mixture models. We present a geometric picture, where all these representations are regarded as free points in the space of probability distributions. Based on minimum description length, we derive a simple geometric principle to learn all these models together. We present a new learning machine with theories, algorithms, and simulations.",http://proceedings.mlr.press/v37/suna15.html,http://proceedings.mlr.press/v37/suna15.pdf,ICML
2185,2015,"Rademacher Observations, Private Data, and Boosting","Richard Nock,         Giorgio Patrini,         Arik Friedman","The minimization of the logistic loss is a popular approach to batch supervised learning. Our paper starts from the surprising observation that, when fitting linear classifiers, the minimization of the logistic loss is \textitequivalent to the minimization of an exponential \textitrado-loss computed (i) over transformed data that we call Rademacher observations (rados), and (ii) over the \textitsame classifier as the one of the logistic loss. Thus, a classifier learnt from rados can be \textitdirectly used to classify \textitobservations. We provide a learning algorithm over rados with boosting-compliant convergence rates on the \textitlogistic loss (computed over examples). Experiments on domains with up to millions of examples, backed up by theoretical arguments, display that learning over a small set of random rados can challenge the state of the art that learns over the \textitcomplete set of examples. We show that rados comply with various privacy requirements that make them good candidates for machine learning in a privacy framework. We give several algebraic, geometric and computational hardness results on reconstructing examples from rados. We also show how it is possible to craft, and efficiently learn from, rados in a differential privacy framework. Tests reveal that learning from differentially private rados brings non-trivial privacy vs accuracy tradeoffs.",http://proceedings.mlr.press/v37/nock15.html,http://proceedings.mlr.press/v37/nock15.pdf,ICML
2186,2015,Rebuilding Factorized Information Criterion: Asymptotically Accurate Marginal Likelihood,"Kohei Hayashi,         Shin-ichi Maeda,         Ryohei Fujimaki","Factorized information criterion (FIC) is a recently developed approximation technique for the marginal log-likelihood, which provides an automatic model selection framework for a few latent variable models (LVMs) with tractable inference algorithms. This paper reconsiders FIC and fills theoretical gaps of previous FIC studies. First, we reveal the core idea of FIC that allows generalization for a broader class of LVMs, including continuous LVMs, in contrast to previous FICs, which are applicable only to binary LVMs. Second, we investigate the model selection mechanism of the generalized FIC. Our analysis provides a formal justification of FIC as a model selection criterion for LVMs and also a systematic procedure for pruning redundant latent variables that have been removed heuristically in previous studies. Third, we provide an interpretation of FIC as a variational free energy and uncover previously-unknown their relationship. A demonstrative study on Bayesian principal component analysis is provided and numerical experiments support our theoretical results.",http://proceedings.mlr.press/v37/hayashi15.html,http://proceedings.mlr.press/v37/hayashi15.pdf,ICML
2187,2015,Hidden Markov Anomaly Detection,"Nico Goernitz,         Mikio Braun,         Marius Kloft","We introduce a new anomaly detection methodology for data with latent dependency structure. As a particular instantiation, we derive a hidden Markov anomaly detector that extends the regular one-class support vector machine. We optimize the approach, which is non-convex, via a DC (difference of convex functions) algorithm, and show that the parameter v can be conveniently used to control the number of outliers in the model. The empirical evaluation on artificial and real data from the domains of computational biology and computational sustainability shows that the approach can achieve significantly higher anomaly detection performance than the regular one-class SVM.",http://proceedings.mlr.press/v37/goernitz15.html,http://proceedings.mlr.press/v37/goernitz15.pdf,ICML
2188,2015,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","Kelvin Xu,         Jimmy Ba,         Ryan Kiros,         Kyunghyun Cho,         Aaron Courville,         Ruslan Salakhudinov,         Rich Zemel,         Yoshua Bengio","Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",http://proceedings.mlr.press/v37/xuc15.html,http://proceedings.mlr.press/v37/xuc15.pdf,ICML
2189,2015,Complete Dictionary Recovery Using Nonconvex Optimization,"Ju Sun,         Qing Qu,         John Wright","We consider the problem of recovering a complete (i.e., square and invertible) dictionary mb A_0, from mb Y = mb A_0 mb X_0 with mb Y ∈\mathbb R^n \times p. This recovery setting is central to the theoretical understanding of dictionary learning. We give the first efficient algorithm that provably recovers mb A_0 when mb X_0 has O(n) nonzeros per column, under suitable probability model for mb X_0. Prior results provide recovery guarantees when mb X_0 has only O(\sqrtn) nonzeros per column. Our algorithm is based on nonconvex optimization with a spherical constraint, and hence is naturally phrased in the language of manifold optimization. Our proofs give a geometric characterization of the high-dimensional objective landscape, which shows that with high probability there are no spurious local minima. Experiments with synthetic data corroborate our theory. Full version of this paper is available online: \urlhttp://arxiv.org/abs/1504.06785.",http://proceedings.mlr.press/v37/sund15.html,http://proceedings.mlr.press/v37/sund15.pdf,ICML
2190,2015,The Power of Randomization: Distributed Submodular Maximization on Massive Datasets,"Rafael Barbosa,         Alina Ene,         Huy Nguyen,         Justin Ward","A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We consider a distributed, greedy algorithm that combines previous approaches with randomization. The result is an algorithm that is embarrassingly parallel and achieves provable, constant factor, worst-case approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting.",http://proceedings.mlr.press/v37/barbosa15.html,http://proceedings.mlr.press/v37/barbosa15.pdf,ICML
2191,2015,Gated Feedback Recurrent Neural Networks,"Junyoung Chung,         Caglar Gulcehre,         Kyunghyun Cho,         Yoshua Bengio","In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.",http://proceedings.mlr.press/v37/chung15.html,http://proceedings.mlr.press/v37/chung15.pdf,ICML
2192,2015,An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization,"Necdet Aybat,         Zi Wang,         Garud Iyengar","We propose a distributed first-order augmented Lagrangian (DFAL) algorithm to minimize the sum of composite convex functions, where each term in the sum is a private cost function belonging to a node, and only nodes connected by an edge can directly communicate with each other. This optimization model abstracts a number of applications in distributed sensing and machine learning. We show that any limit point of DFAL iterates is optimal; and for any eps > 0, an eps-optimal and eps-feasible solution can be computed within O(log(1/eps)) DFAL iterations, which require O(\psi_\textmax^1.5/d_\textmin ⋅1/ε) proximal gradient computations and communications per node in total, where \psi_\textmax denotes the largest eigenvalue of the graph Laplacian, and d_\textmin is the minimum degree of the graph. We also propose an asynchronous version of DFAL by incorporating randomized block coordinate descent methods; and demonstrate the efficiency of DFAL on large scale sparse-group LASSO problems.",http://proceedings.mlr.press/v37/aybat15.html,http://proceedings.mlr.press/v37/aybat15.pdf,ICML
2193,2015,Correlation Clustering in Data Streams,"KookJin Ahn,         Graham Cormode,         Sudipto Guha,         Andrew McGregor,         Anthony Wirth","In this paper, we address the problem of \emphcorrelation clustering in the dynamic data stream model. The stream consists of updates to the edge weights of a graph on n nodes and the goal is to find a node-partition such that the end-points of negative-weight edges are typically in different clusters whereas the end-points of positive-weight edges are typically in the same cluster. We present polynomial-time, O(n⋅\textpolylog n)-space approximation algorithms for natural problems that arise. We first develop data structures based on linear sketches that allow the “quality” of a given node-partition to be measured. We then combine these data structures with convex programming and sampling techniques to solve the relevant approximation problem. However the standard LP and SDP formulations are not obviously solvable in O(n⋅\textpolylog n)-space. Our work presents space-efficient algorithms for the convex programming required, as well as approaches to reduce the adaptivity of the sampling. Note that the improved space and running-time bounds achieved from streaming algorithms are also useful for offline settings such as MapReduce models.",http://proceedings.mlr.press/v37/ahn15.html,http://proceedings.mlr.press/v37/ahn15.pdf,ICML
2194,2015,Stay on path: PCA along graph paths,"Megasthenis Asteris,         Anastasios Kyrillidis,         Alex Dimakis,         Han-Gyol Yi,         Bharath Chandrasekaran","We introduce a variant of (sparse) PCA in which the set of feasible support sets is determined by a graph. In particular, we consider the following setting: given a directed acyclic graph G on p vertices corresponding to variables, the non-zero entries of the extracted principal component must coincide with vertices lying along a path in G. From a statistical perspective, information on the underlying network may potentially reduce the number of observations required to recover the population principal component. We consider the canonical estimator which optimally exploits the prior knowledge by solving a non-convex quadratic maximization on the empirical covariance. We introduce a simple network and analyze the estimator under the spiked covariance model for sparse PCA. We show that side information potentially improves the statistical complexity. We propose two algorithms to approximate the solution of the constrained quadratic maximization, and recover a component with the desired properties. We empirically evaluate our schemes on synthetic and real datasets.",http://proceedings.mlr.press/v37/asteris15.html,http://proceedings.mlr.press/v37/asteris15.pdf,ICML
2195,2015,Online Learning of Eigenvectors,"Dan Garber,         Elad Hazan,         Tengyu Ma","Computing the leading eigenvector of a symmetric real matrix is a fundamental primitive of numerical linear algebra with numerous applications. We consider a natural online extension of the leading eigenvector problem: a sequence of matrices is presented and the goal is to predict for each matrix a unit vector, with the overall goal of competing with the leading eigenvector of the cumulative matrix. Existing regret-minimization algorithms for this problem either require to compute an \textiteigen decompostion every iteration, or suffer from a large dependency of the regret bound on the dimension. In both cases the algorithms are not practical for large scale applications. In this paper we present new algorithms that avoid both issues. On one hand they do not require any expensive matrix decompositions and on the other, they guarantee regret rates with a mild dependence on the dimension at most. In contrast to previous algorithms, our algorithms also admit implementations that enable to leverage sparsity in the data to further reduce computation. We extend our results to also handle non-symmetric matrices.",http://proceedings.mlr.press/v37/garberb15.html,http://proceedings.mlr.press/v37/garberb15.pdf,ICML
2196,2015,Adaptive Stochastic Alternating Direction Method of Multipliers,"Peilin Zhao,         Jinwei Yang,         Tong Zhang,         Ping Li","The Alternating Direction Method of Multipliers (ADMM) has been studied for years. Traditional ADMM algorithms need to compute, at each iteration, an (empirical) expected loss function on all training examples, resulting in a computational complexity proportional to the number of training examples. To reduce the complexity, stochastic ADMM algorithms were proposed to replace the expected loss function with a random loss function associated with one uniformly drawn example plus a Bregman divergence term. The Bregman divergence, however, is derived from a simple 2nd-order proximal function, i.e., the half squared norm, which could be a suboptimal choice. In this paper, we present a new family of stochastic ADMM algorithms with optimal 2nd-order proximal functions, which produce a new family of adaptive stochastic ADMM methods. We theoretically prove that the regret bounds are as good as the bounds which could be achieved by the best proximal function that can be chosen in hindsight. Encouraging empirical results on a variety of real-world datasets confirm the effectiveness and efficiency of the proposed algorithms.",http://proceedings.mlr.press/v37/zhaob15.html,http://proceedings.mlr.press/v37/zhaob15.pdf,ICML
2197,2015,PU Learning for Matrix Completion,"Cho-Jui Hsieh,         Nagarajan Natarajan,         Inderjit Dhillon","In this paper, we consider the matrix completion problem when the observations are one-bit measurements of some underlying matrix M , and in particular the observed samples consist only of ones and no zeros. This problem is motivated by modern applications such as recommender systems and social networks where only “likes” or “friendships” are observed. The problem is an instance of PU (positive-unlabeled) learning, i.e. learning from only positive and unlabeled examples that has been studied in the context of binary classification. Under the assumption that M has bounded nuclear norm, we provide recovery guarantees for two different observation models: 1) M parameterizes a distribution that generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For the first case, we propose a “shifted matrix completion” method that recovers M using only a subset of indices corresponding to ones; for the second case, we propose a “biased matrix completion” method that recovers the (thresholded) binary matrix. Both methods yield strong error bounds — if M ∈R^n \times n, the error is bounded as O(1-ρ) , where 1-ρdenotes the fraction of ones observed. This implies a sample complexity of O(n log n) ones to achieve a small error, when M is dense and n is large. We extend our analysis to the inductive matrix completion problem, where rows and columns of M have associated features. We develop efficient and scalable optimization procedures for both the proposed methods and demonstrate their effectiveness for link prediction (on real-world networks consisting of over 2 million nodes and 90 million links) and semi-supervised clustering tasks.",http://proceedings.mlr.press/v37/hsiehb15.html,http://proceedings.mlr.press/v37/hsiehb15.pdf,ICML
2198,2015,Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions,"Alina Ene,         Huy Nguyen","Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of “simple” functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster \emphlinear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations.",http://proceedings.mlr.press/v37/ene15.html,http://proceedings.mlr.press/v37/ene15.pdf,ICML
2199,2015,Learning Parametric-Output HMMs with Two Aliased States,"Roi Weiss,         Boaz Nadler","In various applications involving hidden Markov models (HMMs), some of the hidden states are aliased, having identical output distributions. The minimality, identifiability and learnability of such aliased HMMs have been long standing problems, with only partial solutions provided thus far. In this paper we focus on parametric-output HMMs, whose output distributions come from a parametric family, and that have exactly two aliased states. For this class, we present a complete characterization of their minimality and identifiability. Furthermore, for a large family of parametric output distributions, we derive computationally efficient and statistically consistent algorithms to detect the presence of aliasing and learn the aliased HMM transition and emission parameters. We illustrate our theoretical analysis by several simulations.",http://proceedings.mlr.press/v37/weiss15.html,http://proceedings.mlr.press/v37/weiss15.pdf,ICML
2200,2015,The Composition Theorem for Differential Privacy,"Peter Kairouz,         Sewoong Oh,         Pramod Viswanath","Interactive querying of a database degrades the privacy level. In this paper we answer the fundamental question of characterizing the level of privacy degradation as a function of the number of adaptive interactions and the differential privacy levels maintained by the individual queries. Our solution is complete: the privacy degradation guarantee is true for every privacy mechanism, and further, we demonstrate a sequence of privacy mechanisms that do degrade in the characterized manner. The key innovation is the introduction of an operational interpretation (involving hypothesis testing) to differential privacy and the use of the corresponding data processing inequalities. Our result improves over the state of the art and has immediate applications to several problems studied in the literature.",http://proceedings.mlr.press/v37/kairouz15.html,http://proceedings.mlr.press/v37/kairouz15.pdf,ICML
2201,2015,Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays,"Junpei Komiyama,         Junya Honda,         Hiroshi Nakagawa","We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al.\,(1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance.",http://proceedings.mlr.press/v37/komiyama15.html,http://proceedings.mlr.press/v37/komiyama15.pdf,ICML
2202,2015,Sparse Variational Inference for Generalized GP Models,"Rishit Sheth,         Yuyang Wang,         Roni Khardon","Gaussian processes (GP) provide an attractive machine learning model due to their non-parametric form, their flexibility to capture many types of observation data, and their generic inference procedures. Sparse GP inference algorithms address the cubic complexity of GPs by focusing on a small set of pseudo-samples. To date, such approaches have focused on the simple case of Gaussian observation likelihoods. This paper develops a variational sparse solution for GPs under general likelihoods by providing a new characterization of the gradients required for inference in terms of individual observation likelihood terms. In addition, we propose a simple new approach for optimizing the sparse variational approximation using a fixed point computation. We demonstrate experimentally that the fixed point operator acts as a contraction in many cases and therefore leads to fast convergence. An experimental evaluation for count regression, classification, and ordinal regression illustrates the generality and advantages of the new approach.",http://proceedings.mlr.press/v37/sheth15.html,http://proceedings.mlr.press/v37/sheth15.pdf,ICML
2203,2015,Bipartite Edge Prediction via Transductive Learning over Product Graphs,"Hanxiao Liu,         Yiming Yang","This paper addresses the problem of predicting the missing edges of a bipartite graph where each side of the vertices has its own intrinsic structure. We propose a new optimization framework to map the two sides of the intrinsic structures onto the manifold structure of the edges via a graph product, and to reduce the original problem to vertex label propagation over the product graph. This framework enjoys flexible choices in the formulation of graph products, and supports a rich family of graph transduction schemes with scalable inference. Experiments on benchmark datasets for collaborative filtering, citation network analysis and prerequisite prediction of online courses show advantageous performance of the proposed approach over other state-of-the-art methods.",http://proceedings.mlr.press/v37/liuc15.html,http://proceedings.mlr.press/v37/liuc15.pdf,ICML
2204,2015,Metadata Dependent Mondrian Processes,"Yi Wang,         Bin Li,         Yang Wang,         Fang Chen","Stochastic partition processes in a product space play an important role in modeling relational data. Recent studies on the Mondrian process have introduced more flexibility into the block structure in relational models. A side-effect of such high flexibility is that, in data sparsity scenarios, the model is prone to overfit. In reality, relational entities are always associated with meta information, such as user profiles in a social network. In this paper, we propose a metadata dependent Mondrian process (MDMP) to incorporate meta information into the stochastic partition process in the product space and the entity allocation process on the resulting block structure. MDMP can not only encourage homogeneous relational interactions within blocks but also discourage meta-label diversity within blocks. Regularized by meta information, MDMP becomes more robust in data sparsity scenarios and easier to converge in posterior inference. We apply MDMP to link prediction and rating prediction and demonstrate that MDMP is more effective than the baseline models in prediction accuracy with a more parsimonious model structure.",http://proceedings.mlr.press/v37/wangd15.html,http://proceedings.mlr.press/v37/wangd15.pdf,ICML
2205,2015,The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling,Michael Betancourt,"Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the scalability of Hamiltonian Monte Carlo.",http://proceedings.mlr.press/v37/betancourt15.html,http://proceedings.mlr.press/v37/betancourt15.pdf,ICML
2206,2015,Optimizing Non-decomposable Performance Measures: A Tale of Two Classes,"Harikrishna Narasimhan,         Purushottam Kar,         Prateek Jain","Modern classification problems frequently present mild to severe label imbalance as well as specific requirements on classification characteristics, and require optimizing performance measures that are non-decomposable over the dataset, such as F-measure. Such measures have spurred much interest and pose specific challenges to learning algorithms since their non-additive nature precludes a direct application of well-studied large scale optimization methods such as stochastic gradient descent. In this paper we reveal that for two large families of performance measures that can be expressed as functions of true positive/negative rates, it is indeed possible to implement point stochastic updates. The families we consider are concave and pseudo-linear functions of TPR, TNR which cover several popularly used performance measures such as F-measure, G-mean and H-mean. Our core contribution is an adaptive linearization scheme for these families, using which we develop optimization techniques that enable truly point-based stochastic updates. For concave performance measures we propose SPADE, a stochastic primal dual solver; for pseudo-linear measures we propose STAMP, a stochastic alternate maximization procedure. Both methods have crisp convergence guarantees, demonstrate significant speedups over existing methods - often by an order of magnitude or more, and give similar or more accurate predictions on test data.",http://proceedings.mlr.press/v37/narasimhana15.html,http://proceedings.mlr.press/v37/narasimhana15.pdf,ICML
2207,2015,Learning Scale-Free Networks by Dynamic Node Specific Degree Prior,"Qingming Tang,         Siqi Sun,         Jinbo Xu","Learning network structure underlying data is an important problem in machine learning. This paper presents a novel degree prior to study the inference of scale-free networks, which are widely used to model social and biological networks. In particular, this paper formulates scale-free network inference using Gaussian Graphical model (GGM) regularized by a node degree prior. Our degree prior not only promotes a desirable global degree distribution, but also exploits the estimated degree of an individual node and the relative strength of all the edges of a single node. To fulfill this, this paper proposes a ranking-based method to dynamically estimate the degree of a node, which makes the resultant optimization problem challenging to solve. To deal with this, this paper presents a novel ADMM (alternating direction method of multipliers) procedure. Our experimental results on both synthetic and real data show that our prior not only yields a scale-free network, but also produces many more correctly predicted edges than existing scale-free inducing prior, hub-inducing prior and the l_1 norm.",http://proceedings.mlr.press/v37/tangb15.html,http://proceedings.mlr.press/v37/tangb15.pdf,ICML
2208,2015,On the Optimality of Multi-Label Classification under Subset Zero-One Loss for Distributions Satisfying the Composition Property,"Maxime Gasse,         Alexandre Aussem,         Haytham Elghazel","The benefit of exploiting label dependence in multi-label classification is known to be closely dependent on the type of loss to be minimized. In this paper, we show that the subsets of labels that appear as irreducible factors in the factorization of the conditional distribution of the label set given the input features play a pivotal role for multi-label classification in the context of subset Zero-One loss minimization, as they divide the learning task into simpler independent multi-class problems. We establish theoretical results to characterize and identify these irreducible label factors for any given probability distribution satisfying the Composition property. The analysis lays the foundation for generic multi-label classification and optimal feature subset selection procedures under this subclass of distributions. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data.",http://proceedings.mlr.press/v37/gasse15.html,http://proceedings.mlr.press/v37/gasse15.pdf,ICML
2209,2015,On the Rate of Convergence and Error Bounds for LSTD(λ),"Manel Tagorti,         Bruno Scherrer","We consider LSTD(λ), the least-squares temporal-difference algorithm with eligibility traces algorithm proposed by Boyan (2002). It computes a linear approximation of the value function of a fixed policy in a large Markov Decision Process. Under a β-mixing assumption, we derive, for any value of λ∈(0,1), a high-probability bound on the rate of convergence of this algorithm to its limit. We deduce a high-probability bound on the error of this algorithm, that extends (and slightly improves) that derived by Lazaric et al. (2012) in the specific case where λ=0. In the context of temporal-difference algorithms with value function approximation, this analysis is to our knowledge the first to provide insight on the choice of the eligibility-trace parameter λwith respect to the approximation quality of the space and the number of samples.",http://proceedings.mlr.press/v37/tagorti15.html,http://proceedings.mlr.press/v37/tagorti15.pdf,ICML
2210,2015,Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization,"Roy Frostig,         Rong Ge,         Sham Kakade,         Aaron Sidford","We develop a family of accelerated stochastic algorithms that optimize sums of convex functions. Our algorithms improve upon the fastest running time for empirical risk minimization (ERM), and in particular linear least-squares regression, across a wide range of problem settings. To achieve this, we establish a framework, based on the classical proximal point algorithm, useful for accelerating recent fast stochastic algorithms in a black-box fashion. Empirically, we demonstrate that the resulting algorithms exhibit notions of stability that are advantageous in practice. Both in theory and in practice, the provided algorithms reap the computational benefits of adding a large strongly convex regularization term, without incurring a corresponding bias to the original ERM problem.",http://proceedings.mlr.press/v37/frostig15.html,http://proceedings.mlr.press/v37/frostig15.pdf,ICML
2211,2015,Attribute Efficient Linear Regression with Distribution-Dependent Sampling,"Doron Kukliansky,         Ohad Shamir","We consider a budgeted learning setting, where the learner can only choose and observe a small subset of the attributes of each training example. We develop efficient algorithms for Ridge and Lasso linear regression, which utilize the geometry of the data by a novel distribution-dependent sampling scheme, and have excess risk bounds which are better a factor of up to O(d/k) over the state-of-the-art, where d is the dimension and k+1 is the number of observed attributes per example. Moreover, under reasonable assumptions, our algorithms are the first in our setting which can provably use *less* attributes than full-information algorithms, which is the main concern in budgeted learning. We complement our theoretical analysis with experiments which support our claims.",http://proceedings.mlr.press/v37/kukliansky15.html,http://proceedings.mlr.press/v37/kukliansky15.pdf,ICML
2212,2015,An Explicit Sampling Dependent Spectral Error Bound for Column Subset Selection,"Tianbao Yang,         Lijun Zhang,         Rong Jin,         Shenghuo Zhu","In this paper, we consider the problem of column subset selection. We present a novel analysis of the spectral norm reconstruction for a simple randomized algorithm and establish a new bound that depends explicitly on the sampling probabilities. The sampling dependent error bound (i) allows us to better understand the tradeoff in the reconstruction error due to sampling probabilities, (ii) exhibits more insights than existing error bounds that exploit specific probability distributions, and (iii) implies better sampling distributions. In particular, we show that a sampling distribution with probabilities proportional to the square root of the statistical leverage scores is better than uniform sampling, and is better than leverage-based sampling when the statistical leverage scores are very nonuniform. And by solving a constrained optimization problem related to the error bound with an efficient bisection search we are able to achieve better performance than using either the leverage-based distribution or that proportional to the square root of the statistical leverage scores. Numerical simulations demonstrate the benefits of the new sampling distributions for low-rank matrix approximation and least square approximation compared to state-of-the art algorithms.",http://proceedings.mlr.press/v37/yanga15.html,http://proceedings.mlr.press/v37/yanga15.pdf,ICML
2213,2015,A Divide and Conquer Framework for Distributed Graph Clustering,"Wenzhuo Yang,         Huan Xu","Graph clustering is about identifying clusters of closely connected nodes, and is a fundamental technique of data analysis with many applications including community detection, VLSI network partitioning, collaborative filtering, and many others. In order to improve the scalability of existing graph clustering algorithms, we propose a novel divide and conquer framework for graph clustering, and establish theoretical guarantees of exact recovery of the clusters. One additional advantage of the proposed framework is that it can identify small clusters – the size of the smallest cluster can be of size o(\sqrtn), in contrast to Ω(\sqrtn) required by standard methods. Extensive experiments on synthetic and real-world datasets demonstrate the efficiency and effectiveness of our framework.",http://proceedings.mlr.press/v37/yange15.html,http://proceedings.mlr.press/v37/yange15.pdf,ICML
2214,2015,Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets,"Dan Garber,         Elad Hazan","The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning. A key advantage of the method is that it avoids projections - the computational bottleneck in many applications - replacing it by a linear optimization step. Despite this advantage, the known convergence rates of the FW method fall behind standard first order methods for most settings of interest. It is an active line of research to derive faster linear optimization-based algorithms for various settings of convex optimization. In this paper we consider the special case of optimization over strongly convex sets, for which we prove that the vanila FW method converges at a rate of \frac1t^2. This gives a quadratic improvement in convergence rate compared to the general case, in which convergence is of the order \frac1t, and known to be tight. We show that various balls induced by \ell_p norms, Schatten norms and group norms are strongly convex on one hand and on the other hand, linear optimization over these sets is straightforward and admits a closed-form solution. We further show how several previous fast-rate results for the FW method follow easily from our analysis.",http://proceedings.mlr.press/v37/garbera15.html,http://proceedings.mlr.press/v37/garbera15.pdf,ICML
2215,2015,High Confidence Policy Improvement,"Philip Thomas,         Georgios Theocharous,         Mohammad Ghavamzadeh","We present a batch reinforcement learning (RL) algorithm that provides probabilistic guarantees about the quality of each policy that it proposes, and which has no hyper-parameter that requires expert tuning. Specifically, the user may select any performance lower-bound and confidence level and our algorithm will ensure that the probability that it returns a policy with performance below the lower bound is at most the specified confidence level. We then propose an incremental algorithm that executes our policy improvement algorithm repeatedly to generate multiple policy improvements. We show the viability of our approach with a simple 4 x 4 gridworld and the standard mountain car problem, as well as with a digital marketing application that uses real world data.",http://proceedings.mlr.press/v37/thomas15.html,http://proceedings.mlr.press/v37/thomas15.pdf,ICML
2216,2015,How Hard is Inference for Structured Prediction?,"Amir Globerson,         Tim Roughgarden,         David Sontag,         Cafer Yildirim","Structured prediction tasks in machine learning involve the simultaneous prediction of multiple labels. This is often done by maximizing a score function on the space of labels, which decomposes as a sum of pairwise elements, each depending on two specific labels. The goal of this paper is to develop a theoretical explanation of the empirical effectiveness of heuristic inference algorithms for solving such structured prediction problems. We study the minimum-achievable expected Hamming error in such problems, highlighting the case of 2D grid graphs, which are common in machine vision applications. Our main theorems provide tight upper and lower bounds on this error, as well as a polynomial-time algorithm that achieves the bound.",http://proceedings.mlr.press/v37/globerson15.html,http://proceedings.mlr.press/v37/globerson15.pdf,ICML
2217,2015,Landmarking Manifolds with Gaussian Processes,"Dawen Liang,         John Paisley","We present an algorithm for finding landmarks along a manifold. These landmarks provide a small set of locations spaced out along the manifold such that they capture the low-dimensional non-linear structure of the data embedded in the high-dimensional space. The approach does not select points directly from the dataset, but instead we optimize each landmark by moving along the continuous manifold space (as approximated by the data) according to the gradient of an objective function. We borrow ideas from active learning with Gaussian processes to define the objective, which has the property that a new landmark is ""repelled"" by those currently selected, allowing for exploration of the manifold. We derive a stochastic algorithm for learning with large datasets and show results on several datasets, including the Million Song Dataset and articles from the New York Times.",http://proceedings.mlr.press/v37/liang15.html,http://proceedings.mlr.press/v37/liang15.pdf,ICML
2218,2015,Context-based Unsupervised Data Fusion for Decision Making,"Erfan Soltanmohammadi,         Mort Naraghi-Pour,         Mihaela Schaar","Big Data received from sources such as social media, in-stream monitoring systems, networks, and markets is often mined for discovering patterns, detecting anomalies, and making decisions or predictions. In distributed learning and real-time processing of Big Data, ensemble-based systems in which a fusion center (FC) is used to combine the local decisions of several classifiers, have shown to be superior to single expert systems. However, optimal design of the FC requires knowledge of the accuracy of the individual classifiers which, in many cases, is not available. Moreover, in many applications supervised training of the FC is not feasible since the true labels of the data set are not available. In this paper, we propose an unsupervised joint estimation-detection scheme to estimate the accuracies of the local classifiers as functions of data context and to fuse the local decisions of the classifiers. Numerical results show the dramatic improvement of the proposed method as compared with the state of the art approaches.",http://proceedings.mlr.press/v37/soltanmohammadi15.html,http://proceedings.mlr.press/v37/soltanmohammadi15.pdf,ICML
2219,2015,Controversy in mechanistic modelling with Gaussian processes,"Benn Macdonald,         Catherine Higham,         Dirk Husmeier","Parameter inference in mechanistic models based on non-affine differential equations is computationally onerous, and various faster alternatives based on gradient matching have been proposed. A particularly promising approach is based on nonparametric Bayesian modelling with Gaussian processes, which exploits the fact that a Gaussian process is closed under differentiation. However, two alternative paradigms have been proposed. The first paradigm, proposed at NIPS 2008 and AISTATS 2013, is based on a product of experts approach and a marginalization over the derivatives of the state variables. The second paradigm, proposed at ICML 2014, is based on a probabilistic generative model and a marginalization over the state variables. The claim has been made that this leads to better inference results. In the present article, we offer a new interpretation of the second paradigm, which highlights the underlying assumptions, approximations and limitations. In particular, we show that the second paradigm suffers from an intrinsic identifiability problem, which the first paradigm is not affected by.",http://proceedings.mlr.press/v37/macdonald15.html,http://proceedings.mlr.press/v37/macdonald15.pdf,ICML
2220,2015,A Modified Orthant-Wise Limited Memory Quasi-Newton Method with Convergence Analysis,"Pinghua Gong,         Jieping Ye","The Orthant-Wise Limited memory Quasi-Newton (OWL-QN) method has been demonstrated to be very effective in solving the \ell_1-regularized sparse learning problem. OWL-QN extends the L-BFGS from solving unconstrained smooth optimization problems to \ell_1-regularized (non-smooth) sparse learning problems. At each iteration, OWL-QN does not involve any \ell_1-regularized quadratic optimization subproblem and only requires matrix-vector multiplications without an explicit use of the (inverse) Hessian matrix, which enables OWL-QN to tackle large-scale problems efficiently. Although many empirical studies have demonstrated that OWL-QN works quite well in practice, several recent papers point out that the existing convergence proof of OWL-QN is flawed and a rigorous convergence analysis for OWL-QN still remains to be established. In this paper, we propose a modified Orthant-Wise Limited memory Quasi-Newton (mOWL-QN) algorithm by slightly modifying the OWL-QN algorithm. As the main technical contribution of this paper, we establish a rigorous convergence proof for the mOWL-QN algorithm. To the best of our knowledge, our work fills the theoretical gap by providing the first rigorous convergence proof for the OWL-QN-type algorithm on solving \ell_1-regularized sparse learning problems. We also provide empirical studies to show that mOWL-QN works well and is as efficient as OWL-QN.",http://proceedings.mlr.press/v37/gonga15.html,http://proceedings.mlr.press/v37/gonga15.pdf,ICML
2221,2015,A Fast Variational Approach for Learning Markov Random Field Language Models,"Yacine Jernite,         Alexander Rush,         David Sontag","Language modelling is a fundamental building block of natural language processing. However, in practice the size of the vocabulary limits the distributions applicable for this task: specifically, one has to either resort to local optimization methods, such as those used in neural language models, or work with heavily constrained distributions. In this work, we take a step towards overcoming these difficulties. We present a method for global-likelihood optimization of a Markov random field language model exploiting long-range contexts in time independent of the corpus size. We take a variational approach to optimizing the likelihood and exploit underlying symmetries to greatly simplify learning. We demonstrate the efficiency of this method both for language modelling and for part-of-speech tagging.",http://proceedings.mlr.press/v37/jernite15.html,http://proceedings.mlr.press/v37/jernite15.pdf,ICML
2222,2015,A Unified Framework for Outlier-Robust PCA-like Algorithms,"Wenzhuo Yang,         Huan Xu","We propose a unified framework for making a wide range of PCA-like algorithms – including the standard PCA, sparse PCA and non-negative sparse PCA, etc. – robust when facing a constant fraction of arbitrarily corrupted outliers. Our theoretic analysis establishes solid performance guarantees of the proposed framework: its estimation error is upper bounded by a term depending on the intrinsic parameters of the data model, the selected PCA-like algorithm and the fraction of outliers. Comprehensive experiments on synthetic and real-world datasets demonstrate that the outlier-robust PCA-like algorithms derived from our framework have outstanding performance.",http://proceedings.mlr.press/v37/yangc15.html,http://proceedings.mlr.press/v37/yangc15.pdf,ICML
2223,2015,Unsupervised Domain Adaptation by Backpropagation,"Yaroslav Ganin,         Victor Lempitsky","Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of ""deep"" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.",http://proceedings.mlr.press/v37/ganin15.html,http://proceedings.mlr.press/v37/ganin15.pdf,ICML
2224,2015,Scaling up Natural Gradient by Sparsely Factorizing the Inverse Fisher Matrix,"Roger Grosse,         Ruslan Salakhudinov","Second-order optimization methods, such as natural gradient, are difficult to apply to high-dimensional problems, because they require approximately solving large linear systems. We present FActorized Natural Gradient (FANG), an approximation to natural gradient descent where the Fisher matrix is approximated with a Gaussian graphical model whose precision matrix can be computed efficiently. We analyze the Fisher matrix for a small RBM and derive an extremely sparse graphical model which is a good match to the covariance of the sufficient statistics. Our experiments indicate that FANG allows RBMs to be trained more efficiently compared with stochastic gradient descent. Additionally, our analysis yields insight into the surprisingly good performance of the “centering trick” for training RBMs.",http://proceedings.mlr.press/v37/grosse15.html,http://proceedings.mlr.press/v37/grosse15.pdf,ICML
2225,2015,Multi-view Sparse Co-clustering via Proximal Alternating Linearized Minimization,"Jiangwen Sun,         Jin Lu,         Tingyang Xu,         Jinbo Bi","When multiple views of data are available for a set of subjects, co-clustering aims to identify subject clusters that agree across the different views. We explore the problem of co-clustering when the underlying clusters exist in different subspaces of each view. We propose a proximal alternating linearized minimization algorithm that simultaneously decomposes multiple data matrices into sparse row and columns vectors. This approach is able to group subjects consistently across the views and simultaneously identify the subset of features in each view that are associated with the clusters. The proposed algorithm can globally converge to a critical point of the problem. A simulation study validates that the proposed algorithm can identify the hypothesized clusters and their associated features. Comparison with several latest multi-view co-clustering methods on benchmark datasets demonstrates the superior performance of the proposed approach.",http://proceedings.mlr.press/v37/sunb15.html,http://proceedings.mlr.press/v37/sunb15.pdf,ICML
2226,2015,Scalable Variational Inference in Log-supermodular Models,"Josip Djolonga,         Andreas Krause","We consider the problem of approximate Bayesian inference in log-supermodular models. These models encompass regular pairwise MRFs with binary variables, but allow to capture high order interactions, which are intractable for existing approximate inference techniques such as belief propagation, mean field and variants. We show that a recently proposed variational approach to inference in log-supermodular models – L-Field – reduces to the widely studied minimum norm problem for submodular minimization. This insight allows to leverage powerful existing tools, and allows solving the variational problem orders of magnitude more efficiently than previously possible. We then provide another natural interpretation of L-Field, demonstrating that it exactly minimizes a specific type of Renyi divergence measure. This insight sheds light on the nature of the variational approximations produced by L-Field. Furthermore, we show how to perform parallel inference as message passing in a suitable factor graph at a linear convergence rate, without having to sum up over all the configurations of the factor. Finally, we apply our approach to a challenging image segmentation task. Our experiments confirm scalability of our approach, high quality of the marginals and the benefit of incorporating higher order potentials.",http://proceedings.mlr.press/v37/djolonga15.html,http://proceedings.mlr.press/v37/djolonga15.pdf,ICML
2227,2015,A trust-region method for stochastic variational inference with applications to streaming data,"Lucas Theis,         Matt Hoffman","Stochastic variational inference allows for fast posterior inference in complex Bayesian models. However, the algorithm is prone to local optima which can make the quality of the posterior approximation sensitive to the choice of hyperparameters and initialization. We address this problem by replacing the natural gradient step of stochastic varitional inference with a trust-region update. We show that this leads to generally better results and reduced sensitivity to hyperparameters. We also describe a new strategy for variational inference on streaming data and show that here our trust-region method is crucial for getting good performance.",http://proceedings.mlr.press/v37/theis15.html,http://proceedings.mlr.press/v37/theis15.pdf,ICML
2228,2015,A Lower Bound for the Optimization of Finite Sums,"Alekh Agarwal,         Leon Bottou","This paper presents a lower bound for optimizing a finite sum of n functions, where each function is L-smooth and the sum is μ-strongly convex. We show that no algorithm can reach an error εin minimizing all functions from this class in fewer than Ω(n + \sqrtn(κ-1)\log(1/ε)) iterations, where κ=L/μis a surrogate condition number. We then compare this lower bound to upper bounds for recently developed methods specializing to this setting. When the functions involved in this sum are not arbitrary, but based on i.i.d. random data, then we further contrast these complexity results with those for optimal first-order methods to directly optimize the sum. The conclusion we draw is that a lot of caution is necessary for an accurate comparison, and identify machine learning scenarios where the new methods help computationally.",http://proceedings.mlr.press/v37/agarwal15.html,http://proceedings.mlr.press/v37/agarwal15.pdf,ICML
2229,2015,Learning Deep Structured Models,"Liang-Chieh Chen,         Alexander Schwing,         Alan Yuille,         Raquel Urtasun","Many problems in real-world applications involve predicting several random variables that are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such dependencies. The goal of this paper is to combine MRFs with deep learning to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as tagging of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.",http://proceedings.mlr.press/v37/chenb15.html,http://proceedings.mlr.press/v37/chenb15.pdf,ICML
2230,2015,On Symmetric and Asymmetric LSHs for Inner Product Search,"Behnam Neyshabur,         Nathan Srebro","We consider the problem of designing locality sensitive hashes (LSH) for inner product similarity, and of the power of asymmetric hashes in this context. Shrivastava and Li (2014a) argue that there is no symmetric LSH for the problem and propose an asymmetric LSH based on different mappings for query and database points. However, we show there does exist a simple symmetric LSH that enjoys stronger guarantees and better empirical performance than the asymmetric LSH they suggest. We also show a variant of the settings where asymmetry is in-fact needed, but there a different asymmetric LSH is required.",http://proceedings.mlr.press/v37/neyshabur15.html,http://proceedings.mlr.press/v37/neyshabur15.pdf,ICML
2231,2015,On TD(0) with function approximation: Concentration bounds and a centered variant with exponential convergence,"Nathaniel Korda,         Prashanth La","We provide non-asymptotic bounds for the well-known temporal difference learning algorithm TD(0) with linear function approximators. These include high-probability bounds as well as bounds in expectation. Our analysis suggests that a step-size inversely proportional to the number of iterations cannot guarantee optimal rate of convergence unless we assume (partial) knowledge of the stationary distribution for the Markov chain underlying the policy considered. We also provide bounds for the iterate averaged TD(0) variant, which gets rid of the step-size dependency while exhibiting the optimal rate of convergence. Furthermore, we propose a variant of TD(0) with linear approximators that incorporates a centering sequence, and establish that it exhibits an exponential rate of convergence in expectation. We demonstrate the usefulness of our bounds on two synthetic experimental settings.",http://proceedings.mlr.press/v37/korda15.html,http://proceedings.mlr.press/v37/korda15.pdf,ICML
2232,2015,Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components,"Philipp Geiger,         Kun Zhang,         Bernhard Schoelkopf,         Mingming Gong,         Dominik Janzing","A widely applied approach to causal inference from a time series X, often referred to as “(linear) Granger causal analysis”, is to simply regress present on past and interpret the regression matrix \hatB causally. However, if there is an unmeasured time series Z that influences X, then this approach can lead to wrong causal conclusions, i.e., distinct from those one would draw if one had additional information such as Z. In this paper we take a different approach: We assume that X together with some hidden Z forms a first order vector autoregressive (VAR) process with transition matrix A, and argue why it is more valid to interpret A causally instead of \hatB. Then we examine under which conditions the most important parts of A are identifiable or almost identifiable from only X. Essentially, sufficient conditions are (1) non-Gaussian, independent noise or (2) no influence from X to Z. We present two estimation algorithms that are tailored towards conditions (1) and (2), respectively, and evaluate them on synthetic and real-world data. We discuss how to check the model using X.",http://proceedings.mlr.press/v37/geiger15.html,http://proceedings.mlr.press/v37/geiger15.pdf,ICML
2233,2015,Off-policy Model-based Learning under Unknown Factored Dynamics,"Assaf Hallak,         Francois Schnitzler,         Timothy Mann,         Shie Mannor","Off-policy learning in dynamic decision problems is essential for providing strong evidence that a new policy is better than the one in use. But how can we prove superiority without testing the new policy? To answer this question, we introduce the G-SCOPE algorithm that evaluates a new policy based on data generated by the existing policy. Our algorithm is both computationally and sample efficient because it greedily learns to exploit factored structure in the dynamics of the environment. We present a finite sample analysis of our approach and show through experiments that the algorithm scales well on high-dimensional problems with few samples.",http://proceedings.mlr.press/v37/hallak15.html,http://proceedings.mlr.press/v37/hallak15.pdf,ICML
2234,2015,A Deeper Look at Planning as Learning from Replay,"Harm Vanseijen,         Rich Sutton","In reinforcement learning, the notions of experience replay, and of planning as learning from replayed experience, have long been used to find good policies with minimal training data. Replay can be seen either as model-based reinforcement learning, where the store of past experiences serves as the model, or as a way to avoid a conventional model of the environment altogether. In this paper, we look more deeply at how replay blurs the line between model-based and model-free methods. First, we show for the first time an exact equivalence between the sequence of value functions found by a model-based policy-evaluation method and by a model-free method with replay. Second, we present a general replay method that can mimic a spectrum of methods ranging from the explicitly model-free (TD(0)) to the explicitly model-based (linear Dyna). Finally, we use insights gained from these relationships to design a new model-based reinforcement learning algorithm for linear function approximation. This method, which we call forgetful LSTD(lambda), improves upon regular LSTD(lambda) because it extends more naturally to online control, and improves upon linear Dyna because it is a multi-step method, enabling it to perform well even in non-Markov problems or, equivalently, in problems with significant function approximation.",http://proceedings.mlr.press/v37/vanseijen15.html,http://proceedings.mlr.press/v37/vanseijen15.pdf,ICML
2235,2015,Structural Maxent Models,"Corinna Cortes,         Vitaly Kuznetsov,         Mehryar Mohri,         Umar Syed","We present a new class of density estimation models, Structural Maxent models, with feature functions selected from possibly very complex families. The design of our models is motivated by data-dependent convergence bounds and benefits from new data-dependent learning bounds expressed in terms of the Rademacher complexities of the sub-families composing the family of features considered. We prove a duality theorem, which we use to derive our Structural Maxent algorithm. We give a full description of our algorithm, including the details of its derivation and report the results of several experiments demonstrating that its performance compares favorably to that of existing regularized Maxent. We further similarly define conditional Structural Maxent models for multi-class classification problems. These are conditional probability models making use of possibly complex feature families. We also prove a duality theorem for these models which shows the connection between these models and existing binary and multi-class deep boosting algorithms.",http://proceedings.mlr.press/v37/cortes15.html,http://proceedings.mlr.press/v37/cortes15.pdf,ICML
2236,2015,From Word Embeddings To Document Distances,"Matt Kusner,         Yu Sun,         Nicholas Kolkin,         Kilian Weinberger","We present the Word Mover’s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to ""travel"" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover’s Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.",http://proceedings.mlr.press/v37/kusnerb15.html,http://proceedings.mlr.press/v37/kusnerb15.pdf,ICML
2237,2015,Budget Allocation Problem with Multiple Advertisers: A Game Theoretic View,"Takanori Maehara,         Akihiro Yabe,         Ken-ichi Kawarabayashi","In marketing planning, advertisers seek to maximize the number of customers by allocating given budgets to each media channel effectively. The budget allocation problem with a bipartite influence model captures this scenario; however, the model is problematic because it assumes there is only one advertiser in the market. In reality, there are many advertisers which are in conflict of advertisement; thus we must extend the model for such a case. By extending the budget allocation problem with a bipartite influence model, we propose a game-theoretic model problem that considers many advertisers. By simulating our model, we can analyze the behavior of a media channel market, e.g., we can estimate which media channels are allocated by an advertiser, and which customers are influenced by an advertiser. Our model has many attractive features. First, our model is a potential game; therefore, it has a pure Nash equilibrium. Second, any Nash equilibrium of our game has 2-optimal social utility, i.e., the price of anarchy is 2. Finally, the proposed model can be simulated very efficiently; thus it can be used to analyze large markets.",http://proceedings.mlr.press/v37/maehara15.html,http://proceedings.mlr.press/v37/maehara15.pdf,ICML
2238,2015,Removing systematic errors for exoplanet search via latent causes,"Bernhard Schölkopf,         David Hogg,         Dun Wang,         Dan Foreman-Mackey,         Dominik Janzing,         Carl-Johann Simon-Gabriel,         Jonas Peters","We describe a method for removing the effect of confounders in order to reconstruct a latent quantity of interest. The method, referred to as \em half-sibling regression, is inspired by recent work in causal inference using additive noise models. We provide a theoretical justification and illustrate the potential of the method in a challenging astronomy application.",http://proceedings.mlr.press/v37/scholkopf15.html,http://proceedings.mlr.press/v37/scholkopf15.pdf,ICML
2239,2015,Learning Local Invariant Mahalanobis Distances,"Ethan Fetaya,         Shimon Ullman","For many tasks and data types, there are natural transformations to which the data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensitive to rotation and translation. This requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. In this paper we propose a novel and computationally efficient way to learn a local Mahalanobis metric per datum, and show how we can learn a local invariant metric to any transformation in order to improve performance.",http://proceedings.mlr.press/v37/fetaya15.html,http://proceedings.mlr.press/v37/fetaya15.pdf,ICML
2240,2015,Learning from Corrupted Binary Labels via Class-Probability Estimation,"Aditya Menon,         Brendan Van Rooyen,         Cheng Soon Ong,         Bob Williamson","Many supervised learning problems involve learning from samples whose labels are corrupted in some way. For example, each sample may have some constant probability of being incorrectly labelled (learning with label noise), or one may have a pool of unlabelled samples in lieu of negative samples (learning from positive and unlabelled data). This paper uses class-probability estimation to study these and other corruption processes belonging to the mutually contaminated distributions framework (Scott et al., 2013), with three conclusions. First, one can optimise balanced error and AUC without knowledge of the corruption process parameters. Second, given estimates of the corruption parameters, one can minimise a range of classification risks. Third, one can estimate the corruption parameters using only corrupted data. Experiments confirm the efficacy of class-probability estimation in learning from corrupted labels.",http://proceedings.mlr.press/v37/menon15.html,http://proceedings.mlr.press/v37/menon15.pdf,ICML
2241,2015,DRAW: A Recurrent Neural Network For Image Generation,"Karol Gregor,         Ivo Danihelka,         Alex Graves,         Danilo Rezende,         Daan Wierstra","This paper introduces the Deep Recurrent Attentive Writer (DRAW) architecture for image generation with neural networks. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it is able to generate images that are indistinguishable from real data with the naked eye.",http://proceedings.mlr.press/v37/gregor15.html,http://proceedings.mlr.press/v37/gregor15.pdf,ICML
2242,2015,Distributed Gaussian Processes,"Marc Deisenroth,         Jun Wei Ng","To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets.",http://proceedings.mlr.press/v37/deisenroth15.html,http://proceedings.mlr.press/v37/deisenroth15.pdf,ICML
2243,2015,A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate,Ohad Shamir,"We describe and analyze a simple algorithm for principal component analysis and singular value decomposition, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly convex optimization, whereas here we apply it to an inherently non-convex problem, using a very different analysis.",http://proceedings.mlr.press/v37/shamir15.html,http://proceedings.mlr.press/v37/shamir15.pdf,ICML
2244,2015,Markov Mixed Membership Models,"Aonan Zhang,         John Paisley","We present a Markov mixed membership model (Markov M3) for grouped data that learns a fully connected graph structure among mixing components. A key feature of Markov M3 is that it interprets the mixed membership assignment as a Markov random walk over this graph of nodes. This is in contrast to tree-structured models in which the assignment is done according to a tree structure on the mixing components. The Markov structure results in a simple parametric model that can learn a complex dependency structure between nodes, while still maintaining full conjugacy for closed-form stochastic variational inference. Empirical results demonstrate that Markov M3 performs well compared with tree structured topic models, and can learn meaningful dependency structure between topics.",http://proceedings.mlr.press/v37/zhangd15.html,http://proceedings.mlr.press/v37/zhangd15.pdf,ICML
2245,2015,Differentially Private Bayesian Optimization,"Matt Kusner,         Jacob Gardner,         Roman Garnett,         Kilian Weinberger","Bayesian optimization is a powerful tool for fine-tuning the hyper-parameters of a wide variety of machine learning models. The success of machine learning has led practitioners in diverse real-world settings to learn classifiers for practical problems. As machine learning becomes commonplace, Bayesian optimization becomes an attractive method for practitioners to automate the process of classifier hyper-parameter tuning. A key observation is that the data used for tuning models in these settings is often sensitive. Certain data such as genetic predisposition, personal email statistics, and car accident history, if not properly private, may be at risk of being inferred from Bayesian optimization outputs. To address this, we introduce methods for releasing the best hyper-parameters and classifier accuracy privately. Leveraging the strong theoretical guarantees of differential privacy and known Bayesian optimization convergence bounds, we prove that under a GP assumption these private quantities are often near-optimal. Finally, even if this assumption is not satisfied, we can use different smoothness guarantees to protect privacy.",http://proceedings.mlr.press/v37/kusnera15.html,http://proceedings.mlr.press/v37/kusnera15.pdf,ICML
2246,2015,On Greedy Maximization of Entropy,"Dravyansh Sharma,         Ashish Kapoor,         Amit Deshpande","Submodular function maximization is one of the key problems that arise in many machine learning tasks. Greedy selection algorithms are the proven choice to solve such problems, where prior theoretical work guarantees (1 - 1/e) approximation ratio. However, it has been empirically observed that greedy selection provides almost optimal solutions in practice. The main goal of this paper is to explore and answer why the greedy selection does significantly better than the theoretical guarantee of (1 - 1/e). Applications include, but are not limited to, sensor selection tasks which use both entropy and mutual information as a maximization criteria. We give a theoretical justification for the nearly optimal approximation ratio via detailed analysis of the curvature of these objective functions for Gaussian RBF kernels.",http://proceedings.mlr.press/v37/sharma15.html,http://proceedings.mlr.press/v37/sharma15.pdf,ICML
2247,2015,Finding Galaxies in the Shadows of Quasars with Gaussian Processes,"Roman Garnett,         Shirley Ho,         Jeff Schneider","We develop an automated technique for detecting damped Lyman-αabsorbers (DLAs) along spectroscopic sightlines to quasi-stellar objects (QSOs or quasars). The detection of DLAs in large-scale spectroscopic surveys such as SDSS–III is critical to address outstanding cosmological questions, such as the nature of galaxy formation. We use nearly 50000 QSO spectra to learn a tailored Gaussian process model for quasar emission spectra, which we apply to the DLA detection problem via Bayesian model selection. We demonstrate our method’s effectiveness with a large-scale validation experiment on over 100000 spectra, with excellent performance.",http://proceedings.mlr.press/v37/garnett15.html,http://proceedings.mlr.press/v37/garnett15.pdf,ICML
2248,2015,Non-Stationary Approximate Modified Policy Iteration,"Boris Lesner,         Bruno Scherrer","We consider the infinite-horizon γ-discounted optimal control problem formalized by Markov Decision Processes. Running any instance of Modified Policy Iteration—a family of algorithms that can interpolate between Value and Policy Iteration—with an error εat each iteration is known to lead to stationary policies that are at least \frac2γε(1-γ)^2-optimal. Variations of Value and Policy Iteration, that build \ell-periodic non-stationary policies, have recently been shown to display a better \frac2γε(1-γ)(1-γ^\ell)-optimality guarantee. Our first contribution is to describe a new algorithmic scheme, Non-Stationary Modified Policy Iteration, a family of algorithms parameterized by two integers m \ge 0 and \ell \ge 1 that generalizes all the above mentionned algorithms. While m allows to interpolate between Value-Iteration-style and Policy-Iteration-style updates, \ell specifies the period of the non-stationary policy that is output. We show that this new family of algorithms also enjoys the improved \frac2γε(1-γ)(1-γ^\ell)-optimality guarantee. Perhaps more importantly, we show, by exhibiting an original problem instance, that this guarantee is tight for all m and \ell; this tightness was to our knowledge only proved two specific cases, Value Iteration (m=0,\ell=1) and Policy Iteration (m=∞,\ell=1).",http://proceedings.mlr.press/v37/lesner15.html,http://proceedings.mlr.press/v37/lesner15.pdf,ICML
2249,2015,Bimodal Modelling of Source Code and Natural Language,"Miltos Allamanis,         Daniel Tarlow,         Andrew Gordon,         Yi Wei","We consider the problem of building probabilistic models that jointly model short natural language utterances and source code snippets. The aim is to bring together recent work on statistical modelling of source code and work on bimodal models of images and natural language. The resulting models are useful for a variety of tasks that involve natural language and source code. We demonstrate their performance on two retrieval tasks: retrieving source code snippets given a natural language query, and retrieving natural language descriptions given a source code query (i.e., source code captioning). The experiments show there to be promise in this direction, and that modelling the structure of source code is helpful towards the retrieval tasks.",http://proceedings.mlr.press/v37/allamanis15.html,http://proceedings.mlr.press/v37/allamanis15.pdf,ICML
2250,2015,Tracking Approximate Solutions of Parameterized Optimization Problems over Multi-Dimensional (Hyper-)Parameter Domains,"Katharina Blechschmidt,         Joachim Giesen,         Soeren Laue","Many machine learning methods are given as parameterized optimization problems. Important examples of such parameters are regularization- and kernel hyperparameters. These parameters have to be tuned carefully since the choice of their values can have a significant impact on the statistical performance of the learning methods. In most cases the parameter space does not carry much structure and parameter tuning essentially boils down to exploring the whole parameter space. The case when there is only one parameter received quite some attention over the years. First, algorithms for tracking an optimal solution for several machine learning optimization problems over regularization- and hyperparameter intervals had been developed, but since these algorithms can suffer from numerical problems more robust and efficient approximate path tracking algorithms have been devised and analyzed recently. By now approximate path tracking algorithms are known for regularization-and kernel hyperparameter paths with optimal path complexities that depend only on the prescribed approximation error. Here we extend the work on approximate path tracking algorithms with approximation guarantees to multi-dimensional parameter domains. We show a lower bound on the complexity of approximately exploring a multi-dimensional parameter domain that is the product of the corresponding path complexities. We also show a matching upper bound that can be turned into a theoretically and practically efficient algorithm. Experimental results for kernelized support vector machines and the elastic net confirm the theoretical complexity analysis.",http://proceedings.mlr.press/v37/blechschmidt15.html,http://proceedings.mlr.press/v37/blechschmidt15.pdf,ICML
2251,2015,Double Nyström Method: An Efficient and Accurate Nyström Scheme for Large-Scale Data Sets,"Woosang Lim,         Minhwan Kim,         Haesun Park,         Kyomin Jung","The Nyström method has been one of the most effective techniques for kernel-based approach that scales well to large data sets. Since its introduction, there has been a large body of work that improves the approximation accuracy while maintaining computational efficiency. In this paper, we present a novel Nyström method that improves both accuracy and efficiency based on a new theoretical analysis. We first provide a generalized sampling scheme, CAPS, that minimizes a novel error bound based on the subspace distance. We then present our double Nyström method that reduces the size of the decomposition in two stages. We show that our method is highly efficient and accurate compared to other state-of-the-art Nyström methods by evaluating them on a number of real data sets.",http://proceedings.mlr.press/v37/lima15.html,http://proceedings.mlr.press/v37/lima15.pdf,ICML
2252,2015,A New Generalized Error Path Algorithm for Model Selection,"Bin Gu,         Charles Ling","Model selection with cross validation (CV) is very popular in machine learning. However, CV with grid and other common search strategies cannot guarantee to find the model with minimum CV error, which is often the ultimate goal of model selection. Recently, various solution path algorithms have been proposed for several important learning algorithms including support vector classification, Lasso, and so on. However, they still do not guarantee to find the model with minimum CV error.In this paper, we first show that the solution paths produced by various algorithms have the property of piecewise linearity. Then, we prove that a large class of error (or loss) functions are piecewise constant, linear, or quadratic w.r.t. the regularization parameter, based on the solution path. Finally, we propose a new generalized error path algorithm (GEP), and prove that it will find the model with minimum CV error for the entire range of the regularization parameter. The experimental results on a variety of datasets not only confirm our theoretical findings, but also show that the best model with our GEP has better generalization error on the test data, compared to the grid search, manual search, and random search.",http://proceedings.mlr.press/v37/gu15.html,http://proceedings.mlr.press/v37/gu15.pdf,ICML
2253,2015,Bayesian and Empirical Bayesian Forests,"Taddy Matthew,         Chun-Sheng Chen,         Jun Yu,         Mitch Wyle","We derive ensembles of decision trees through a nonparametric Bayesian model, allowing us to view such ensembles as samples from a posterior distribution. This insight motivates a class of Bayesian Forest (BF) algorithms that provide small gains in performance and large gains in interpretability. Based on the BF framework, we are able to show that high-level tree hierarchy is stable in large samples. This motivates an empirical Bayesian Forest (EBF) algorithm for building approximate BFs on massive distributed datasets and we show that EBFs outperform sub-sampling based alternatives by a large margin.",http://proceedings.mlr.press/v37/matthew15.html,http://proceedings.mlr.press/v37/matthew15.pdf,ICML
2254,2015,Accelerated Online Low Rank Tensor Learning for Multivariate Spatiotemporal Streams,"Rose Yu,         Dehua Cheng,         Yan Liu","Low-rank tensor learning has many applications in machine learning. A series of batch learning algorithms have achieved great successes. However, in many emerging applications, such as climate data analysis, we are confronted with large-scale tensor streams, which poses significant challenges to existing solution in terms of computational costs and limited response time. In this paper, we propose an online accelerated low-rank tensor learning algorithm (ALTO) to solve the problem. At each iteration, we project the current tensor to the subspace of low-rank tensors in order to perform efficient tensor decomposition, then recover the decomposition of the new tensor. By randomly glancing at additional subspaces, we successfully avoid local optima at negligible extra computational cost. We evaluate our method on two tasks in streaming multivariate spatio-temporal analysis: online forecasting and multi-model ensemble, which shows that our method achieves comparable predictive accuracy with significant boost in run time.",http://proceedings.mlr.press/v37/yua15.html,http://proceedings.mlr.press/v37/yua15.pdf,ICML
2255,2015,Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network,"Seunghoon Hong,         Tackgeun You,         Suha Kwak,         Bohyung Han","We propose an online visual tracking algorithm by learning discriminative saliency map using Convolutional Neural Network (CNN). Given a CNN pre-trained on a large-scale image repository in offline, our algorithm takes outputs from hidden layers of the network as feature descriptors since they show excellent representation performance in various general visual recognition problems. The features are used to learn discriminative target appearance models using an online Support Vector Machine (SVM). In addition, we construct target-specific saliency map by back-projecting CNN features with guidance of the SVM, and obtain the final tracking result in each frame based on the appearance model generatively constructed with the saliency map. Since the saliency map reveals spatial configuration of target effectively, it improves target localization accuracy and enables us to achieve pixel-level target segmentation. We verify the effectiveness of our tracking algorithm through extensive experiment on a challenging benchmark, where our method illustrates outstanding performance compared to the state-of-the-art tracking algorithms.",http://proceedings.mlr.press/v37/hong15.html,http://proceedings.mlr.press/v37/hong15.pdf,ICML
2256,2015,An Aligned Subtree Kernel for Weighted Graphs,"Lu Bai,         Luca Rossi,         Zhihong Zhang,         Edwin Hancock","In this paper, we develop a new entropic matching kernel for weighted graphs by aligning depth-based representations. We demonstrate that this kernel can be seen as an \textbfaligned subtree kernel that incorporates explicit subtree correspondences, and thus addresses the drawback of neglecting the relative locations between substructures that arises in the R-convolution kernels. Experiments on standard datasets demonstrate that our kernel can easily outperform state-of-the-art graph kernels in terms of classification accuracy.",http://proceedings.mlr.press/v37/bai15.html,http://proceedings.mlr.press/v37/bai15.pdf,ICML
2257,2015,Efficient Learning in Large-Scale Combinatorial Semi-Bandits,"Zheng Wen,         Branislav Kveton,         Azin Ashkan","A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to combinatorial constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose two learning algorithms called Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both algorithms are computationally efficient as long as the offline version of the combinatorial problem can be solved efficiently. We establish that CombLinTS and CombLinUCB are also provably statistically efficient under reasonable assumptions, by developing regret bounds that are independent of the problem scale (number of items) and sublinear in time. We also evaluate CombLinTS on a variety of problems with thousands of items. Our experiment results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines.",http://proceedings.mlr.press/v37/wen15.html,http://proceedings.mlr.press/v37/wen15.pdf,ICML
2258,2015,Inference in a Partially Observed Queuing Model with Applications in Ecology,"Kevin Winner,         Garrett Bernstein,         Dan Sheldon","We consider the problem of inference in a probabilistic model for transient populations where we wish to learn about arrivals, departures, and population size over all time, but the only available data are periodic counts of the population size at specific observation times. The underlying model arises in queueing theory (as an M/G/inf queue) and also in ecological models for short-lived animals such as insects. Our work applies to both systems. Previous work in the ecology literature focused on maximum likelihood estimation and made a simplifying independence assumption that prevents inference over unobserved random variables such as arrivals and departures. The contribution of this paper is to formulate a latent variable model and develop a novel Gibbs sampler based on Markov bases to perform inference using the correct, but intractable, likelihood function. We empirically validate the convergence behavior of our sampler and demonstrate the ability of our model to make much finer-grained inferences than the previous approach.",http://proceedings.mlr.press/v37/winner15.html,http://proceedings.mlr.press/v37/winner15.pdf,ICML
2259,2015,Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top,"Arun Rajkumar,         Suprovat Ghoshal,         Lek-Heng Lim,         Shivani Agarwal","We consider the problem of ranking n items from stochastically sampled pairwise preferences. It was shown recently that when the underlying pairwise preferences are acyclic, several algorithms including the Rank Centrality algorithm, the Matrix Borda algorithm, and the SVM-RankAggregation algorithm succeed in recovering a ranking that minimizes a global pairwise disagreement error (Rajkumar and Agarwal, 2014). In this paper, we consider settings where pairwise preferences can contain cycles. In such settings, one may still like to be able to recover ‘good’ items at the top of the ranking. For example, if a Condorcet winner exists that beats every other item, it is natural to ask that this be ranked at the top. More generally, several tournament solution concepts such as the top cycle, Copeland set, Markov set and others have been proposed in the social choice literature for choosing a set of winners in the presence of cycles. We show that existing algorithms can fail to perform well in terms of ranking Condorcet winners and various natural tournament solution sets at the top. We then give alternative ranking algorithms that provably rank Condorcet winners, top cycles, and other tournament solution sets of interest at the top. In all cases, we give finite sample complexity bounds for our algorithms to recover such winners. As a by-product of our analysis, we also obtain an improved sample complexity bound for the Rank Centrality algorithm to recover an optimal ranking under a Bradley-Terry-Luce (BTL) condition, which answers an open question of Rajkumar and Agarwal (2014).",http://proceedings.mlr.press/v37/rajkumar15.html,http://proceedings.mlr.press/v37/rajkumar15.pdf,ICML
2260,2015,Adaptive Belief Propagation,"Georgios Papachristoudis,         John Fisher","Graphical models are widely used in inference problems. In practice, one may construct a single large-scale model to explain a phenomenon of interest, which may be utilized in a variety of settings. The latent variables of interest, which can differ in each setting, may only represent a small subset of all variables. The marginals of variables of interest may change after the addition of measurements at different time points. In such adaptive settings, naive algorithms, such as standard belief propagation (BP), may utilize many unnecessary computations by propagating messages over the entire graph. Here, we formulate an efficient inference procedure, termed adaptive BP (AdaBP), suitable for adaptive inference settings. We show that it gives exact results for trees in discrete and Gaussian Markov Random Fields (MRFs), and provide an extension to Gaussian loopy graphs. We also provide extensions on finding the most likely sequence of the entire latent graph. Lastly, we compare the proposed method to standard BP and to that of (Sumer et al., 2011), which tackles the same problem. We show in synthetic and real experiments that it outperforms standard BP by orders of magnitude and explore the settings that it is advantageous over (Sumer et al., 2011).",http://proceedings.mlr.press/v37/papachristoudis15.html,http://proceedings.mlr.press/v37/papachristoudis15.pdf,ICML
2261,2015,Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection,"Julie Nutini,         Mark Schmidt,         Issam Laradji,         Michael Friedlander,         Hoyt Koepke","There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of  Nesterov [SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, as it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that—except in extreme cases—it’s convergence rate is faster than choosing random coordinates. Further, in this work we (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule.",http://proceedings.mlr.press/v37/nutini15.html,http://proceedings.mlr.press/v37/nutini15.pdf,ICML
2262,2015,Adding vs. Averaging in Distributed Primal-Dual Optimization,"Chenxin Ma,         Virginia Smith,         Martin Jaggi,         Michael Jordan,         Peter Richtarik,         Martin Takac","Distributed optimization methods for large-scale machine learning suffer from a communication bottleneck. It is difficult to reduce this bottleneck while still efficiently and accurately aggregating partial work from different machines. In this paper, we present a novel generalization of the recent communication-efficient primal-dual framework (COCOA) for distributed optimization. Our framework, COCOA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allow conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both COCOA as well as our new variants, and generalize the theory for both methods to cover non-smooth convex loss functions. We provide an extensive experimental comparison that shows the markedly improved performance of COCOA+ on several real-world distributed datasets, especially when scaling up the number of machines.",http://proceedings.mlr.press/v37/mab15.html,http://proceedings.mlr.press/v37/mab15.pdf,ICML
2263,2015,A Bayesian nonparametric procedure for comparing algorithms,"Alessio Benavoli,         Giorgio Corani,         Francesca Mangili,         Marco Zaffalon","A fundamental task in machine learning is to compare the performance of multiple algorithms. This is typically performed by frequentist tests (usually the Friedman test followed by a series of multiple pairwise comparisons). This implies dealing with null hypothesis significance tests and p-values, although the shortcomings of such methods are well known. First, we propose a nonparametric Bayesian version of the Friedman test using a Dirichlet process (DP) based prior. Our derivations show that, from a Bayesian perspective, the Friedman test is an inference for a multivariate mean based on an ellipsoid inclusion test. Second, we derive a joint procedure for the analysis of the multiple comparisons which accounts for their dependencies and which is based on the posterior probability computed through the DP. The proposed approach allows verifying the null hypothesis, not only rejecting it. Third, we apply our test to perform algorithms racing, i.e., the problem of identifying the best algorithm among a large set of candidates. We show by simulation that our approach is competitive both in terms of accuracy and speed in identifying the best algorithm.",http://proceedings.mlr.press/v37/benavoli15.html,http://proceedings.mlr.press/v37/benavoli15.pdf,ICML
2264,2015,Intersecting Faces: Non-negative Matrix Factorization With New Guarantees,"Rong Ge,         James Zou","Non-negative matrix factorization (NMF) is a natural model of admixture and is widely used in science and engineering. A plethora of algorithms have been developed to tackle NMF, but due to the non-convex nature of the problem, there is little guarantee on how well these methods work. Recently a surge of research have focused on a very restricted class of NMFs, called separable NMF, where provably correct algorithms have been developed. In this paper, we propose the notion of subset-separable NMF, which substantially generalizes the property of separability. We show that subset-separability is a natural necessary condition for the factorization to be unique or to have minimum volume. We developed the Face-Intersect algorithm which provably and efficiently solves subset-separable NMF under natural conditions, and we prove that our algorithm is robust to small noise. We explored the performance of Face-Intersect on simulations and discuss settings where it empirically outperformed the state-of-art methods. Our work is a step towards finding provably correct algorithms that solve large classes of NMF problems.",http://proceedings.mlr.press/v37/geb15.html,http://proceedings.mlr.press/v37/geb15.pdf,ICML
2265,2015,JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes,"Jonathan Huggins,         Karthik Narasimhan,         Ardavan Saeedi,         Vikash Mansinghka","Markov jump processes (MJPs) are used to model a wide range of phenomenon from disease progression to RNA path folding. However, existing methods suffer from a number of shortcomings: degenerate trajectories in the case of ML estimation of parametric models and poor inferential performance in the case of nonparametric models. We take a small-variance asymptotics (SVA) approach to overcome these limitations. We derive the small-variance asymptotics for parametric and nonparametric MJPs for both directly observed and hidden state models. In the parametric case we obtain a novel objective function which leads to non-degenerate trajectories. To derive the nonparametric version we introduce the gamma-gamma process, a novel extension to the gamma-exponential process. We propose algorithms for each of these formulations, which we call \emphJUMP-means. Our experiments demonstrate that JUMP-means is competitive with or outperforms widely used MJP inference approaches in terms of both speed and reconstruction accuracy.",http://proceedings.mlr.press/v37/hugginsa15.html,http://proceedings.mlr.press/v37/hugginsa15.pdf,ICML
2266,2015,Harmonic Exponential Families on Manifolds,"Taco Cohen,         Max Welling","In a range of fields including the geosciences, molecular biology, robotics and computer vision, one encounters problems that involve random variables on manifolds. Currently, there is a lack of flexible probabilistic models on manifolds that are fast and easy to train. We define an extremely flexible class of exponential family distributions on manifolds such as the torus, sphere, and rotation groups, and show that for these distributions the gradient of the log-likelihood can be computed efficiently using a non-commutative generalization of the Fast Fourier Transform (FFT). We discuss applications to Bayesian camera motion estimation (where harmonic exponential families serve as conjugate priors), and modelling of the spatial distribution of earthquakes on the surface of the earth. Our experimental results show that harmonic densities yield a significantly higher likelihood than the best competing method, while being orders of magnitude faster to train.",http://proceedings.mlr.press/v37/cohenb15.html,http://proceedings.mlr.press/v37/cohenb15.pdf,ICML
2267,2015,Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions,"Taehoon Lee,         Sungroh Yoon","Splicing refers to the elimination of non-coding regions in transcribed pre-messenger ribonucleic acid (RNA). Discovering splice sites is an important machine learning task that helps us not only to identify the basic units of genetic heredity but also to understand how different proteins are produced. Existing methods for splicing prediction have produced promising results, but often show limited robustness and accuracy. In this paper, we propose a deep belief network-based methodology for computational splice junction prediction. Our proposal includes a novel method for training restricted Boltzmann machines for class-imbalanced prediction. The proposed method addresses the limitations of conventional contrastive divergence and provides regularization for datasets that have categorical features. We tested our approach using public human genome datasets and obtained significantly improved accuracy and reduced runtime compared to state-of-the-art alternatives. The proposed approach was less sensitive to the length of input sequences and more robust for handling false splicing signals. Furthermore, we could discover non-canonical splicing patterns that were otherwise difficult to recognize using conventional methods. Given the efficiency and robustness of our methodology, we anticipate that it can be extended to the discovery of primary structural patterns of other subtle genomic elements.",http://proceedings.mlr.press/v37/leeb15.html,http://proceedings.mlr.press/v37/leeb15.pdf,ICML
2268,2015,Asymmetric Transfer Learning with Deep Gaussian Processes,Melih Kandemir,"We introduce a novel Gaussian process based Bayesian model for asymmetric transfer learning. We adopt a two-layer feed-forward deep Gaussian process as the task learner of source and target domains. The first layer projects the data onto a separate non-linear manifold for each task. We perform knowledge transfer by projecting the target data also onto the source domain and linearly combining its representations on the source and target domain manifolds. Our approach achieves the state-of-the-art in a benchmark real-world image categorization task, and improves on it in cross-tissue tumor detection from histopathology tissue slide images.",http://proceedings.mlr.press/v37/kandemir15.html,http://proceedings.mlr.press/v37/kandemir15.pdf,ICML
2269,2015,Paired-Dual Learning for Fast Training of Latent Variable Hinge-Loss MRFs,"Stephen Bach,         Bert Huang,         Jordan Boyd-Graber,         Lise Getoor","Latent variables allow probabilistic graphical models to capture nuance and structure in important domains such as network science, natural language processing, and computer vision. Naive approaches to learning such complex models can be prohibitively expensive—because they require repeated inferences to update beliefs about latent variables—so lifting this restriction for useful classes of models is an important problem. Hinge-loss Markov random fields (HL-MRFs) are graphical models that allow highly scalable inference and learning in structured domains, in part by representing structured problems with continuous variables. However, this representation leads to challenges when learning with latent variables. We introduce paired-dual learning, a framework that greatly speeds up training by using tractable entropy surrogates and avoiding repeated inferences. Paired-dual learning optimizes an objective with a pair of dual inference problems. This allows fast, joint optimization of parameters and dual variables. We evaluate on social-group detection, trust prediction in social networks, and image reconstruction, finding that paired-dual learning trains models as accurate as those trained by traditional methods in much less time, often before traditional methods make even a single parameter update.",http://proceedings.mlr.press/v37/bach15.html,http://proceedings.mlr.press/v37/bach15.pdf,ICML
2270,2015,Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP),"Andrew Wilson,         Hannes Nickisch","We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.",http://proceedings.mlr.press/v37/wilson15.html,http://proceedings.mlr.press/v37/wilson15.pdf,ICML
2271,2015,Exponential Integration for Hamiltonian Monte Carlo,"Wei-Lun Chao,         Justin Solomon,         Dominik Michels,         Fei Sha","We investigate numerical integration of ordinary differential equations (ODEs) for Hamiltonian Monte Carlo (HMC). High-quality integration is crucial for designing efficient and effective proposals for HMC. While the standard method is leapfrog (Stormer-Verlet) integration, we propose the use of an exponential integrator, which is robust to stiff ODEs with highly-oscillatory components. This oscillation is difficult to reproduce using leapfrog integration, even with carefully selected integration parameters and preconditioning. Concretely, we use a Gaussian distribution approximation to segregate stiff components of the ODE. We integrate this term analytically for stability and account for deviation from the approximation using variation of constants. We consider various ways to derive Gaussian approximations and conduct extensive empirical studies applying the proposed “exponential HMC” to several benchmarked learning problems. We compare to state-of-the-art methods for improving leapfrog HMC and demonstrate the advantages of our method in generating many effective samples with high acceptance rates in short running times.",http://proceedings.mlr.press/v37/chao15.html,http://proceedings.mlr.press/v37/chao15.pdf,ICML
2272,2015,Variational Inference for Gaussian Process Modulated Poisson Processes,"Chris Lloyd,         Tom Gunter,         Michael Osborne,         Stephen Roberts","We present the first fully variational Bayesian inference scheme for continuous Gaussian-process-modulated Poisson processes. Such point processes are used in a variety of domains, including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our scheme: requires no discretisation of the domain; scales linearly in the number of observed events; and is many orders of magnitude faster than previous sampling based approaches. The resulting algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster data and in the prediction of Malaria incidences in Kenya.",http://proceedings.mlr.press/v37/lloyd15.html,http://proceedings.mlr.press/v37/lloyd15.pdf,ICML
2273,2015,Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices,"Jie Wang,         Jieping Ye","Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule—that is based on the dual projection onto convex sets (DPC)—to quickly identify the inactive features—that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features—especially for high dimensional data—which leads to a speedup up to several orders of magnitude.",http://proceedings.mlr.press/v37/wangf15.html,http://proceedings.mlr.press/v37/wangf15.pdf,ICML
2274,2015,Celeste: Variational inference for a generative model of astronomical images,"Jeffrey Regier,         Andrew Miller,         Jon McAuliffe,         Ryan Adams,         Matt Hoffman,         Dustin Lang,         David Schlegel,         Mr Prabhat","We present a new, fully generative model of optical telescope image sets, along with a variational procedure for inference. Each pixel intensity is treated as a Poisson random variable, with a rate parameter dependent on latent properties of stars and galaxies. Key latent properties are themselves random, with scientific prior distributions constructed from large ancillary data sets. We check our approach on synthetic images. We also run it on images from a major sky survey, where it exceeds the performance of the current state-of-the-art method for locating celestial bodies and measuring their colors.",http://proceedings.mlr.press/v37/regier15.html,http://proceedings.mlr.press/v37/regier15.pdf,ICML
2275,2015,Discovering Temporal Causal Relations from Subsampled Data,"Mingming Gong,         Kun Zhang,         Bernhard Schoelkopf,         Dacheng Tao,         Philipp Geiger","Granger causal analysis has been an important tool for causal analysis for time series in various fields, including neuroscience and economics, and recently it has been extended to include instantaneous effects between the time series to explain the contemporaneous dependence in the residuals. In this paper, we assume that the time series at the true causal frequency follow the vector autoregressive model. We show that when the data resolution becomes lower due to subsampling, neither the original Granger causal analysis nor the extended one is able to discover the underlying causal relations. We then aim to answer the following question: can we estimate the temporal causal relations at the right causal frequency from the subsampled data? Traditionally this suffers from the identifiability problems: under the Gaussianity assumption of the data, the solutions are generally not unique. We prove that, however, if the noise terms are non-Gaussian, the underlying model for the high frequency data is identifiable from subsampled data under mild conditions. We then propose an Expectation-Maximization (EM) approach and a variational inference approach to recover temporal causal relations from such subsampled data. Experimental results on both simulated and real data are reported to illustrate the performance of the proposed approaches.",http://proceedings.mlr.press/v37/gongb15.html,http://proceedings.mlr.press/v37/gongb15.pdf,ICML
2276,2015,Simple regret for infinitely many armed bandits,"Alexandra Carpentier,         Michal Valko","We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter βcharacterizing the distribution of the near-optimal arms. We prove that depending on β, our algorithm is minimax optimal either up to a multiplicative constant or up to a \log(n) factor. We also provide extensions to several important cases: when βis unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon.",http://proceedings.mlr.press/v37/carpentier15.html,http://proceedings.mlr.press/v37/carpentier15.pdf,ICML
2277,2015,Entropy evaluation based on confidence intervals of frequency estimates : Application to the learning of decision trees,"Mathieu Serrurier,         Henri Prade","Entropy gain is widely used for learning decision trees. However, as we go deeper downward the tree, the examples become rarer and the faithfulness of entropy decreases. Thus, misleading choices and over-fitting may occur and the tree has to be adjusted by using an early-stop criterion or post pruning algorithms. However, these methods still depends on the choices previously made, which may be unsatisfactory. We propose a new cumulative entropy function based on confidence intervals on frequency estimates that together considers the entropy of the probability distribution and the uncertainty around the estimation of its parameters. This function takes advantage of the ability of a possibility distribution to upper bound a family of probabilities previously estimated from a limited set of examples and of the link between possibilistic specificity order and entropy. The proposed measure has several advantages over the classical one. It performs significant choices of split and provides a statistically relevant stopping criterion that allows the learning of trees whose size is well-suited w.r.t. the available data. On the top of that, it also provides a reasonable estimator of the performances of a decision tree. Finally, we show that it can be used for designing a simple and efficient online learning algorithm.",http://proceedings.mlr.press/v37/serrurier15.html,http://proceedings.mlr.press/v37/serrurier15.pdf,ICML
2278,2015,Latent Topic Networks: A Versatile Probabilistic Programming Framework for Topic Models,"James Foulds,         Shachi Kumar,         Lise Getoor","Topic models have become increasingly prominent text-analytic machine learning tools for research in the social sciences and the humanities. In particular, custom topic models can be developed to answer specific research questions. The design of these models requires a non-trivial amount of effort and expertise, motivating general-purpose topic modeling frameworks. In this paper we introduce latent topic networks, a flexible class of richly structured topic models designed to facilitate applied research. Custom models can straightforwardly be developed in our framework with an intuitive first-order logical probabilistic programming language. Latent topic networks admit scalable training via a parallelizable EM algorithm which leverages ADMM in the M-step. We demonstrate the broad applicability of the models with case studies on modeling influence in citation networks, and U.S. Presidential State of the Union addresses.",http://proceedings.mlr.press/v37/foulds15.html,http://proceedings.mlr.press/v37/foulds15.pdf,ICML
2279,2015,A low variance consistent test of relative dependency,"Wacha Bounliphone,         Arthur Gretton,         Arthur Tenenhaus,         Matthew Blaschko","We describe a novel non-parametric statistical hypothesis test of relative dependence between a source variable and two candidate target variables. Such a test enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). We test whether the first dependence measure is significantly larger than the second. Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by sub-sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. Source code is available for download at https://github.com/wbounliphone/reldep/.",http://proceedings.mlr.press/v37/bounliphone15.html,http://proceedings.mlr.press/v37/bounliphone15.pdf,ICML
2280,2015,Ordinal Mixed Membership Models,"Seppo Virtanen,         Mark Girolami","We present a novel class of mixed membership models for joint distributions of groups of observations that co-occur with ordinal response variables for each group for learning statistical associations between the ordinal response variables and the observation groups. The class of proposed models addresses a requirement for predictive and diagnostic methods in a wide range of practical contemporary applications. In this work, by way of illustration, we apply the models to a collection of consumer-generated reviews of mobile software applications, where each review contains unstructured text data accompanied with an ordinal rating, and demonstrate that the models infer useful and meaningful recurring patterns of consumer feedback. We also compare the developed models to relevant existing works, which rely on improper statistical assumptions for ordinal variables, showing significant improvements both in predictive ability and knowledge extraction.",http://proceedings.mlr.press/v37/virtanen15.html,http://proceedings.mlr.press/v37/virtanen15.pdf,ICML
2281,2015,Blitz: A Principled Meta-Algorithm for Scaling Sparse Optimization,"Tyler Johnson,         Carlos Guestrin","By reducing optimization to a sequence of small subproblems, working set methods achieve fast convergence times for many challenging problems. Despite excellent performance, theoretical understanding of working sets is limited, and implementations often resort to heuristics to determine subproblem size, makeup, and stopping criteria. We propose Blitz, a fast working set algorithm accompanied by useful guarantees. Making no assumptions on data, our theory relates subproblem size to progress toward convergence. This result motivates methods for optimizing algorithmic parameters and discarding irrelevant variables as iterations progress. Applied to L1-regularized learning, Blitz convincingly outperforms existing solvers in sequential, limited-memory, and distributed settings. Blitz is not specific to L1-regularized learning, making the algorithm relevant to many applications involving sparsity or constraints.",http://proceedings.mlr.press/v37/johnson15.html,http://proceedings.mlr.press/v37/johnson15.pdf,ICML
2282,2015,How Can Deep Rectifier Networks Achieve Linear Separability and Preserve Distances?,"Senjian An,         Farid Boussaid,         Mohammed Bennamoun","This paper investigates how hidden layers of deep rectifier networks are capable of transforming two or more pattern sets to be linearly separable while preserving the distances with a guaranteed degree, and proves the universal classification power of such distance preserving rectifier networks. Through the nearly isometric nonlinear transformation in the hidden layers, the margin of the linear separating plane in the output layer and the margin of the nonlinear separating boundary in the original data space can be closely related so that the maximum margin classification in the input data space can be achieved approximately via the maximum margin linear classifiers in the output layer. The generalization performance of such distance preserving deep rectifier neural networks can be well justified by the distance-preserving properties of their hidden layers and the maximum margin property of the linear classifiers in the output layer.",http://proceedings.mlr.press/v37/an15.html,http://proceedings.mlr.press/v37/an15.pdf,ICML
2283,2015,Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms and Lower Bounds,"Yuchen Zhang,         Martin Wainwright,         Michael Jordan","We study the following generalized matrix rank estimation problem: given an n-by-n matrix and a constant c > 0, estimate the number of eigenvalues that are greater than c. In the distributed setting, the matrix of interest is the sum of m matrices held by separate machines. We show that any deterministic algorithm solving this problem must communicate Ω(n^2) bits, which is order-equivalent to transmitting the whole matrix. In contrast, we propose a randomized algorithm that communicates only O(n) bits. The upper bound is matched by an Ω(n) lower bound on the randomized communication complexity. We demonstrate the practical effectiveness of the proposed algorithm with some numerical experiments.",http://proceedings.mlr.press/v37/zhangc15.html,http://proceedings.mlr.press/v37/zhangc15.pdf,ICML
2284,2015,Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems,"Qiang Zhou,         Qi Zhao","Nuclear norm regularization has been shown very promising for pursing a low rank matrix solution in various machine learning problems. Many efforts have been devoted to develop efficient algorithms for solving the optimization problem in nuclear norm regularization. Solving it for large-scale matrix variables, however, is still a challenging task since the complexity grows fast with the size of matrix variable. In this work, we propose a novel method called safe subspace screening (SSS), to improve the efficiency of the solver for nuclear norm regularized least squares problems. Motivated by the fact that the low rank solution can be represented by a few subspaces, the proposed method accurately discards a predominant percentage of inactive subspaces prior to solving the problem to reduce problem size. Consequently, a much smaller problem is required to solve, making it more efficient than optimizing the original problem. The proposed SSS is safe, in that its solution is identical to the solution from the solver. In addition, the proposed SSS can be used together with any existing nuclear norm solver since it is independent of the solver. Extensive results on several synthetic and real data sets show that the proposed SSS is very effective in inactive subspace screening.",http://proceedings.mlr.press/v37/zhoua15.html,http://proceedings.mlr.press/v37/zhoua15.pdf,ICML
2285,2015,Learning Word Representations with Hierarchical Sparse Coding,"Dani Yogatama,         Manaal Faruqui,         Chris Dyer,         Noah Smith","We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens. Experiments on various benchmark tasks—word similarity ranking, syntactic and semantic analogies, sentence completion, and sentiment analysis—demonstrate that the method outperforms or is competitive with state-of-the-art methods.",http://proceedings.mlr.press/v37/yogatama15.html,http://proceedings.mlr.press/v37/yogatama15.pdf,ICML
2286,2015,Large-scale Distributed Dependent Nonparametric Trees,"Zhiting Hu,         Ho Qirong,         Avinava Dubey,         Eric Xing","Practical applications of Bayesian nonparametric (BNP) models have been limited, due to their high computational complexity and poor scaling on large data. In this paper, we consider dependent nonparametric trees (DNTs), a powerful infinite model that captures time-evolving hierarchies, and develop a large-scale distributed training system. Our major contributions include: (1) an effective memoized variational inference for DNTs, with a novel birth-merge strategy for exploring the unbounded tree space; (2) a model-parallel scheme for concurrent tree growing/pruning and efficient model alignment, through conflict-free model partitioning and lightweight synchronization; (3) a data-parallel scheme for variational parameter updates that allows distributed processing of massive data. Using 64 cores in 36 hours, our system learns a 10K-node DNT topic model on 8M documents that captures both high-frequency and long-tail topics. Our data and model scales are orders-of-magnitude larger than recent results on the hierarchical Dirichlet process, and the near-linear scalability indicates great potential for even bigger problem sizes.",http://proceedings.mlr.press/v37/hu15.html,http://proceedings.mlr.press/v37/hu15.pdf,ICML
2287,2015,A Theoretical Analysis of Metric Hypothesis Transfer Learning,"Michaël Perrot,         Amaury Habrard","We consider the problem of transferring some a priori knowledge in the context of supervised metric learning approaches. While this setting has been successfully applied in some empirical contexts, no theoretical evidence exists to justify this approach. In this paper, we provide a theoretical justification based on the notion of algorithmic stability adapted to the regularized metric learning setting. We propose an on-average-replace-two-stability model allowing us to prove fast generalization rates when an auxiliary source metric is used to bias the regularizer. Moreover, we prove a consistency result from which we show the interest of considering biased weighted regularized formulations and we provide a solution to estimate the associated weight. We also present some experiments illustrating the interest of the approach in standard metric learning tasks and in a transfer learning problem where few labelled data are available.",http://proceedings.mlr.press/v37/perrot15.html,http://proceedings.mlr.press/v37/perrot15.pdf,ICML
2288,2015,Gradient-based Hyperparameter Optimization through Reversible Learning,"Dougal Maclaurin,         David Duvenaud,         Ryan Adams","Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.",http://proceedings.mlr.press/v37/maclaurin15.html,http://proceedings.mlr.press/v37/maclaurin15.pdf,ICML
2289,2015,Moderated and Drifting Linear Dynamical Systems,"Jinyan Guan,         Kyle Simek,         Ernesto Brau,         Clayton Morrison,         Emily Butler,         Kobus Barnard","We consider linear dynamical systems, particularly coupled linear oscillators, where the parameters represent meaningful values in a domain theory and thus learning what affects them contributes to explanation. Rather than allow perturbations of latent states, we assume that temporal variation beyond noise is explained by parameter drift, and variation across coupled systems is a function of moderating variables. This change of focus reduces opportunities for efficient inference, and we propose sampling procedures to learn and fit the models. We test our approach on a real dataset of physiological measures of heterosexual couples engaged in a conversation about a potentially emotional topic, with body mass index (BMI) being considered as a moderator. We evaluate several models on their ability to predict future conversation dynamics (the last 20% of the data for each test couple), with shared parameters being learned using held out data. As proof of concept, we validate the hypothesis that BMI affects the conversation dynamic in the experimentally chosen topic.",http://proceedings.mlr.press/v37/guan15.html,http://proceedings.mlr.press/v37/guan15.pdf,ICML
2290,2015,Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret,"Haitham Bou Ammar,         Rasul Tutunov,         Eric Eaton","Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases, and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.",http://proceedings.mlr.press/v37/ammar15.html,http://proceedings.mlr.press/v37/ammar15.pdf,ICML
2291,2015,Reified Context Models,"Jacob Steinhardt,         Percy Liang","A classic tension exists between exact inference in a simple model and approximate inference in a complex model. The latter offers expressivity and thus accuracy, but the former provides coverage of the space, an important property for confidence estimation and learning with indirect supervision. In this work, we introduce a new approach, reified context models, to reconcile this tension. Specifically, we let the choice of factors in a graphical model (the contexts) be random variables inside the model itself. In this sense, the contexts are reified and can be chosen in a data-dependent way. Empirically, we show that our approach obtains expressivity and coverage on three sequence modeling tasks.",http://proceedings.mlr.press/v37/steinhardta15.html,http://proceedings.mlr.press/v37/steinhardta15.pdf,ICML
2292,2015,Safe Exploration for Optimization with Gaussian Processes,"Yanan Sui,         Alkis Gotovos,         Joel Burdick,         Andreas Krause","We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multi-armed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified ""safety"" threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SafeOpt, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SafeOpt on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation.",http://proceedings.mlr.press/v37/sui15.html,http://proceedings.mlr.press/v37/sui15.pdf,ICML
2293,2015,Dealing with small data: On the generalization of context trees,"Ralf Eggeling,         Mikko Koivisto,         Ivo Grosse","Context trees (CT) are a widely used tool in machine learning for representing context-specific independences in conditional probability distributions. Parsimonious context trees (PCTs) are a recently proposed generalization of CTs that can enable statistically more efficient learning due to a higher structural flexibility, which is particularly useful for small-data settings. However, this comes at the cost of a computationally expensive structure learning algorithm, which is feasible only for domains with small alphabets and tree depths. In this work, we investigate to which degree CTs can be generalized to increase statistical efficiency while still keeping the learning computationally feasible. Approaching this goal from two different angles, we (i) propose algorithmic improvements to the PCT learning algorithm, and (ii) study further generalizations of CTs, which are inspired by PCTs, but trade structural flexibility for computational efficiency. By empirical studies both on simulated and real-world data, we demonstrate that the synergy of combining of both orthogonal approaches yields a substantial improvement in obtaining statistically efficient and computationally feasible generalizations of CTs.",http://proceedings.mlr.press/v37/eggeling15.html,http://proceedings.mlr.press/v37/eggeling15.pdf,ICML
2294,2015,Qualitative Multi-Armed Bandits: A Quantile-Based Approach,"Balazs Szorenyi,         Robert Busa-Fekete,         Paul Weng,         Eyke Hüllermeier","We formalize and study the multi-armed bandit (MAB) problem in a generalized stochastic setting, in which rewards are not assumed to be numerical. Instead, rewards are measured on a qualitative scale that allows for comparison but invalidates arithmetic operations such as averaging. Correspondingly, instead of characterizing an arm in terms of the mean of the underlying distribution, we opt for using a quantile of that distribution as a representative value. We address the problem of quantile-based online learning both for the case of a finite (pure exploration) and infinite time horizon (cumulative regret minimization). For both cases, we propose suitable algorithms and analyze their properties. These properties are also illustrated by means of first experimental studies.",http://proceedings.mlr.press/v37/szorenyi15.html,http://proceedings.mlr.press/v37/szorenyi15.pdf,ICML
2295,2015,A Multitask Point Process Predictive Model,"Wenzhao Lian,         Ricardo Henao,         Vinayak Rao,         Joseph Lucas,         Lawrence Carin","Point process data are commonly observed in fields like healthcare and social science. Designing predictive models for such event streams is an under-explored problem, due to often scarce training data. In this work we propose a multitask point process model, leveraging information from all tasks via a hierarchical Gaussian process (GP). Nonparametric learning functions implemented by a GP, which map from past events to future rates, allow analysis of flexible arrival patterns. To facilitate efficient inference, we propose a sparse construction for this hierarchical model, and derive a variational Bayes method for learning and inference. Experimental results are shown on both synthetic data and an application on real electronic health records.",http://proceedings.mlr.press/v37/lian15.html,http://proceedings.mlr.press/v37/lian15.pdf,ICML
2296,2015,A General Analysis of the Convergence of ADMM,"Robert Nishihara,         Laurent Lessard,         Ben Recht,         Andrew Packard,         Michael Jordan","We provide a new proof of the linear convergence of the alternating direction method of multipliers (ADMM) when one of the objective terms is strongly convex. Our proof is based on a framework for analyzing optimization algorithms introduced in Lessard et al. (2014), reducing algorithm convergence to verifying the stability of a dynamical system. This approach generalizes a number of existing results and obviates any assumptions about specific choices of algorithm parameters. On a numerical example, we demonstrate that minimizing the derived bound on the convergence rate provides a practical approach to selecting algorithm parameters for particular ADMM instances. We complement our upper bound by constructing a nearly-matching lower bound on the worst-case rate of convergence.",http://proceedings.mlr.press/v37/nishihara15.html,http://proceedings.mlr.press/v37/nishihara15.pdf,ICML
2297,2015,An Online Learning Algorithm for Bilinear Models,"Yuanbin Wu,         Shiliang Sun","We investigate the bilinear model, which is a matrix form linear model with the rank 1 constraint. A new online learning algorithm is proposed to train the model parameters. Our algorithm runs in the manner of online mirror descent, and gradients are computed by the power iteration. To analyze it, we give a new second order approximation of the squared spectral norm, which helps us to get a regret bound. Experiments on two sequential labelling tasks give positive results.",http://proceedings.mlr.press/v37/wua15.html,http://proceedings.mlr.press/v37/wua15.pdf,ICML
2298,2015,Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE),"Maurizio Filippone,         Raphael Engler","In applications of Gaussian processes where quantification of uncertainty is of primary interest, it is necessary to accurately characterize the posterior distribution over covariance parameters. This paper proposes an adaptation of the Stochastic Gradient Langevin Dynamics algorithm to draw samples from the posterior distribution over covariance parameters with negligible bias and without the need to compute the marginal likelihood. In Gaussian process regression, this has the enormous advantage that stochastic gradients can be computed by solving linear systems only. A novel unbiased linear systems solver based on parallelizable covariance matrix-vector products is developed to accelerate the unbiased estimation of gradients. The results demonstrate the possibility to enable scalable and exact (in a Monte Carlo sense) quantification of uncertainty in Gaussian processes without imposing any special structure on the covariance or reducing the number of input vectors.",http://proceedings.mlr.press/v37/filippone15.html,http://proceedings.mlr.press/v37/filippone15.pdf,ICML
2299,2015,Convex Learning of Multiple Tasks and their Structure,"Carlo Ciliberto,         Youssef Mroueh,         Tomaso Poggio,         Lorenzo Rosasco","Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem. We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum.",http://proceedings.mlr.press/v37/ciliberto15.html,http://proceedings.mlr.press/v37/ciliberto15.pdf,ICML
2300,2015,Robust Estimation of Transition Matrices in High Dimensional Heavy-tailed Vector Autoregressive Processes,"Huitong Qiu,         Sheng Xu,         Fang Han,         Han Liu,         Brian Caffo","Gaussian vector autoregressive (VAR) processes have been extensively studied in the literature. However, Gaussian assumptions are stringent for heavy-tailed time series that frequently arises in finance and economics. In this paper, we develop a unified framework for modeling and estimating heavy-tailed VAR processes. In particular, we generalize the Gaussian VAR model by an elliptical VAR model that naturally accommodates heavy-tailed time series. Under this model, we develop a quantile-based robust estimator for the transition matrix of the VAR process. We show that the proposed estimator achieves parametric rates of convergence in high dimensions. This is the first work in analyzing heavy-tailed high dimensional VAR processes. As an application of the proposed framework, we investigate Granger causality in the elliptical VAR process, and show that the robust transition matrix estimator induces sign-consistent estimators of Granger causality. The empirical performance of the proposed methodology is demonstrated by both synthetic and real data. We show that the proposed estimator is robust to heavy tails, and exhibit superior performance in stock price prediction.",http://proceedings.mlr.press/v37/qiu15.html,http://proceedings.mlr.press/v37/qiu15.pdf,ICML
2301,2015,Spectral Clustering via the Power Method - Provably,"Christos Boutsidis,         Prabhanjan Kambadur,         Alex Gittens","Spectral clustering is one of the most important algorithms in data mining and machine intelligence; however, its computational complexity limits its application to truly large scale data analysis. The computational bottleneck in spectral clustering is computing a few of the top eigenvectors of the (normalized) Laplacian matrix corresponding to the graph representing the data to be clustered. One way to speed up the computation of these eigenvectors is to use the “power method” from the numerical linear algebra literature. Although the power method has been empirically used to speed up spectral clustering, the theory behind this approach, to the best of our knowledge, remains unexplored. This paper provides the first such rigorous theoretical justification, arguing that a small number of power iterations suffices to obtain near-optimal partitionings using the approximate eigenvectors. Specifically, we prove that solving the k-means clustering problem on the approximate eigenvectors obtained via the power method gives an additive-error approximation to solving the k-means problem on the optimal eigenvectors.",http://proceedings.mlr.press/v37/boutsidis15.html,http://proceedings.mlr.press/v37/boutsidis15.pdf,ICML
2302,2015,Multi-instance multi-label learning in the presence of novel class instances,"Anh Pham,         Raviv Raich,         Xiaoli Fern,         Jesús Pérez Arriaga","Multi-instance multi-label learning (MIML) is a framework for learning in the presence of label ambiguity. In MIML, experts provide labels for groups of instances (bags), instead of directly providing a label for every instance. When labeling efforts are focused on a set of target classes, instances outside this set will not be appropriately modeled. For example, ornithologists label bird audio recordings with a list of species present. Other additional sound instances, e.g., a rain drop or a moving vehicle sound, are not labeled. The challenge is due to the fact that for a given bag, the presence or absence of novel instances is latent. In this paper, this problem is addressed using a discriminative probabilistic model that accounts for novel instances. We propose an exact and efficient implementation of the maximum likelihood approach to determine the model parameters and consequently learn an instance-level classifier for all classes including the novel class. Experiments on both synthetic and real datasets illustrate the effectiveness of the proposed approach.",http://proceedings.mlr.press/v37/pham15.html,http://proceedings.mlr.press/v37/pham15.pdf,ICML
2303,2015,Universal Value Function Approximators,"Tom Schaul,         Daniel Horgan,         Karol Gregor,         David Silver","Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.",http://proceedings.mlr.press/v37/schaul15.html,http://proceedings.mlr.press/v37/schaul15.pdf,ICML
2304,2015,Fixed-point algorithms for learning determinantal point processes,"Zelda Mariet,         Suvrit Sra","Determinantal point processes (DPPs) offer an elegant tool for encoding probabilities over subsets of a ground set. Discrete DPPs are parametrized by a positive semidefinite matrix (called the DPP kernel), and estimating this kernel is key to learning DPPs from observed data. We consider the task of learning the DPP kernel, and develop for it a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefits over previous approaches: (a) it is much simpler; (b) it yields equally good and sometimes even better local maxima; and (c) it runs an order of magnitude faster on large problems. We present experimental results on both real and simulated data to illustrate the numerical performance of our technique.",http://proceedings.mlr.press/v37/mariet15.html,http://proceedings.mlr.press/v37/mariet15.pdf,ICML
2305,2015,Deep Edge-Aware Filters,"Li Xu,         Jimmy Ren,         Qiong Yan,         Renjie Liao,         Jiaya Jia","There are many edge-aware filters varying in their construction forms and filtering properties. It seems impossible to uniformly represent and accelerate them in a single framework. We made the attempt to learn a big and important family of edge-aware operators from data. Our method is based on a deep convolutional neural network with a gradient domain training procedure, which gives rise to a powerful tool to approximate various filters without knowing the original models and implementation details. The only difference among these operators in our system becomes merely the learned parameters. Our system enables fast approximation for complex edge-aware filters and achieves up to 200x acceleration, regardless of their originally very different implementation. Fast speed can also be achieved when creating new effects using spatially varying filter or filter combination, bearing out the effectiveness of our deep edge-aware filters.",http://proceedings.mlr.press/v37/xub15.html,http://proceedings.mlr.press/v37/xub15.pdf,ICML
2306,2015,Active Nearest Neighbors in Changing Environments,"Christopher Berlind,         Ruth Urner","While classic machine learning paradigms assume training and test data are generated from the same process, domain adaptation addresses the more realistic setting in which the learner has large quantities of labeled data from some source task but limited or no labeled data from the target task it is attempting to learn. In this work, we give the first formal analysis showing that using active learning for domain adaptation yields a way to address the statistical challenges inherent in this setting. We propose a novel nonparametric algorithm, ANDA, that combines an active nearest neighbor querying strategy with nearest neighbor prediction. We provide analyses of its querying behavior and of finite sample convergence rates of the resulting classifier under covariate shift. Our experiments show that ANDA successfully corrects for dataset bias in multi-class image categorization.",http://proceedings.mlr.press/v37/berlind15.html,http://proceedings.mlr.press/v37/berlind15.pdf,ICML
2307,2015,Stochastic Dual Coordinate Ascent with Adaptive Probabilities,"Dominik Csiba,         Zheng Qu,         Peter Richtarik","This paper introduces AdaSDCA: an adaptive variant of stochastic dual coordinate ascent (SDCA) for solving the regularized empirical risk minimization problems. Our modification consists in allowing the method adaptively change the probability distribution over the dual variables throughout the iterative process. AdaSDCA achieves provably better complexity bound than SDCA with the best fixed probability distribution, known as importance sampling. However, it is of a theoretical character as it is expensive to implement. We also propose AdaSDCA+: a practical variant which in our experiments outperforms existing non-adaptive methods.",http://proceedings.mlr.press/v37/csiba15.html,http://proceedings.mlr.press/v37/csiba15.pdf,ICML
2308,2015,Distributed Box-Constrained Quadratic Optimization for Dual Linear SVM,"Ching-Pei Lee,         Dan Roth","Training machine learning models sometimes needs to be done on large amounts of data that exceed the capacity of a single machine, motivating recent works on developing algorithms that train in a distributed fashion. This paper proposes an efficient box-constrained quadratic optimization algorithm for distributedly training linear support vector machines (SVMs) with large data. Our key technical contribution is an analytical solution to the problem of computing the optimal step size at each iteration, using an efficient method that requires only O(1) communication cost to ensure fast convergence. With this optimal step size, our approach is superior to other methods by possessing global linear convergence, or, equivalently, O(\log(1/ε)) iteration complexity for an epsilon-accurate solution, for distributedly solving the non-strongly-convex linear SVM dual problem. Experiments also show that our method is significantly faster than state-of- the-art distributed linear SVM algorithms including DSVM-AVE, DisDCA and TRON.",http://proceedings.mlr.press/v37/leea15.html,http://proceedings.mlr.press/v37/leea15.pdf,ICML
2309,2015,"\ell_1,p-Norm Regularization: Error Bounds and Convergence Rate Analysis of First-Order Methods","Zirui Zhou,         Qi Zhang,         Anthony Man-Cho So","Recently, \ell_1,p-regularization has been widely used to induce structured sparsity in the solutions to various optimization problems. Motivated by the desire to analyze the convergence rate of first-order methods, we show that for a large class of \ell_1,p-regularized problems, an error bound condition is satisfied when p∈[1,2] or p=∞but fails to hold for any p∈(2,∞). Based on this result, we show that many first-order methods enjoy an asymptotic linear rate of convergence when applied to \ell_1,p-regularized linear or logistic regression with p∈[1,2] or p=∞. By contrast, numerical experiments suggest that for the same class of problems with p∈(2,∞), the aforementioned methods may not converge linearly.",http://proceedings.mlr.press/v37/zhoub15.html,http://proceedings.mlr.press/v37/zhoub15.pdf,ICML
2310,2015,Pushing the Limits of Affine Rank Minimization by Adapting Probabilistic PCA,"Bo Xin,         David Wipf","Many applications require recovering a matrix of minimal rank within an affine constraint set, with matrix completion a notable special case. Because the problem is NP-hard in general, it is common to replace the matrix rank with the nuclear norm, which acts as a convenient convex surrogate. While elegant theoretical conditions elucidate when this replacement is likely to be successful, they are highly restrictive and convex algorithms fail when the ambient rank is too high or when the constraint set is poorly structured. Non-convex alternatives fare somewhat better when carefully tuned; however, convergence to locally optimal solutions remains a continuing source of failure. Against this backdrop we derive a deceptively simple and parameter-free probabilistic PCA-like algorithm that is capable, over a wide battery of empirical tests, of successful recovery even at the theoretical limit where the number of measurements equals the degrees of freedom in the unknown low-rank matrix. Somewhat surprisingly, this is possible even when the affine constraint set is highly ill-conditioned. While proving general recovery guarantees remains evasive for non-convex algorithms, Bayesian-inspired or otherwise, we nonetheless show conditions whereby the underlying cost function has a unique stationary point located at the global optimum; no existing cost function we are aware of satisfies this property. The algorithm has also been successfully deployed on a computer vision application involving image rectification and a standard collaborative filtering benchmark.",http://proceedings.mlr.press/v37/xin15.html,http://proceedings.mlr.press/v37/xin15.pdf,ICML
2311,2015,Weight Uncertainty in Neural Network,"Charles Blundell,         Julien Cornebise,         Koray Kavukcuoglu,         Daan Wierstra","We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.",http://proceedings.mlr.press/v37/blundell15.html,http://proceedings.mlr.press/v37/blundell15.pdf,ICML
2312,2015,Guaranteed Tensor Decomposition: A Moment Approach,"Gongguo Tang,         Parikshit Shah","We develop a theoretical and computational framework to perform guaranteed tensor decomposition, which also has the potential to accomplish other tensor tasks such as tensor completion and denoising. We formulate tensor decomposition as a problem of measure estimation from moments. By constructing a dual polynomial, we demonstrate that measure optimization returns the correct CP decomposition under an incoherence condition on the rank-one factors. To address the computational challenge, we present a hierarchy of semidefinite programs based on sums-of-squares relaxations of the measure optimization problem. By showing that the constructed dual polynomial is a sum-of-squares modulo the sphere, we prove that the smallest SDP in the relaxation hierarchy is exact and the decomposition can be extracted from the solution under the same incoherence condition. One implication is that the tensor nuclear norm can be computed exactly using the smallest SDP as long as the rank-one factors of the tensor are incoherent. Numerical experiments are conducted to test the performance of the moment approach.",http://proceedings.mlr.press/v37/tanga15.html,http://proceedings.mlr.press/v37/tanga15.pdf,ICML
2313,2015,Complex Event Detection using Semantic Saliency and Nearly-Isotonic SVM,"Xiaojun Chang,         Yi Yang,         Eric Xing,         Yaoliang Yu","We aim to detect complex events in long Internet videos that may last for hours. A major challenge in this setting is that only a few shots in a long video are relevant to the event of interest while others are irrelevant or even misleading. Instead of indifferently pooling the shots, we first define a novel notion of semantic saliency that assesses the relevance of each shot with the event of interest. We then prioritize the shots according to their saliency scores since shots that are semantically more salient are expected to contribute more to the final event detector. Next, we propose a new isotonic regularizer that is able to exploit the semantic ordering information. The resulting nearly-isotonic SVM classifier exhibits higher discriminative power. Computationally, we develop an efficient implementation using the proximal gradient algorithm, and we prove new, closed-form proximal steps. We conduct extensive experiments on three real-world video datasets and confirm the effectiveness of the proposed approach.",http://proceedings.mlr.press/v37/changa15.html,http://proceedings.mlr.press/v37/changa15.pdf,ICML
2314,2015,Distributed Inference for Dirichlet Process Mixture Models,"Hong Ge,         Yutian Chen,         Moquan Wan,         Zoubin Ghahramani","Bayesian nonparametric mixture models based on the Dirichlet process (DP) have been widely used for solving problems like clustering, density estimation and topic modelling. These models make weak assumptions about the underlying process that generated the observed data. Thus, when more data are collected, the complexity of these models can change accordingly. These theoretical properties often lead to superior predictive performance when compared to traditional finite mixture models. However, despite the increasing amount of data available, the application of Bayesian nonparametric mixture models is so far limited to relatively small data sets. In this paper, we propose an efficient distributed inference algorithm for the DP and the HDP mixture model. The proposed method is based on a variant of the slice sampler for DPs. Since this sampler does not involve a pre-determined truncation, the stationary distribution of the sampling algorithm is unbiased. We provide both local thread-level and distributed machine-level parallel implementations and study the performance of this sampler through an extensive set of experiments on image and text data. When compared to existing inference algorithms, the proposed method exhibits state-of-the-art accuracy and strong scalability with up to 512 cores.",http://proceedings.mlr.press/v37/gea15.html,http://proceedings.mlr.press/v37/gea15.pdf,ICML
2315,2015,Dynamic Sensing: Better Classification under Acquisition Constraints,"Oran Richman,         Shie Mannor","In many machine learning applications the quality of the data is limited by resource constraints (may it be power, bandwidth, memory, ...). In such cases, the constraints are on the average resources allocated, therefore there is some control over each sample’s quality. In most cases this option remains unused and the data’s quality is uniform over the samples. In this paper we propose to actively allocate resources to each sample such that resources are used optimally overall. We propose a method to compute the optimal resource allocation. We further derive generalization bounds for the case where the problem’s model is unknown. We demonstrate the potential benefit of this approach on both simulated and real-life problems.",http://proceedings.mlr.press/v37/richman15.html,http://proceedings.mlr.press/v37/richman15.pdf,ICML
2316,2015,MADE: Masked Autoencoder for Distribution Estimation,"Mathieu Germain,         Karol Gregor,         Iain Murray,         Hugo Larochelle","There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.",http://proceedings.mlr.press/v37/germain15.html,http://proceedings.mlr.press/v37/germain15.pdf,ICML
2317,2015,Cascading Bandits: Learning to Rank in the Cascade Model,"Branislav Kveton,         Csaba Szepesvari,         Zheng Wen,         Azin Ashkan","A search engine usually outputs a list of K web pages. The user examines this list, from the first web page to the last, and chooses the first attractive page. This model of user behavior is known as the cascade model. In this paper, we propose cascading bandits, a learning variant of the cascade model where the objective is to identify K most attractive items. We formulate our problem as a stochastic combinatorial partial monitoring problem. We propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. We also prove gap-dependent upper bounds on the regret of these algorithms and derive a lower bound on the regret in cascading bandits. The lower bound matches the upper bound of CascadeKL-UCB up to a logarithmic factor. We experiment with our algorithms on several problems. The algorithms perform surprisingly well even when our modeling assumptions are violated.",http://proceedings.mlr.press/v37/kveton15.html,http://proceedings.mlr.press/v37/kveton15.pdf,ICML
2318,2015,Strongly Adaptive Online Learning,"Amit Daniely,         Alon Gonen,         Shai Shalev-Shwartz","Strongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems.",http://proceedings.mlr.press/v37/daniely15.html,http://proceedings.mlr.press/v37/daniely15.pdf,ICML
2319,2015,Approval Voting and Incentives in Crowdsourcing,"Nihar Shah,         Dengyong Zhou,         Yuval Peres","The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a (""strictly proper"") incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach.",http://proceedings.mlr.press/v37/shaha15.html,http://proceedings.mlr.press/v37/shaha15.pdf,ICML
2320,2015,Alpha-Beta Divergences Discover Micro and Macro Structures in Data,"Karthik Narayan,         Ali Punjani,         Pieter Abbeel","Although recent work in non-linear dimensionality reduction investigates multiple choices of divergence measure during optimization \citeyang2013icml,bunte2012neuro, little work discusses the direct effects that divergence measures have on visualization. We study this relationship, theoretically and through an empirical analysis over 10 datasets. Our works shows how the αand βparameters of the generalized alpha-beta divergence can be chosen to discover hidden macro-structures (categories, e.g. birds) or micro-structures (fine-grained classes, e.g. toucans). Our method, which generalizes t-SNE \citetsne, allows us to discover such structure without extensive grid searches over (α, β) due to our theoretical analysis: such structure is apparent with particular choices of (α, β) that generalize across datasets. We also discuss efficient parallel CPU and GPU schemes which are non-trivial due to the tree-structures employed in optimization and the large datasets that do not fully fit into GPU memory. Our method runs 20x faster than the fastest published code \citefmm. We conclude with detailed case studies on the following very large datasets: ILSVRC 2012, a standard computer vision dataset with 1.2M images; SUSY, a particle physics dataset with 5M instances; and HIGGS, another particle physics dataset with 11M instances. This represents the largest published visualization attained by SNE methods. We have open-sourced our visualization code: \texttthttp://rll.berkeley.edu/absne/.",http://proceedings.mlr.press/v37/narayan15.html,http://proceedings.mlr.press/v37/narayan15.pdf,ICML
2321,2015,An Empirical Exploration of Recurrent Network Architectures,"Rafal Jozefowicz,         Wojciech Zaremba,         Ilya Sutskever","The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM’s architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM’s forget gate closes the gap between the LSTM and the GRU.",http://proceedings.mlr.press/v37/jozefowicz15.html,http://proceedings.mlr.press/v37/jozefowicz15.pdf,ICML
2322,2015,Surrogate Functions for Maximizing Precision at the Top,"Purushottam Kar,         Harikrishna Narasimhan,         Prateek Jain","The problem of maximizing precision at the top of a ranked list, often dubbed Precision@k (prec@k), finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalance. However, despite its popularity, there exist significant gaps in our understanding of this problem and its associated performance measure. The most notable of these is the lack of a convex upper bounding surrogate for prec@k. We also lack scalable perceptron and stochastic gradient descent algorithms for optimizing this performance measure. In this paper we make key contributions in these directions. At the heart of our results is a family of truly upper bounding surrogates for prec@k. These surrogates are motivated in a principled manner and enjoy attractive properties such as consistency to prec@k under various natural margin/noise conditions. These surrogates are then used to design a class of novel perceptron algorithms for optimizing prec@k with provable mistake bounds. We also devise scalable stochastic gradient descent style methods for this problem with provable convergence bounds. Our proofs rely on novel uniform convergence bounds which require an in-depth analysis of the structural properties of prec@k and its surrogates. We conclude with experimental results comparing our algorithms with state-of-the-art cutting plane and stochastic gradient algorithms for maximizing prec@k.",http://proceedings.mlr.press/v37/kar15.html,http://proceedings.mlr.press/v37/kar15.pdf,ICML
2323,2015,"Proteins, Particles, and Pseudo-Max-Marginals: A Submodular Approach","Jason Pacheco,         Erik Sudderth","Variants of max-product (MP) belief propagation effectively find modes of many complex graphical models, but are limited to discrete distributions. Diverse particle max-product (D-PMP) robustly approximates max-product updates in continuous MRFs using stochastically sampled particles, but previous work was specialized to tree-structured models. Motivated by the challenging problem of protein side chain prediction, we extend D-PMP in several key ways to create a generic MAP inference algorithm for loopy models. We define a modified diverse particle selection objective that is provably submodular, leading to an efficient greedy algorithm with rigorous optimality guarantees, and corresponding max-marginal error bounds. We further incorporate tree-reweighted variants of the MP algorithm to allow provable verification of global MAP recovery in many models. Our general-purpose Matlab library is applicable to a wide range of pairwise graphical models, and we validate our approach using optical flow benchmarks. We further demonstrate superior side chain prediction accuracy compared to baseline algorithms from the state-of-the-art Rosetta package.",http://proceedings.mlr.press/v37/pacheco15.html,http://proceedings.mlr.press/v37/pacheco15.pdf,ICML
2324,2015,Manifold-valued Dirichlet Processes,"Hyunwoo Kim,         Jia Xu,         Baba Vemuri,         Vikas Singh","Statistical models for manifold-valued data permit capturing the intrinsic nature of the curved spaces in which the data lie and have been a topic of research for several decades. Typically, these formulations use geodesic curves and distances defined locally for most cases - this makes it hard to design parametric models globally on smooth manifolds. Thus, most (manifold specific) parametric models available today assume that the data lie in a small neighborhood on the manifold. To address this ’locality’ problem, we propose a novel nonparametric model which unifies multivariate general linear models (MGLMs) using multiple tangent spaces. Our framework generalizes existing work on (both Euclidean and non-Euclidean) general linear models providing a recipe to globally extend the locally-defined parametric models (using a mixture of local models). By grouping observations into sub-populations at multiple tangent spaces, our method provides insights into the hidden structure (geodesic relationships) in the data. This yields a framework to group observations and discover geodesic relationships between covariates X and manifold-valued responses Y, which we call Dirichlet process mixtures of multivariate general linear models (DP-MGLM) on Riemannian manifolds. Finally, we present proof of concept experiments to validate our model.",http://proceedings.mlr.press/v37/kim15.html,http://proceedings.mlr.press/v37/kim15.pdf,ICML
2325,2015,Atomic Spatial Processes,"Sean Jewell,         Neil Spencer,         Alexandre Bouchard-Côté","The emergence of compact GPS systems and the establishment of open data initiatives has resulted in widespread availability of spatial data for many urban centres. These data can be leveraged to develop data-driven intelligent resource allocation systems for urban issues such as policing, sanitation, and transportation. We employ techniques from Bayesian non-parametric statistics to develop a process which captures a common characteristic of urban spatial datasets. Specifically, our new spatial process framework models events which occur repeatedly at discrete spatial points, the number and locations of which are unknown a priori. We develop a representation of our spatial process which facilitates posterior simulation, resulting in an interpretable and computationally tractable model. The framework’s superiority over both empirical grid-based models and Dirichlet process mixture models is demonstrated by fitting, interpreting, and comparing models of graffiti prevalence for both downtown Vancouver and Manhattan.",http://proceedings.mlr.press/v37/jewell15.html,http://proceedings.mlr.press/v37/jewell15.pdf,ICML
2326,2015,DiSCO: Distributed Optimization for Self-Concordant Empirical Loss,"Yuchen Zhang,         Xiao Lin","We propose a new distributed algorithm for empirical risk minimization in machine learning. The algorithm is based on an inexact damped Newton method, where the inexact Newton steps are computed by a distributed preconditioned conjugate gradient method. We analyze its iteration complexity and communication efficiency for minimizing self-concordant empirical loss functions, and discuss the results for distributed ridge regression, logistic regression and binary classification with a smoothed hinge loss. In a standard setting for supervised learning, where the n data points are i.i.d. sampled and when the regularization parameter scales as 1/\sqrtn, we show that the proposed algorithm is communication efficient: the required round of communication does not increase with the sample size n, and only grows slowly with the number of machines.",http://proceedings.mlr.press/v37/zhangb15.html,http://proceedings.mlr.press/v37/zhangb15.pdf,ICML
2327,2015,Low Rank Approximation using Error Correcting Coding Matrices,"Shashanka Ubaru,         Arya Mazumdar,         Yousef Saad","Low-rank matrix approximation is an integral component of tools such as principal component analysis (PCA), as well as is an important instrument used in applications like web search models, text mining and computer vision, e.g., face recognition. Recently, randomized algorithms were proposed to effectively construct low rank approximations of large matrices. In this paper, we show how matrices from error correcting codes can be used to find such low rank approximations. The benefits of using these code matrices are the following: (i) They are easy to generate and they reduce randomness significantly. (ii) Code matrices have low coherence and have a better chance of preserving the geometry of an entire subspace of vectors; (iii) Unlike Fourier transforms or Hadamard matrices, which require sampling O(k\log k) columns for a rank-k approximation, the log factor is not necessary in the case of code matrices. (iv) Under certain conditions, the approximation errors can be better and the singular values obtained can be more accurate, than those obtained using Gaussian random matrices and other structured random matrices.",http://proceedings.mlr.press/v37/ubaru15.html,http://proceedings.mlr.press/v37/ubaru15.pdf,ICML
2328,2015,Variational Generative Stochastic Networks with Collaborative Shaping,"Philip Bachman,         Doina Precup","We develop an approach to training generative models based on unrolling a variational auto-encoder into a Markov chain, and shaping the chain’s trajectories using a technique inspired by recent work in Approximate Bayesian computation. We show that the global minimizer of the resulting objective is achieved when the generative model reproduces the target distribution. To allow finer control over the behavior of the models, we add a regularization term inspired by techniques used for regularizing certain types of policy search in reinforcement learning. We present empirical results on the MNIST and TFD datasets which show that our approach offers state-of-the-art performance, both quantitatively and from a qualitative point of view.",http://proceedings.mlr.press/v37/bachman15.html,http://proceedings.mlr.press/v37/bachman15.pdf,ICML
2329,2015,The Kendall and Mallows Kernels for Permutations,"Yunlong Jiao,         Jean-Philippe Vert","We show that the widely used Kendall tau correlation coefficient is a positive definite kernel for permutations. It offers a computationally attractive alternative to more complex kernels on the symmetric group to learn from rankings, or to learn to rank. We show how to extend it to partial rankings or rankings with uncertainty, and demonstrate promising results on high-dimensional classification problems in biomedical applications.",http://proceedings.mlr.press/v37/jiao15.html,http://proceedings.mlr.press/v37/jiao15.pdf,ICML
2330,2015,Convex Formulation for Learning from Positive and Unlabeled Data,"Marthinus Du Plessis,         Gang Niu,         Masashi Sugiyama","We discuss binary classification from only from positive and unlabeled data (PU classification), which is conceivable in various real-world machine learning problems. Since unlabeled data consists of both positive and negative data, simply separating positive and unlabeled data yields a biased solution. Recently, it was shown that the bias can be canceled by using a particular non-convex loss such as the ramp loss. However, classifier training with a non-convex loss is not straightforward in practice. In this paper, we discuss a convex formulation for PU classification that can still cancel the bias. The key idea is to use different loss functions for positive and unlabeled samples. However, in this setup, the hinge loss is not permissible. As an alternative, we propose the double hinge loss. Theoretically, we prove that the estimators converge to the optimal solutions at the optimal parametric rate. Experimentally, we demonstrate that PU classification with the double hinge loss performs as accurate as the non-convex method, with a much lower computational cost.",http://proceedings.mlr.press/v37/plessis15.html,http://proceedings.mlr.press/v37/plessis15.pdf,ICML
2331,2015,Mind the duality gap: safer rules for the Lasso,"Olivier Fercoq,         Alexandre Gramfort,         Joseph Salmon","Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the so-called \textitsafe rules for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules.",http://proceedings.mlr.press/v37/fercoq15.html,http://proceedings.mlr.press/v37/fercoq15.pdf,ICML
2332,2015,Low-Rank Matrix Recovery from Row-and-Column Affine Measurements,"Or Zuk,         Avishai Wagner","We propose and study a row-and-column affine measurement scheme for low-rank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix X. This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition (SVD) and least-squares (LS), which we term alg. We prove that (a simplified version of) our algorithm can recover X exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-and-column design and alg algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed row-and-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction.",http://proceedings.mlr.press/v37/zuk15.html,http://proceedings.mlr.press/v37/zuk15.pdf,ICML
2333,2015,Trust Region Policy Optimization,"John Schulman,         Sergey Levine,         Pieter Abbeel,         Michael Jordan,         Philipp Moritz","In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",http://proceedings.mlr.press/v37/schulman15.html,http://proceedings.mlr.press/v37/schulman15.pdf,ICML
2334,2015,Non-Gaussian Discriminative Factor Models via the Max-Margin Rank-Likelihood,"Xin Yuan,         Ricardo Henao,         Ephraim Tsalik,         Raymond Langley,         Lawrence Carin","We consider the problem of discriminative factor analysis for data that are in general non-Gaussian. A Bayesian model based on the ranks of the data is proposed. We first introduce a max-margin version of the rank-likelihood. A discriminative factor model is then developed, integrating the new max-margin rank-likelihood and (linear) Bayesian support vector machines, which are also built on the max-margin principle. The discriminative factor model is further extended to the nonlinear case through mixtures of local linear classifiers, via Dirichlet processes. Fully local conjugacy of the model yields efficient inference with both Markov Chain Monte Carlo and variational Bayes approaches. Extensive experiments on benchmark and real data demonstrate superior performance of the proposed model and its potential for applications in computational biology.",http://proceedings.mlr.press/v37/yuan15.html,http://proceedings.mlr.press/v37/yuan15.pdf,ICML
2335,2015,Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo,"Yu-Xiang Wang,         Stephen Fienberg,         Alex Smola","We consider the problem of Bayesian learning on sensitive datasets and present two simple but somewhat surprising results that connect Bayesian learning to “differential privacy”, a cryptographic approach to protect individual-level privacy while permitting database-level utility. Specifically, we show that under standard assumptions, getting one sample from a posterior distribution is differentially private “for free”; and this sample as a statistical estimator is often consistent, near optimal, and computationally tractable. Similarly but separately, we show that a recent line of work that use stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve differentially privacy with minor or no modifications of the algorithmic procedure at all, these observations lead to an “anytime” algorithm for Bayesian learning under privacy constraint. We demonstrate that it performs much better than the state-of-the-art differential private methods on synthetic and real datasets.",http://proceedings.mlr.press/v37/wangg15.html,http://proceedings.mlr.press/v37/wangg15.pdf,ICML
2336,2015,Message Passing for Collective Graphical Models,"Tao Sun,         Dan Sheldon,         Akshat Kumar","Collective graphical models (CGMs) are a formalism for inference and learning about a population of independent and identically distributed individuals when only noisy aggregate data are available. We highlight a close connection between approximate MAP inference in CGMs and marginal inference in standard graphical models. The connection leads us to derive a novel Belief Propagation (BP) style algorithm for collective graphical models. Mathematically, the algorithm is a strict generalization of BP—it can be viewed as an extension to minimize the Bethe free energy plus additional energy terms that are non-linear functions of the marginals. For CGMs, the algorithm is much more efficient than previous approaches to inference. We demonstrate its performance on two synthetic experiments concerning bird migration and collective human mobility.",http://proceedings.mlr.press/v37/sunc15.html,http://proceedings.mlr.press/v37/sunc15.pdf,ICML
2337,2015,Robust partially observable Markov decision process,Takayuki Osogami,"We seek to find the robust policy that maximizes the expected cumulative reward for the worst case when a partially observable Markov decision process (POMDP) has uncertain parameters whose values are only known to be in a given region. We prove that the robust value function, which represents the expected cumulative reward that can be obtained with the robust policy, is convex with respect to the belief state. Based on the convexity, we design a value-iteration algorithm for finding the robust policy. We prove that our value iteration converges for an infinite horizon. We also design point-based value iteration for fining the robust policy more efficiency possibly with approximation. Numerical experiments show that our point-based value iteration can adequately find robust policies.",http://proceedings.mlr.press/v37/osogami15.html,http://proceedings.mlr.press/v37/osogami15.pdf,ICML
2338,2015,Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes,"Yves-Laurent Kom Samo,         Stephen Roberts","In this paper we propose an efficient, scalable non-parametric Gaussian process model for inference on Poisson point processes. Our model does not resort to gridding the domain or to introducing latent thinning points. Unlike competing models that scale as O(n^3) over n data points, our model has a complexity O(nk^2) where k << n. We propose a MCMC sampler and show that the model obtained is faster, more accurate and generates less correlated samples than competing approaches on both synthetic and real-life data. Finally, we show that our model easily handles data sizes not considered thus far by alternate approaches.",http://proceedings.mlr.press/v37/samo15.html,http://proceedings.mlr.press/v37/samo15.pdf,ICML
2339,2015,Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data,"Yarin Gal,         Yutian Chen,         Zoubin Ghahramani","Multivariate categorical data occur in many applications of machine learning. One of the main difficulties with these vectors of categorical variables is sparsity. The number of possible observations grows exponentially with vector length, but dataset diversity might be poor in comparison. Recent models have gained significant improvement in supervised tasks with this data. These models embed observations in a continuous space to capture similarities between them. Building on these ideas we propose a Bayesian model for the unsupervised task of distribution estimation of multivariate categorical data. We model vectors of categorical variables as generated from a non-linear transformation of a continuous latent space. Non-linearity captures multi-modality in the distribution. The continuous representation addresses sparsity. Our model ties together many existing models, linking the linear categorical latent Gaussian model, the Gaussian process latent variable model, and Gaussian process classification. We derive inference for our model based on recent developments in sampling based variational inference. We show empirically that the model outperforms its linear and discrete counterparts in imputation tasks of sparse data.",http://proceedings.mlr.press/v37/gala15.html,http://proceedings.mlr.press/v37/gala15.pdf,ICML
2340,2015,Statistical and Algorithmic Perspectives on Randomized Sketching for Ordinary Least-Squares,"Garvesh Raskutti,         Michael Mahoney","We consider statistical and algorithmic aspects of solving large-scale least-squares (LS) problems using randomized sketching algorithms. Prior results show that, from an \emphalgorithmic perspective, when using sketching matrices constructed from random projections and leverage-score sampling, if the number of samples r much smaller than the original sample size n, then the worst-case (WC) error is the same as solving the original problem, up to a very small relative error. From a \emphstatistical perspective, one typically considers the mean-squared error performance of randomized sketching algorithms, when data are generated according to a statistical linear model. In this paper, we provide a rigorous comparison of both perspectives leading to insights on how they differ. To do this, we first develop a framework for assessing, in a unified manner, algorithmic and statistical aspects of randomized sketching methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and we use our framework to provide upper bounds for several types of random projection and random sampling algorithms. Among other results, we show that the RE can be upper bounded when r is much smaller than n, while the PE typically requires the number of samples r to be substantially larger. Lower bounds developed in subsequent work show that our upper bounds on PE can not be improved.",http://proceedings.mlr.press/v37/raskutti15.html,http://proceedings.mlr.press/v37/raskutti15.pdf,ICML
2341,2015,A Convex Optimization Framework for Bi-Clustering,"Shiau Hong Lim,         Yudong Chen,         Huan Xu","We present a framework for biclustering and clustering where the observations are general labels. Our approach is based on the maximum likelihood estimator and its convex relaxation, and generalizes recent works in graph clustering to the biclustering setting. In addition to standard biclustering setting where one seeks to discover clustering structure simultaneously in two domain sets, we show that the same algorithm can be as effective when clustering structure only occurs in one domain. This allows for an alternative approach to clustering that is more natural in some scenarios. We present theoretical results that provide sufficient conditions for the recovery of the true underlying clusters under a generalized stochastic block model. These are further validated by our empirical results on both synthetic and real data.",http://proceedings.mlr.press/v37/limb15.html,http://proceedings.mlr.press/v37/limb15.pdf,ICML
2342,2015,Consistent estimation of dynamic and multi-layer block models,"Qiuyi Han,         Kevin Xu,         Edoardo Airoldi","Significant progress has been made recently on theoretical analysis of estimators for the stochastic block model (SBM). In this paper, we consider the multi-graph SBM, which serves as a foundation for many application settings including dynamic and multi-layer networks. We explore the asymptotic properties of two estimators for the multi-graph SBM, namely spectral clustering and the maximum-likelihood estimate (MLE), as the number of layers of the multi-graph increases. We derive sufficient conditions for consistency of both estimators and propose a variational approximation to the MLE that is computationally feasible for large networks. We verify the sufficient conditions via simulation and demonstrate that they are practical. In addition, we apply the model to two real data sets: a dynamic social network and a multi-layer social network with several types of relations.",http://proceedings.mlr.press/v37/hanb15.html,http://proceedings.mlr.press/v37/hanb15.pdf,ICML
2343,2015,A Nearly-Linear Time Framework for Graph-Structured Sparsity,"Chinmay Hegde,         Piotr Indyk,         Ludwig Schmidt","We introduce a framework for sparsity structures defined via graphs. Our approach is flexible and generalizes several previously studied sparsity models. Moreover, we provide efficient projection algorithms for our sparsity model that run in nearly-linear time. In the context of sparse recovery, we show that our framework achieves an information-theoretically optimal sample complexity for a wide range of parameters. We complement our theoretical analysis with experiments demonstrating that our algorithms improve on prior work also in practice.",http://proceedings.mlr.press/v37/hegde15.html,http://proceedings.mlr.press/v37/hegde15.pdf,ICML
2344,2015,High Dimensional Bayesian Optimisation and Bandits via Additive Models,"Kirthevasan Kandasamy,         Jeff Schneider,         Barnabas Poczos","Bayesian Optimisation (BO) is a technique used in optimising a D-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on D even though the function depends on all D dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.",http://proceedings.mlr.press/v37/kandasamy15.html,http://proceedings.mlr.press/v37/kandasamy15.pdf,ICML
2345,2015,A Linear Dynamical System Model for Text,"David Belanger,         Sham Kakade","Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words’ local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous states, where words’ representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple co-occurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity.",http://proceedings.mlr.press/v37/belanger15.html,http://proceedings.mlr.press/v37/belanger15.pdf,ICML
2346,2015,Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems,"Christopher De Sa,         Christopher Re,         Kunle Olukotun","Stochastic gradient descent (SGD) on a low-rank factorization is commonly employed to speed up matrix problems including matrix completion, subspace tracking, and SDP relaxation. In this paper, we exhibit a step size scheme for SGD on a low-rank least-squares problem, and we prove that, under broad sampling conditions, our method converges globally from a random starting point within O(ε^-1 n \log n) steps with constant probability for constant-rank problems. Our modification of SGD relates it to stochastic power iteration. We also show some experiments to illustrate the runtime and convergence of the algorithm.",http://proceedings.mlr.press/v37/sa15.html,http://proceedings.mlr.press/v37/sa15.pdf,ICML
2347,2015,A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models,"En-Hsu Yen,         Xin Lin,         Kai Zhong,         Pradeep Ravikumar,         Inderjit Dhillon","MAD-Bayes (MAP-based Asymptotic Derivations) has been recently proposed as a general technique to derive scalable algorithm for Bayesian Nonparametric models. However, the combinatorial nature of objective functions derived from MAD-Bayes results in hard optimization problem, for which current practice employs heuristic algorithms analogous to k-means to find local minimum. In this paper, we consider the exemplar-based version of MAD-Bayes formulation for DP and Hierarchical DP (HDP) mixture model. We show that an exemplar-based MAD-Bayes formulation can be relaxed to a convex structural-regularized program that, under cluster-separation conditions, shares the same optimal solution to its combinatorial counterpart. An algorithm based on Alternating Direction Method of Multiplier (ADMM) is then proposed to solve such program. In our experiments on several benchmark data sets, the proposed method finds optimal solution of the combinatorial problem and significantly improves existing methods in terms of the exemplar-based objective.",http://proceedings.mlr.press/v37/yen15.html,http://proceedings.mlr.press/v37/yen15.pdf,ICML
2348,2015,Log-Euclidean Metric Learning on Symmetric Positive Definite Manifold with Application to Image Set Classification,"Zhiwu Huang,         Ruiping Wang,         Shiguang Shan,         Xianqiu Li,         Xilin Chen","The manifold of Symmetric Positive Definite (SPD) matrices has been successfully used for data representation in image set classification. By endowing the SPD manifold with Log-Euclidean Metric, existing methods typically work on vector-forms of SPD matrix logarithms. This however not only inevitably distorts the geometrical structure of the space of SPD matrix logarithms but also brings low efficiency especially when the dimensionality of SPD matrix is high. To overcome this limitation, we propose a novel metric learning approach to work directly on logarithms of SPD matrices. Specifically, our method aims to learn a tangent map that can directly transform the matrix logarithms from the original tangent space to a new tangent space of more discriminability. Under the tangent map framework, the novel metric learning can then be formulated as an optimization problem of seeking a Mahalanobis-like matrix, which can take the advantage of traditional metric learning techniques. Extensive evaluations on several image set classification tasks demonstrate the effectiveness of our proposed metric learning method.",http://proceedings.mlr.press/v37/huanga15.html,http://proceedings.mlr.press/v37/huanga15.pdf,ICML
2349,2015,Risk and Regret of Hierarchical Bayesian Learners,"Jonathan Huggins,         Josh Tenenbaum","Common statistical practice has shown that the full power of Bayesian methods is not realized until hierarchical priors are used, as these allow for greater “robustness” and the ability to “share statistical strength.” Yet it is an ongoing challenge to provide a learning-theoretically sound formalism of such notions that: offers practical guidance concerning when and how best to utilize hierarchical models; provides insights into what makes for a good hierarchical prior; and, when the form of the prior has been chosen, can guide the choice of hyperparameter settings. We present a set of analytical tools for understanding hierarchical priors in both the online and batch learning settings. We provide regret bounds under log-loss, which show how certain hierarchical models compare, in retrospect, to the best single model in the model class. We also show how to convert a Bayesian log-loss regret bound into a Bayesian risk bound for any bounded loss, a result which may be of independent interest. Risk and regret bounds for Student’s t and hierarchical Gaussian priors allow us to formalize the concepts of “robustness” and “sharing statistical strength.” Priors for feature selection are investigated as well. Our results suggest that the learning-theoretic benefits of using hierarchical priors can often come at little cost on practical problems.",http://proceedings.mlr.press/v37/hugginsb15.html,http://proceedings.mlr.press/v37/hugginsb15.pdf,ICML
2350,2015,Deterministic Independent Component Analysis,"Ruitong Huang,         Andras Gyorgy,         Csaba Szepesvári","We study independent component analysis with noisy observations. We present, for the first time in the literature, consistent, polynomial-time algorithms to recover non-Gaussian source signals and the mixing matrix with a reconstruction error that vanishes at a 1/\sqrtT rate using T observations and scales only polynomially with the natural parameters of the problem. Our algorithms and analysis also extend to deterministic source signals whose empirical distributions are approximately independent.",http://proceedings.mlr.press/v37/huangb15.html,http://proceedings.mlr.press/v37/huangb15.pdf,ICML
2351,2015,Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer,"Yan-Fu Liu,         Cheng-Yu Hsu,         Shan-Hung Wu","The Cross Domain Collaborative Filtering (CDCF) exploits the rating matrices from multiple domains to make better recommendations. Existing CDCF methods adopt the sub-structure sharing technique that can only transfer linearly correlated knowledge between domains. In this paper, we propose the notion of Hyper-Structure Transfer (HST) that requires the rating matrices to be explained by the projections of some more complex structure, called the hyper-structure, shared by all domains, and thus allows the non-linearly correlated knowledge between domains to be identified and transferred. Extensive experiments are conducted and the results demonstrate the effectiveness of our HST models empirically.",http://proceedings.mlr.press/v37/liua15.html,http://proceedings.mlr.press/v37/liua15.pdf,ICML
2352,2015,Compressing Neural Networks with the Hashing Trick,"Wenlin Chen,         James Wilson,         Stephen Tyree,         Kilian Weinberger,         Yixin Chen","As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.",http://proceedings.mlr.press/v37/chenc15.html,http://proceedings.mlr.press/v37/chenc15.pdf,ICML
2353,2015,Phrase-based Image Captioning,"Remi Lebret,         Pedro Pinheiro,         Ronan Collobert","Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely linear model to embed an image representation (generated from a previously trained Convolutional Neural Network) into a multimodal space that is common to the images and the phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on the sentence description statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.",http://proceedings.mlr.press/v37/lebret15.html,http://proceedings.mlr.press/v37/lebret15.pdf,ICML
2354,2015,Variational Inference with Normalizing Flows,"Danilo Rezende,         Shakir Mohamed","The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.",http://proceedings.mlr.press/v37/rezende15.html,http://proceedings.mlr.press/v37/rezende15.pdf,ICML
2355,2015,Counterfactual Risk Minimization: Learning from Logged Bandit Feedback,"Adith Swaminathan,         Thorsten Joachims","We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method – called Policy Optimizer for Exponential Models (POEM) – for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. POEM is evaluated on several multi-label classification problems showing substantially improved robustness and generalization performance compared to the state-of-the-art.",http://proceedings.mlr.press/v37/swaminathan15.html,http://proceedings.mlr.press/v37/swaminathan15.pdf,ICML
2356,2015,Improved Regret Bounds for Undiscounted Continuous Reinforcement Learning,"K. Lakshmanan,         Ronald Ortner,         Daniil Ryabko","We consider the problem of undiscounted reinforcement learning in continuous state space. Regret bounds in this setting usually hold under various assumptions on the structure of the reward and transition function. Under the assumption that the rewards and transition probabilities are Lipschitz, for 1-dimensional state space a regret bound of O(T^3/4) after any T steps has been given by Ortner and Ryabko (2012). Here we improve upon this result by using non-parametric kernel density estimation for estimating the transition probability distributions, and obtain regret bounds that depend on the smoothness of the transition probability distributions. In particular, under the assumption that the transition probability functions are smoothly differentiable, the regret bound is shown to be O(T^2/3) asymptotically for reinforcement learning in 1-dimensional state space. Finally, we also derive improved regret bounds for higher dimensional state space.",http://proceedings.mlr.press/v37/lakshmanan15.html,http://proceedings.mlr.press/v37/lakshmanan15.pdf,ICML
2357,2015,Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup,"Yufei Ding,         Yue Zhao,         Xipeng Shen,         Madanlal Musuvathi,         Todd Mytkowicz","This paper presents Yinyang K-means, a new algorithm for K-means clustering. By clustering the centers in the initial stage, and leveraging efficiently maintained lower and upper bounds between a point and centers, it more effectively avoids unnecessary distance calculations than prior algorithms. It significantly outperforms classic K-means and prior alternative K-means algorithms consistently across all experimented data sets, cluster numbers, and machine configurations. The consistent, superior performance—plus its simplicity, user-control of overheads, and guarantee in producing the same clustering results as the standard K-means does—makes Yinyang K-means a drop-in replacement of the classic K-means with an order of magnitude higher performance.",http://proceedings.mlr.press/v37/ding15.html,http://proceedings.mlr.press/v37/ding15.pdf,ICML
2358,2015,Learning Submodular Losses with the Lovasz Hinge,"Jiaqian Yu,         Matthew Blaschko","Learning with non-modular losses is an important problem when sets of predictions are made simultaneously. The main tools for constructing convex surrogate loss functions for set prediction are margin rescaling and slack rescaling. In this work, we show that these strategies lead to tight convex surrogates iff the underlying loss function is increasing in the number of incorrect predictions. However, gradient or cutting-plane computation for these functions is NP-hard for non-supermodular loss functions. We propose instead a novel convex surrogate loss function for submodular losses, the Lovasz hinge, which leads to O(p log p) complexity with O(p) oracle accesses to the loss function to compute a gradient or cutting-plane. As a result, we have developed the first tractable convex surrogates in the literature for submodular losses. We demonstrate the utility of this novel convex surrogate through a real world image labeling task.",http://proceedings.mlr.press/v37/yub15.html,http://proceedings.mlr.press/v37/yub15.pdf,ICML
2359,2015,PeakSeg: constrained optimal segmentation and supervised penalty learning for peak detection in count data,"Toby Hocking,         Guillem Rigaill,         Guillaume Bourque","Peak detection is a central problem in genomic data analysis, and current algorithms for this task are unsupervised and mostly effective for a single data type and pattern (e.g. H3K4me3 data with a sharp peak pattern). We propose PeakSeg, a new constrained maximum likelihood segmentation model for peak detection with an efficient inference algorithm: constrained dynamic programming. We investigate unsupervised and supervised learning of penalties for the critical model selection problem. We show that the supervised method has state-of-the-art peak detection across all data sets in a benchmark that includes both sharp H3K4me3 and broad H3K36me3 patterns.",http://proceedings.mlr.press/v37/hocking15.html,http://proceedings.mlr.press/v37/hocking15.pdf,ICML
2360,2015,BilBOWA: Fast Bilingual Distributed Representations without Word Alignments,"Stephan Gouws,         Yoshua Bengio,         Greg Corrado","We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperforms state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on the WMT11 data.",http://proceedings.mlr.press/v37/gouws15.html,http://proceedings.mlr.press/v37/gouws15.pdf,ICML
2361,2015,DP-space: Bayesian Nonparametric Subspace Clustering with Small-variance Asymptotics,"Yining Wang,         Jun Zhu","Subspace clustering separates data points approximately lying on union of affine subspaces into several clusters. This paper presents a novel nonparametric Bayesian subspace clustering model that infers both the number of subspaces and the dimension of each subspace from the observed data. Though the posterior inference is hard, our model leads to a very efficient deterministic algorithm, DP-space, which retains the nonparametric ability under a small-variance asymptotic analysis. DP-space monotonically minimizes an intuitive objective with an explicit tradeoff between data fitness and model complexity. Experimental results demonstrate that DP-space outperforms various competitors in terms of clustering accuracy and at the same time it is highly efficient.",http://proceedings.mlr.press/v37/wanga15.html,http://proceedings.mlr.press/v37/wanga15.pdf,ICML
2362,2015,Hashing for Distributed Data,"Cong Leng,         Jiaxiang Wu,         Jian Cheng,         Xi Zhang,         Hanqing Lu","Recently, hashing based approximate nearest neighbors search has attracted much attention. Extensive centralized hashing algorithms have been proposed and achieved promising performance. However, due to the large scale of many applications, the data is often stored or even collected in a distributed manner. Learning hash functions by aggregating all the data into a fusion center is infeasible because of the prohibitively expensive communication and computation overhead. In this paper, we develop a novel hashing model to learn hash functions in a distributed setting. We cast a centralized hashing model as a set of subproblems with consensus constraints. We find these subproblems can be analytically solved in parallel on the distributed compute nodes. Since no training data is transmitted across the nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large scale datasets containing up to 100 million samples demonstrate the efficacy of our method.",http://proceedings.mlr.press/v37/leng15.html,http://proceedings.mlr.press/v37/leng15.pdf,ICML
2363,2015,Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games,"Julien Perolat,         Bruno Scherrer,         Bilal Piot,         Olivier Pietquin","This paper provides an analysis of error propagation in Approximate Dynamic Programming applied to zero-sum two-player Stochastic Games. We provide a novel and unified error propagation analysis in L_p-norm of three well-known algorithms adapted to Stochastic Games (namely Approximate Value Iteration, Approximate Policy Iteration and Approximate Generalized Policy Iteration). We show that we can achieve a stationary policy which is \frac2γ(1 - γ)^2 ε+ \frac1(1 - γ)^2ε’-optimal, where εis the value function approximation error and ε’ is the approximate greedy operator error. In addition, we provide a practical algorithm (AGPI-Q) to solve infinite horizon γ-discounted two-player zero-sum stochastic games in a batch setting. It is an extension of the Fitted-Q algorithm (which solves Markov Decisions Processes in a batch setting) and can be non-parametric. Finally, we demonstrate experimentally the performance of AGPI-Q on a simultaneous two-player game, namely Alesia.",http://proceedings.mlr.press/v37/perolat15.html,http://proceedings.mlr.press/v37/perolat15.pdf,ICML
2364,2015,Large-scale log-determinant computation through stochastic Chebyshev expansions,"Insu Han,         Dmitry Malioutov,         Jinwoo Shin","Logarithms of determinants of large positive definite matrices appear ubiquitously in machine learning applications including Gaussian graphical and Gaussian process models, partition functions of discrete graphical models, minimum-volume ellipsoids and metric and kernel learning. Log-determinant computation involves the Cholesky decomposition at the cost cubic in the number of variables (i.e., the matrix dimension), which makes it prohibitive for large-scale applications. We propose a linear-time randomized algorithm to approximate log-determinants for very large-scale positive definite and general non-singular matrices using a stochastic trace approximation, called the Hutchinson method, coupled with Chebyshev polynomial expansions that both rely on efficient matrix-vector multiplications. We establish rigorous additive and multiplicative approximation error bounds depending on the condition number of the input matrix. In our experiments, the proposed algorithm can provide very high accuracy solutions at orders of magnitude faster time than the Cholesky decomposition and Shur completion, and enables us to compute log-determinants of matrices involving tens of millions of variables.",http://proceedings.mlr.press/v37/hana15.html,http://proceedings.mlr.press/v37/hana15.pdf,ICML
2365,2015,Functional Subspace Clustering with Application to Time Series,"Mohammad Taha Bahadori,         David Kale,         Yingying Fan,         Yan Liu","Functional data, where samples are random functions, are increasingly common and important in a variety of applications, such as health care and traffic analysis. They are naturally high dimensional and lie along complex manifolds. These properties warrant use of the subspace assumption, but most state-of-the-art subspace learning algorithms are limited to linear or other simple settings. To address these challenges, we propose a new framework called Functional Subspace Clustering (FSC). FSC assumes that functional samples lie in deformed linear subspaces and formulates the subspace learning problem as a sparse regression over operators. The resulting problem can be efficiently solved via greedy variable selection, given access to a fast deformation oracle. We provide theoretical guarantees for FSC and show how it can be applied to time series with warped alignments. Experimental results on both synthetic data and real clinical time series show that FSC outperforms both standard time series clustering and state-of-the-art subspace clustering.",http://proceedings.mlr.press/v37/bahadori15.html,http://proceedings.mlr.press/v37/bahadori15.pdf,ICML
2366,2015,Long Short-Term Memory Over Recursive Structures,"Xiaodan Zhu,         Parinaz Sobihani,         Hongyu Guo","The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.",http://proceedings.mlr.press/v37/zhub15.html,http://proceedings.mlr.press/v37/zhub15.pdf,ICML
2367,2015,Ordered Stick-Breaking Prior for Sequential MCMC Inference of Bayesian Nonparametric Models,"Mrinal Das,         Trapit Bansal,         Chiranjib Bhattacharyya","This paper introduces ordered stick-breaking process (OSBP), where the atoms in a stick-breaking process (SBP) appear in order. The choice of weights on the atoms of OSBP ensure that; (1) probability of adding new atoms exponentially decrease, and (2) OSBP, though non-exchangeable, admit predictive probability functions (PPFs). In a Bayesian nonparametric (BNP) setting, OSBP serves as a natural prior over sequential mini-batches, facilitating exchange of relevant statistical information by sharing the atoms of OSBP. One of the major contributions of this paper is SUMO, an MCMC algorithm, for solving the inference problem arising from applying OSBP to BNP models. SUMO uses the PPFs of OSBP to obtain a Gibbs-sampling based truncation-free algorithm which applies generally to BNP models. For large scale inference problems existing algorithms such as particle filtering (PF) are not practical and variational procedures such as TSVI (Wang & Blei, 2012) are the only alternative. For Dirichlet process mixture model (DPMM), SUMO outperforms TSVI on perplexity by 33% on 3 datasets with million data points, which are beyond the scope of PF, using only 3GB RAM.",http://proceedings.mlr.press/v37/das15.html,http://proceedings.mlr.press/v37/das15.pdf,ICML
2368,2015,Multiview Triplet Embedding: Learning Attributes in Multiple Maps,"Ehsan Amid,         Antti Ukkonen","For humans, it is usually easier to make statements about the similarity of objects in relative, rather than absolute terms. Moreover, subjective comparisons of objects can be based on a number of different and independent attributes. For example, objects can be compared based on their shape, color, etc. In this paper, we consider the problem of uncovering these hidden attributes given a set of relative distance judgments in the form of triplets. The attribute that was used to generate a particular triplet in this set is unknown. Such data occurs, e.g., in crowdsourcing applications where the triplets are collected from a large group of workers. We propose the Multiview Triplet Embedding (MVTE) algorithm that produces a number of low-dimensional maps, each corresponding to one of the hidden attributes. The method can be used to assess how many different attributes were used to create the triplets, as well as to assess the difficulty of a distance comparison task, and find objects that have multiple interpretations in relation to the other objects.",http://proceedings.mlr.press/v37/amid15.html,http://proceedings.mlr.press/v37/amid15.pdf,ICML
2369,2015,An Empirical Study of Stochastic Variational Inference Algorithms for the Beta Bernoulli Process,"Amar Shah,         David Knowles,         Zoubin Ghahramani","Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we also show that different posterior dependencies are important in BPFA relative to LDA.",http://proceedings.mlr.press/v37/shahb15.html,http://proceedings.mlr.press/v37/shahb15.pdf,ICML
2370,2015,The Hedge Algorithm on a Continuum,"Walid Krichene,         Maximilian Balandat,         Claire Tomlin,         Alexandre Bayen","We consider an online optimization problem on a subset S of R^n (not necessarily convex), in which a decision maker chooses, at each iteration t, a probability distribution x^(t) over S, and seeks to minimize a cumulative expected loss, where each loss is a Lipschitz function revealed at the end of iteration t. Building on previous work, we propose a generalized Hedge algorithm and show a O(\sqrtt \log t) bound on the regret when the losses are uniformly Lipschitz and S is uniformly fat (a weaker condition than convexity). Finally, we propose a generalization to the dual averaging method on the set of Lebesgue-continuous distributions over S.",http://proceedings.mlr.press/v37/krichene15.html,http://proceedings.mlr.press/v37/krichene15.pdf,ICML
2371,2015,On Deep Multi-View Representation Learning,"Weiran Wang,         Raman Arora,         Karen Livescu,         Jeff Bilmes","We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for representation learning while only one view is available at test time. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them experimentally on visual, speech, and language domains. To our knowledge this is the first head-to-head comparison of a variety of such techniques on multiple tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE).",http://proceedings.mlr.press/v37/wangb15.html,http://proceedings.mlr.press/v37/wangb15.pdf,ICML
2372,2015,On the Relationship between Sum-Product Networks and Bayesian Networks,"Han Zhao,         Mazen Melibari,         Pascal Poupart","In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of \em normal SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN.",http://proceedings.mlr.press/v37/zhaoc15.html,http://proceedings.mlr.press/v37/zhaoc15.pdf,ICML
2373,2014,Robust Distance Metric Learning via Simultaneous L1-Norm Minimization and Maximization,"Hua Wang,         Feiping Nie,         Heng Huang","Traditional distance metric learning with side information usually formulates the objectives using the covariance matrices of the data point pairs in the two constraint sets of must-links and cannot-links. Because the covariance matrix computes the sum of the squared L2-norm distances, it is prone to both outlier samples and outlier features. To develop a robust distance metric learning method, in this paper we propose a new objective for distance metric learning using the L1-norm distances. However, the resulted objective is very challenging to solve, because it simultaneously minimizes and maximizes (minmax) a number of non-smooth L1-norm terms. As an important theoretical contribution of this paper, we systematically derive an efficient iterative algorithm to solve the general L1-norm minmax problem, which is rarely studied in literature. We have performed extensive empirical evaluations, where our new distance metric learning method outperforms related state-of-the-art methods in a variety of experimental settings to cluster both noiseless and noisy data.",http://proceedings.mlr.press/v32/wangj14.html,http://proceedings.mlr.press/v32/wangj14.pdf,ICML
2374,2014,Fast Stochastic Alternating Direction Method of Multipliers,"Wenliang Zhong,         James Kwok","We propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms,  it improves the convergence rate on convex problems from \mO(1/\sqrtT) to \mO(1/T), where T is the number of iterations. This matches the  convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms.",http://proceedings.mlr.press/v32/zhong14.html,http://proceedings.mlr.press/v32/zhong14.pdf,ICML
2375,2014,Learning Character-level Representations for Part-of-Speech Tagging,"Cicero Dos Santos,         Bianca Zadrozny","Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Information about word morphology and shape is normally ignored when learning word representations. However, for tasks like part-of-speech tagging, intra-word information is extremely useful, specially when dealing with morphologically rich languages. In this paper, we propose a deep neural network that learns character-level representation of words and associate them with usual word representations to perform POS tagging. Using the proposed approach, while avoiding the use of any handcrafted feature, we produce state-of-the-art POS taggers for two languages: English, with 97.32% accuracy on the Penn Treebank WSJ corpus; and Portuguese, with 97.47% accuracy on the Mac-Morpho corpus, where the latter represents an error reduction of 12.2% on the best previous known result.",http://proceedings.mlr.press/v32/santos14.html,http://proceedings.mlr.press/v32/santos14.pdf,ICML
2376,2014,Learning Graphs with a Few Hubs,"Rashish Tandon,         Pradeep Ravikumar","We consider the problem of recovering the graph structure of a “hub-networked” Ising model given iid samples, under high-dimensional settings, where number of nodes p could be potentially larger than the number of samples n. By a “hub-networked” graph, we mean a graph with a few “hub nodes” with very large degrees. State of the art estimators for Ising models have a sample complexity that scales polynomially with the maximum node-degree, and are thus ill-suited to recovering such graphs with a few hub nodes. Some recent proposals for specifically recovering hub graphical models do not come with theoretical guarantees, and even empirically provide limited improvements over vanilla Ising model estimators. Here, we show that under such low sample settings, instead of estimating “difficult” components such as hub-neighborhoods, we can use quantitative indicators of our inability to do so, and thereby identify hub-nodes. This simple procedure allows us to recover hub-networked graphs with very strong statistical guarantees even under very low sample settings.",http://proceedings.mlr.press/v32/tandon14.html,http://proceedings.mlr.press/v32/tandon14.pdf,ICML
2377,2014,A Physics-Based Model Prior for Object-Oriented MDPs,"Jonathan Scholz,         Martin Levihn,         Charles Isbell,         David Wingate","One of the key challenges in using reinforcement learning in robotics is the need for models that capture natural world structure. There are, methods that formalize multi-object dynamics using relational representations, but these methods are not sufficiently compact for  real-world robotics. We present a physics-based approach that exploits modern simulation tools to efficiently parameterize physical dynamics.  Our results show that this representation can result in much faster learning, by virtue of its strong but appropriate inductive bias in  physical environments.",http://proceedings.mlr.press/v32/scholz14.html,http://proceedings.mlr.press/v32/scholz14.pdf,ICML
2378,2014,Skip Context Tree Switching,"Marc Bellemare,         Joel Veness,         Erik Talvitie","Context Tree Weighting (CTW) is a powerful probabilistic sequence prediction technique that efficiently performs Bayesian model averaging over the class of all prediction suffix trees of bounded depth. In this paper we show how to generalize this technique to the class of K-skip prediction suffix trees. Contrary to regular prediction suffix trees, K-skip prediction suffix trees are permitted to ignore up to K contiguous portions of the context. This allows for significant improvements in predictive accuracy when irrelevant variables are present, a case which often occurs within record-aligned data and images. We provide a regret-based analysis of our approach, and empirically evaluate it on the Calgary corpus and a set of Atari 2600 screen prediction tasks.",http://proceedings.mlr.press/v32/bellemare14.html,http://proceedings.mlr.press/v32/bellemare14.pdf,ICML
2379,2014,"Anti-differentiating approximation algorithms:A case study with min-cuts, spectral, and flow","David Gleich,         Michael Mahoney","We formalize and illustrate the general concept of algorithmic anti-differentiation: given an algorithmic procedure, e.g., an approximation algorithm for which worst-case approximation guarantees are available or a heuristic that has been engineered to be practically-useful but for which a precise theoretical understanding is lacking, an algorithmic anti-derivative is a precise statement of an optimization problem that is exactly solved by that procedure. We explore this concept with a case study of approximation algorithms for finding locally-biased partitions in data graphs, demonstrating connections between min-cut objectives, a personalized version of the popular PageRank vector, and the highly effective ""push"" procedure for computing an approximation to personalized PageRank. We show, for example, that this latter algorithm solves (exactly, but implicitly) an l1-regularized l2-regression problem, a fact that helps to explain its excellent performance in practice. We expect that, when available, these implicit optimization problems will be critical for rationalizing and predicting the performance of many approximation algorithms on realistic data.",http://proceedings.mlr.press/v32/gleich14.html,http://proceedings.mlr.press/v32/gleich14.pdf,ICML
2380,2014,Nonparametric Estimation of Multi-View Latent Variable Models,"Le Song,         Animashree Anandkumar,         Bo Dai,         Bo Xie","Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixtures of discrete or Gaussian distributions. In this paper, we propose a kernel method for learning multi-view latent variable models, allowing each mixture component to be nonparametric and learned from data in an unsupervised fashion. The key idea of our method is to embed the joint distribution of a multi-view latent variable model into a reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. We establish that the  sample complexity for the proposed method is quadratic in the number of latent components and is a low order polynomial in the other relevant parameters. Thus, our nonparametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. As a special case of our framework, we also obtain a first unsupervised conditional density estimator of the kind with provable guarantees. In both synthetic and real world datasets, the nonparametric tensor power method compares favorably to EM algorithm and other spectral algorithms.",http://proceedings.mlr.press/v32/songa14.html,http://proceedings.mlr.press/v32/songa14.pdf,ICML
2381,2014,Nonnegative Sparse PCA with Provable Guarantees,"Megasthenis Asteris,         Dimitris Papailiopoulos,         Alexandros Dimakis","We introduce a novel algorithm to compute nonnegative sparse principal components of positive semidefinite (PSD) matrices. Our algorithm comes with approximation guarantees   contingent on the spectral profile of the input matrix A:  the sharper the eigenvalue decay, the better the approximation quality.    If the eigenvalues decay like any asymptotically vanishing function, we can approximate nonnegative sparse PCA within any accuracy εin time polynomial in the matrix size n and desired sparsity k, but not in 1/ε. Further, we obtain a data-dependent bound that is computed by executing an algorithm on a given data set. This bound is significantly tighter than a-priori bounds and can be used to show that for all tested datasets our algorithm is provably within 40%-90% from the unknown optimum.     Our algorithm is combinatorial and explores a subspace defined by the leading eigenvectors of A. We test our scheme on several data sets, showing that it matches or outperforms the previous state of the art.",http://proceedings.mlr.press/v32/asteris14.html,http://proceedings.mlr.press/v32/asteris14.pdf,ICML
2382,2014,"Topic Modeling using Topics from Many Domains, Lifelong Learning and Big Data","Zhiyuan Chen,         Bing Liu","Topic modeling has been commonly used to discover topics from document collections. However, unsupervised models can generate many incoherent topics. To address this problem, several knowledge-based topic models have been proposed to incorporate prior domain knowledge from the user. This work advances this research much further and shows that without any user input, we can mine the prior knowledge automatically and dynamically from topics already found from a large number of domains. This paper first proposes a novel method to mine such prior knowledge dynamically in the modeling process, and then a new topic model to use the knowledge to guide the model inference. What is also interesting is that this approach offers a novel lifelong learning algorithm for topic discovery, which exploits the big (past) data and knowledge gained from such data for subsequent modeling. Our experimental results using product reviews from 50 domains demonstrate the effectiveness of the proposed approach.",http://proceedings.mlr.press/v32/chenf14.html,http://proceedings.mlr.press/v32/chenf14.pdf,ICML
2383,2014,Online Clustering of Bandits,"Claudio Gentile,         Shuai Li,         Giovanni Zappella","We introduce a novel algorithmic approach to content recommendation based on adaptive clustering of exploration-exploitation (“bandit"") strategies. We provide a sharp regret analysis of this algorithm in a standard stochastic noise setting, demonstrate its scalability properties, and prove its effectiveness on a number of artificial and real-world datasets. Our experiments show a significant increase in prediction performance over state-of-the-art methods for bandit problems.",http://proceedings.mlr.press/v32/gentile14.html,http://proceedings.mlr.press/v32/gentile14.pdf,ICML
2384,2014,Optimal Mean Robust Principal Component Analysis,"Feiping Nie,         Jianjun Yuan,         Heng Huang","Dimensionality reduction techniques extract low-dimensional structure from high-dimensional data and are widespread in machine learning research. In practice, due to lacking labeled data, the unsupervised dimensionality reduction algorithms are more desired. Among them, Principal Component Analysis (PCA) is the most widely used approach. In recent research, several robust PCA algorithms were presented to enhance the robustness of PCA model. However, all existing robust PCA methods incorrectly center the data using the L2-norm distance to calculate the mean, which actually is not the optimal mean due to the L1-norm used in the objective functions. It is non-trivial to remove the optimal mean in the robust PCA, because of the sparsity-inducing norms used in the robust formulations. In this paper, we propose novel robust PCA objective functions with removing optimal mean automatically. We naturally integrate the mean calculation into the dimensionality reduction optimization, such that the optimal mean can be obtained to enhance the dimensionality reduction. Both theoretical analysis and empirical studies demonstrate our new methods can more effectively reduce data dimensionality than previous robust PCA methods.",http://proceedings.mlr.press/v32/nieb14.html,http://proceedings.mlr.press/v32/nieb14.pdf,ICML
2385,2014,"A Convergence Rate Analysis for LogitBoost, MART and Their Variant","Peng Sun,         Tong Zhang,         Jie Zhou","LogitBoost, MART and their variant can be viewed as additive tree regression using logistic loss and boosting style optimization. We analyze their convergence rates based on a new weak learnability formulation. We show that it has O(\frac1T) rate when using gradient descent only, while a linear rate is achieved when using Newton descent. Moreover, introducing Newton descent when growing the trees, as LogitBoost does, leads to a faster linear rate. Empirical results on UCI datasets support our analysis.",http://proceedings.mlr.press/v32/sunc14.html,http://proceedings.mlr.press/v32/sunc14.pdf,ICML
2386,2014,High Order Regularization for Semi-Supervised Learning of Structured Output Problems,"Yujia Li,         Rich Zemel","Semi-supervised learning, which uses unlabeled data to help learn a discriminative model, is especially important for structured output problems, as considerably more effort is needed to label its multidimensional outputs versus standard single output problems. We propose a new max-margin framework for semi-supervised structured output learning, that allows the use of powerful discrete optimization algorithms and high order regularizers defined directly on model predictions for the unlabeled examples. We show that our framework is closely related to Posterior Regularization, and the two frameworks optimize special cases of the same objective. The new framework is instantiated on two image segmentation tasks, using both a graph regularizer and a cardinality regularizer. Experiments also demonstrate that this framework can utilize unlabeled data from a different source than the labeled data to significantly improve performance while saving labeling effort.",http://proceedings.mlr.press/v32/lif14.html,http://proceedings.mlr.press/v32/lif14.pdf,ICML
2387,2014,Generalized Exponential Concentration Inequality for Renyi Divergence Estimation,"Shashank Singh,         Barnabas Poczos","Estimating divergences between probability distributions in a consistent way is of great importance in many machine learning tasks. Although this is a fundamental problem in nonparametric statistics, to the best of our knowledge there has been no finite sample exponential inequality convergence bound derived for any divergence estimators. The main contribution of our work is to provide such a bound for an estimator of Renyi divergence for a smooth Holder class of densities on the d-dimensional unit cube. We also illustrate our theoretical results with a numerical experiment.",http://proceedings.mlr.press/v32/singh14.html,http://proceedings.mlr.press/v32/singh14.pdf,ICML
2388,2014,Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm,"Jacob Steinhardt,         Percy Liang","We present an adaptive variant of the exponentiated gradient algorithm. Leveraging the optimistic learning framework of Rakhlin & Sridharan (2012), we obtain regret bounds that in the learning from experts setting depend on the variance and path length of the best expert, improving on results by Hazan & Kale (2008) and Chiang et al. (2012), and resolving an open problem posed by Kale (2012). Our techniques naturally extend to matrix-valued loss functions, where we present an adaptive matrix exponentiated gradient algorithm. To obtain the optimal regret bound in the matrix case, we generalize the Follow-the-Regularized-Leader algorithm to vector-valued payoffs, which may be of independent interest.",http://proceedings.mlr.press/v32/steinhardtb14.html,http://proceedings.mlr.press/v32/steinhardtb14.pdf,ICML
2389,2014,Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery,"Cun Mu,         Bo Huang,         John Wright,         Donald Goldfarb","Recovering a low-rank tensor from incomplete information is a recurring problem in signal processing and machine learning. The most popular convex relaxation of this problem minimizes the sum of the nuclear norms (SNN) of the unfolding matrices of the tensor. We show that this approach can be substantially suboptimal: reliably recovering a K-way n\timesn\times⋯\times n tensor of Tucker rank (r, r, \ldots, r) from Gaussian measurements requires Ω( r n^K-1 ) observations. In contrast, a certain (intractable) nonconvex formulation needs only O(r^K + nrK) observations. We introduce a simple, new convex relaxation, which partially bridges this gap. Our new formulation succeeds with O(r^⌊K/2 ⌋n^⌈K/2 ⌉) observations. The lower bound for the SNN model follows from our new result on recovering signals with multiple structures (e.g. sparse, low rank), which indicates the significant suboptimality of the common approach of minimizing the sum of individual sparsity inducing norms (e.g. \ell_1, nuclear norm). Our new tractable formulation for low-rank tensor recovery shows how the sample complexity can be reduced by designing convex regularizers that exploit several structures jointly.",http://proceedings.mlr.press/v32/mu14.html,http://proceedings.mlr.press/v32/mu14.pdf,ICML
2390,2014,Latent Semantic Representation Learning for Scene Classification,"Xin Li,         Yuhong Guo","The performance of machine learning methods is heavily dependent on the choice of data representation. In real world applications such as scene recognition problems, the widely used low-level input features can fail to explain the high-level semantic label concepts. In this work, we address this problem by proposing a novel patch-based latent variable model to integrate latent contextual representation learning and classification model training in one joint optimization framework. Within this framework, the latent layer of variables bridge the gap between inputs and outputs by providing discriminative explanations for the semantic output labels, while being predictable from the low-level input features. Experiments conducted on standard scene recognition tasks demonstrate the efficacy of the proposed approach, comparing to the state-of-the-art scene recognition methods.",http://proceedings.mlr.press/v32/lid14.html,http://proceedings.mlr.press/v32/lid14.pdf,ICML
2391,2014,Bayesian Optimization with Inequality Constraints,"Jacob Gardner,         Matt Kusner,          Zhixiang,         Kilian Weinberger,         John Cunningham","Bayesian optimization is a powerful framework for minimizing expensive objective functions while using very few function evaluations.  It has been successfully applied to a variety of problems, including hyperparameter tuning and experimental design.  However, this framework has not been extended to the inequality-constrained optimization setting, particularly the setting in which evaluating feasibility is just as expensive as evaluating the objective.  Here we present constrained Bayesian optimization, which places a prior distribution on both the objective and the constraint functions.  We evaluate our method on simulated and real data, demonstrating that constrained Bayesian optimization can quickly find optimal and feasible points, even when small feasible regions cause standard methods to fail.",http://proceedings.mlr.press/v32/gardner14.html,http://proceedings.mlr.press/v32/gardner14.pdf,ICML
2392,2014,Demystifying Information-Theoretic Clustering,"Greg Ver Steeg,         Aram Galstyan,         Fei Sha,         Simon DeDeo","We propose a novel method for clustering data which is grounded in information-theoretic principles and requires no parametric assumptions.  Previous attempts to use information theory to define clusters in an assumption-free way are based on maximizing mutual information between data and cluster labels. We demonstrate that this intuition suffers from a fundamental conceptual flaw that causes clustering performance to deteriorate as the amount of data increases. Instead, we return to the axiomatic foundations of information theory to define a meaningful clustering measure based on the notion of consistency under coarse-graining for finite data.",http://proceedings.mlr.press/v32/steeg14.html,http://proceedings.mlr.press/v32/steeg14.pdf,ICML
2393,2014,Scalable Gaussian Process Structured Prediction for Grid Factor Graph Applications,"Sebastien Bratieres,         Novi Quadrianto,         Sebastian Nowozin,         Zoubin Ghahramani","Structured prediction is an important and well studied problem with many applications across machine learning. GPstruct is a recently proposed structured prediction model that offers appealing properties such as being kernelised, non-parametric, and supporting Bayesian inference (Bratières et al. 2013).   The model places a Gaussian process prior over energy functions which describe relationships between input variables and structured output variables.  However, the memory demand of GPstruct is quadratic in the number of latent variables and training runtime scales cubically.   This prevents GPstruct from being applied to problems involving grid factor graphs, which are prevalent in computer vision and spatial statistics applications.     Here we explore a scalable approach to learning GPstruct models based on ensemble learning, with weak learners (predictors) trained on subsets of the latent variables and bootstrap data, which can easily be distributed.  We show experiments with 4M latent variables on image segmentation.  Our method outperforms widely-used conditional random field models trained with pseudo-likelihood.   Moreover, in image segmentation problems it improves over recent state-of-the-art marginal optimisation methods in terms of predictive performance and uncertainty calibration. Finally, it generalises well on all training set sizes.",http://proceedings.mlr.press/v32/bratieres14.html,http://proceedings.mlr.press/v32/bratieres14.pdf,ICML
2394,2014,Sparse meta-Gaussian information bottleneck,"Melani Rey,         Volker Roth,         Thomas Fuchs","We present a new sparse compression technique based on the information  bottleneck (IB) principle, which takes into account side information. This is achieved by introducing a sparse variant of IB which preserves the information in only a few selected dimensions of the original data through compression. By assuming a Gaussian copula we can capture arbitrary non-Gaussian margins, continuous or discrete. We apply our model to select a sparse number of biomarkers relevant to the evolution of malignant melanoma and show that our sparse selection  provides reliable predictors.",http://proceedings.mlr.press/v32/rey14.html,http://proceedings.mlr.press/v32/rey14.pdf,ICML
2395,2014,Coordinate-descent for learning orthogonal matrices through Givens rotations,"Uri Shalit,         Gal Chechik","Optimizing over the set of orthogonal matrices is a central component in problems like sparse-PCA or tensor decomposition. Unfortunately, such optimization is hard since simple operations on orthogonal matrices easily break orthogonality, and correcting orthogonality usually costs a large amount of computation.  Here we propose a framework for optimizing orthogonal matrices, that is the parallel of coordinate-descent in Euclidean spaces. It is based on \em Givens-rotations, a fast-to-compute operation that affects a small number of entries in the learned matrix, and preserves orthogonality.  We show two applications of this approach: an algorithm for tensor decompositions used in learning mixture models, and an algorithm for sparse-PCA. We study the parameter regime where a  Givens rotation approach converges faster and achieves a superior model on a genome-wide brain-wide mRNA expression dataset.",http://proceedings.mlr.press/v32/shalit14.html,http://proceedings.mlr.press/v32/shalit14.pdf,ICML
2396,2014,Nonlinear Information-Theoretic Compressive Measurement Design,"Liming Wang,         Abolfazl Razi,         Miguel Rodrigues,         Robert Calderbank,         Lawrence Carin","We investigate design of general nonlinear functions for mapping high-dimensional data into a lower-dimensional (compressive) space. The nonlinear measurements are assumed contaminated by additive Gaussian noise. Depending on the application, we are either interested in recovering the high-dimensional data from the nonlinear compressive measurements, or performing classification directly based on these measurements. The latter case corresponds to classification based on nonlinearly constituted and noisy features. The nonlinear measurement functions are designed based on constrained mutual-information optimization. New analytic results are developed for the gradient of mutual information in this setting, for arbitrary input-signal  statistics. We make connections to kernel-based methods, such as the support vector machine. Encouraging results are presented on multiple datasets, for both signal recovery and classification. The nonlinear approach is shown to be particularly valuable in high-noise scenarios.",http://proceedings.mlr.press/v32/wangh14.html,http://proceedings.mlr.press/v32/wangh14.pdf,ICML
2397,2014,A Clockwork RNN,"Jan Koutnik,         Klaus Greff,         Faustino Gomez,         Juergen Schmidhuber","Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is  required.    This paper introduces a simple, yet powerful modification to the  simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate.    Rather than making the standard RNN models more complex, CW-RNN  reduces the number of SRN parameters, improves the performance  significantly in the tasks tested, and speeds up the network evaluation.    The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word classification,  where it outperforms both SRN and LSTM networks, and online handwriting recognition, where it outperforms SRNs.",http://proceedings.mlr.press/v32/koutnik14.html,http://proceedings.mlr.press/v32/koutnik14.pdf,ICML
2398,2014,Towards Minimax Online Learning with Unknown Time Horizon,"Haipeng Luo,         Robert Schapire","We consider online learning when the time horizon is unknown. We apply a minimax analysis, beginning with the fixed horizon case, and then moving on to two unknown-horizon settings, one that assumes the horizon is chosen randomly according to some distribution, and the other which allows the adversary full control over the horizon. For the random horizon setting with restricted losses, we derive a fully optimal minimax algorithm. And for the adversarial horizon setting, we prove a nontrivial lower bound which shows that the adversary obtains strictly more power than when the horizon is fixed and known. Based on the minimax solution of the random horizon setting, we then propose a new adaptive algorithm which “pretends” that the horizon is drawn from a distribution from a special family, but no matter how the actual horizon is chosen,  the worst-case regret is of the optimal rate. Furthermore, our algorithm can be combined and applied in many ways, for instance, to online convex optimization, follow the perturbed leader, exponential weights algorithm and first order bounds. Experiments show that our algorithm outperforms many other existing algorithms in an online linear optimization setting.",http://proceedings.mlr.press/v32/luo14.html,http://proceedings.mlr.press/v32/luo14.pdf,ICML
2399,2014,Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts,"Tien Vu Nguyen,         Dinh Phung,         Xuanlong Nguyen,         Swetha Venkatesh,         Hung Bui","We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-speciﬁc contexts results in the nDP mixture over content variables. We provide a Polya-urn view of the model and an efﬁcient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.",http://proceedings.mlr.press/v32/nguyenb14.html,http://proceedings.mlr.press/v32/nguyenb14.pdf,ICML
2400,2014,Deep Boosting,"Corinna Cortes,         Mehryar Mohri,         Umar Syed","We present a new ensemble learning algorithm, DeepBoost, which can use as base classifiers a hypothesis set containing deep decision trees, or members of other rich or complex families, and succeed in achieving high accuracy without overfitting the data. The key to the success of the algorithm is a ‘capacity-conscious’ criterion for the selection of the hypotheses.  We give new data-dependent learning bounds for convex ensembles expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. Our algorithm directly benefits from these guarantees since it seeks to minimize the corresponding learning bound. We give a full description of our algorithm, including the details of its derivation, and report the results of several experiments showing that its performance compares favorably to that of AdaBoost and Logistic Regression and their L_1-regularized variants.",http://proceedings.mlr.press/v32/cortesb14.html,http://proceedings.mlr.press/v32/cortesb14.pdf,ICML
2401,2014,A Highly Scalable Parallel Algorithm for Isotropic Total Variation Models,"Jie Wang,         Qingyang Li,         Sen Yang,         Wei Fan,         Peter Wonka,         Jieping Ye","Total variation (TV) models are among the most popular and successful tools in signal processing. However, due to the complex nature of the TV term, it is challenging to efficiently compute a solution for large-scale problems. State-of-the-art algorithms that are based on the alternating direction method of multipliers (ADMM)  often involve solving large-size linear systems. In this paper, we propose a highly scalable parallel algorithm for TV models that is based on a novel decomposition strategy of the problem domain. As a result, the TV models can be decoupled into a set of small and independent subproblems, which admit closed form solutions. This makes our approach particularly suitable for parallel implementation. Our algorithm is guaranteed to converge to its global minimum. With N variables and n_p processes, the time complexity is O(N/(εn_p)) to reach an epsilon-optimal solution. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art algorithms, especially in dealing with high-resolution, mega-size images.",http://proceedings.mlr.press/v32/wangb14.html,http://proceedings.mlr.press/v32/wangb14.pdf,ICML
2402,2014,Inferning with High Girth Graphical Models,"Uri Heinemann,         Amir Globerson","Unsupervised learning of graphical models is an important task in many domains. Although maximum likelihood learning is computationally hard, there do exist consistent learning algorithms (e.g., psuedo-likelihood and its variants). However, inference in the learned models is still hard, and thus they are not directly usable. In other words, given a probabilistic query they are not guaranteed to provide an answer that is close to the true one.   In the current paper, we provide a learning algorithm that is guaranteed to provide approximately correct probabilistic inference. We focus on a particular class of models, namely high girth graphs in the correlation decay regime. It is well known that approximate inference (e.g, using loopy BP) in such models yields marginals that are close to the true ones. Motivated by this, we propose an algorithm that always returns models of this type, and hence in the models it returns inference is approximately correct. We derive finite sample results guaranteeing that beyond a certain sample size, the resulting models will answer probabilistic queries with a high level of accuracy.   Results on synthetic data show that the models we learn indeed outperform those obtained by other algorithms, which do not return high girth graphs.",http://proceedings.mlr.press/v32/heinemann14.html,http://proceedings.mlr.press/v32/heinemann14.pdf,ICML
2403,2014,Neural Variational Inference and Learning in Belief Networks,"Andriy Mnih,         Karol Gregor","Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.",http://proceedings.mlr.press/v32/mnih14.html,http://proceedings.mlr.press/v32/mnih14.pdf,ICML
2404,2014,Ensemble Methods for Structured Prediction,"Corinna Cortes,         Vitaly Kuznetsov,         Mehryar Mohri","We present a series of learning algorithms and theoretical guarantees for designing accurate ensembles of structured prediction tasks. This includes several randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones, and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels. We give a detailed study of all these algorithms, including the description of new on-line-to-batch conversions and learning guarantees. We also report the results of extensive experiments with these algorithms in several structured prediction tasks.",http://proceedings.mlr.press/v32/cortesa14.html,http://proceedings.mlr.press/v32/cortesa14.pdf,ICML
2405,2014,Scaling SVM and Least Absolute Deviations via Exact Data Reduction,"Jie Wang,         Peter Wonka,         Jieping Ye","The support vector machine (SVM) is a widely used method for classification. Although many efforts have been devoted to develop efficient solvers, it remains challenging to apply SVM to large-scale problems. A nice property of SVM is that the non-support vectors have no effect on the resulting classifier. Motivated by this observation, we present fast and efficient screening rules to discard non-support vectors by analyzing the dual problem of SVM via variational inequalities (DVI). As a result, the number of data instances to be entered into the optimization can be substantially reduced. Some appealing features of our screening method are: (1) DVI is safe in the sense that the vectors discarded by DVI are guaranteed to be non-support vectors; (2) the data set needs to be scanned only once to run the screening, and its computational cost is negligible compared to that of solving the SVM problem; (3) DVI is independent of the solvers and can be integrated with any existing efficient solver. We also show that the DVI technique can be extended to detect non-support vectors in the least absolute deviations regression (LAD). To the best of our knowledge, there are currently no screening methods for LAD.  We have evaluated DVI on both synthetic and real data sets. Experiments indicate that DVI significantly outperforms the existing state-of-the-art screening rules for SVM, and it is very effective in discarding non-support vectors for LAD. The speedup gained by DVI rules can be up to two orders of magnitude.",http://proceedings.mlr.press/v32/wangd14.html,http://proceedings.mlr.press/v32/wangd14.pdf,ICML
2406,2014,Discovering Latent Network Structure in Point Process Data,"Scott Linderman,         Ryan Adams","Networks play a central role in modern data analysis, enabling us to reason about systems by studying the relationships between their parts.  Most often in network analysis, the edges are given.  However, in many systems it is difficult or impossible to measure the network directly.  Examples of latent networks include economic interactions linking financial instruments and patterns of reciprocity in gang violence.  In these cases, we are limited to noisy observations of events associated with each node.  To enable analysis of these implicit networks, we develop a probabilistic model that combines mutually-exciting point processes with random graph models.  We show how the Poisson superposition principle enables an elegant auxiliary variable formulation and a fully-Bayesian, parallel inference algorithm.  We evaluate this new model empirically on several datasets.",http://proceedings.mlr.press/v32/linderman14.html,http://proceedings.mlr.press/v32/linderman14.pdf,ICML
2407,2014,Boosting with Online Binary Learners for the Multiclass Bandit Problem,"Shang-Tse Chen,         Hsuan-Tien Lin,         Chi-Jen Lu","We consider the problem of online multiclass prediction in the bandit setting. Compared with the full-information setting, in which the learner can receive the true label as feedback after making each prediction, the bandit setting assumes that the learner can only know the correctness of the predicted label. Because the bandit setting is more restricted, it is difficult to design good bandit learners and currently there are not many bandit learners. In this paper, we propose an approach that systematically converts existing online binary classifiers to promising bandit learners with strong theoretical guarantee. The approach matches the idea of boosting, which has been shown to be powerful for batch learning as well as online learning. In particular, we establish the weak-learning condition on the online binary classifiers, and show that the condition allows automatically constructing a bandit learner with arbitrary strength by combining several of those classifiers. Experimental results on several real-world data sets demonstrate the effectiveness of the proposed approach.",http://proceedings.mlr.press/v32/chenb14.html,http://proceedings.mlr.press/v32/chenb14.pdf,ICML
2408,2014,Structured Generative Models of Natural Source Code,"Chris Maddison,         Daniel Tarlow","We study the problem of building generative models of natural source code (NSC); that is, source code written and understood by humans. Our primary contribution is to describe a family of generative models for NSC that have two key properties: First, they incorporate both sequential and hierarchical structure. Second, they are capable of integrating closely with a compiler, which allows leveraging compiler logic and abstractions when building structure into the model. We also develop an extension that includes more complex structure, refining how the model generates identifier tokens based on what variables are currently in scope.  Our models can be learned efficiently, and we show empirically that including appropriate structure greatly improves the probability of generating test programs.",http://proceedings.mlr.press/v32/maddison14.html,http://proceedings.mlr.press/v32/maddison14.pdf,ICML
2409,2014,Multi-label Classification via Feature-aware Implicit Label Space Encoding,"Zijia Lin,         Guiguang Ding,         Mingqing Hu,         Jianmin Wang","To tackle a multi-label classification problem with many classes, recently label space dimension reduction (LSDR) is proposed. It encodes the original label space to a low-dimensional latent space and uses a decoding process for recovery. In this paper, we propose a novel method termed FaIE to perform LSDR via Feature-aware Implicit label space Encoding. Unlike most previous work, the proposed FaIE makes no assumptions about the encoding process and directly learns a code matrix, i.e. the encoding result of some implicit encoding function, and a linear decoding matrix. To learn both matrices, FaIE jointly maximizes the recoverability of the original label space from the latent space, and the predictability of the latent space from the feature space, thus making itself feature-aware. FaIE can also be specified to learn an explicit encoding function, and extended with kernel tricks to handle non-linear correlations between the feature space and the latent space. Extensive experiments conducted on benchmark datasets well demonstrate its effectiveness.",http://proceedings.mlr.press/v32/linc14.html,http://proceedings.mlr.press/v32/linc14.pdf,ICML
2410,2014,Gaussian Process Classification and Active Learning with Multiple Annotators,"Filipe Rodrigues,         Francisco Pereira,         Bernardete Ribeiro","Learning from multiple annotators took a valuable step towards modelling data that does not fit the usual single annotator setting. However, multiple annotators sometimes offer varying degrees of expertise. When disagreements arise, the establishment of the correct label through trivial solutions such as majority voting may not be adequate, since without considering heterogeneity in the annotators, we risk generating a flawed model.   In this paper, we extend GP classification in order to account for multiple annotators with different levels expertise. By explicitly handling uncertainty, Gaussian processes (GPs) provide a natural framework to build proper multiple-annotator models. We empirically show that our model significantly outperforms other commonly used approaches, such as majority voting, without a significant increase in the computational cost of approximate Bayesian inference. Furthermore, an active learning methodology is proposed, which is able to reduce annotation cost even further.",http://proceedings.mlr.press/v32/rodrigues14.html,http://proceedings.mlr.press/v32/rodrigues14.pdf,ICML
2411,2014,Input Warping for Bayesian Optimization of Non-Stationary Functions,"Jasper Snoek,         Kevin Swersky,         Rich Zemel,         Ryan Adams","Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions.  The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization.  Although Gaussian processes provide a flexible prior over functions, there are various classes of functions that remain difficult to model.  One of the most frequently occurring of these is the class of non-stationary functions.  The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in ""log-space"", to mitigate the effects of spatially-varying length scale.  We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function.  We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.",http://proceedings.mlr.press/v32/snoek14.html,http://proceedings.mlr.press/v32/snoek14.pdf,ICML
2412,2014,PAC-inspired Option Discovery in Lifelong Reinforcement Learning,"Emma Brunskill,         Lihong Li","A key goal of AI is to create lifelong learning agents that can leverage prior experience to improve performance on later tasks. In reinforcement-learning problems, one way to summarize prior experience for future use is through options, which are temporally extended actions (subpolicies) for how to behave. Options can then be used to potentially accelerate learning in new reinforcement learning tasks. In this work, we provide the first formal analysis of the sample complexity, a measure of learning speed, of reinforcement learning with options.  This analysis helps shed light on some interesting  prior empirical results on when and how options may accelerate learning. We then quantify the benefit of options in reducing sample complexity of a lifelong learning agent. Finally, the new theoretical insights inspire a novel option-discovery algorithm that aims at minimizing overall sample complexity in lifelong reinforcement learning.",http://proceedings.mlr.press/v32/brunskill14.html,http://proceedings.mlr.press/v32/brunskill14.pdf,ICML
2413,2014,GEV-Canonical Regression for Accurate Binary Class Probability Estimation when One Class is Rare,"Arpit Agarwal,         Harikrishna Narasimhan,         Shivaram Kalyanakrishnan,         Shivani Agarwal","We consider the problem of binary class probability estimation (CPE) when one class is rare compared to the other. It is well known that standard algorithms such as logistic regression do not perform well on this task as they tend to under-estimate the probability of the rare class. Common fixes include under-sampling and weighting, together with various correction schemes. Recently, Wang & Dey (2010) suggested the use of a parametrized family of asymmetric link functions based on the generalized extreme value (GEV) distribution, which has been used for modeling rare events in statistics. The approach showed promising initial results, but combined with the logarithmic CPE loss implicitly used in their work, it results in a non-convex composite loss that is difficult to optimize. In this paper, we use tools from the theory of proper composite losses (Buja et al, 2005; Reid & Williamson, 2010) to construct a canonical underlying CPE loss corresponding to the GEV link, which yields a convex proper composite loss that we call the GEV-canonical loss; this loss is tailored for the task of CPE when one class is rare, and is easy to minimize using an IRLS-type algorithm similar to that used for logistic regression. Our experiments on both synthetic and real data demonstrate that the resulting algorithm – which we term GEV-canonical regression – outperforms common approaches such as under-sampling and weights correction for this problem.",http://proceedings.mlr.press/v32/agarwalc14.html,http://proceedings.mlr.press/v32/agarwalc14.pdf,ICML
2414,2014,Spherical Hamiltonian Monte Carlo for Constrained Target Distributions,"Shiwei Lan,         Bo Zhou,         Babak Shahbaba","Statistical models with constrained probability distributions are abundant in machine learning. Some examples include regression models with norm constraints (e.g., Lasso), probit models, many copula models, and Latent Dirichlet Allocation (LDA) models. Bayesian inference involving probability distributions confined to constrained domains could be quite challenging for commonly used sampling algorithms. For such problems, we propose a novel Markov Chain Monte Carlo (MCMC) method that provides a general and computationally efficient framework for handling boundary conditions. Our method first maps the D-dimensional constrained domain of parameters to the unit ball \bf B_0^D(1), then augments it to the D-dimensional sphere \bf S^D such that the original boundary corresponds to the equator of \bf S^D. This way, our method handles the constraints implicitly by moving freely on sphere generating proposals that remain within boundaries when mapped back to the original space. To improve the computational efficiency of our algorithm, we divide the dynamics into several parts such that the resulting split dynamics has a partial analytical solution as a geodesic flow on the sphere. We apply our method to several examples including truncated Gaussian, Bayesian Lasso, Bayesian bridge regression, and a copula model for identifying synchrony among multiple neurons. Our results show that the proposed method can provide a natural and efficient framework for handling several types of constraints on target distributions.",http://proceedings.mlr.press/v32/lan14.html,http://proceedings.mlr.press/v32/lan14.pdf,ICML
2415,2014,On p-norm Path Following in Multiple Kernel Learning for Non-linear Feature Selection,"Pratik Jawanpuria,         Manik Varma,         Saketha Nath","Our objective is to develop formulations and algorithms for efficiently computing the feature selection path – i.e. the variation in classification accuracy as the fraction of selected features is varied from null to unity. Multiple Kernel Learning subject to l_p\geq1 regularization (l_p-MKL) has been demonstrated to be one of the most effective techniques for non-linear feature selection. However, state-of-the-art l_p-MKL algorithms are too computationally expensive to be invoked thousands of times to determine the entire path.    We propose a novel conjecture which states that, for certain l_p-MKL formulations, the number of features selected in the optimal solution monotonically decreases as p is decreased from an initial value to unity. We prove the conjecture, for a generic family of kernel target alignment based formulations, and show that the feature weights themselves decay (grow) monotonically once they are below (above) a certain threshold at optimality. This allows us to develop a path following algorithm that systematically generates optimal feature sets of decreasing size. The proposed algorithm sets certain feature weights directly to zero for potentially large intervals of p thereby reducing optimization costs while simultaneously providing approximation guarantees.    We empirically demonstrate that our formulation can lead to classification accuracies which are as much as 10% higher on benchmark data sets not only as compared to other l_p-MKL formulations and uniform kernel baselines but also leading feature selection methods. We further demonstrate that our algorithm reduces training time significantly over other path following algorithms and state-of-the-art l_p-MKL optimizers such as SMO-MKL. In particular, we generate the entire feature selection path for data sets with a hundred thousand features in approximately half an hour on standard hardware.",http://proceedings.mlr.press/v32/jawanpuria14.html,http://proceedings.mlr.press/v32/jawanpuria14.pdf,ICML
2416,2014,A Divide-and-Conquer Solver for Kernel Support Vector Machines,"Cho-Jui Hsieh,         Si Si,         Inderjit Dhillon","The kernel support vector machine (SVM) is one of the most widely used classification methods; however, the amount of computation required becomes the bottleneck when facing millions of samples. In this paper, we propose and analyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In the division step, we partition the kernel SVM problem into smaller subproblems by clustering the data, so that each subproblem can be solved independently and efficiently. We show theoretically that the support vectors identified by the subproblem solution are likely to be support vectors of the entire kernel SVM problem, provided that the problem is partitioned appropriately by kernel clustering. In the conquer step, the local solutions from the subproblems are used to initialize a global coordinate descent solver, which converges quickly as suggested by our analysis. By extending this idea, we develop a multilevel Divide-and-Conquer SVM algorithm with adaptive clustering and early prediction strategy, which outperforms state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on the covtype dataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM in obtaining the exact SVM solution (to within 10^-6 relative error) which achieves 96.15% prediction accuracy. Moreover, with our proposed early prediction strategy, DC-SVM achieves about 96% accuracy in only 12 minutes, which is more than 100 times faster than LIBSVM.",http://proceedings.mlr.press/v32/hsieha14.html,http://proceedings.mlr.press/v32/hsieha14.pdf,ICML
2417,2014,Optimization Equivalence of Divergences Improves Neighbor Embedding,"Zhirong Yang,         Jaakko Peltonen,         Samuel Kaski","Visualization methods that arrange data objects in 2D or 3D layouts have followed two main schools, methods oriented for graph layout and methods oriented for vectorial embedding. We show the two previously separate approaches are tied by an optimization equivalence, making it possible to relate methods from the two approaches and to build new methods that take the best of both worlds.  In detail, we prove a theorem of optimization equivalences between beta- and gamma-, as well as alpha- and Renyi-divergences through a connection scalar. Through the equivalences we represent several nonlinear dimensionality reduction and graph drawing methods in a generalized stochastic neighbor embedding setting, where information divergences are minimized between similarities in input and output spaces, and the optimal connection scalar provides a natural choice for the tradeoff between attractive and repulsive forces. We give two examples of developing new visualization methods through the equivalences: 1) We develop weighted symmetric stochastic neighbor embedding (ws-SNE) from Elastic Embedding and analyze its benefits, good performance for both vectorial and network data; in experiments ws-SNE has good performance across data sets of different types, whereas comparison methods fail for some of the data sets; 2) we develop a gamma-divergence version of a PolyLog layout method; the new method is scale invariant in the output space and makes it possible to efficiently use large-scale smoothed neighborhoods.",http://proceedings.mlr.press/v32/yange14.html,http://proceedings.mlr.press/v32/yange14.pdf,ICML
2418,2014,Joint Inference of Multiple Label Types in Large Networks,"Deepayan Chakrabarti,         Stanislav Funiak,         Jonathan Chang,         Sofus Macskassy","We tackle the problem of inferring node labels in a partially labeled  graph where each node in the graph has multiple label types and  each label type has a large number of possible labels.  Our primary  example, and the focus of this paper, is the joint inference of label  types such as hometown, current city, and employers, for users  connected by a social network.  Standard label propagation fails to  consider the properties of the label types and the interactions  between them.  Our proposed method, called EdgeExplain, explicitly  models these, while still enabling scalable inference under a  distributed message-passing architecture.  On a billion-node subset of the Facebook social network,  EdgeExplain significantly outperforms label propagation for several  label types, with lifts of up to 120% for recall@1 and 60% for  recall@3.",http://proceedings.mlr.press/v32/chakrabarti14.html,http://proceedings.mlr.press/v32/chakrabarti14.pdf,ICML
2419,2014,Towards an optimal stochastic alternating direction method of multipliers,"Samaneh Azadi,         Suvrit Sra","We study regularized stochastic convex optimization subject to linear equality constraints. This class of problems was recently also studied by Ouyang et al. (2013) and Suzuki (2013); both introduced similar stochastic alternating direction method of multipliers (SADMM) algorithms. However, the analysis of both papers led to suboptimal convergence rates. This paper presents two new SADMM methods: (i) the first attains the minimax optimal rate of O(1/k) for nonsmooth strongly-convex stochastic problems; while (ii) the second progresses towards an optimal rate by exhibiting an O(1/k^2) rate for the smooth part. We present several experiments with our new methods; the results indicate improved performance over competing ADMM methods.",http://proceedings.mlr.press/v32/azadi14.html,http://proceedings.mlr.press/v32/azadi14.pdf,ICML
2420,2014,Robust Principal Component Analysis with Complex Noise,"Qian Zhao,         Deyu Meng,         Zongben Xu,         Wangmeng Zuo,         Lei Zhang","The research on robust principal component analysis (RPCA) has been attracting much attention recently. The original RPCA model assumes sparse noise, and use the L_1-norm to characterize the error term. In practice, however, the noise is much more complex and it is not appropriate to simply use a certain L_p-norm for noise modeling. We propose a generative RPCA model under the Bayesian framework by modeling data noise as a mixture of Gaussians (MoG). The MoG is a universal approximator to continuous distributions and thus our model is able to fit a wide range of noises such as Laplacian, Gaussian, sparse noises and any combinations of them. A variational Bayes algorithm is presented to infer the posterior of the proposed model. All involved parameters can be recursively updated in closed form. The advantage of our method is demonstrated by extensive experiments on synthetic data, face modeling and background subtraction.",http://proceedings.mlr.press/v32/zhao14.html,http://proceedings.mlr.press/v32/zhao14.pdf,ICML
2421,2014,On learning to localize objects with minimal supervision,"Hyun Oh Song,         Ross Girshick,         Stefanie Jegelka,         Julien Mairal,         Zaid Harchaoui,         Trevor Darrell","Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasi-Newton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection.",http://proceedings.mlr.press/v32/songb14.html,http://proceedings.mlr.press/v32/songb14.pdf,ICML
2422,2014,Computing Parametric Ranking Models via Rank-Breaking,"Hossein Azari Soufiani,         David Parkes,         Lirong Xia","Rank breaking is a methodology introduced by Azari Soufiani et al. (2013a) for applying a Generalized Method of Moments (GMM) algorithm to the estimation of parametric ranking models. Breaking takes full rankings and breaks, or splits them up, into counts for pairs of alternatives that occur in particular positions (e.g., first place and second place, second place and third place). GMMs are of interest because they can achieve significant speed-up relative to maximum likelihood approaches and comparable statistical efficiency. We characterize the breakings for which the estimator is consistent for random utility models (RUMs) including Plackett-Luce and Normal-RUM, develop a general sufficient condition for a full breaking to be the only consistent breaking, and provide a trichotomy theorem in regard to single-edge breakings. Experimental results are presented to show the computational efficiency along with statistical performance of the proposed method.",http://proceedings.mlr.press/v32/soufiani14.html,http://proceedings.mlr.press/v32/soufiani14.pdf,ICML
2423,2014,Learning Polynomials with Neural Networks,"Alexandr Andoni,         Rina Panigrahy,         Gregory Valiant,         Li Zhang","We study the effectiveness of learning low degree polynomials using   neural networks by the gradient descent method.  While neural   networks have been shown to have great expressive power, and gradient   descent has been widely used in practice for learning neural   networks, few theoretical guarantees are known for such methods.  In   particular, it is well known that gradient descent can get stuck at   local minima, even for simple classes of target functions.  In this   paper, we present several positive theoretical results to support the   effectiveness of neural networks.  We focus on two-layer neural   networks (i.e. one hidden layer) where the top layer node is a linear   function, similar to \citebarron93.  First we show that for a   randomly initialized neural network with sufficiently many hidden   units, the gradient descent method can learn any low degree   polynomial.  Secondly, we show that if we use complex-valued weights   (the target function can still be real), then under suitable   conditions, there are no “robust local minima”: the neural network   can always escape a local minimum by performing a random   perturbation. This property does not hold for real-valued weights.   Thirdly, we discuss whether sparse polynomials can be learned   with \emphsmall neural networks, where the size is dependent on the   sparsity of the target function.",http://proceedings.mlr.press/v32/andoni14.html,http://proceedings.mlr.press/v32/andoni14.pdf,ICML
2424,2014,Asynchronous Distributed ADMM for Consensus Optimization,"Ruiliang Zhang,         James Kwok","Distributed optimization algorithms are highly attractive for solving big data problems. In particular, many machine learning problems can be formulated as the global consensus optimization problem, which can then be solved in a distributed manner by the alternating direction method of multipliers (ADMM) algorithm. However, this suffers from the straggler problem as its updates have to be synchronized. In this paper, we propose an asynchronous ADMM algorithm by using two conditions to control the asynchrony: partial barrier and bounded delay. The proposed algorithm has a simple structure and good convergence guarantees (its convergence rate can be reduced to that of its synchronous counterpart). Experiments on different distributed ADMM applications show that asynchrony reduces the time on network waiting, and achieves faster convergence than its synchronous counterpart in terms of the wall clock time.",http://proceedings.mlr.press/v32/zhange14.html,http://proceedings.mlr.press/v32/zhange14.pdf,ICML
2425,2014,Optimal Budget Allocation: Theoretical Guarantee and Efficient Algorithm,"Tasuku Soma,         Naonori Kakimura,         Kazuhiro Inaba,         Ken-ichi Kawarabayashi","We consider the budget allocation problem over bipartite influence model proposed by Alon et al. This problem can be viewed as the well-known influence maximization problem with budget constraints.     We first show that this problem and its much more general form  fall into a general setting; namely the monotone submodular function maximization over integer lattice subject to a knapsack constraint.  Our framework includes Alon et al.’s model, even with a competitor and with cost.  We then give a (1-1/e)-approximation algorithm for this more general problem. Furthermore, when influence probabilities are nonincreasing, we obtain a faster (1-1/e)-approximation algorithm, which runs essentially in linear time in the number of nodes. This allows us to implement our algorithm up to almost 10M edges (indeed, our experiments tell us that we can implement our algorithm up to 1 billion edges. It would approximately take us only 500 seconds.).",http://proceedings.mlr.press/v32/soma14.html,http://proceedings.mlr.press/v32/soma14.pdf,ICML
2426,2014,Hierarchical Quasi-Clustering Methods for Asymmetric Networks,"Gunnar Carlsson,         Facundo Mémoli,         Alejandro Ribeiro,         Santiago Segarra","This paper introduces hierarchical quasi-clustering methods, a generalization of hierarchical clustering for asymmetric networks where the output structure preserves the asymmetry of the input data. We show that this output structure is equivalent to a finite quasi-ultrametric space and study admissibility with respect to two desirable properties. We prove that a modified version of single linkage is the only admissible quasi-clustering method. Moreover, we show stability of the proposed method and we establish invariance properties fulfilled by it. Algorithms are further developed and the value of quasi-clustering analysis is illustrated with a study of internal migration within United States.",http://proceedings.mlr.press/v32/carlsson14.html,http://proceedings.mlr.press/v32/carlsson14.pdf,ICML
2427,2014,Scaling Up Approximate Value Iteration with Options: Better Policies with Fewer Iterations,"Timothy Mann,         Shie Mannor","We show how options, a class of control structures encompassing primitive and temporally extended actions, can play a valuable role in planning in MDPs with continuous state-spaces. Analyzing the convergence rate of Approximate Value Iteration with options reveals that for pessimistic initial value function estimates, options can speed up convergence compared to planning with only primitive actions even when the temporally extended actions are suboptimal and sparsely scattered throughout the state-space. Our experimental results in an optimal replacement task and a complex inventory management task demonstrate the potential for options to speed up convergence in practice. We show that options induce faster convergence to the optimal value function, which implies deriving better policies with fewer iterations.",http://proceedings.mlr.press/v32/mann14.html,http://proceedings.mlr.press/v32/mann14.pdf,ICML
2428,2014,Marginalized Denoising Auto-encoders for Nonlinear Representations,"Minmin Chen,         Kilian Weinberger,         Fei Sha,         Yoshua Bengio","Denoising auto-encoders (DAEs) have been successfully  used to learn new representations for a  wide range of machine learning tasks. During  training, DAEs make many passes over the training  dataset and reconstruct it from partial corruption  generated from a pre-specified corrupting  distribution. This process learns robust representation,  though at the expense of requiring many  training epochs, in which the data is explicitly  corrupted. In this paper we present the marginalized  Denoising Auto-encoder (mDAE), which  (approximately) marginalizes out the corruption  during training. Effectively, the mDAE takes  into account infinitely many corrupted copies of  the training data in every epoch, and therefore is  able to match or outperform the DAE with much  fewer training epochs. We analyze our proposed  algorithm and show that it can be understood as  a classic auto-encoder with a special form of regularization.  In empirical evaluations we show  that it attains 1-2 order-of-magnitude speedup in  training time over other competing approaches.",http://proceedings.mlr.press/v32/cheng14.html,http://proceedings.mlr.press/v32/cheng14.pdf,ICML
2429,2014,Dynamic Programming Boosting for Discriminative Macro-Action Discovery,"Leonidas Lefakis,         Francois Fleuret","We consider the problem of automatic macro-action discovery in imitation learning, which we cast as one of change-point detection. Unlike prior work in change-point detection, the present work leverages discriminative learning algorithms. Our main contribution is a novel supervised learning algorithm which extends the classical Boosting framework by combining it with dynamic programming. The resulting process alternatively improves the performance of individual strong predictors and the estimated change-points in the training sequence. Empirical evaluation is presented for the proposed method on tasks where change-points arise naturally as part of a classification problem. Finally we show the applicability of the algorithm to macro-action discovery in imitation learning and demonstrate it allows us to solve complex image-based goal-planning problems with thousands of features.",http://proceedings.mlr.press/v32/lefakis14.html,http://proceedings.mlr.press/v32/lefakis14.pdf,ICML
2430,2014,Structured Recurrent Temporal Restricted Boltzmann Machines,"Roni Mittelman,         Benjamin Kuipers,         Silvio Savarese,         Honglak Lee","The Recurrent temporal restricted Boltzmann machine (RTRBM) is a probabilistic model for temporal data, that has been shown to effectively capture both short and long-term dependencies in time-series. The topology of the RTRBM graphical model, however, assumes full connectivity between all the pairs of visible and hidden units, therefore ignoring the dependency structure between the different observations. Learning this structure has the potential to not only improve the prediction performance, but it can also reveal important patterns in the data. For example, given an econometric dataset, we could identify interesting dependencies between different market sectors; given a meteorological dataset, we could identify regional weather patterns. In this work we propose a new class of RTRBM, which explicitly uses a dependency graph to model the structure in the problem and to define the energy function. We refer to the new model as the structured RTRBM (SRTRBM). Our technique is related to methods such as graphical lasso, which are used to learn the topology of Gaussian graphical models. We also develop a spike-and-slab version of the RTRBM, and combine it with our method to learn structure in datasets with real valued observations. Our experimental results using synthetic and real datasets, demonstrate that the SRTRBM can improve the prediction performance of the RTRBM, particularly when the number of visible units is large and the size of the training set is small. It also reveals the structure underlying our benchmark datasets.",http://proceedings.mlr.press/v32/mittelman14.html,http://proceedings.mlr.press/v32/mittelman14.pdf,ICML
2431,2014,Scalable Bayesian Low-Rank Decomposition of Incomplete Multiway Tensors,"Piyush Rai,         Yingjian Wang,         Shengbo Guo,         Gary Chen,         David Dunson,         Lawrence Carin","We present a scalable Bayesian framework for low-rank decomposition of multiway tensor data with missing observations. The key issue of pre-specifying the rank of the decomposition is sidestepped in a principled manner using a multiplicative gamma process prior. Both continuous and binary data can be analyzed under the framework, in a coherent way using fully conjugate Bayesian analysis. In particular, the analysis in the non-conjugate binary case is facilitated via the use of the Pólya-Gamma sampling strategy which elicits closed-form Gibbs sampling updates. The resulting samplers are efficient and enable us to apply our framework to large-scale problems, with time-complexity that is linear in the number of observed entries in the tensor. This is especially attractive in analyzing very large but sparsely observed tensors with very few known entries. Moreover, our method admits easy extension to the supervised setting where entities in one or more tensor modes have labels. Our method outperforms several state-of-the-art tensor decomposition methods on various synthetic and benchmark real-world datasets.",http://proceedings.mlr.press/v32/rai14.html,http://proceedings.mlr.press/v32/rai14.pdf,ICML
2432,2014,A Single-Pass Algorithm for Efficiently Recovering Sparse Cluster Centers of High-dimensional Data,"Jinfeng Yi,         Lijun Zhang,         Jun Wang,         Rong Jin,         Anil Jain","Learning a statistical model for high-dimensional data is an important topic in machine learning. Although this problem has been well studied in the supervised setting, little is known about its unsupervised counterpart. In this work, we focus on the problem of clustering high-dimensional data with sparse centers.  In particular, we address the following open question in unsupervised learning: “is it possible to reliably cluster high-dimensional data when the number of samples is smaller than the data dimensionality?"" We develop an efficient clustering algorithm that is able to estimate sparse cluster centers with a single pass over the data. Our theoretical analysis shows that the proposed algorithm is able to accurately recover cluster centers with only O(s\log d) number of samples (data points), provided all the cluster centers are s-sparse vectors in a d dimensional space. Experimental results verify both the effectiveness and efficiency of the proposed clustering algorithm compared to the state-of-the-art algorithms on several benchmark datasets.",http://proceedings.mlr.press/v32/yib14.html,http://proceedings.mlr.press/v32/yib14.pdf,ICML
2433,2014,Scalable and Robust Bayesian Inference via the Median Posterior,"Stanislav Minsker,         Sanvesh Srivastava,         Lizhen Lin,         David Dunson","Many Bayesian learning methods for massive data benefit from working with small subsets of observations.  In particular, significant progress has been made in scalable Bayesian learning via stochastic approximation.  However, Bayesian learning methods in distributed computing environments are often problem- or distribution-specific and use ad hoc techniques.   We propose a novel general approach to Bayesian inference that is scalable and robust to corruption in the data.  Our technique is based on the idea of splitting the data into several non-overlapping subgroups, evaluating the posterior distribution given each independent subgroup, and then combining the results.  The main novelty is the proposed aggregation step which is based on finding the geometric median of posterior distributions.    We present both theoretical and numerical results illustrating the advantages of our approach.",http://proceedings.mlr.press/v32/minsker14.html,http://proceedings.mlr.press/v32/minsker14.pdf,ICML
2434,2014,Stochastic Variational Inference for Bayesian Time Series Models,"Matthew Johnson,         Alan Willsky","Bayesian models provide powerful tools for analyzing complex time series data, but performing inference with large datasets is a challenge.  Stochastic variational inference (SVI) provides a new framework for approximating model posteriors with only a small number of passes through the data, enabling such models to be fit at scale.  However, its application to time series models has not been studied.    In this paper we develop SVI algorithms for several common Bayesian time series models, namely the hidden Markov model (HMM), hidden semi-Markov model (HSMM), and the nonparametric HDP-HMM and HDP-HSMM.  In addition, because HSMM inference can be expensive even in the minibatch setting of SVI, we develop fast approximate updates for HSMMs with durations distributions that are negative binomials or mixtures of negative binomials.",http://proceedings.mlr.press/v32/johnson14.html,http://proceedings.mlr.press/v32/johnson14.pdf,ICML
2435,2014,Model-Based Relational RL When Object Existence is Partially Observable,"Ngo Ahn Vien,         Marc Toussaint","We consider learning and planning in relational MDPs when object existence is uncertain and new objects may appear or disappear depending on previous actions or properties of other objects. Optimal policies actively need to discover  objects to achieve a goal; planning in such domains in general amounts to a POMDP problem, where the belief is about the existence and properties of potential not-yet-discovered objects. We propose a computationally efficient extension of model-based relational RL methods that approximates these beliefs using discrete uncertainty predicates. In this formulation the belief update  is learned using probabilistic rules and planning in the approximated belief space can be achieved  using an extension of existing planners. We prove that the learned belief update rules encode an approximation of the exact belief updates of a POMDP formulation and demonstrate experimentally that the proposed approach successfully  learns a set of relational rules appropriate to solve  such problems.",http://proceedings.mlr.press/v32/ngo14.html,http://proceedings.mlr.press/v32/ngo14.pdf,ICML
2436,2014,Learning the Parameters of Determinantal Point Process Kernels,"Raja Hafiz Affandi,         Emily Fox,         Ryan Adams,         Ben Taskar","Determinantal point processes (DPPs) are  well-suited for modeling repulsion and have  proven useful in applications where diversity  is desired. While DPPs have many appealing  properties, learning the parameters of a DPP  is difficult, as the likelihood is non-convex  and is infeasible to compute in many scenarios. Here we propose Bayesian methods for  learning the DPP kernel parameters. These methods are applicable in large-scale discrete  and continuous DPP settings, even when the  likelihood can only be bounded. We demonstrate  the utility of our DPP learning methods  in studying the progression of diabetic neuropathy  based on the spatial distribution of  nerve fibers, and in studying human perception  of diversity in images.",http://proceedings.mlr.press/v32/affandi14.html,http://proceedings.mlr.press/v32/affandi14.pdf,ICML
2437,2014,Composite Quantization for Approximate Nearest Neighbor Search,"Ting Zhang,         Chao Du,         Jingdong Wang","This paper presents a novel compact coding approach, composite quantization, for approximate nearest neighbor search. The idea is to use the composition of several elements selected from the dictionaries to accurately approximate a vector and to represent the vector by a short code composed of the indices of the selected elements. To efficiently compute the approximate distance of a query to a database vector using the short code, we introduce an extra constraint, constant inter-dictionary-element-product, resulting in that  approximating the distance only using the distance of the query to each selected element is enough for nearest neighbor search. Experimental comparison with state-of-the-art algorithms over several benchmark datasets demonstrates the efficacy of the proposed approach.",http://proceedings.mlr.press/v32/zhangd14.html,http://proceedings.mlr.press/v32/zhangd14.pdf,ICML
2438,2014,Learning the Consistent Behavior of Common Users for Target Node Prediction across Social Networks,"Shan-Hung Wu,         Hao-Heng Chien,         Kuan-Hua Lin,         Philip Yu","We study the target node prediction problem: given two social networks, identify those nodes/users from one network (called the source network) who are likely to join another (called the target network, with nodes called target nodes). Although this problem can be solved using existing techniques in the field of cross domain classification, we observe that in many real-world situations the cross-domain classifiers perform sub-optimally due to the heterogeneity between source and target networks that prevents the knowledge from being transferred. In this paper, we propose learning the consistent behavior of common users to help the knowledge transfer. We first present the Consistent Incidence Co-Factorization (CICF) for identifying the consistent users, i.e., common users that behave consistently across networks. Then we introduce the Domain-UnBiased (DUB) classifiers that transfer knowledge only through those consistent users. Extensive experiments are conducted and the results show that our proposal copes with heterogeneity and improves prediction accuracy.",http://proceedings.mlr.press/v32/wu14.html,http://proceedings.mlr.press/v32/wu14.pdf,ICML
2439,2014,Online Multi-Task Learning for Policy Gradient Methods,"Haitham Bou Ammar,         Eric Eaton,         Paul Ruvolo,         Matthew Taylor","Policy gradient algorithms have shown considerable recent success in solving high-dimensional sequential decision making tasks, particularly in robotics.  However, these methods often require extensive experience in a domain to achieve high performance.  To make agents more sample-efficient, we developed a multi-task policy gradient method to learn decision making tasks consecutively, transferring knowledge between tasks to accelerate learning.  Our approach provides robust theoretical guarantees, and we show empirically that it dramatically accelerates learning on a variety of dynamical systems, including an application to quadrotor control.",http://proceedings.mlr.press/v32/ammar14.html,http://proceedings.mlr.press/v32/ammar14.pdf,ICML
2440,2014,Kernel Adaptive Metropolis-Hastings,"Dino Sejdinovic,         Heiko Strathmann,         Maria Lomeli Garcia,         Christophe Andrieu,         Arthur Gretton","A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for the purpose of sampling from a target distribution with strongly nonlinear support. The algorithm embeds the trajectory of the Markov chain into a reproducing kernel Hilbert space (RKHS), such that the feature space covariance of the samples informs the choice of proposal. The procedure is computationally efficient and straightforward to implement, since the RKHS moves can be integrated out analytically: our proposal distribution in the original space is a normal distribution whose mean and covariance depend on where the current sample lies in the support of the target distribution, and adapts to its local covariance structure. Furthermore, the procedure requires neither gradients nor any other higher order information about the target, making it particularly attractive for contexts such as Pseudo-Marginal MCMC. Kernel Adaptive Metropolis-Hastings outperforms competing fixed and adaptive samplers on multivariate, highly nonlinear target distributions, arising in both real-world and synthetic examples.",http://proceedings.mlr.press/v32/sejdinovic14.html,http://proceedings.mlr.press/v32/sejdinovic14.pdf,ICML
2441,2014,A Deep Semi-NMF Model for Learning Hidden Representations,"George Trigeorgis,         Konstantinos Bousmalis,         Stefanos Zafeiriou,         Bjoern Schuller","Semi-NMF is a matrix factorization technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original features contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies can not interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering  according to different, unknown attributes of a given dataset. We show that by doing so, our model is able to learn low-dimensional representations that are better suited for clustering, outperforming Semi-NMF, but also other NMF variants.",http://proceedings.mlr.press/v32/trigeorgis14.html,http://proceedings.mlr.press/v32/trigeorgis14.pdf,ICML
2442,2014,Kernel Mean Estimation and Stein Effect,"Krikamol Muandet,         Kenji Fukumizu,         Bharath Sriperumbudur,         Arthur Gretton,         Bernhard Schoelkopf","A mean function in reproducing kernel Hilbert space (RKHS), or a kernel mean, is an important part of many algorithms ranging from kernel principal component analysis to Hilbert-space embedding of distributions. Given a finite sample, an empirical average is the standard estimate for the true kernel mean. We show that this estimator can be improved due to a well-known phenomenon in statistics called Stein phenomenon. After consideration, our theoretical analysis reveals the existence of a wide class of estimators that are better than the standard one. Focusing on a subset of this class, we propose efficient shrinkage estimators for the kernel mean. Empirical evaluations on several applications clearly demonstrate that the proposed estimators outperform the standard kernel mean estimator.",http://proceedings.mlr.press/v32/muandet14.html,http://proceedings.mlr.press/v32/muandet14.pdf,ICML
2443,2014,Guess-Averse Loss Functions For Cost-Sensitive Multiclass Boosting,"Oscar Beijbom,         Mohammad Saberian,         David Kriegman,         Nuno Vasconcelos","Cost-sensitive multiclass classification has recently  acquired significance in several applications, through the introduction  of multiclass datasets with well-defined misclassification  costs. The design of classification algorithms for this  setting is considered. It is argued that the unreliable performance  of current algorithms is due to the inability of the underlying  loss functions to enforce a certain fundamental underlying property.   This property, denoted guess-aversion, is that  the loss should encourage correct classifications over the arbitrary guessing  that ensues when all classes are equally scored by the classifier.  While guess-aversion holds trivially for binary classification, this is not true in  the multiclass setting. A new family of cost-sensitive guess-averse   loss functions is derived, and used to design new cost-sensitive multiclass   boosting algorithms, denoted GEL- and GLL-MCBoost.  Extensive experiments demonstrate (1) the general importance of   guess-aversion and (2) that the GLL loss function outperforms other loss functions for multiclass boosting.",http://proceedings.mlr.press/v32/beijbom14.html,http://proceedings.mlr.press/v32/beijbom14.pdf,ICML
2444,2014,Spectral Regularization for Max-Margin Sequence Tagging,"Ariadna Quattoni,         Borja Balle,         Xavier Carreras,         Amir Globerson","We frame max-margin learning of latent variable structured prediction models as a convex optimization problem, making use of scoring functions computed by input-output observable operator models. This learning problem can be expressed as an optimization involving a low-rank Hankel matrix that represents the input-output operator model. The direct outcome of our work is a new spectral regularization method for max-margin structured prediction.  Our experiments confirm that our proposed regularization framework leads to an effective way of controlling the capacity of structured prediction models.",http://proceedings.mlr.press/v32/quattoni14.html,http://proceedings.mlr.press/v32/quattoni14.pdf,ICML
2445,2014,Graph-based Semi-supervised Learning: Realizing Pointwise Smoothness Probabilistically,"Yuan Fang,         Kevin Chang,         Hady Lauw","As the central notion in semi-supervised learning, smoothness is often realized on a graph representation of the data. In this paper, we study two complementary dimensions of smoothness: its pointwise nature and probabilistic modeling. While no existing graph-based work exploits them in conjunction, we encompass both in a novel framework of Probabilistic Graph-based Pointwise Smoothness (PGP), building upon two foundational models of data closeness and label coupling. This new form of smoothness axiomatizes a set of probability constraints, which ultimately enables class prediction. Theoretically, we provide an error and robustness analysis of PGP. Empirically, we conduct extensive experiments to show the advantages of PGP.",http://proceedings.mlr.press/v32/fang14.html,http://proceedings.mlr.press/v32/fang14.pdf,ICML
2446,2014,Learning Sum-Product Networks with Direct and Indirect Variable Interactions,"Amirmohammad Rooshenas,         Daniel Lowd","Sum-product networks (SPNs) are a deep probabilistic representation that allows for efficient, exact inference.  SPNs generalize many other tractable models, including thin junction trees, latent tree models, and many types of mixtures.  Previous work on learning SPN structure has mainly focused on using top-down or bottom-up clustering to find mixtures, which capture variable interactions indirectly through implicit latent variables.  In contrast, most work on learning graphical models, thin junction trees, and arithmetic circuits has focused on finding direct interactions among variables.  In this paper, we present ID-SPN, a new algorithm for learning SPN structure that unifies the two approaches. In experiments on 20 benchmark datasets, we find that the combination of direct and indirect interactions leads to significantly better accuracy than several state-of-the-art algorithms for learning SPNs and other tractable models.",http://proceedings.mlr.press/v32/rooshenas14.html,http://proceedings.mlr.press/v32/rooshenas14.pdf,ICML
2447,2014,Lower Bounds for the Gibbs Sampler over Mixtures of Gaussians,"Christopher Tosh,         Sanjoy Dasgupta","The mixing time of a Markov chain is the minimum time t necessary for the total variation distance between the distribution of the Markov chain’s current state X_t and its stationary distribution to fall below some ε> 0. In this paper, we present lower bounds for the mixing time of the Gibbs sampler over Gaussian mixture models with Dirichlet priors.",http://proceedings.mlr.press/v32/tosh14.html,http://proceedings.mlr.press/v32/tosh14.pdf,ICML
2448,2014,Deterministic Policy Gradient Algorithms,"David Silver,         Guy Lever,         Nicolas Heess,         Thomas Degris,         Daan Wierstra,         Martin Riedmiller","In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.",http://proceedings.mlr.press/v32/silver14.html,http://proceedings.mlr.press/v32/silver14.pdf,ICML
2449,2014,"Estimating Diffusion Network Structures: Recovery Conditions, Sample Complexity & Soft-thresholding Algorithm","Hadi Daneshmand,         Manuel Gomez-Rodriguez,         Le Song,         Bernhard Schoelkopf","Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable guarantees?    Despite the increasing availability of cascade data and methods for inferring networks from these data, a thorough theoretical understanding of the above questions remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous-time diffusion models using an l1-regularized likelihood maximization framework. We show that, as long as the cascade sampling process satisfies a natural incoherence condition, our framework can recover the correct network structure with high probability if we observe O(d^3 log N) cascades, where d is the maximum number of parents of a node and N is the total number of nodes. Moreover, we develop a simple and efficient soft-thresholding inference algorithm, which we use to illustrate the consequences of our theoretical results, and show that our framework outperforms other alternatives in practice.",http://proceedings.mlr.press/v32/daneshmand14.html,http://proceedings.mlr.press/v32/daneshmand14.pdf,ICML
2450,2014,A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data,"Arun Rajkumar,         Shivani Agarwal","There has been much interest recently in the problem of rank aggregation from pairwise data. A natural question that arises is: under what sorts of statistical assumptions do various rank aggregation algorithms converge to an ‘optimal’ ranking? In this paper, we consider this question in a natural setting where pairwise comparisons are drawn randomly and independently from some underlying probability distribution. We first show that, under a ‘time-reversibility’ or Bradley-Terry-Luce (BTL) condition on the distribution generating the outcomes of the pairwise comparisons, the rank centrality (PageRank) and least squares (HodgeRank) algorithms both converge to an optimal ranking. Next, we show that a matrix version of the Borda count algorithm, and more surprisingly, an algorithm which performs maximal likelihood estimation under a BTL assumption, both converge to an optimal ranking under a ‘low-noise’ condition that is strictly more general than BTL. Finally, we propose a new SVM-based algorithm for rank aggregation from pairwise data, and show that this converges to an optimal ranking under an even more general condition that we term ‘generalized low-noise’. In all cases, we provide explicit sample complexity bounds for exact recovery of an optimal ranking. Our experiments confirm our theoretical findings and help to shed light on the statistical behavior of various rank aggregation algorithms.",http://proceedings.mlr.press/v32/rajkumar14.html,http://proceedings.mlr.press/v32/rajkumar14.pdf,ICML
2451,2014,Gaussian Process Optimization with Mutual Information,"Emile Contal,         Vianney Perchet,         Nicolas Vayatis","In this paper, we analyze a generic algorithm scheme for sequential global optimization using Gaussian processes. The upper bounds we derive on the cumulative regret for this generic algorithm improve by an exponential factor the previously known bounds for algorithms like GP-UCB. We also introduce the novel Gaussian Process Mutual Information algorithm (GP-MI), which significantly improves further these upper bounds for the cumulative regret. We confirm the efficiency of this algorithm on synthetic and real tasks against the natural competitor, GP-UCB, and also the Expected Improvement heuristic.",http://proceedings.mlr.press/v32/contal14.html,http://proceedings.mlr.press/v32/contal14.pdf,ICML
2452,2014,Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits,"Alekh Agarwal,         Daniel Hsu,         Satyen Kale,         John Langford,         Lihong Li,         Robert Schapire","We present a new algorithm for the contextual bandit learning problem,  where the learner repeatedly takes one of K \emphactions in response to the  observed \emphcontext, and observes the \emphreward only for that  action. Our method assumes access to an oracle for solving fully  supervised cost-sensitive classification problems and achieves the  statistically optimal regret guarantee with only \otil(\sqrtKT)  oracle calls across all T rounds. By doing so, we obtain the most  practical contextual bandit learning algorithm amongst approaches that  work for general policy classes.  We conduct a  proof-of-concept experiment which demonstrates the excellent  computational and statistical performance of (an online variant of) our  algorithm relative to several strong baselines.",http://proceedings.mlr.press/v32/agarwalb14.html,http://proceedings.mlr.press/v32/agarwalb14.pdf,ICML
2453,2014,Large-Margin Metric Learning for Constrained Partitioning Problems,"Rémi Lajugie,         Francis Bach,         Sylvain Arlot","We consider unsupervised partitioning problems based explicitly or implicitly on the minimization of Euclidean distortions, such as clustering, image or video segmentation, and other change-point detection problems. We emphasize on cases with specific structure, which include many practical situations ranging from mean-based change-point detection to image segmentation problems. We aim at learning a Mahalanobis metric for these unsupervised problems, leading to feature weighting and/or selection. This is done in a supervised way by assuming the availability of several (partially) labeled datasets that share the same metric. We cast the metric learning problem as a large-margin structured prediction problem, with proper definition of regularizers and losses, leading to a convex optimization problem which can be solved efficiently. Our experiments show how learning the metric can significantly improve performance on bioinformatics, video  or image segmentation problems.",http://proceedings.mlr.press/v32/lajugie14.html,http://proceedings.mlr.press/v32/lajugie14.pdf,ICML
2454,2014,Probabilistic Partial Canonical Correlation Analysis,"Yusuke Mukuta,          Harada","Partial canonical correlation analysis (partial CCA) is a statistical method that estimates a pair of linear projections onto a low dimensional space, where the correlation between two multidimensional variables is maximized after eliminating the influence of a third variable. Partial CCA is known to be closely related to a causality measure between two time series. However, partial CCA requires the inverses of covariance matrices, so the calculation is not stable. This is particularly the case for high-dimensional data or small sample sizes. Additionally, we cannot estimate the optimal dimension of the subspace in the model. In this paper, we have addressed these problems by proposing a probabilistic interpretation of partial CCA and deriving a Bayesian estimation method based on the probabilistic model. Our numerical experiments demonstrated that our methods can stably estimate the model parameters, even in high dimensions or when there are a small number of samples.",http://proceedings.mlr.press/v32/mukuta14.html,http://proceedings.mlr.press/v32/mukuta14.pdf,ICML
2455,2014,A Consistent Histogram Estimator for Exchangeable Graph Models,"Stanley Chan,         Edoardo Airoldi","Exchangeable graph models (ExGM) subsume a number of popular network models. The mathematical object that characterizes an ExGM is  termed a graphon. Finding scalable estimators of graphons, provably consistent, remains an open issue. In this paper, we propose a histogram estimator of a graphon that is provably consistent and numerically efficient. The proposed estimator is based on a sorting-and-smoothing (SAS) algorithm, which first sorts the empirical degree of a graph, then smooths the sorted graph using total variation minimization. The consistency of the SAS algorithm is proved by leveraging  sparsity concepts from compressed sensing.",http://proceedings.mlr.press/v32/chan14.html,http://proceedings.mlr.press/v32/chan14.pdf,ICML
2456,2014,Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques,"Jérémie Mary,         Philippe Preux,         Olivier Nicol","In many recommendation applications such as news recommendation, the  items that can be recommended come and go at a very fast pace.  This  is a challenge for recommender systems (RS) to face this setting.  Online learning algorithms seem to be the most straight forward  solution. The contextual bandit framework was introduced for that very  purpose. In general the evaluation of a RS is a critical issue. Live  evaluation is often avoided due to the potential loss of revenue,  hence the need for offline evaluation methods. Two options are  available. Model based methods are biased by nature and are thus  difficult to trust when used alone. Data driven methods are therefore  what we consider here. Evaluating online learning algorithms with past  data is not simple but some methods exist in the  literature. Nonetheless their accuracy is not satisfactory mainly due  to their mechanism of data rejection that only allow the exploitation  of a small fraction of the data. We precisely address this issue in  this paper. After highlighting the limitations of the previous  methods, we present a new method, based on bootstrapping  techniques. This new method comes with two important improvements: it  is much more accurate and it provides a measure of quality of its  estimation. The latter is a highly desirable property in order to  minimize the risks entailed by putting online a RS for the first  time. We provide both theoretical and experimental proofs of its  superiority compared to state-of-the-art methods, as well as an  analysis of the convergence of the measure of quality.",http://proceedings.mlr.press/v32/mary14.html,http://proceedings.mlr.press/v32/mary14.pdf,ICML
2457,2014,Robust Learning under Uncertain Test Distributions: Relating Covariate Shift to Model Misspecification,"Junfeng Wen,         Chun-Nam Yu,         Russell Greiner","Many learning situations involve learning the conditional distribution p(y|x) when the training instances are drawn from the training distribution p_tr(x), even though it will later be used to predict for instances drawn from a different test distribution p_te(x).   Most current approaches focus on learning how to reweigh the training examples, to make them resemble the test distribution.   However, reweighing does not always help, because (we show that) the test error also depends on the correctness of the underlying model class.   This paper analyses this situation by viewing the problem of learning under changing distributions as a game between a learner and an adversary.   We characterize when such reweighing is needed, and also provide an algorithm, robust covariate shift adjustment (RCSA), that provides relevant weights.   Our empirical studies, on UCI datasets and a real-world cancer prognostic prediction dataset, show that our analysis applies, and that our RCSA works effectively.",http://proceedings.mlr.press/v32/wen14.html,http://proceedings.mlr.press/v32/wen14.pdf,ICML
2458,2014,One Practical Algorithm for Both Stochastic and Adversarial Bandits,"Yevgeny Seldin,         Aleksandrs Slivkins","We present an algorithm for multiarmed bandits that achieves almost optimal performance in both stochastic and adversarial regimes without prior knowledge about the nature of the environment. Our algorithm is based on augmentation of the EXP3 algorithm with a new control lever in the form of exploration parameters that are tailored individually for each arm. The algorithm simultaneously applies the “old” control lever, the learning rate, to control the regret in the adversarial regime and the new control lever to detect and exploit gaps between the arm losses. This secures problem-dependent “logarithmic” regret when gaps are present without compromising on the worst-case performance guarantee in the adversarial regime. We show that the algorithm can exploit both the usual expected gaps between the arm losses in the stochastic regime and deterministic gaps between the arm losses in the adversarial regime. The algorithm retains “logarithmic” regret guarantee in the stochastic regime even when some observations are contaminated by an adversary, as long as on average the contamination does not reduce the gap by more than a half. Our results for the stochastic regime are supported by experimental validation.",http://proceedings.mlr.press/v32/seldinb14.html,http://proceedings.mlr.press/v32/seldinb14.pdf,ICML
2459,2014,Learning Ordered Representations with Nested Dropout,"Oren Rippel,         Michael Gelbart,         Ryan Adams","In this paper, we present results on ordered representations of data in which different dimensions have different degrees of importance. To learn these representations we introduce nested dropout, a procedure for stochastically removing coherent nested sets of hidden units in a neural network. We first present a sequence of theoretical results in the simple case of a semi-linear autoencoder.  We rigorously show that the application of nested dropout enforces identifiability of the units, which leads to an exact equivalence with PCA.  We then extend the algorithm to deep models and demonstrate the relevance of ordered representations to a number of applications.  Specifically, we use the ordered property of the learned codes to construct hash-based data structures that permit very fast retrieval, achieving retrieval in time logarithmic in the database size and independent of the dimensionality of the representation. This allows the use of codes that are hundreds of times longer than currently feasible for retrieval.  We therefore avoid the diminished quality associated with short codes, while still performing retrieval that is competitive in speed with existing methods.  We also show that ordered representations are a promising way to learn adaptive compression for efficient online data reconstruction.",http://proceedings.mlr.press/v32/rippel14.html,http://proceedings.mlr.press/v32/rippel14.pdf,ICML
2460,2014,Beta Diffusion Trees,"Creighton Heaukulani,         David Knowles,         Zoubin Ghahramani","We define the beta diffusion tree, a random tree structure with a set of leaves that defines a collection of overlapping subsets of objects, known as a feature allocation. The generative process for the tree is defined in terms of particles (representing the objects) diffusing in some continuous space, analogously to the Dirichlet and Pitman-Yor diffusion trees (Neal, 2003b; Knowles & Ghahramani, 2011), both of which define tree structures over clusters of the particles. With the beta diffusion tree, however, multiple copies of a particle may exist and diffuse to multiple locations in the continuous space, resulting in (a random number of) possibly overlapping clusters of the objects. We demonstrate how to build a hierarchically-clustered factor analysis model with the beta diffusion tree and how to perform inference over the random tree structures with a Markov chain Monte Carlo algorithm. We conclude with several numerical experiments on missing data problems with data sets of gene expression arrays, international development statistics, and intranational socioeconomic measurements.",http://proceedings.mlr.press/v32/heaukulani14.html,http://proceedings.mlr.press/v32/heaukulani14.pdf,ICML
2461,2014,Compositional Morphology for Word Representations and Language Modelling,"Jan Botha,         Phil Blunsom","This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models.",http://proceedings.mlr.press/v32/botha14.html,http://proceedings.mlr.press/v32/botha14.pdf,ICML
2462,2014,Prediction with Limited Advice and Multiarmed Bandits with Paid Observations,"Yevgeny Seldin,         Peter Bartlett,         Koby Crammer,         Yasin Abbasi-Yadkori","We study two problems of online learning under restricted information access. In the first problem, \emphprediction with limited advice, we consider a game of prediction with expert advice, where on each round of the game we query the advice of a subset of M out of N experts. We present an algorithm that achieves O(\sqrt(N/M)T\ln N) regret on T rounds of this game. The second problem, the \emphmultiarmed bandit with paid  observations, is a variant of the adversarial N-armed bandit game, where on round t of the game we can observe the reward of any number of arms, but each observation has a cost c. We present an algorithm that achieves O((cN\ln N)^1/3 T^2/3 + \sqrtT \ln N) regret on T rounds of this game in the worst case. Furthermore, we present a number of refinements that treat arm- and time-dependent observation costs and achieve lower regret under benign conditions. We present lower bounds that show that, apart from the logarithmic factors, the worst-case regret bounds cannot be improved.",http://proceedings.mlr.press/v32/seldin14.html,http://proceedings.mlr.press/v32/seldin14.pdf,ICML
2463,2014,Exchangeable Variable Models,"Mathias Niepert,         Pedro Domingos","A sequence of random variables is exchangeable if its joint distribution is invariant under variable permutations. We introduce exchangeable variable models (EVMs) as a novel class of probabilistic models whose basic building blocks are partially exchangeable sequences, a generalization of exchangeable sequences. We prove that a family of tractable EVMs is optimal under zero-one loss for a large class of functions, including parity and threshold functions, and strictly subsumes existing tractable independence-based model families. Extensive experiments show that EVMs outperform state of the art classifiers such as SVMs and  probabilistic models which are solely based on independence assumptions.",http://proceedings.mlr.press/v32/niepert14.html,http://proceedings.mlr.press/v32/niepert14.pdf,ICML
2464,2014,Robust RegBayes: Selectively Incorporating First-Order Logic Domain Knowledge into Bayesian Models,"Shike Mei,         Jun Zhu,         Jerry Zhu","Much research in Bayesian modeling has been done to elicit a prior distribution that incorporates domain knowledge. We present a novel and more direct approach by imposing First-Order Logic (FOL) rules on the posterior distribution. Our approach unifies FOL and Bayesian modeling under the regularized Bayesian framework. In addition, our approach automatically estimates the uncertainty of FOL rules when they are produced by humans, so that reliable rules are incorporated while unreliable ones are ignored. We apply our approach to latent topic modeling tasks and demonstrate that by combining FOL knowledge and Bayesian modeling, we both improve the task performance and discover more structured latent representations in unsupervised and supervised learning.",http://proceedings.mlr.press/v32/mei14.html,http://proceedings.mlr.press/v32/mei14.pdf,ICML
2465,2014,Anomaly Ranking as Supervised Bipartite Ranking,"Stephan Clémençon,         Sylvain Robbiano","The Mass Volume (MV) curve is a visual  tool to evaluate the performance of a scoring  function with regard to its capacity to rank  data in the same order as the underlying density function. Anomaly ranking refers to the  unsupervised learning task which consists in  building a scoring function, based on unlabeled data, with a MV curve as low as possible at any point. In this paper, it is proved  that, in the case where the data generating probability distribution has compact support, anomaly ranking is equivalent to (supervised) bipartite ranking, where the goal is  to discriminate between the underlying probability distribution and the uniform distribution with same support. In this situation, the  MV curve can be then seen as a simple transform of the corresponding ROC curve. Exploiting this view, we then show how to use  bipartite ranking algorithms, possibly combined with random sampling, to solve the  MV curve minimization problem. Numerical experiments based on a variety of bipartite ranking algorithms well-documented in  the literature are displayed in order to illustrate the relevance of our approach.",http://proceedings.mlr.press/v32/clemencon14.html,http://proceedings.mlr.press/v32/clemencon14.pdf,ICML
2466,2014,Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem,"Masrour Zoghi,         Shimon Whiteson,         Remi Munos,         Maarten Rijke","This paper proposes a new method for the K-armed dueling bandit problem, a variation on the regular K-armed bandit problem that offers only relative feedback about pairs of arms. Our approach extends the Upper Confidence Bound algorithm to the relative setting by using estimates of the pairwise probabilities to select a promising arm and applying Upper Confidence Bound with the winner as a benchmark. We prove a sharp finite-time regret bound of order O(K log t) on a very general class of dueling bandit problems that matches a lower bound proven in (Yue et al., 2012). In addition, our empirical results using real data from an information retrieval application show that it greatly outperforms the state of the art.",http://proceedings.mlr.press/v32/zoghi14.html,http://proceedings.mlr.press/v32/zoghi14.pdf,ICML
2467,2014,A Kernel Independence Test for Random Processes,"Kacper Chwialkowski,         Arthur Gretton","A non-parametric approach to the problem of testing the independence of two random processes is developed.  The test statistic is the Hilbert-Schmidt Independence Criterion (HSIC), which was used previously in testing independence for i.i.d. pairs of variables. The asymptotic behaviour of HSIC is established when computed from samples drawn from random processes. It is shown that earlier bootstrap procedures which worked in the i.i.d. case will fail for random processes, and an alternative consistent estimate of the p-values is proposed. Tests on artificial data and real-world forex data indicate that the new test procedure discovers dependence which is missed by linear approaches, while the earlier bootstrap procedure returns an elevated number of false positives.",http://proceedings.mlr.press/v32/chwialkowski14.html,http://proceedings.mlr.press/v32/chwialkowski14.pdf,ICML
2468,2014,Learning Complex Neural Network Policies with Trajectory Optimization,"Sergey Levine,         Vladlen Koltun","Direct policy search methods offer the promise of automatically learning controllers for complex, high-dimensional tasks. However, prior applications of policy search often required specialized, low-dimensional policy classes, limiting their generality. In this work, we introduce a policy search algorithm that can directly learn high-dimensional, general-purpose policies, represented by neural networks. We formulate the policy search problem as an optimization over trajectory distributions, alternating between optimizing the policy to match the trajectories, and optimizing the trajectories to match the policy and minimize expected cost. Our method can learn policies for complex tasks such as bipedal push recovery and walking on uneven terrain, while outperforming prior methods.",http://proceedings.mlr.press/v32/levine14.html,http://proceedings.mlr.press/v32/levine14.pdf,ICML
2469,2014,K-means recovers ICA filters when independent components are sparse,"Alon Vinnikov,         Shai Shalev-Shwartz","Unsupervised feature learning is the task of using unlabeled examples  for building a representation of objects as vectors. This task has  been extensively studied in recent years, mainly in the context of  unsupervised pre-training of neural networks. Recently, (Coates et al., 2011)  conducted extensive experiments, comparing the accuracy of a linear  classifier that has been trained using features learnt by several  unsupervised feature learning methods.  Surprisingly, the best  performing method was the simplest feature learning approach that was  based on applying the K-means clustering algorithm after a whitening  of the data. The goal of this work is to shed light on the success of  K-means with whitening for the task of unsupervised feature learning.  Our main result is a close connection between K-means and ICA  (Independent Component Analysis).  Specifically, we show that K-means  and similar clustering algorithms can be used to recover the ICA  mixing matrix or its inverse, the ICA filters. It is well known that  the independent components found by ICA form useful features for  classification (Le et al., 2012; 2011; 2010), hence the connection between K-mean and ICA explains  the empirical success of K-means as a feature learner. Moreover, our  analysis underscores the significance of the whitening operation, as was also  observed in the experiments reported in (Coates et al., 2011).  Finally, our  analysis leads to a better initialization of K-means for the task of feature learning.",http://proceedings.mlr.press/v32/vinnikov14.html,http://proceedings.mlr.press/v32/vinnikov14.pdf,ICML
2470,2014,Sample-based approximate regularization,"Philip Bachman,         Amir-Massoud Farahmand,         Doina Precup","We introduce a method for regularizing linearly parameterized functions using general derivative-based penalties, which relies on sampling as well as finite-difference approximations of the relevant derivatives. We call this approach sample-based approximate regularization (SAR). We provide theoretical guarantees on the fidelity of such regularizers, compared to those they approximate, and prove that the approximations converge efficiently. We also examine the empirical performance of SAR on several datasets.",http://proceedings.mlr.press/v32/bachman14.html,http://proceedings.mlr.press/v32/bachman14.pdf,ICML
2471,2014,Provable Bounds for Learning Some Deep Representations,"Sanjeev Arora,         Aditya Bhaskara,         Rong Ge,         Tengyu Ma","We give  algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an n node multilayer neural net that has degree at most n^γ for some γ< 1 and each edge has a random edge weight in [-1,1]. Our algorithm learns  almost all networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model.  The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis  of the algorithm reveals interesting structure of  neural nets with random edge weights.",http://proceedings.mlr.press/v32/arora14.html,http://proceedings.mlr.press/v32/arora14.pdf,ICML
2472,2014,Affinity Weighted Embedding,"Jason Weston,         Ron Weiss,         Hector Yee","Supervised linear embedding models like Wsabie (Weston et al., 2011) and supervised semantic indexing (Bai et al., 2010) have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and we believe they typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our approach works by reweighting each component of the embedding of features and labels with a potentially nonlinear affinity function. We describe several variants of the family, and show  its usefulness on several datasets.",http://proceedings.mlr.press/v32/weston14.html,http://proceedings.mlr.press/v32/weston14.pdf,ICML
2473,2014,Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function,"Yong Liu,         Shali Jiang,         Shizhong Liao","Model selection is one of the key issues both in recent research and application of kernel methods. Cross-validation is a commonly employed and widely accepted model selection criterion. However, it requires multiple times of training the algorithm under consideration, which is computationally intensive. In this paper, we present a novel strategy for approximating the cross-validation based on the Bouligand influence function (BIF), which only requires the solution of the algorithm once. The BIF measures the impact of an infinitesimal small amount of contamination of the original distribution. We first establish the link between the concept of BIF and the concept of cross-validation. The BIF is related to the first order term of a Taylor expansion. Then, we calculate the BIF and higher order BIFs, and apply these theoretical results to approximate the cross-validation error in practice. Experimental results demonstrate that our approximate cross-validation criterion is sound and efficient.",http://proceedings.mlr.press/v32/liua14.html,http://proceedings.mlr.press/v32/liua14.pdf,ICML
2474,2014,A Statistical Perspective on Algorithmic Leveraging,"Ping Ma,         Michael Mahoney,         Bin Yu","One popular method for dealing with large-scale data sets is sampling. Using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. Existing work has focused on algorithmic issues, but none of it addresses statistical aspects of this method.  Here, we provide an effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model.   In particular, for several versions of leverage-based sampling, we derive results for the bias and variance, both conditional and unconditional on the observed data. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms: one constructs a smaller least-squares problem with “shrinked” leverage scores (SLEV), and the other solves a smaller and unweighted (or biased) least-squares problem (LEVUNW). The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance.",http://proceedings.mlr.press/v32/ma14.html,http://proceedings.mlr.press/v32/ma14.pdf,ICML
2475,2014,Approximate Policy Iteration Schemes: A Comparison,Bruno Scherrer,"We consider the infinite-horizon discounted optimal control problem  formalized by Markov Decision Processes. We focus on several  approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration, Conservative Policy Iteration  (CPI), a natural adaptation of the Policy Search by  Dynamic Programming algorithm to the  infinite-horizon case (PSDP_∞), and the recently proposed  Non-Stationary Policy iteration (NSPI(m)). For all  algorithms, we describe performance bounds, and  make a comparison by paying a particular attention to the  concentrability constants involved, the number of iterations and the  memory required. Our analysis highlights the following points: 1) The  performance guarantee of CPI can be arbitrarily better than that of  API/API(α), but this comes at the cost of a  relative—exponential in \frac1ε—increase of the  number of iterations. 2) PSDP_∞enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that  requires a constant memory, the memory needed by CPI and PSDP_∞is  proportional to their number of iterations, which may be problematic  when the discount factor γis close to 1 or the  approximation error εis close to 0; we show that  the NSPI(m) algorithm allows to make an overall trade-off between  memory and performance. Simulations with these schemes confirm our  analysis.",http://proceedings.mlr.press/v32/scherrer14.html,http://proceedings.mlr.press/v32/scherrer14.pdf,ICML
2476,2014,Nuclear Norm Minimization via Active Subspace Selection,"Cho-Jui Hsieh,         Peder Olsen","We describe a novel approach to optimizing matrix problems involving nuclear norm regularization and apply it to the matrix completion problem. We combine methods from non-smooth and smooth optimization. At each step we use the proximal gradient to select an active subspace. We then find a smooth, convex relaxation of the smaller subspace problems and solve these using second order methods. We apply our methods to matrix completion problems including Netflix dataset, and show that they are more than 6 times faster than state-of-the-art nuclear norm solvers. Also, this is the first paper to scale nuclear norm solvers to the Yahoo-Music dataset, and the first time in the literature that the efficiency of nuclear norm solvers can be compared and even compete with non-convex solvers like Alternating Least Squares (ALS).",http://proceedings.mlr.press/v32/hsiehb14.html,http://proceedings.mlr.press/v32/hsiehb14.pdf,ICML
2477,2014,On Measure Concentration of Random Maximum A-Posteriori Perturbations,"Francesco Orabona,         Tamir Hazan,         Anand Sarwate,         Tommi Jaakkola","The maximum a-posteriori (MAP) perturbation framework has emerged as a useful approach for inference and learning in high dimensional complex models.  By maximizing a randomly perturbed potential function, MAP perturbations generate unbiased samples from the Gibbs distribution.  Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive.  More efficient algorithms use sequential sampling strategies based on the expected value of low dimensional MAP perturbations. This paper develops new measure concentration inequalities that bound the number of samples needed to estimate such expected values. Applying the general result to MAP perturbations can yield a more efficient algorithm to approximate sampling from the Gibbs distribution.  The measure concentration result is of general interest and may be applicable to other areas involving Monte Carlo estimation of expectations.",http://proceedings.mlr.press/v32/orabona14.html,http://proceedings.mlr.press/v32/orabona14.pdf,ICML
2478,2014,Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets,"Diederik Kingma,         Max Welling","Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the non-centered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",http://proceedings.mlr.press/v32/kingma14.html,http://proceedings.mlr.press/v32/kingma14.pdf,ICML
2479,2014,Finding Dense Subgraphs via Low-Rank Bilinear Optimization,"Dimitris Papailiopoulos,         Ioannis Mitliagkas,         Alexandros Dimakis,         Constantine Caramanis","Given a graph, the Densest k-Subgraph (\DkS) problem asks for the subgraph on k vertices that contains the largest number of edges. In this work, we develop a novel algorithm for \DkS that searches a low-dimensional space for provably good solutions.  We obtain provable performance bounds that depend on the graph spectrum.  One of our results is that if there exists a k-subgraph that contains a constant fraction of all the edges, we can approximate \DkS within a factor arbitrarily close to two in polynomial time.     Our algorithm runs in nearly linear time, under spectral assumptions satisfied by   most graphs found in applications. Moreover, it is highly scalable and parallelizable.  We demonstrate this by implementing it in MapReduce and executing numerous experiments on  massive real-world graphs that have up to billions of edges.  We empirically show that our algorithm can find subgraphs of significantly higher density compared to the previous state of the art.",http://proceedings.mlr.press/v32/papailiopoulos14.html,http://proceedings.mlr.press/v32/papailiopoulos14.pdf,ICML
2480,2014,Aggregating  Ordinal Labels from Crowds by Minimax Conditional Entropy,"Dengyong Zhou,         Qiang Liu,         John Platt,         Christopher Meek",We propose a method to aggregate noisy ordinal labels collected from a crowd of workers or annotators.  Eliciting ordinal labels is important in tasks such as judging web search quality and consumer satisfaction. Our method is  motivated by the observation that workers usually have difficulty distinguishing between two adjacent ordinal classes whereas distinguishing between two classes which are far away from each other is much easier. We develop the method  through  minimax conditional entropy subject to constraints which encode this observation. Empirical  evaluations on real datasets demonstrate significant improvements over existing methods.,http://proceedings.mlr.press/v32/zhouc14.html,http://proceedings.mlr.press/v32/zhouc14.pdf,ICML
2481,2014,An Asynchronous Parallel Stochastic Coordinate Descent Algorithm,"Ji Liu,         Steve Wright,         Christopher Re,         Victor Bittorf,         Srikrishna Sridhar","We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate (1/K) on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is O(n^1/2) in unconstrained optimization and O(n^1/4) in the separable-constrained case, where n is the number of variables. We  describe results from implementation on 40-core processors.",http://proceedings.mlr.press/v32/liud14.html,http://proceedings.mlr.press/v32/liud14.pdf,ICML
2482,2014,Nearest Neighbors Using Compact Sparse Codes,Anoop Cherian,"In this paper, we propose a novel scheme for approximate nearest neighbor (ANN) retrieval based on dictionary learning and sparse coding. Our key innovation is to build compact codes, dubbed SpANN codes, using the active set of sparse coded data. These codes are then used to index an inverted file table for fast retrieval. The active sets are often found to be sensitive to small differences among data points, resulting in only near duplicate retrieval. We show that this sensitivity is related to the coherence of the dictionary; small coherence resulting in better retrieval. To this end, we propose a novel dictionary learning formulation with incoherence constraints and an efficient method to solve it. Experiments are conducted on two state-of-the-art computer vision datasets with 1M data points and show an order of magnitude improvement in retrieval accuracy without sacrificing memory and query time compared to the state-of-the-art methods.",http://proceedings.mlr.press/v32/cherian14.html,http://proceedings.mlr.press/v32/cherian14.pdf,ICML
2483,2014,Cold-start Active Learning with Robust Ordinal Matrix Factorization,"Neil Houlsby,         Jose Miguel Hernandez-Lobato,         Zoubin Ghahramani","We present a new matrix factorization model for rating data and a corresponding active learning strategy to address the cold-start problem. Cold-start is one of the most challenging tasks for recommender systems: what to recommend with new users or items for which one has little or no data. An approach is to use active learning to collect the most useful initial ratings. However, the performance of active learning depends strongly upon having accurate estimates of i) the uncertainty in model parameters and ii) the intrinsic noisiness of the data. To achieve these estimates we propose a heteroskedastic Bayesian model for ordinal matrix factorization. We also present a computationally efficient framework for Bayesian active learning with this type of complex probabilistic model. This algorithm successfully distinguishes between informative and noisy data points. Our model yields state-of-the-art predictive performance and, coupled with our active learning strategy, enables us to gain useful information in the cold-start setting from the very first active sample.",http://proceedings.mlr.press/v32/houlsby14.html,http://proceedings.mlr.press/v32/houlsby14.pdf,ICML
2484,2014,The f-Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation,"Sven Kurras,         Ulrike Luxburg,         Gilles Blanchard","Consider a neighborhood graph, for example a k-nearest neighbor graph, that is constructed on sample points drawn according to some density p. Our goal is to re-weight the graph’s edges such that all cuts and volumes behave as if the graph was built on a different sample drawn from an alternative density q. We introduce the f-adjusted graph and prove that it provides the correct cuts and volumes as the sample size tends to infinity. From an algebraic perspective, we show that its normalized Laplacian, denoted as the f-adjusted Laplacian, represents a natural family of diagonal perturbations of the original normalized Laplacian. Our technique allows to apply any cut and volume based algorithm to the f-adjusted graph, for example spectral clustering, in order to study the given graph as if it were built on an unaccessible sample from a different density. We point out applications in sample bias correction, data uniformization, and multi-scale analysis of graphs.",http://proceedings.mlr.press/v32/kurras14.html,http://proceedings.mlr.press/v32/kurras14.pdf,ICML
2485,2014,A Unified Framework for Consistency of Regularized Loss Minimizers,"Jean Honorio,         Tommi Jaakkola","We characterize a family of regularized loss minimization problems that satisfy three properties: scaled uniform convergence, super-norm regularization, and norm-loss monotonicity. We show several theoretical guarantees within this framework, including loss consistency, norm consistency, sparsistency (i.e. support recovery) as well as sign consistency. A number of regularization problems can be shown to fall within our framework and we provide several examples. Our results can be seen as a concise summary of existing guarantees but we also extend them to new settings. Our formulation enables us to assume very little about the hypothesis class, data distribution, the loss, or the regularization. In particular, many of our results do not require a bounded hypothesis class, or identically distributed samples. Similarly, we do not assume boundedness, convexity or smoothness of the loss nor the regularizer. We only assume approximate optimality of the empirical minimizer. In terms of recovery, in contrast to existing results, our sparsistency and sign consistency results do not require knowledge of the sub-differential of the objective function.",http://proceedings.mlr.press/v32/honorio14.html,http://proceedings.mlr.press/v32/honorio14.pdf,ICML
2486,2014,Multiresolution Matrix Factorization,"Risi Kondor,         Nedelina Teneva,         Vikas Garg","The types of large matrices that appear in modern Machine Learning problems often have complex hierarchical structures that go beyond what can be found by traditional linear algebra tools, such as eigendecompositions. Inspired by ideas from multiresolution analysis,   this paper introduces a new notion of matrix factorization that can capture structure in matrices at multiple different scales. The resulting Multiresolution Matrix Factorizations (MMFs) not only provide a wavelet basis for sparse approximation, but can also be used for matrix compression (similar to Nystrom approximations) and as a prior for matrix completion.",http://proceedings.mlr.press/v32/kondor14.html,http://proceedings.mlr.press/v32/kondor14.pdf,ICML
2487,2014,Efficient Dimensionality Reduction for High-Dimensional Network Estimation,"Safiye Celik,         Benjamin Logsdon,         Su-In Lee","We propose module graphical lasso (MGL), an aggressive dimensionality reduction and network estimation technique for a high-dimensional Gaussian graphical model (GGM). MGL achieves scalability, interpretability and robustness by exploiting the modularity property of many real-world networks. Variables are organized into tightly coupled modules and a graph structure is estimated to determine the conditional independencies among modules. MGL iteratively learns the module assignment of variables, the latent variables, each corresponding to a module, and the parameters of the GGM of the latent variables. In synthetic data experiments, MGL outperforms the standard graphical lasso and three other methods that incorporate latent variables into GGMs. When applied to gene expression data from ovarian cancer, MGL outperforms standard clustering algorithms in identifying functionally coherent gene sets and predicting survival time of patients. The learned modules and their dependencies provide novel insights into cancer biology as well as identifying possible novel drug targets.",http://proceedings.mlr.press/v32/celik14.html,http://proceedings.mlr.press/v32/celik14.pdf,ICML
2488,2014,Weighted Graph Clustering with Non-Uniform Uncertainties,"Yudong Chen,         Shiau Hong Lim,         Huan Xu","We study the graph clustering problem where each observation (edge or no-edge between a pair of nodes) may have a different level of confidence/uncertainty. We propose a clustering algorithm that is based on optimizing an appropriate weighted objective, where larger weights are given to observations with lower uncertainty. Our approach leads to a convex optimization problem that is efficiently solvable. We analyze our approach under a natural generative model, and establish theoretical guarantees for recovering the underlying clusters. Our main result is a general theorem that applies to any given weight and distribution for the uncertainty. By optimizing over the weights, we derive a provably optimal weighting scheme, which matches the information theoretic lower bound up to logarithmic factors and leads to strong performance bounds in several specific settings. By optimizing over the uncertainty distribution, we show that non-uniform uncertainties can actually help. In particular, if the graph is built by spending a limited amount of resource to take measurement on each node pair, then it is beneficial to allocate the resource in a non-uniform fashion to obtain accurate measurements on a few pairs of nodes, rather than obtaining inaccurate measurements on many pairs. We provide simulation results that validate our theoretical findings.",http://proceedings.mlr.press/v32/chenh14.html,http://proceedings.mlr.press/v32/chenh14.pdf,ICML
2489,2014,Bias in Natural Actor-Critic Algorithms,Philip Thomas,"We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(lambda) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics is concave, so policy gradient methods are guaranteed to converge to globally optimal policies as well.",http://proceedings.mlr.press/v32/thomas14.html,http://proceedings.mlr.press/v32/thomas14.pdf,ICML
2490,2014,Concept Drift Detection Through Resampling,"Maayan Harel,         Shie Mannor,         Ran El-Yaniv,         Koby Crammer","Detecting changes in data-streams is an important part of enhancing learning quality in dynamic environments. We devise a procedure for detecting concept drifts in data-streams that relies on analyzing the empirical loss of learning algorithms. Our method is based on obtaining statistics from the loss distribution by reusing the data multiple times via resampling. We present theoretical guarantees for the proposed procedure based on the stability of the underlying learning algorithms. Experimental results show that the detection method has high recall and precision, and performs well in the presence of noise.",http://proceedings.mlr.press/v32/harel14.html,http://proceedings.mlr.press/v32/harel14.pdf,ICML
2491,2014,Geodesic Distance Function Learning via Heat Flow on Vector Fields,"Binbin Lin,         Ji Yang,         Xiaofei He,         Jieping Ye","Learning a distance function or metric on a given data manifold is of great importance in machine learning and pattern recognition. Many of the previous works first embed the manifold to Euclidean space and then learn the distance function. However, such a scheme might not faithfully preserve the distance function if the original manifold is not Euclidean. In this paper, we propose to learn the distance function directly on the manifold without embedding. We first provide a theoretical characterization of the distance function by its gradient field. Based on our theoretical analysis, we propose to first learn the gradient field of the distance function and then learn the distance function itself. Specifically, we set the gradient field of a local distance function as an initial vector field. Then we transport it to the whole manifold via heat flow on vector fields. Finally, the geodesic distance function can be obtained by requiring its gradient field to be close to the normalized vector field. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm.",http://proceedings.mlr.press/v32/linb14.html,http://proceedings.mlr.press/v32/linb14.pdf,ICML
2492,2014,An Analysis of State-Relevance Weights and Sampling Distributions on L1-Regularized Approximate Linear Programming Approximation Accuracy,"Gavin Taylor,         Connor Geer,         David Piekut","Recent interest in the use of L_1 regularization in the use of value function approximation includes Petrik et al.’s introduction of L_1-Regularized Approximate Linear Programming (RALP).  RALP is unique among L_1-regularized approaches in that it approximates the optimal value function using off-policy samples.  Additionally, it produces policies which outperform those of previous methods, such as LSPI.  RALP’s value function approximation quality is affected heavily by the choice of state-relevance weights in the objective function of the linear program, and by the distribution from which samples are drawn; however, there has been no discussion of these considerations in the previous literature.  In this paper, we discuss and explain the effects of choices in the state-relevance weights and sampling distribution on approximation quality, using both theoretical and experimental illustrations.  The results provide insight not only onto these effects, but also provide intuition into the types of MDPs which are especially well suited for approximation with RALP.",http://proceedings.mlr.press/v32/taylor14.html,http://proceedings.mlr.press/v32/taylor14.pdf,ICML
2493,2014,Fast Computation of Wasserstein Barycenters,"Marco Cuturi,         Arnaud Doucet","We present new algorithms to compute the mean of a set of N empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter \citepagueh2011barycenters,rabin2012, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of \citetcuturi2013sinkhorn, we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem.",http://proceedings.mlr.press/v32/cuturi14.html,http://proceedings.mlr.press/v32/cuturi14.pdf,ICML
2494,2014,Variational Inference for Sequential Distance Dependent Chinese Restaurant Process,"Sergey Bartunov,         Dmitry Vetrov","Recently proposed distance dependent Chinese Restaurant Process (ddCRP) generalizes extensively used Chinese Restaurant Process (CRP) by accounting for dependencies between data points. Its posterior is intractable and so far only MCMC methods were used for inference. Because of very different nature of ddCRP no prior developments in variational methods for Bayesian nonparametrics are appliable. In this paper we propose novel variational inference for important sequential case of ddCRP (seqddCRP) by revealing its connection with Laplacian of random graph constructed by the process. We develop efficient algorithm for optimizing variational lower bound and demonstrate its efficiency comparing to Gibbs sampler. We also apply our variational approximation to CRP-equivalent seqddCRP-mixture model, where it could be considered as alternative to one based on truncated stick-breaking representation. This allowed us to achieve significantly better variational lower bound than variational approximation based on truncated stick breaking for Dirichlet process.",http://proceedings.mlr.press/v32/bartunov14.html,http://proceedings.mlr.press/v32/bartunov14.pdf,ICML
2495,2014,Learning Mixtures of Linear Classifiers,"Yuekai Sun,         Stratis Ioannidis,         Andrea Montanari","We consider a discriminative learning (regression) problem, whereby the regression function is a convex combination of k linear classifiers. Existing approaches are based on the EM algorithm, or similar techniques, without provable guarantees. We develop a simple method based on spectral techniques and a ‘mirroring’ trick, that discovers the subspace spanned by the classifiers’ parameter vectors. Under a probabilistic assumption on the  feature vector distribution, we prove that this approach has nearly optimal statistical efficiency.",http://proceedings.mlr.press/v32/sunb14.html,http://proceedings.mlr.press/v32/sunb14.pdf,ICML
2496,2014,Large-scale Multi-label Learning with Missing Labels,"Hsiang-Fu Yu,         Prateek Jain,         Purushottam Kar,         Inderjit Dhillon","The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges: (a) scaling up to problems with a large number (say millions) of labels, and (b) handling data with missing labels. In this paper, we directly address both these problems by studying the multi-label problem in a generic empirical risk minimization (ERM) framework. Our framework, despite being simple, is surprisingly able to encompass several recent label-compression based methods which can be derived as special cases of our method. To optimize the ERM problem, we develop techniques that exploit the structure of specific loss functions - such as the squared loss function - to obtain efficient algorithms. We further show that our learning framework admits excess risk bounds even in the presence of missing labels. Our bounds are tight and demonstrate better generalization performance for low-rank promoting trace-norm regularization when compared to (rank insensitive) Frobenius norm regularization. Finally, we present extensive empirical results on a variety of benchmark datasets and show that our methods perform significantly better than existing label compression based methods and can scale up to very large datasets such as a Wikipedia dataset that has more than 200,000 labels.",http://proceedings.mlr.press/v32/yu14.html,http://proceedings.mlr.press/v32/yu14.pdf,ICML
2497,2014,Gaussian Approximation of Collective Graphical Models,"Liping Liu,         Daniel Sheldon,         Thomas Dietterich","The Collective Graphical Model (CGM) models a population of  independent and identically distributed individuals when only  collective statistics (i.e., counts of individuals) are   observed. Exact inference in CGMs is intractable, and previous work  has explored Markov Chain Monte Carlo (MCMC) and MAP approximations  for learning and inference. This paper studies Gaussian approximations  to the CGM. As the population grows large, we show that the CGM   distribution converges to a multivariate Gaussian distribution (GCGM)  that maintains the conditional independence properties of the original  CGM.  If the observations are exact marginals of the CGM or marginals  that are corrupted by Gaussian noise, inference in the GCGM  approximation can be computed efficiently in closed form. If the   observations follow a different noise model (e.g., Poisson), then  expectation propagation provides efficient and accurate approximate  inference. The accuracy and speed of GCGM inference is compared to the   MCMC and MAP methods on a simulated bird migration problem. The GCGM  matches or exceeds the accuracy of the MAP method while being significantly  faster.",http://proceedings.mlr.press/v32/liuf14.html,http://proceedings.mlr.press/v32/liuf14.pdf,ICML
2498,2014,Learning the Irreducible Representations of Commutative Lie Groups,"Taco Cohen,         Max Welling","We present a new probabilistic model of compact commutative Lie groups that produces invariant-equivariant and disentangled representations of data. To define the notion of disentangling, we borrow a fundamental principle from physics that is used to derive the elementary particles of a system from its symmetries. Our model employs a newfound Bayesian conjugacy relation that enables fully tractable probabilistic inference over compact commutative Lie groups – a class that includes the groups that describe the rotation and cyclic translation of images. We train the model on pairs of transformed image patches, and show that the learned invariant representation is highly effective for classification.",http://proceedings.mlr.press/v32/cohen14.html,http://proceedings.mlr.press/v32/cohen14.pdf,ICML
2499,2014,Statistical analysis of stochastic gradient methods for generalized linear models,"Panagiotis Toulis,         Edoardo Airoldi,         Jason Rennie","We study the statistical properties of stochastic gradient descent (SGD) using   explicit and implicit updates for fitting generalized linear models (GLMs).  Initially, we develop a computationally   efficient algorithm to implement implicit SGD learning of GLMs.  Next, we obtain exact formulas for the bias and variance  of both updates which leads to two important observations on their   comparative statistical properties.  First, in small samples, the estimates from the implicit procedure   are more biased than the estimates from the explicit one,   but their empirical variance is smaller and they are more robust to   learning rate misspecification.   Second, the two procedures are statistically identical in the limit:   they are both unbiased, converge at the same rate and have the   same asymptotic variance. Our set of experiments confirm our theory and   more broadly suggest that the implicit procedure can be a competitive choice   for fitting large-scale  models, especially when robustness is a concern.",http://proceedings.mlr.press/v32/toulis14.html,http://proceedings.mlr.press/v32/toulis14.pdf,ICML
2500,2014,Coupled Group Lasso for Web-Scale CTR Prediction in Display Advertising,"Ling Yan,         Wu-Jun Li,         Gui-Rong Xue,         Dingyi Han","In display advertising, click through rate(CTR) prediction is the problem of estimating the probability  that an advertisement (ad) is clicked when displayed to a user in a specific context. Due to its easy implementation and promising performance, logistic regression(LR) model has been widely used for CTR prediction, especially in industrial systems. However, it is not easy for LR to capture the nonlinear information, such as the conjunction information, from user features and ad features. In this paper, we propose a novel model, called coupled group lasso(CGL), for CTR prediction in display advertising. CGL can seamlessly integrate the conjunction information from user features and ad features for modeling. Furthermore, CGL can automatically eliminate useless features for both users and ads, which may facilitate fast online prediction. Scalability of CGL is ensured through feature hashing and distributed implementation. Experimental results on real-world data sets show that our CGL model can achieve state-of-the-art performance on web-scale CTR prediction tasks.",http://proceedings.mlr.press/v32/yan14.html,http://proceedings.mlr.press/v32/yan14.pdf,ICML
2501,2014,The Inverse Regression Topic Model,"Maxim Rabinovich,         David Blei","\citettaddy13mnir proposed multinomial inverse regression (MNIR) as a new model of annotated text based on the influence of metadata and response variables on the distribution of words in a document. While effective, MNIR has no way to exploit structure in the corpus to improve its predictions or facilitate exploratory data analysis. On the other hand, traditional probabilistic topic models (like latent Dirichlet allocation) capture natural heterogeneity in a collection but do not account for external variables. In this paper, we introduce the inverse regression topic model (IRTM), a mixed-membership extension of MNIR that combines the strengths of both methodologies. We present two inference algorithms for the IRTM: an efficient batch estimation  algorithm and an online variant, which is suitable for large corpora.  We apply these methods to a corpus of 73K Congressional press releases  and another of 150K Yelp reviews, demonstrating that the IRTM  outperforms both MNIR and supervised topic models on the prediction task.  Further, we give examples showing that the IRTM enables systematic  discovery of in-topic lexical variation, which is not possible with previous supervised topic models.",http://proceedings.mlr.press/v32/rabinovich14.html,http://proceedings.mlr.press/v32/rabinovich14.pdf,ICML
2502,2014,An Efficient Approach for Assessing Hyperparameter Importance,"Frank Hutter,         Holger Hoos,         Kevin Leyton-Brown","The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that—even in very high-dimensional cases—most performance variation is attributable to just a few hyperparameters.",http://proceedings.mlr.press/v32/hutter14.html,http://proceedings.mlr.press/v32/hutter14.pdf,ICML
2503,2014,A Compilation Target for Probabilistic Programming Languages,"Brooks Paige,         Frank Wood","Forward inference techniques such as sequential Monte Carlo and particle Markov chain Monte Carlo for probabilistic programming can be implemented in any programming language by creative use of standardized operating system functionality including processes, forking, mutexes, and shared memory.   Exploiting this we have defined, developed, and tested a probabilistic programming language intermediate representation language we call probabilistic C, which itself can be compiled to machine code by standard compilers and linked to operating system libraries yielding an efficient, scalable, portable probabilistic programming compilation target.  This opens up a new hardware and systems research path for optimizing probabilistic programming systems.",http://proceedings.mlr.press/v32/paige14.html,http://proceedings.mlr.press/v32/paige14.pdf,ICML
2504,2014,Learning Theory and Algorithms for revenue optimization in second price auctions with reserve,"Mehryar Mohri,         Andres Munoz Medina",Second-price auctions with reserve play a critical role for    modern search engine and popular online sites since the revenue of   these companies often directly depends on the outcome of such   auctions. The choice of the reserve price is the main mechanism   through which the auction revenue can be influenced in these   electronic markets. We cast the problem of selecting the reserve   price to optimize revenue as a learning problem and present a full   theoretical analysis dealing with the complex properties of the   corresponding loss function (it is non-convex and discontinuous). We further give novel algorithms for solving this problem and report the results of encouraging experiments   demonstrating their effectiveness.,http://proceedings.mlr.press/v32/mohri14.html,http://proceedings.mlr.press/v32/mohri14.pdf,ICML
2505,2014,Online Stochastic Optimization  under Correlated Bandit Feedback,"Mohammad Gheshlaghi azar,         Alessandro Lazaric,         Emma Brunskill","In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high-confidence tree (HCT) algorithm, a novel anytime \mathcal X-armed bandit algorithm, and derive regret bounds matching the performance of state-of-the-art algorithms in terms of the dependency on number of steps and the near-optimality dimension. The main advantage of HCT is that it handles the challenging case of correlated bandit feedback (reward), whereas existing methods require rewards to be conditionally independent. HCT also improves on the state-of-the-art in terms of the memory requirement, as well as requiring a weaker smoothness assumption on the mean-reward function in comparison with the existing anytime algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results.",http://proceedings.mlr.press/v32/azar14.html,http://proceedings.mlr.press/v32/azar14.pdf,ICML
2506,2014,Memory and Computation Efficient PCA via Very Sparse Random Projections,"Farhad Pourkamali Anaraki,         Shannon Hughes","Algorithms that can efficiently recover principal components in very high-dimensional, streaming, and/or distributed data settings have become an important topic in the literature. In this paper, we propose an approach to principal component estimation that utilizes projections onto very sparse random vectors with Bernoulli-generated nonzero entries. Indeed, our approach is simultaneously efficient in memory/storage space, efficient in computation, and produces accurate PC estimates, while also allowing for rigorous theoretical performance analysis. Moreover, one can tune the sparsity of the random vectors deliberately to achieve a desired point on the tradeoffs between memory, computation, and accuracy. We rigorously characterize these tradeoffs and provide statistical performance guarantees. In addition to these very sparse random vectors, our analysis also applies to more general random projections. We present experimental results demonstrating that this approach allows for simultaneously achieving a substantial reduction of the computational complexity and memory/storage space, with little loss in accuracy, particularly for very high-dimensional data.",http://proceedings.mlr.press/v32/anaraki14.html,http://proceedings.mlr.press/v32/anaraki14.pdf,ICML
2507,2014,Online Learning in Markov Decision Processes with Changing Cost Sequences,"Travis Dick,         Andras Gyorgy,         Csaba Szepesvari","In this paper we consider online learning in finite Markov decision processes (MDPs) with changing cost sequences under full and bandit-information.  We propose to view this problem as an instance of online linear optimization.  We propose two methods for this problem: MD^2 (mirror descent with approximate projections) and the continuous exponential weights algorithm with Dikin walks.  We provide a rigorous complexity analysis of these techniques, while providing near-optimal regret-bounds (in particular, we take into account the computational costs of performing approximate projections in MD^2).  In the case of full-information feedback, our results complement existing ones. In the case of bandit-information feedback we consider the online stochastic shortest path problem, a special case of the above MDP problems, and manage to improve the existing results by removing the previous restrictive assumption that the state-visitation probabilities are uniformly bounded away from zero under all policies.",http://proceedings.mlr.press/v32/dick14.html,http://proceedings.mlr.press/v32/dick14.pdf,ICML
2508,2014,Factorized Point Process Intensities: A Spatial Analysis of Professional Basketball,"Andrew Miller,         Luke Bornn,         Ryan Adams,         Kirk Goldsberry","We develop a machine learning approach to represent and analyze the underlying spatial structure that governs shot selection among professional basketball players in the NBA.  Typically, NBA players are discussed and compared in an heuristic, imprecise manner that relies on unmeasured intuitions about player behavior.  This makes it difficult to draw comparisons between players and make accurate player specific predictions.  Modeling shot attempt data as a point process, we create a low dimensional representation of offensive player types in the NBA.  Using non-negative matrix factorization (NMF), an unsupervised dimensionality reduction technique, we show that a low-rank spatial decomposition summarizes the shooting habits of NBA players.  The spatial representations discovered by the algorithm correspond to intuitive descriptions of NBA player types, and can be used to model other spatial effects, such as shooting accuracy.",http://proceedings.mlr.press/v32/miller14.html,http://proceedings.mlr.press/v32/miller14.pdf,ICML
2509,2014,Sparse Reinforcement Learning via Convex Optimization,"Zhiwei Qin,         Weichang Li,         Firdaus Janoos","We propose two new algorithms for the sparse reinforcement learning problem based on different formulations.  The first algorithm is an off-line method based on the alternating direction method of multipliers for solving a constrained formulation that explicitly controls the projected Bellman residual.  The second algorithm is an online stochastic approximation algorithm that employs the regularized dual averaging technique, using the Lagrangian formulation.  The convergence of both algorithms are established. We demonstrate the performance of these algorithms through two classical examples.",http://proceedings.mlr.press/v32/qin14.html,http://proceedings.mlr.press/v32/qin14.pdf,ICML
2510,2014,Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning,"François Denis,         Mattias Gybels,         Amaury Habrard","Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution. These methods rely on a singular value decomposition of a matrix H_S, called the Hankel matrix, that records the frequencies of (some of) the observed strings. The accuracy of the learned distribution depends both on the quantity of information embedded in H_S and on the distance between H_S and its mean H_r. Existing concentration bounds seem to indicate that the concentration over H_r gets looser with its size, suggesting to make a trade-off between the quantity of used information and the size of H_r. We propose new dimension-free concentration bounds for several variants of Hankel matrices. Experiments demonstrate that these bounds are tight and that they significantly improve existing bounds. These results suggest that the concentration rate of the Hankel matrix around its mean does not constitute an argument for limiting its size.",http://proceedings.mlr.press/v32/denis14.html,http://proceedings.mlr.press/v32/denis14.pdf,ICML
2511,2014,An Adaptive Accelerated Proximal Gradient Method and its Homotopy Continuation for Sparse Optimization,"Qihang Lin,         Lin Xiao","We first propose an adaptive accelerated proximal gradient(APG) method for minimizing strongly convex composite functions with unknown convexity parameters. This method incorporates a restarting scheme to automatically estimate the strong convexity parameter and achieves a nearly optimal iteration complexity. Then we consider the ℓ1-regularized least-squares (ℓ1-LS) problem in the high-dimensional setting. Although such an objective function is not strongly convex, it has restricted strong convexity over sparse vectors. We exploit this property by combining the adaptive  APG method with a homotopy continuation scheme, which generates a sparse solution path towards optimality. This method obtains a global linear rate of convergence and its overall iteration complexity has a weaker dependency on the restricted condition number than previous work.",http://proceedings.mlr.press/v32/lin14.html,http://proceedings.mlr.press/v32/lin14.pdf,ICML
2512,2014,A Discriminative Latent Variable Model for Online Clustering,"Rajhans Samdani,         Kai-Wei Chang,         Dan Roth","This paper presents a latent variable structured prediction model for discriminative supervised clustering of items called the Latent Left-linking Model (L3M). We present an online clustering algorithm for L3M based on a feature-based item similarity function. We provide a learning framework for estimating the similarity function and present a fast stochastic gradient-based learning technique. In our experiments on coreference resolution and document clustering, L3 M outperforms several existing online as well as batch supervised clustering techniques.",http://proceedings.mlr.press/v32/samdani14.html,http://proceedings.mlr.press/v32/samdani14.pdf,ICML
2513,2014,Circulant Binary Embedding,"Felix Yu,         Sanjiv Kumar,         Yunchao Gong,         Shih-Fu Chang","Binary embedding of high-dimensional data requires long codes to preserve the discriminative power of the input space. Traditional binary coding methods often suffer from very high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure enables the use of Fast Fourier Transformation to speed up the computation. Compared to methods that use unstructured matrices, the proposed method improves the time complexity from \mathcalO(d^2) to \mathcalO(d\logd), and the space complexity from \mathcalO(d^2) to \mathcalO(d) where d is the input dimensionality. We also propose a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. We show by extensive experiments that the proposed approach gives much better performance than the state-of-the-art approaches for fixed time, and provides much faster computation with no performance degradation for fixed number of bits.",http://proceedings.mlr.press/v32/yub14.html,http://proceedings.mlr.press/v32/yub14.pdf,ICML
2514,2014,Distributed Stochastic Gradient MCMC,"Sungjin Ahn,         Babak Shahbaba,         Max Welling",Probabilistic inference on a big data scale is becoming increasingly relevant to both the machine learning and statistics communities. Here we introduce the first fully distributed MCMC algorithm based on stochastic gradients. We argue that stochastic gradient MCMC algorithms are particularly suited for distributed inference because individual chains can draw minibatches from their local pool of data for a flexible amount of time before jumping to or syncing with other chains. This greatly reduces communication overhead and allows adaptive load balancing. Our experiments for LDA on Wikipedia and Pubmed show that relative to the state of the art in distributed MCMC we reduce compute time from 27 hours to half an hour in order to reach the same perplexity level.,http://proceedings.mlr.press/v32/ahn14.html,http://proceedings.mlr.press/v32/ahn14.pdf,ICML
2515,2014,Multimodal Neural Language Models,"Ryan Kiros,         Ruslan Salakhutdinov,         Rich Zemel","We introduce two multimodal neural language models: models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase descriptions given image queries, as well as generate text conditioned on images. We show that in the case of image-text modelling we can jointly learn word representations and image features by training our models together with a convolutional network. Unlike many of the existing methods, our approach can generate sentence descriptions for images without the use of templates, structured prediction, and/or syntactic trees. While we focus on image-text modelling, our algorithms can be easily applied to other modalities such as audio.",http://proceedings.mlr.press/v32/kiros14.html,http://proceedings.mlr.press/v32/kiros14.pdf,ICML
2516,2014,Nonmyopic ε-Bayes-Optimal Active Learning of Gaussian Processes,"Trong Nghia Hoang,         Bryan Kian Hsiang Low,         Patrick Jaillet,         Mohan Kankanhalli","A fundamental issue in active learning of Gaussian processes is that of the exploration-exploitation trade-off. This paper presents a novel nonmyopic ε-Bayes-optimal active learning (ε-BAL) approach that jointly and naturally optimizes the trade-off.   In contrast, existing works have primarily developed myopic/greedy algorithms or performed exploration and exploitation separately. To perform active learning in real time, we then propose an anytime algorithm based on ε-BAL with performance guarantee and empirically demonstrate using synthetic and real-world datasets that, with limited budget, it outperforms the state-of-the-art algorithms.",http://proceedings.mlr.press/v32/hoang14.html,http://proceedings.mlr.press/v32/hoang14.pdf,ICML
2517,2014,Outlier Path: A Homotopy Algorithm for Robust SVM,"Shinya Suzumura,         Kohei Ogawa,         Masashi Sugiyama,         Ichiro Takeuchi","In recent applications with massive but less reliable data (e.g., labels obtained by a semi-supervised learning method or crowdsourcing), non-robustness of the support vector machine (SVM) often causes considerable performance deterioration. Although improving the robustness of SVM has been investigated for long time, robust SVM (RSVM) learning still poses two major challenges: obtaining a good (local) solution from a non-convex optimization problem and optimally controlling the robustness-efficiency trade-off. In this paper, we address these two issues simultaneously in an integrated way by introducing a novel homotopy approach to RSVM learning. Based on theoretical investigation of the geometry of RSVM solutions, we show that a path of local RSVM solutions can be computed efficiently when the influence of outliers is gradually suppressed as simulated annealing. We experimentally demonstrate that our algorithm tends to produce better local solutions than the alternative approach based on the concave-convex procedure, with the ability of stable and efficient model selection for controlling the influence of outliers.",http://proceedings.mlr.press/v32/suzumura14.html,http://proceedings.mlr.press/v32/suzumura14.pdf,ICML
2518,2014,Stochastic Backpropagation and Approximate Inference in Deep Generative Models,"Danilo Jimenez Rezende,         Shakir Mohamed,         Daan Wierstra","We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.",http://proceedings.mlr.press/v32/rezende14.html,http://proceedings.mlr.press/v32/rezende14.pdf,ICML
2519,2014,Influence Function Learning in Information Diffusion Networks,"Nan Du,         Yingyu Liang,         Maria Balcan,         Le Song","Can we learn the influence of a set of people in a social network from cascades of information diffusion? This question is often addressed by a two-stage approach: first learn a diffusion model, and then calculate the influence based on the learned model. Thus, the success of this approach relies heavily on the correctness of the diffusion model which is hard to verify for real world data. In this paper, we exploit the insight that the influence functions in many diffusion models are coverage functions, and propose a novel parameterization of such functions using a convex combination of random basis functions. Moreover, we propose an efficient maximum likelihood based algorithm to learn such functions directly from cascade data, and hence bypass the need to specify a particular diffusion model in advance. We provide both theoretical and empirical analysis for our approach, showing that the proposed approach can provably learn the influence function with low sample complexity, be robust to the unknown diffusion models, and significantly outperform existing approaches in both synthetic and real world data.",http://proceedings.mlr.press/v32/du14.html,http://proceedings.mlr.press/v32/du14.pdf,ICML
2520,2014,Estimating Latent-Variable Graphical Models using Moments and Likelihoods,"Arun Tejasvi Chaganty,         Percy Liang","Recent work in method of moments provide consistent estimates for  latent-variable models, avoiding local optima issues, but these methods can  only be applied to certain types of graphical models. In this work, we show  that the method of moments in conjunction with a composite marginal likelihood  objective yields consistent parameter estimates for a much broader class of  directed and undirected graphical models, including loopy graphs with high  treewidth. Specifically, we use tensor factorization to reveal partial  information about the hidden variables, rendering the otherwise non-convex  negative log-likelihood convex. Our approach gracefully extends to models  outside our class by incorporating the partial information via posterior  regulraization.",http://proceedings.mlr.press/v32/chaganty14.html,http://proceedings.mlr.press/v32/chaganty14.pdf,ICML
2521,2014,Maximum Margin Multiclass Nearest Neighbors,"Aryeh Kontorovich,         Roi Weiss","We develop a general framework for margin-based multicategory classification in metric spaces. The basic work-horse is a margin-regularized version of the nearest-neighbor classifier. We prove generalization bounds that match the state of the art in sample size n and significantly improve the dependence on the number of classes k. Our point of departure is a nearly Bayes-optimal finite-sample risk bound independent of k. Although k-free, this bound is unregularized and non-adaptive, which motivates our main result: Rademacher and scale-sensitive margin bounds with a logarithmic dependence on k. As the best previous risk estimates  in this setting were of order \sqrt k, our bound is exponentially sharper. From the algorithmic standpoint, in doubling metric spaces our classifier may be trained on n examples in  O(n^2\log n) time and evaluated on new points in O(\log n) time.",http://proceedings.mlr.press/v32/kontorovichb14.html,http://proceedings.mlr.press/v32/kontorovichb14.pdf,ICML
2522,2014,Discriminative Features via Generalized Eigenvectors,"Nikos Karampatziakis,         Paul Mineiro","Representing examples in a way that is compatible with the underlying classifier can greatly enhance the performance of a learning system. In this paper we investigate scalable techniques for inducing discriminative features by taking advantage of simple second order structure in the data. We focus on multiclass classification and show that features extracted from the generalized eigenvectors of the class conditional second moments lead to classifiers with excellent empirical performance. Moreover, these features have attractive theoretical properties, such as inducing representations that are invariant to linear transformations of the input. We evaluate classifiers built from these features on three different tasks, obtaining state of the art results.",http://proceedings.mlr.press/v32/karampatziakis14.html,http://proceedings.mlr.press/v32/karampatziakis14.pdf,ICML
2523,2014,Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction,"Jian Zhou,         Olga Troyanskaya","Predicting protein secondary structure is a fundamental problem in protein structure prediction. Here we present a new supervised generative stochastic network (GSN) based method to predict local secondary structure with deep hierarchical representations. GSN is a recently proposed deep learning technique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative model. We present the supervised extension of GSN, which learns a Markov chain to sample from a conditional distribution, and applied it to protein structure prediction. To scale the model to full-sized, high-dimensional data, like protein sequences with hundreds of amino-acids, we introduce a convolutional architecture, which allows efficient learning across multiple layers of hierarchical representations. Our architecture uniquely focuses on predicting structured low-level labels informed with both low and high-level representations learned by the model. In our application this corresponds to labeling the secondary structure state of each amino-acid residue. We trained and tested the model on separate sets of non-homologous proteins sharing less than 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513 dataset, better than the previously reported best performance 64.9% (Wang et al., 2011) for this challenging secondary structure prediction problem.",http://proceedings.mlr.press/v32/zhou14.html,http://proceedings.mlr.press/v32/zhou14.pdf,ICML
2524,2014,Learnability of the Superset Label Learning Problem,"Liping Liu,         Thomas Dietterich","In the Superset Label Learning (SLL) problem, weak supervision is  provided in the form of a \it superset of labels that contains the  true label.  If the classifier predicts a label outside of the  superset, it commits a \it superset error.  Most existing SLL  algorithms learn a multiclass classifier by minimizing the superset  error. However, only limited theoretical analysis has been dedicated  to this approach. In this paper, we analyze Empirical Risk Minimizing  learners that use the superset error as the empirical risk measure.  SLL data can arise either in the form of independent instances or as  multiple-instance bags. For both scenarios, we give the conditions for  ERM learnability and sample complexity for the realizable case.",http://proceedings.mlr.press/v32/liug14.html,http://proceedings.mlr.press/v32/liug14.pdf,ICML
2525,2014,Asymptotically consistent estimation of the number of change points in highly dependent time series,"Azadeh Khaleghi,         Daniil Ryabko","The problem of change point estimation is considered in a general framework where the  data are generated by arbitrary unknown stationary ergodic process distributions. This means that the data may have  long-range dependencies of an arbitrary form. In this context the consistent estimation of the number of change points is provably impossible. A formulation is proposed which overcomes this obstacle:   it is possible to find the correct number of change points at the  expense of introducing the additional constraint that the correct number of  process distributions that generate the data is provided. This additional parameter has a natural interpretation  in many real-world applications.  It turns out that in this formulation change point estimation can be reduced to time series clustering. Based on this reduction, an algorithm is proposed that finds the number of change points and locates the changes.  This algorithm is shown to be asymptotically consistent.  The theoretical results are complemented with empirical evaluations.",http://proceedings.mlr.press/v32/khaleghi14.html,http://proceedings.mlr.press/v32/khaleghi14.pdf,ICML
2526,2014,Robust and Efficient Kernel Hyperparameter Paths with Guarantees,"Joachim Giesen,         Soeren Laue,         Patrick Wieschollek","Algorithmically, many machine learning tasks boil down to solving  parameterized optimization problems. Finding good values for the  parameters has significant influence on the statistical performance  of these methods. Thus supporting the choice of parameter values  algorithmically has received quite some attention recently,  especially algorithms for computing the whole solution path of  parameterized optimization problem. These algorithms can be used,  for instance, to track the solution of a regularized learning  problem along the regularization parameter path, or for tracking the  solution of kernelized problems along a kernel hyperparameter  path. Since exact path following algorithms can be numerically  unstable, robust and efficient approximate path tracking algorithms  became popular for regularized learning problems. By now algorithms  with optimal path complexity are known for many regularized learning  problems. That is not the case for kernel hyperparameter path  tracking algorithms, where the exact path tracking algorithms can  also suffer from numerical instabilities. The robust approximation  algorithms for regularization path tracking can not be used directly  for kernel hyperparameter path tracking problems since the latter  fall into a different problem class. Here we address this problem by  devising a robust and efficient path tracking algorithm that can  also handle kernel hyperparameter paths and has asymptotically  optimal complexity. We use this algorithm to compute approximate  kernel hyperparamter solution paths for support vector machines and  robust kernel regression. Experimental results for this problem  applied to various data sets confirms the theoretical complexity  analysis.",http://proceedings.mlr.press/v32/giesen14.html,http://proceedings.mlr.press/v32/giesen14.pdf,ICML
2527,2014,Globally Convergent Parallel MAP LP Relaxation Solver using the Frank-Wolfe Algorithm,"Alexander Schwing,         Tamir Hazan,         Marc Pollefeys,         Raquel Urtasun","While MAP inference is typically intractable for many real-world applications, linear programming relaxations have been proven very effective. Dual block-coordinate descent methods are among the most efficient solvers, however, they are prone to get stuck in sub-optimal  points. Although subgradient approaches achieve global convergence, they are typically slower in practice. To improve convergence speed, algorithms which compute the steepest ε-descent direction by solving a quadratic program have been proposed. In this paper we suggest to decouple the quadratic program based on the Frank-Wolfe approach. This allows us to obtain an efficient and easy to parallelize algorithm while retaining the global convergence properties. Our method proves superior when compared to existing algorithms on a set of spin-glass models and protein design tasks.",http://proceedings.mlr.press/v32/schwing14.html,http://proceedings.mlr.press/v32/schwing14.pdf,ICML
2528,2014,Multivariate Maximal Correlation Analysis,"Hoang Vu Nguyen,         Emmanuel Müller,         Jilles Vreeken,         Pavel Efros,         Klemens Böhm","Correlation analysis is one of the key elements of statistics, and has various applications in data analysis. Whereas most existing measures can only detect pairwise correlations between two dimensions, modern analysis aims at detecting correlations in multi-dimensional spaces.    We propose MAC, a novel multivariate correlation measure designed for discovering multi-dimensional patterns. It belongs to the powerful class of maximal correlation analysis, for which we propose a generalization to multivariate domains. We highlight the limitations of current methods in this class, and address these with MAC. Our experiments show that MAC outperforms existing solutions, is robust to noise, and discovers interesting and useful patterns.",http://proceedings.mlr.press/v32/nguyenc14.html,http://proceedings.mlr.press/v32/nguyenc14.pdf,ICML
2529,2014,Combinatorial Partial Monitoring Game with Linear Feedback and Its Applications,"Tian Lin,         Bruno Abrahao,         Robert Kleinberg,         John Lui,         Wei Chen","In online learning, a player chooses actions to play and receives reward and feedback from the environment with the goal of maximizing her reward over time. In this paper, we propose the model of combinatorial partial monitoring games with linear feedback, a model which simultaneously addresses limited feedback, infinite outcome space of the environment and exponentially large action space of the player. We present the Global Confidence Bound (GCB) algorithm, which integrates ideas from both combinatorial multi-armed bandits and finite partial monitoring games to handle all the above issues. GCB only requires feedback on a small set of actions and achieves O(T^\frac23\log T) distribution-independent regret and O(\log T) distribution-dependent regret (the latter assuming unique optimal action), where T is the total time steps played. Moreover, the regret bounds only depend linearly on \log |X| rather than |X|, where X is the action space. GCB isolates offline optimization tasks from online learning and avoids explicit enumeration of all actions in the online learning part. We demonstrate that our model and algorithm can be applied to a crowdsourcing application leading to both an efficient learning algorithm and low regret, and argue that they can be applied to a wide range of combinatorial applications constrained with limited feedback.",http://proceedings.mlr.press/v32/lind14.html,http://proceedings.mlr.press/v32/lind14.pdf,ICML
2530,2014,Recurrent Convolutional Neural Networks for Scene Labeling,"Pedro Pinheiro,         Ronan Collobert","The goal of the scene labeling task is to assign a class label to each pixel in an image.  To ensure a good visual coherence and a high class accuracy, it is essential for a model to capture long range  pixel) label dependencies in images. In a feed-forward architecture, this can be achieved simply by considering a sufficiently large input context patch, around each pixel to be labeled.  We propose an approach that consists of a recurrent convolutional neural network which allows us to consider a large input context while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation technique nor any task-specific features. The system is trained in an end-to-end manner over raw pixels, and models complex spatial dependencies with low inference cost. As the context size increases with the built-in recurrence, the system identifies and corrects its own errors. Our approach yields state-of-the-art performance on both the Stanford Background Dataset and the SIFT Flow Dataset, while remaining very fast at test time.",http://proceedings.mlr.press/v32/pinheiro14.html,http://proceedings.mlr.press/v32/pinheiro14.pdf,ICML
2531,2014,Dual Query: Practical Private Query Release for High Dimensional Data,"Marco Gaboardi,         Emilio Jesus Gallego Arias,         Justin Hsu,         Aaron Roth,         Zhiwei Steven Wu","We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example,  our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude.",http://proceedings.mlr.press/v32/gaboardi14.html,http://proceedings.mlr.press/v32/gaboardi14.pdf,ICML
2532,2014,Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels,"Jiyan Yang,         Vikas Sindhwani,         Haim Avron,         Michael Mahoney","We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large datasets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo (QMC) approximations instead  where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem.",http://proceedings.mlr.press/v32/yangb14.html,http://proceedings.mlr.press/v32/yangb14.pdf,ICML
2533,2014,Admixture of Poisson MRFs: A Topic Model with Word Dependencies,"David Inouye,         Pradeep Ravikumar,         Inderjit Dhillon","This paper introduces a new topic model based on an admixture of Poisson Markov Random Fields (APM), which can model dependencies between words as opposed to previous independent topic models such as PLSA (Hofmann, 1999), LDA (Blei et al., 2003) or SAM (Reisinger et al., 2010). We propose a class of admixture models that generalizes previous topic models and show an equivalence between the conditional distribution of LDA and independent Poissons—suggesting that APM subsumes the modeling power of LDA. We present a tractable method for estimating the parameters of an APM based on the pseudo log-likelihood and demonstrate the benefits of APM over previous models by preliminary qualitative and quantitative experiments.",http://proceedings.mlr.press/v32/inouye14.html,http://proceedings.mlr.press/v32/inouye14.pdf,ICML
2534,2014,Convex Total Least Squares,"Dmitry Malioutov,         Nikolai Slavov","We study the total least squares (TLS) problem that generalizes least squares regression by allowing measurement errors in both dependent and independent variables. TLS is widely used in applied fields including computer vision, system identification and econometrics.  The special case when all dependent and independent variables have the same  level of uncorrelated Gaussian noise, known  as ordinary TLS, can be solved by singular  value decomposition (SVD). However, SVD cannot solve many important practical TLS  problems with realistic noise structure, such  as having varying measurement noise, known  structure on the errors, or large outliers requiring robust error-norms. To solve such  problems, we develop convex relaxation approaches for a general class of structured  TLS (STLS). We show both theoretically  and experimentally, that while the plain nuclear  norm relaxation incurs large approximation errors for STLS, the re-weighted nuclear  norm approach is very effective, and achieves better accuracy on challenging STLS  problems than popular non-convex solvers.  We describe a fast solution based on augmented  Lagrangian formulation, and apply our approach to an important class of biological  problems that use population average measurements to infer cell-type and  physiological-state specific expression levels that are very hard to measure directly.",http://proceedings.mlr.press/v32/malioutov14.html,http://proceedings.mlr.press/v32/malioutov14.pdf,ICML
2535,2014,Ensemble-Based Tracking: Aggregating Crowdsourced Structured Time Series Data,"Naiyan Wang,         Dit-Yan Yeung","We study the problem of aggregating the contributions of multiple contributors in a crowdsourcing setting.  The data involved is in a form not typically considered in most crowdsourcing tasks, in that the data is structured and has a temporal dimension.  In particular, we study the visual tracking problem in which the unknown data to  be estimated is in the form of a sequence of bounding boxes representing the trajectory of the target object being tracked.  We propose a factorial hidden Markov model (FHMM) for ensemble-based tracking by learning jointly the unknown trajectory of the target and the reliability of each tracker in the ensemble.  For efficient online inference of the FHMM, we devise a conditional particle filter algorithm by exploiting the structure of the joint posterior distribution of the hidden variables.  Using the largest open benchmark for visual tracking, we empirically compare two ensemble methods constructed from five state-of-the-art trackers with the individual trackers.  The promising experimental results provide empirical evidence for our ensemble approach to ""get the best of all worlds"".",http://proceedings.mlr.press/v32/wangg14.html,http://proceedings.mlr.press/v32/wangg14.pdf,ICML
2536,2014,Adaptive Monte Carlo via Bandit Allocation,"James Neufeld,         Andras Gyorgy,         Csaba Szepesvari,         Dale Schuurmans","We consider the problem of sequentially choosing between a set of unbiased Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final combined estimate. By reducing this task to a stochastic multi-armed bandit problem, we show that well developed allocation strategies can be used to achieve an MSE that approaches that of the best estimator chosen in retrospect. We then extend these developments to a scenario where alternative estimators have different, possibly stochastic, costs. The outcome is a new set of adaptive Monte Carlo strategies that provide stronger guarantees than previous approaches while offering practical advantages.",http://proceedings.mlr.press/v32/neufeld14.html,http://proceedings.mlr.press/v32/neufeld14.pdf,ICML
2537,2014,Scaling Up Robust MDPs using Function Approximation,"Aviv Tamar,         Shie Mannor,         Huan Xu","We consider large-scale Markov decision processes (MDPs) with parameter uncertainty, under the robust MDP paradigm. Previous studies showed that robust MDPs, based on a minimax approach to handling uncertainty, can be solved using dynamic programming for small to medium sized problems. However, due to the ""curse of dimensionality"", MDPs that model real-life problems are typically prohibitively large for such approaches. In this work we employ a reinforcement learning approach to tackle this planning problem: we develop a robust approximate dynamic programming method based on a projected fixed point equation to approximately solve large scale robust MDPs. We show that the proposed method provably succeeds under certain technical conditions, and demonstrate its effectiveness through simulation of an option pricing problem. To the best of our knowledge, this is the first attempt to scale up the robust MDP paradigm.",http://proceedings.mlr.press/v32/tamar14.html,http://proceedings.mlr.press/v32/tamar14.pdf,ICML
2538,2014,Towards End-To-End Speech Recognition with Recurrent Neural Networks,"Alex Graves,         Navdeep Jaitly","This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.",http://proceedings.mlr.press/v32/graves14.html,http://proceedings.mlr.press/v32/graves14.pdf,ICML
2539,2014,A reversible infinite HMM using normalised random measures,"David Knowles,         Zoubin Ghahramani,         Konstantina Palla","We present a nonparametric prior over reversible Markov chains. We use completely random measures, specifically gamma processes, to construct a countably infinite graph with weighted edges.  By enforcing symmetry to make the edges undirected we define a prior over random walks on graphs that results in a reversible Markov chain. The resulting prior over infinite transition matrices is closely related to the hierarchical Dirichlet process but enforces reversibility. A reinforcement scheme has recently been proposed with similar properties, but the de Finetti measure is not well characterised. We take the alternative approach of explicitly constructing the mixing measure, which allows more straightforward and efficient inference at the cost of no longer having a closed form predictive distribution. We use our process to construct a reversible infinite HMM which we apply to two real datasets, one from epigenomics and one ion channel recording.",http://proceedings.mlr.press/v32/knowles14.html,http://proceedings.mlr.press/v32/knowles14.pdf,ICML
2540,2014,Low-density Parity Constraints for Hashing-Based Discrete Integration,"Stefano Ermon,         Carla Gomes,         Ashish Sabharwal,         Bart Selman","In recent years, a number of probabilistic inference and counting techniques have been proposed that exploit pairwise independent hash functions to infer properties of succinctly defined high-dimensional sets. While providing desirable statistical guarantees, typical constructions of such hash functions are themselves not amenable to efficient inference. Inspired by the success of LDPC codes, we propose the use of low-density parity constraints to make inference more tractable in practice. While not strongly universal, we show that such sparse constraints belong to a new class of hash functions that we call Average Universal. These weaker hash functions retain the desirable statistical guarantees needed by most such probabilistic inference methods. Thus, they continue to provide provable accuracy guarantees while at the same time making a number of algorithms significantly more scalable in practice. Using this technique, we provide new, tighter bounds for challenging discrete integration and model counting problems.",http://proceedings.mlr.press/v32/ermon14.html,http://proceedings.mlr.press/v32/ermon14.pdf,ICML
2541,2014,Compact Random Feature Maps,"Raffay Hamid,         Ying Xiao,         Alex Gittens,         Dennis Decoste","Kernel approximation using randomized feature maps has recently gained a lot of interest. In this work, we identify that previous approaches for polynomial kernel approximation create maps that are rank deficient, and therefore do not utilize the capacity of the projected feature space effectively. To address this challenge, we propose compact random feature maps (CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efficiently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class classifiers. We present experiments on multiple standard data-sets with performance competitive with state-of-the-art results.",http://proceedings.mlr.press/v32/hamid14.html,http://proceedings.mlr.press/v32/hamid14.pdf,ICML
2542,2014,Effective Bayesian Modeling of Groups of Related Count Time Series,Nicolas Chapados,"Time series of counts arise in a variety of forecasting applications, for which traditional models are generally inappropriate. This paper introduces a hierarchical Bayesian formulation applicable to count time series that can easily account for explanatory variables and share statistical strength across groups of related time series. We derive an efficient approximate inference technique, and illustrate its performance on a number of datasets from supply chain planning.",http://proceedings.mlr.press/v32/chapados14.html,http://proceedings.mlr.press/v32/chapados14.pdf,ICML
2543,2014,On Modelling Non-linear Topical Dependencies,"Zhixing Li,         Siqiang Wen,         Juanzi Li,         Peng Zhang,         Jie Tang","Probabilistic topic models such as Latent Dirichlet Allocation (LDA) discover latent topics from large corpora by exploiting words’ co-occurring relation. By observing the topical similarity between words, we find that some other relations, such as semantic or syntax relation between words, lead to strong dependence between their topics. In this paper, sentences are represented as dependency trees and a Global Topic Random Field (GTRF) is presented to model the non-linear dependencies between words. To infer our model, a new global factor is defined over all edges and the normalization factor of GRF is proven to be a constant. As a result, no independent assumption is needed when inferring our model. Based on it, we develop an efficient expectation-maximization (EM) procedure for parameter estimation. Experimental results on four data sets show that GTRF achieves much lower perplexity than LDA and linear dependency topic models and produces better topic coherence.",http://proceedings.mlr.press/v32/lib14.html,http://proceedings.mlr.press/v32/lib14.pdf,ICML
2544,2014,Nonparametric Estimation of Renyi Divergence and Friends,"Akshay Krishnamurthy,         Kirthevasan Kandasamy,         Barnabas Poczos,         Larry Wasserman","We consider nonparametric estimation of L_2, Renyi-αand Tsallis-αdivergences between continuous distributions. Our approach is to construct estimators for particular integral functionals of two densities and translate them into divergence estimators. For the integral functionals, our estimators are based on corrections of a preliminary plug-in estimator. We show that these estimators achieve the parametric convergence rate of n^-1/2 when the densities’ smoothness, s, are both at least d/4 where d is the dimension. We also derive minimax lower bounds for this problem which confirm that s > d/4 is necessary to achieve the n^-1/2 rate of convergence. We validate our theoretical guarantees with a number of simulations.",http://proceedings.mlr.press/v32/krishnamurthy14.html,http://proceedings.mlr.press/v32/krishnamurthy14.pdf,ICML
2545,2014,Communication-Efficient Distributed Optimization using an Approximate Newton-type Method,"Ohad Shamir,         Nati Srebro,         Tong Zhang","We present a novel Newton-type method for distributed optimization,  which is particularly well suited for stochastic optimization and  learning problems.  For quadratic objectives, the method enjoys a  linear rate of convergence which provably \emphimproves with the  data size, requiring an essentially constant number of iterations  under reasonable assumptions.  We provide theoretical and empirical  evidence of the advantages of our method compared to other  approaches, such as one-shot parameter averaging and ADMM.",http://proceedings.mlr.press/v32/shamir14.html,http://proceedings.mlr.press/v32/shamir14.pdf,ICML
2546,2014,Learning Latent Variable Gaussian Graphical Models,"Zhaoshi Meng,         Brian Eriksson,         Al Hero","Gaussian graphical models (GGM) have been widely used in many high-dimensional applications ranging from biological and financial data to recommender systems. Sparsity in GGM plays a central role both statistically and computationally. Unfortunately, real-world data often does not fit well to sparse graphical models.  In this paper, we focus on a family of latent variable Gaussian graphical models (LVGGM), where the model is conditionally sparse given latent variables, but marginally non-sparse. In LVGGM, the inverse covariance matrix has a low-rank plus sparse structure, and can be learned in a regularized maximum likelihood framework. We derive novel parameter estimation error bounds for LVGGM under mild conditions in the high-dimensional setting. These results complement the existing theory on the structural learning, and open up new possibilities of using LVGGM for statistical inference.",http://proceedings.mlr.press/v32/meng14.html,http://proceedings.mlr.press/v32/meng14.pdf,ICML
2547,2014,Transductive Learning with Multi-class Volume Approximation,"Gang Niu,         Bo Dai,         Christoffel Plessis,         Masashi Sugiyama","Given a hypothesis space, the large volume principle by Vladimir Vapnik prioritizes equivalence classes according to their volume in the hypothesis space. The volume approximation has hitherto been successfully applied to binary learning problems. In this paper, we propose a novel generalization to multiple classes, allowing applications of the large volume principle on more learning problems such as multi-class, multi-label and serendipitous learning in a transductive manner. Although the resultant learning method involves a non-convex optimization problem, the globally optimal solution is almost surely unique and can be obtained using O(n^3) time. Novel theoretical analyses are presented for the proposed method, and experimental results show it compares favorably with the one-vs-rest extension.",http://proceedings.mlr.press/v32/niu14.html,http://proceedings.mlr.press/v32/niu14.pdf,ICML
2548,2014,(Near) Dimension Independent Risk Bounds for Differentially Private Learning,"Prateek Jain,         Abhradeep Guha Thakurta","In this paper, we study the problem of differentially private risk minimization where the goal is to provide differentially private algorithms that have small excess risk. In particular we address the following open problem: \emphIs it possible to design computationally efficient differentially private risk minimizers with  excess risk bounds that do not explicitly depend on dimensionality (p) and do not require  structural assumptions like restricted strong convexity?  In this paper, we answer the question in the affirmative for a variant of the well-known  \emphoutput and \emphobjective perturbation algorithms [Chaudhuri et al., 2011]. In particular, we show that  in generalized linear model, variants of both output and objective perturbation algorithms have no \em explicit dependence on p. Our results assume that the underlying loss function is a 1-Lipschitz convex function and we show that the excess risk depends only on  L_2 norm of the true risk minimizer and that of training points.  Next, we present a novel privacy preserving algorithm for risk minimization over simplex in the generalized linear model, where the loss function is  a doubly differentiable convex function. Assuming that the training points have bounded L_∞-norm, our algorithm provides risk bound that has only \em logarithmic dependence on p. We also apply our technique to the online learning setting and obtain a regret bound with similar logarithmic dependence on p. In contrast, the existing differentially private online learning methods incur O(\sqrtp)  dependence.",http://proceedings.mlr.press/v32/jain14.html,http://proceedings.mlr.press/v32/jain14.pdf,ICML
2549,2014,Min-Max Problems on Factor Graphs,"Siamak Ravanbakhsh,         Christopher Srinivasa,         Brendan Frey,         Russell Greiner","We study the min-max problem in factor graphs, which seeks the assignment that minimizes the maximum value over all factors. We reduce this problem to both min-sum and sum-product inference, and focus on the later. This approach reduces the min-max inference problem to a sequence of constraint satisfaction problems (CSPs) which allows us to sample from a uniform distribution over the set of solutions. We demonstrate how this scheme provides a message passing solution to several NP-hard combinatorial problems, such as min-max clustering (a.k.a. K-clustering), the asymmetric K-center problem, K-packing and the bottleneck traveling salesman problem. Furthermore we theoretically relate the min-max reductions to several NP hard decision problems, such as clique cover, set cover, maximum clique and Hamiltonian cycle, therefore also providing message passing solutions for these problems. Experimental results suggest that message passing often provides near optimal min-max solutions for moderate size instances.",http://proceedings.mlr.press/v32/ravanbakhsh14.html,http://proceedings.mlr.press/v32/ravanbakhsh14.pdf,ICML
2550,2014,Diagnosis determination: decision trees optimizing simultaneously worst and expected testing cost,"Ferdinando Cicalese,         Eduardo Laber,         Aline Medeiros Saettler","In several applications of automatic diagnosis  and active learning a central problem is the evaluation of a discrete function by adaptively querying the values of its variables until the values read uniquely determine the value of the function.   In general reading the value of a variable is done at the expense of some cost (computational or possibly a fee to pay the corresponding experiment). The goal is to design a strategy for evaluating the function incurring little cost (in the worst case or in expectation according to a prior distribution on the possible variables’ assignments).  We provide an algorithm that builds a strategy (decision tree) with both expected cost and worst cost which are at most an O(\log n) factor away  from, respectively, the minimum possible expected cost and the minimum possible worst cost.  Our algorithm provides the best possible approximation simultaneously with respect to both criteria. In fact,  there is no algorithm that can guarantee o(\log n) approximation, under the assumption  that \cal P ≠\cal NP.",http://proceedings.mlr.press/v32/cicalese14.html,http://proceedings.mlr.press/v32/cicalese14.pdf,ICML
2551,2014,Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis,"Jian Tang,         Zhaoshi Meng,         Xuanlong Nguyen,         Qiaozhu Mei,         Ming Zhang","Topic models such as the latent Dirichlet allocation (LDA) have become a standard staple in the modeling toolbox of machine learning. They have been applied to a vast variety of data sets, contexts, and tasks to varying degrees of success. However, to date there is almost no formal theory explicating the LDA’s behavior, and despite its familiarity there is very little systematic analysis of and guidance on the properties of the data that affect the inferential performance of the model. This paper seeks to address this gap, by providing a systematic analysis of factors which characterize the LDA’s performance.  We present theorems elucidating the posterior contraction rates of the topics as the amount of data increases, and a thorough supporting empirical study using synthetic and real data sets, including news and web-based articles and tweet messages. Based on these results we provide practical guidance on how to identify suitable data sets for topic models, and how to specify particular model parameters.",http://proceedings.mlr.press/v32/tang14.html,http://proceedings.mlr.press/v32/tang14.pdf,ICML
2552,2014,Distributed Representations of Sentences and Documents,"Quoc Le,         Tomas Mikolov","Many machine learning algorithms require the  input to be represented as a fixed length feature  vector. When it comes to texts, one of the most  common representations is bag-of-words. Despite their popularity, bag-of-words models have  two major weaknesses: they lose the ordering  of the words and they also ignore semantics of  the words. For example, ""powerful,"" ""strong""  and ""Paris"" are equally distant. In this paper,  we propose an unsupervised algorithm that learns  vector representations of sentences and text documents. This algorithm represents each document by a dense vector which is trained to predict  words in the document. Its construction gives our  algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that our technique outperforms bag-of-words models as well as other techniques for  text representations. Finally, we achieve new  state-of-the-art results on several text classification and sentiment analysis tasks.",http://proceedings.mlr.press/v32/le14.html,http://proceedings.mlr.press/v32/le14.pdf,ICML
2553,2014,Hierarchical Dirichlet Scaling Process,"Dongwoo Kim,         Alice Oh","We present the hierarchical Dirichlet scaling process (HDSP), a Bayesian nonparametric mixed membership model for multi-labeled data. We construct the HDSP based on the gamma representation of the hierarchical Dirichlet process (HDP) which allows scaling the mixture components. With such construction, HDSP allocates a latent location to each label and mixture component in a space, and uses the distance between them to guide membership probabilities. We develop a variational Bayes algorithm for the approximate posterior inference of the HDSP. Through experiments on synthetic datasets as well as datasets of newswire, medical journal articles, and Wikipedia, we show that the HDSP results in better predictive performance than HDP, labeled LDA and partially labeled LDA.",http://proceedings.mlr.press/v32/kim14.html,http://proceedings.mlr.press/v32/kim14.pdf,ICML
2554,2014,Time-Regularized Interrupting Options (TRIO),"Timothy Mann,         Daniel Mankowitz,         Shie Mannor","High-level skills relieve planning algorithms from low-level details. But when the skills are poorly designed for the domain, the resulting plan may be severely suboptimal. Sutton et al. 1999 made an important step towards resolving this problem by introducing a rule that automatically improves a set of skills called options. This rule terminates an option early whenever switching to another option gives a higher value than continuing with the current option. However, they only analyzed the case where the improvement rule is applied once. We show conditions where this rule converges to the optimal set of options. A new Bellman-like operator that simultaneously improves the set of options is at the core of our analysis. One problem with the update rule is that it tends to favor lower-level skills. Therefore we introduce a regularization term that favors longer duration skills. Experimental results demonstrate that this approach can derive a good set of high-level skills even when the original set of skills cannot solve the problem.",http://proceedings.mlr.press/v32/mannb14.html,http://proceedings.mlr.press/v32/mannb14.pdf,ICML
2555,2014,Learning to Disentangle Factors of Variation with Manifold Interaction,"Scott Reed,         Kihyuk Sohn,         Yuting Zhang,         Honglak Lee","Many latent factors of variation interact to generate sensory data; for example pose, morphology and expression in face images. We propose to learn manifold coordinates for the relevant factors of variation and to model their joint interaction. Most existing feature learning algorithms focus on a single task and extract features that are sensitive to the task-relevant factors and invariant to all others. However, models that just extract a single set of invariant features do not exploit the relationships among the latent factors. To address this we propose a higher-order Boltzmann machine that incorporates multiplicative interactions among groups of hidden units that each learn to encode a factor of variation. Furthermore, we propose a manifold-based training strategy that allows effective disentangling, meaning that units in each group encode a distinct type of variation. Our model achieves state-of-the-art emotion recognition and face verification performance on the Toronto Face Database, and we also demonstrate disentangled features learned on the CMU Multi-PIE dataset.",http://proceedings.mlr.press/v32/reed14.html,http://proceedings.mlr.press/v32/reed14.pdf,ICML
2556,2014,Deterministic Anytime Inference for Stochastic Continuous-Time Markov Processes,"E. Busra Celikkaya,         Christian Shelton","We describe a deterministic anytime method for calculating  filtered and smoothed distributions in large variable-based continuous time  Markov processes.  Prior non-random algorithms do not converge to the true  distribution in the limit of infinite computation time.  Sampling  algorithms give different results each time run, which can lead to  instability when used inside expectation-maximization or other algorithms.  Our method combines the anytime convergent properties of sampling with the  non-random nature of variational approaches.  It is built upon a sum of  time-ordered products, an expansion of the matrix exponential.  We  demonstrate that our method performs as well as or better than the current  best sampling approaches on benchmark problems.",http://proceedings.mlr.press/v32/celikkaya14.html,http://proceedings.mlr.press/v32/celikkaya14.pdf,ICML
2557,2014,"Structured Low-Rank Matrix Factorization: Optimality, Algorithm, and Applications to Image Processing","Benjamin Haeffele,         Eric Young,         Rene Vidal","Recently, convex solutions to low-rank matrix factorization problems have received increasing attention in machine learning. However, in many applications the data can display other structures beyond simply being low-rank. For example, images and videos present complex spatio-temporal structures, which are largely ignored by current low-rank methods. In this paper we explore a matrix factorization technique suitable for large datasets that captures additional structure in the factors by using a projective tensor norm, which includes classical image regularizers such as total variation and the nuclear norm as particular cases. Although the resulting optimization problem is not convex, we show that under certain conditions on the factors, any local minimizer for the factors yields a global minimizer for their product. Examples in biomedical video segmentation and hyperspectral compressed recovery show the advantages of our approach on high-dimensional datasets.",http://proceedings.mlr.press/v32/haeffele14.html,http://proceedings.mlr.press/v32/haeffele14.pdf,ICML
2558,2014,"Finito: A faster, permutable incremental gradient method for big data problems","Aaron Defazio,         Justin Domke,          Caetano","Recent advances in optimization theory have shown that smooth strongly convex finite sums can be minimized faster than by treating them as a black box ""batch"" problem. In this work we introduce a new method in this class with a theoretical convergence rate four times faster than existing methods, for sums with sufficiently many terms. This method is also amendable to a sampling without replacement scheme that in practice gives further speed-ups. We give empirical results showing state of the art performance.",http://proceedings.mlr.press/v32/defazio14.html,http://proceedings.mlr.press/v32/defazio14.pdf,ICML
2559,2014,Stable and Efficient Representation Learning with Nonnegativity Constraints,"Tsung-Han Lin,         H. T. Kung","Orthogonal matching pursuit (OMP) is an efficient approximation algorithm for computing sparse representations. However, prior research has shown that the representations computed by OMP may be of inferior quality, as they deliver suboptimal classification accuracy on several im- age datasets. We have found that this problem is caused by OMP’s relatively weak stability under data variations, which leads to unreliability in supervised classifier training. We show that by imposing a simple nonnegativity constraint, this nonnegative variant of OMP (NOMP) can mitigate OMP’s stability issue and is resistant to noise overfitting. In this work, we provide extensive analysis and experimental results to examine and validate the stability advantage of NOMP. In our experiments, we use a multi-layer deep architecture for representation learning, where we use K-means for feature learning and NOMP for representation encoding. The resulting learning framework is not only efficient and scalable to large feature dictionaries, but also is robust against input noise. This framework achieves the state-of-the-art accuracy on the STL-10 dataset.",http://proceedings.mlr.press/v32/line14.html,http://proceedings.mlr.press/v32/line14.pdf,ICML
2560,2014,Pitfalls in the use of Parallel Inference for the Dirichlet Process,"Yarin Gal,         Zoubin Ghahramani","Recent work done by Lovell, Adams, and Mansingka (2012) and Williamson, Dubey, and Xing (2013) has suggested an alternative parametrisation for the Dirichlet process in order to derive non-approximate parallel MCMC inference for it - work which has been picked-up and implemented in several different fields. In this paper we show that the approach suggested is impractical due to an extremely unbalanced distribution of the data. We characterise the requirements of efficient parallel inference for the Dirichlet process and show that the proposed inference fails most of these requirements (while approximate approaches often satisfy most of them). We present both theoretical and experimental evidence, analysing the load balance for the inference and showing that it is independent of the size of the dataset and the number of nodes available in the parallel implementation. We end with suggestions of alternative paths of research for efficient non-approximate parallel inference for the Dirichlet process.",http://proceedings.mlr.press/v32/gal14.html,http://proceedings.mlr.press/v32/gal14.pdf,ICML
2561,2014,Max-Margin Infinite Hidden Markov Models,"Aonan Zhang,         Jun Zhu,         Bo Zhang","Infinite hidden Markov models (iHMMs) are nonparametric Bayesian extensions of hidden Markov models (HMMs) with an infinite number of states. Though flexible in describing sequential data, the generative formulation of iHMMs could limit their discriminative ability in sequential prediction tasks. Our paper introduces max-margin infinite HMMs (M2iHMMs), new infinite HMMs that explore the max-margin principle for discriminative learning. By using the theory of Gibbs classifiers and data augmentation, we develop efficient beam sampling algorithms without making restricting mean-field assumptions or truncated approximation. For single variate classification, M2iHMMs reduce to a new formulation of DP mixtures of max-margin machines. Empirical results on synthetic and real data sets show that our methods obtain superior performance than other competitors in both single variate classification and sequential prediction tasks.",http://proceedings.mlr.press/v32/zhangb14.html,http://proceedings.mlr.press/v32/zhangb14.pdf,ICML
2562,2014,Learning from Contagion (Without Timestamps),"Kareem Amin,         Hoda Heidari,         Michael Kearns",We introduce and study new models for learning from contagion processes in a network. A learning algorithm is allowed to either choose or passively observe an initial set of seed infections. This seed set then induces a final set of infections resulting from the underlying stochastic contagion dynamics. Our models differ from prior work in that detailed vertex-by-vertex timestamps for the spread of the contagion are not observed. The goal of learning is to infer the unknown network structure. Our main theoretical results are efficient and provably correct algorithms for exactly learning trees. We provide empirical evidence that our algorithm performs well more generally on realistic sparse graphs.,http://proceedings.mlr.press/v32/amin14.html,http://proceedings.mlr.press/v32/amin14.pdf,ICML
2563,2014,Putting MRFs on a Tensor Train,"Alexander Novikov,         Anton Rodomanov,         Anton Osokin,         Dmitry Vetrov",In the paper we present a new framework for dealing with probabilistic graphical models. Our approach relies on the recently proposed Tensor Train format (TT-format) of a tensor that while being  compact allows for efficient application of linear algebra operations. We present a way to convert the energy of a Markov random field to the TT-format and show how one can exploit the properties of the TT-format to attack the tasks of the partition function estimation and the MAP-inference. We provide theoretical guarantees on the accuracy of the proposed algorithm for estimating the partition function and compare our methods against several state-of-the-art algorithms.,http://proceedings.mlr.press/v32/novikov14.html,http://proceedings.mlr.press/v32/novikov14.pdf,ICML
2564,2014,A Bayesian Wilcoxon signed-rank test based on the Dirichlet process,"Alessio Benavoli,         Giorgio Corani,         Francesca Mangili,         Marco Zaffalon,         Fabrizio Ruggeri","Bayesian methods are ubiquitous in machine learning.  Nevertheless, the analysis of empirical results is typically   performed  by frequentist tests. This implies dealing with  null hypothesis significance tests and  p-values, even though the   shortcomings of such methods are well known.   We propose  a nonparametric Bayesian version of the Wilcoxon   signed-rank test using a Dirichlet process (DP) based prior.  We address in two different ways the problem of how to choose  the   infinite dimensional parameter that characterizes the DP.   The proposed  test has all the traditional strengths of the Bayesian   approach; for instance, unlike the frequentist tests,   it allows verifying the null hypothesis, not only rejecting it, and   taking decision which minimize the expected loss.  Moreover, one of the solutions proposed to model the infinitedimensional parameter of the DP, allows isolating instances in which the traditional frequentist test is guessing at random.   We show results dealing with the comparison of two classifiers using real and simulated data.",http://proceedings.mlr.press/v32/benavoli14.html,http://proceedings.mlr.press/v32/benavoli14.pdf,ICML
2565,2014,Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms,"Richard Combes,         Alexandre Proutiere","We consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms. This important class of problems has been recently investigated in (Cope 2009, Yu 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. For discrete unimodal bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm, and propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm optimally exploits the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. We also provide a regret upper bound for OSUB in non-stationary environments where the expected rewards smoothly evolve over time. The analytical results are supported by numerical experiments showing that OSUB performs significantly better than the state-of-the-art algorithms. For continuous sets of arms, we provide a brief discussion. We show that combining an appropriate discretization of the set of arms with the UCB algorithm yields an order-optimal regret, and in practice, outperforms recently proposed algorithms designed to exploit the unimodal structure.",http://proceedings.mlr.press/v32/combes14.html,http://proceedings.mlr.press/v32/combes14.pdf,ICML
2566,2014,An Information Geometry of Statistical Manifold Learning,"Ke Sun,         Stéphane Marchand-Maillet","Manifold learning seeks low-dimensional representations of high-dimensional data. The main tactics have been exploring the geometry in an input data space and an output embedding space. We develop a manifold learning theory in a hypothesis space consisting of models. A model means a specific instance of a collection of points, e.g., the input data collectively or the output embedding collectively. The semi-Riemannian metric of this hypothesis space is uniquely derived in closed form based on the information geometry of probability distributions. There, manifold learning is interpreted as a trajectory of intermediate models. The volume of a continuous region reveals an amount of information. It can be measured to define model complexity and embedding quality. This provides deep unified perspectives of manifold learning theory.",http://proceedings.mlr.press/v32/suna14.html,http://proceedings.mlr.press/v32/suna14.pdf,ICML
2567,2014,Linear and Parallel Learning of Markov Random Fields,"Yariv Mizrahi,         Misha Denil,         Nando De Freitas","We introduce a new embarrassingly parallel parameter learning algorithm for Markov random fields which is efficient for a large class of practical models.  Our algorithm parallelizes naturally over cliques and, for graphs of bounded degree, its complexity is linear in the number of cliques. Unlike its competitors, our algorithm is fully parallel and for log-linear models it is also data efficient, requiring only the local sufficient statistics of the data to estimate parameters.",http://proceedings.mlr.press/v32/mizrahi14.html,http://proceedings.mlr.press/v32/mizrahi14.pdf,ICML
2568,2014,Consistency of Causal Inference under the Additive Noise Model,"Samory Kpotufe,         Eleni Sgouritsa,         Dominik Janzing,         Bernhard Schölkopf","We analyze a family of methods for statistical  causal inference from sample under the so-called  Additive Noise Model. While most work  on the subject has concentrated on establishing  the soundness of the Additive Noise Model, the  statistical consistency of the resulting inference  methods has received little attention. We derive  general conditions under which the given family  of inference methods consistently infers the  causal direction in a nonparametric setting.",http://proceedings.mlr.press/v32/kpotufe14.html,http://proceedings.mlr.press/v32/kpotufe14.pdf,ICML
2569,2014,Fast Multi-stage Submodular Maximization,"Kai Wei,         Rishabh Iyer,         Jeff Bilmes","We introduce a new multi-stage algorithmic framework for submodular maximization. We are motivated by extremely large scale machine learning problems, where both storing the whole data for function evaluation and running the standard accelerated greedy algorithm are prohibitive. We propose a multi-stage framework (called MultGreed), where at each stage we apply an approximate greedy procedure to maximize surrogate submodular functions. The surrogates serve as proxies for a target submodular function but require less memory and are easy to evaluate. We theoretically analyze the performance guarantee of the multi-stage framework, and give examples on how to design instances of MultGreed for a broad range of natural submodular functions. We show that MultGreed  performs very close to the standard greedy algorithm, given appropriate surrogate functions, and argue how our framework can easily be integrated with distributive algorithms for optimization. We complement our theory by empirically evaluating on several real world problems, including data subset selection on millions of speech samples, where MultGreed yields at least a thousand times speedup and superior results over the state-of-the-art selection methods.",http://proceedings.mlr.press/v32/wei14.html,http://proceedings.mlr.press/v32/wei14.pdf,ICML
2570,2014,Riemannian Pursuit for Big Matrix Recovery,"Mingkui Tan,         Ivor W. Tsang,         Li Wang,         Bart Vandereycken,         Sinno Jialin Pan","Low rank matrix recovery is a fundamental task in many real-world  applications. The performance of existing methods, however,   deteriorates significantly when applied to ill-conditioned or large-scale matrices.  In this paper, we therefore propose an efficient method, called  Riemannian Pursuit (RP), that aims to address these two problems  simultaneously. Our method consists of a sequence of fixed-rank  optimization problems. Each subproblem, solved by a nonlinear  Riemannian conjugate gradient method, aims to correct the solution  in the most important subspace of increasing size.   Theoretically, RP converges linearly under mild conditions and  experimental results show that it substantially outperforms existing  methods when applied to   large-scale and ill-conditioned matrices.",http://proceedings.mlr.press/v32/tan14.html,http://proceedings.mlr.press/v32/tan14.pdf,ICML
2571,2014,Latent Confusion Analysis by Normalized Gamma Construction,"Issei Sato,         Hisashi Kashima,         Hiroshi Nakagawa","We developed a flexible framework for modeling the annotation and judgment processes of humans, which we called “normalized gamma construction of a confusion matrix.”  This framework enabled us to model three properties: (1) the abilities of humans, (2) a confusion matrix with labeling, and (3) the difficulty with which items are correctly annotated.  We also provided the concept of “latent confusion analysis (LCA),” whose main purpose was to analyze the principal confusions behind human annotations and judgments.  It is assumed in LCA that confusion matrices are shared between persons, which we called “latent confusions”, in tribute to the “latent topics” of topic modeling.  We aim at summarizing the workers’ confusion matrices with the small number of latent principal confusion matrices because many personal confusion matrices is difficult to analyze.  We used LCA to analyze latent confusions regarding the effects of radioactivity on  fish and shellfish following the Fukushima Daiichi nuclear disaster in 2011.",http://proceedings.mlr.press/v32/satob14.html,http://proceedings.mlr.press/v32/satob14.pdf,ICML
2572,2014,Safe Screening with Variational Inequalities and Its Application to Lasso,"Jun Liu,         Zheng Zhao,         Jie Wang,         Jieping Ye","Sparse learning techniques have been routinely used for feature selection as the resulting model usually has a small number of non-zero entries.  Safe screening, which eliminates the features that are guaranteed to have zero coefficients for a certain value of the regularization parameter, is a technique for improving the computational efficiency. Safe screening is gaining increasing attention since 1) solving sparse learning formulations usually has a high computational cost especially when the number of features is large and 2) one needs to try several regularization parameters to select a suitable model. In this paper, we propose an approach called “Sasvi"" (Safe screening with variational inequalities). Sasvi makes use of the variational inequality that provides the sufficient and necessary optimality condition for the dual problem. Several existing approaches for Lasso screening can be casted as relaxed versions of the proposed Sasvi, thus Sasvi provides a stronger safe screening rule. We further study the monotone properties of Sasvi for Lasso, based on which a sure removal regularization parameter can be identified for each feature. Experimental results on both synthetic and real data sets are reported to demonstrate the effectiveness of the proposed Sasvi for Lasso screening.",http://proceedings.mlr.press/v32/liuc14.html,http://proceedings.mlr.press/v32/liuc14.pdf,ICML
2573,2014,Learning by Stretching Deep Networks,"Gaurav Pandey,         Ambedkar Dukkipati","In recent years, deep architectures have gained a lot of prominence for learning complex AI tasks  because of their capability to incorporate complex variations in data within the model. However, these models often need to be trained for a long time in order to obtain good results. In this paper, we propose a technique, called ‘stretching’, that allows the same models to perform considerably better with very little training.  We show that learning can be done tractably, even when the weight matrix is stretched to infinity, for some specific models. We also study tractable algorithms for implementing stretching in deep convolutional architectures in an iterative manner and derive bounds for its convergence. Our experimental results suggest that the proposed stretched deep convolutional networks are capable of achieving good performance for many object recognition tasks. More importantly, for a fixed network architecture, one can achieve much better accuracy using stretching rather than learning the weights using backpropagation.",http://proceedings.mlr.press/v32/pandey14.html,http://proceedings.mlr.press/v32/pandey14.pdf,ICML
2574,2014,Multi-period Trading Prediction Markets with Connections to Machine Learning,"Jinli Hu,         Amos Storkey","We present a new model for prediction markets, in which we use risk measures to model agents and introduce a market maker to describe the trading process. This specific choice of modelling approach enables us to show that the whole market approaches a global objective, despite the fact that the market is designed such that each agent only cares about its own goal. In addition, the market dynamic provides a sensible algorithm for optimising the global objective. An intimate connection between machine learning and our markets is thus established, such that we could 1) analyse a market by applying machine learning methods to the global objective; and 2) solve machine learning problems by setting up and running certain markets.",http://proceedings.mlr.press/v32/hu14.html,http://proceedings.mlr.press/v32/hu14.pdf,ICML
2575,2014,Universal Matrix Completion,"Srinadh Bhojanapalli,         Prateek Jain","The problem of low-rank matrix completion has recently generated a lot of interest leading to several results that offer exact solutions to the problem. However, in order to do so, these methods make assumptions that can be quite restrictive in practice. More specifically, the methods assume that: a) the observed indices are sampled uniformly at random, and b) for every new matrix, the observed indices are sampled \emphafresh. In this work, we address these issues by providing a universal recovery guarantee for matrix completion that works for a variety of sampling schemes. In particular, we show that if the set of sampled indices come from the edges of a bipartite graph with large spectral gap (i.e. gap between the first and the second singular value), then the nuclear norm minimization based method exactly recovers all low-rank matrices that satisfy certain incoherence properties.Moreover, we also show that under certain stricter incoherence conditions, O(nr^2) uniformly sampled entries are enough to recover any rank-r n\times n matrix, in contrast to the O(nr\log n) sample complexity required by other matrix completion algorithms as well as existing analyses of the nuclear norm method.",http://proceedings.mlr.press/v32/bhojanapalli14.html,http://proceedings.mlr.press/v32/bhojanapalli14.pdf,ICML
2576,2014,Active Detection via Adaptive Submodularity,"Yuxin Chen,         Hiroaki Shioi,         Cesar Fuentes Montesinos,         Lian Pin Koh,         Serge Wich,         Andreas Krause","Efficient detection of multiple object instances is one of the fundamental challenges in computer vision. For certain object categories, even the best automatic systems are yet unable to produce high-quality detection results, and fully manual annotation would be an expensive process. How can detection algorithms interplay with human expert annotators? To make the best use of scarce (human) labeling resources, one needs to decide when to invoke the expert, such that the best possible performance can be achieved while requiring a minimum amount of supervision.   In this paper, we propose a principled approach to active object detection, and show that for a rich class of base detectors algorithms, one can derive a natural sequential decision problem for deciding when to invoke expert supervision. We further show that the objective function satisfies adaptive submodularity, which allows us to derive strong performance guarantees for our algorithm. We demonstrate the proposed algorithm on three real-world tasks, including a problem for biodiversity monitoring from micro UAVs in the Sumatra rain forest. Our results show that active detection not only outperforms its passive counterpart; for certain tasks, it also works significantly better than straightforward application of existing active learning techniques. To the best of our knowledge, our approach is the first to rigorously address the active detection problem from both empirical and theoretical perspectives.",http://proceedings.mlr.press/v32/chena14.html,http://proceedings.mlr.press/v32/chena14.pdf,ICML
2577,2014,Memory (and Time) Efficient Sequential Monte Carlo,"Seong-Hwan Jun,         Alexandre Bouchard-Côté","Memory efficiency is an important issue in Sequential Monte Carlo (SMC) algorithms, arising for example in inference of high-dimensional latent variables via Rao-Blackwellized SMC algorithms, where the size of individual particles combined with the required number of particles can stress the main memory. Standard SMC methods have a memory requirement that scales linearly in the number of particles present at all stage of the algorithm.   Our contribution is a simple scheme that makes the memory cost of SMC methods depends on the number of distinct particles that survive resampling.   We show that this difference has a large empirical impact on the quality of the approximation in realistic scenarios, and also—since memory access is generally slow—on the running time.    The method is based on a two pass generation of the particles, which are represented implicitly in the first pass.   We parameterize the accuracy of our algorithm with a memory budget rather than with a fixed number of particles. Our algorithm adaptively selects an optimal number of particle to exploit this fixed memory budget. We show that this adaptation does not interfere with the usual consistency guarantees that come with SMC algorithms.",http://proceedings.mlr.press/v32/jun14.html,http://proceedings.mlr.press/v32/jun14.pdf,ICML
2578,2014,Concentration in unbounded metric spaces and algorithmic stability,Aryeh Kontorovich,"We prove an extension of McDiarmid’s inequality for metric spaces with unbounded diameter.  To this end, we introduce the notion of the \em subgaussian diameter,  which is a distribution-dependent refinement of the metric diameter.  Our technique provides an alternative approach to that of Kutin and Niyogi’s   method of weakly difference-bounded functions, and yields nontrivial,   dimension-free results in some interesting cases where the former does not.  As an application, we give apparently the first generalization bound in the  algorithmic stability setting that holds for unbounded loss functions.  This yields a novel risk bound for some regularized metric regression algorithms.  We give two extensions of the basic concentration result.  The first enables one to replace the independence assumption by appropriate strong mixing.  The second generalizes the subgaussian technique to other Orlicz norms.",http://proceedings.mlr.press/v32/kontorovicha14.html,http://proceedings.mlr.press/v32/kontorovicha14.pdf,ICML
2579,2014,Least Squares Revisited: Scalable Approaches for Multi-class Prediction,"Alekh Agarwal,         Sham Kakade,         Nikos Karampatziakis,         Le Song,         Gregory Valiant","This work provides simple algorithms for multi-class (and multi-label) prediction in settings where both the number of examples n and the data dimension d are relatively    large. These robust and parameter free algorithms are essentially    iterative least-squares updates and very versatile both in theory and in practice. On the theoretical front, we present several variants with convergence guarantees. Owing to their effective use of second-order structure, these algorithms are substantially better than first-order methods in many practical scenarios. On the empirical side, we show how to scale our approach to high dimensional datasets, achieving dramatic computational speedups over popular optimization packages such as Liblinear and Vowpal Wabbit on standard datasets (MNIST and CIFAR-10), while attaining state-of-the-art accuracies.",http://proceedings.mlr.press/v32/agarwala14.html,http://proceedings.mlr.press/v32/agarwala14.pdf,ICML
2580,2014,Signal recovery from Pooling Representations,"Joan Bruna Estrach,         Arthur Szlam,         Yann LeCun","Pooling operators construct non-linear representations  by cascading a redundant linear transform, followed by   a point-wise nonlinearity and a local aggregation, typically  implemented with a \ell_p norm.   Their efficiency in recognition architectures is based   on their ability to locally contract the input space,   but also on their capacity to retain as much stable information   as possible.  We address this latter question by computing the upper and   lower Lipschitz bounds of \ell_p pooling operators for p=1, 2, ∞as well as their half-rectified equivalents, which give  sufficient conditions for the design of invertible pooling layers.  Numerical experiments on MNIST and image patches confirm that  pooling layers can be inverted with phase recovery algorithms. Moreover,  the regularity of the inverse pooling, controlled by the lower Lipschitz constant,   is empirically verified with a nearest neighbor regression.",http://proceedings.mlr.press/v32/estrach14.html,http://proceedings.mlr.press/v32/estrach14.pdf,ICML
2581,2014,Robust Inverse Covariance Estimation under Noisy Measurements,"Jun-Kun Wang,         Shou-de Lin","This paper proposes a robust method to estimate the inverse covariance under noisy measurements. The method is based on the estimation of each column in the inverse covariance matrix independently via robust regression, which enables parallelization. Different from previous linear programming based methods that cannot guarantee a positive semi-definite covariance matrix, our method adjusts the learned matrix to satisfy this condition, which further facilitates the tasks of forecasting future values. Experiments on time series prediction and classification under  noisy condition demonstrate the effectiveness of the approach.",http://proceedings.mlr.press/v32/wangf14.html,http://proceedings.mlr.press/v32/wangf14.pdf,ICML
2582,2014,Bayesian Max-margin Multi-Task Learning with Data Augmentation,"Chengtao Li,         Jun Zhu,         Jianfei Chen","Both max-margin and Bayesian methods have been extensively studied in multi-task learning, but have rarely been considered together. We present Bayesian max-margin multi-task learning, which conjoins the two schools of methods, thus allowing the discriminative max-margin methods to enjoy the great flexibility of Bayesian methods on incorporating rich prior information as well as performing nonparametric Bayesian feature learning with the latent dimensionality resolved from data. We develop Gibbs sampling algorithms by exploring data augmentation to deal with the non-smooth hinge loss. For nonparametric models, our algorithms do not need to make mean-field assumptions or truncated approximation. Empirical results demonstrate superior performance than competitors in both multi-task classification and regression.",http://proceedings.mlr.press/v32/lic14.html,http://proceedings.mlr.press/v32/lic14.pdf,ICML
2583,2014,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,"Jeff Donahue,         Yangqing Jia,         Oriol Vinyals,         Judy Hoffman,         Ning Zhang,         Eric Tzeng,         Trevor Darrell","We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks.  Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks.  We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges.  We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges.  We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.",http://proceedings.mlr.press/v32/donahue14.html,http://proceedings.mlr.press/v32/donahue14.pdf,ICML
2584,2014,Near-Optimally Teaching the Crowd to Classify,"Adish Singla,         Ilija Bogunovic,         Gabor Bartok,         Amin Karbasi,         Andreas Krause","How should we present training examples to learners to teach them classification rules? This is a natural problem when training workers for crowdsourcing labeling tasks, and is also motivated by challenges in data-driven online education. We propose a natural stochastic model of the learners, modeling them as randomly switching among hypotheses based on observed feedback. We then develop STRICT, an efficient algorithm for selecting examples to teach to workers. Our solution greedily maximizes a submodular surrogate objective function in order to select examples to show to the learners. We prove that our strategy is competitive with the optimal teaching policy. Moreover, for the special case of linear separators, we prove that an exponential reduction in error probability can be achieved. Our experiments on simulated workers as well as three real image annotation tasks on Amazon Mechanical Turk show the effectiveness of our teaching algorithm.",http://proceedings.mlr.press/v32/singla14.html,http://proceedings.mlr.press/v32/singla14.pdf,ICML
2585,2014,Buffer k-d Trees: Processing Massive Nearest Neighbor Queries on GPUs,"Fabian Gieseke,         Justin Heinermann,         Cosmin Oancea,         Christian Igel","We present a new approach for combining k-d trees and graphics processing units for nearest neighbor search. It is well known that a direct combination of these tools leads to a non-satisfying performance due to conditional computations and suboptimal memory accesses. To alleviate these problems, we propose a variant of the classical k-d tree data structure, called buffer k-d tree, which can be used to reorganize the search. Our experiments show that we can take advantage of both the hierarchical subdivision induced by k-d trees and the huge computational resources provided by today’s many-core devices. We demonstrate the potential of our approach in astronomy, where hundreds of million nearest neighbor queries have to be processed.",http://proceedings.mlr.press/v32/gieseke14.html,http://proceedings.mlr.press/v32/gieseke14.pdf,ICML
2586,2014,Approximation Analysis of Stochastic Gradient Langevin Dynamics  by using Fokker-Planck Equation and Ito Process ,"Issei Sato,         Hiroshi Nakagawa","The stochastic gradient Langevin dynamics (SGLD) algorithm is appealing for large scale Bayesian learning.  The SGLD algorithm seamlessly transit stochastic optimization and Bayesian posterior sampling.  However, solid theories, such as convergence proof, have not been developed.  We theoretically analyze the SGLD algorithm with constant stepsize in two ways.  First, we show  by using the Fokker-Planck equation that the probability distribution of random variables generated by the SGLD algorithm converges to the Bayesian posterior.  Second, we analyze the convergence of the SGLD algorithm by using the Ito process, which reveals that the SGLD algorithm does not strongly but weakly converges.  This result indicates that the SGLD algorithm can be an approximation method for posterior averaging.",http://proceedings.mlr.press/v32/satoa14.html,http://proceedings.mlr.press/v32/satoa14.pdf,ICML
2587,2014,Global graph kernels using geometric embeddings,"Fredrik Johansson,         Vinay Jethava,         Devdatt Dubhashi,         Chiranjib Bhattacharyya","Applications of machine learning methods increasingly deal with graph structured data through kernels. Most existing graph kernels compare graphs in terms of features defined on small subgraphs such as walks, paths or graphlets, adopting an inherently local perspective. However, several interesting properties such as girth or chromatic number are global properties of the graph, and are not captured in local substructures. This paper presents two graph kernels defined on unlabeled graphs which capture global properties of graphs using the celebrated Lovász number and its associated orthonormal representation. We make progress towards theoretical results aiding kernel choice, proving a result about the separation margin of our kernel for classes of graphs. We give empirical results on classification of synthesized graphs with important global properties as well as established benchmark graph datasets, showing that the accuracy of our kernels is better than or competitive to existing graph kernels.",http://proceedings.mlr.press/v32/johansson14.html,http://proceedings.mlr.press/v32/johansson14.pdf,ICML
2588,2014,Alternating Minimization for Mixed Linear Regression,"Xinyang Yi,         Constantine Caramanis,         Sujay Sanghavi","Mixed linear regression involves the recovery of two (or more) unknown vectors from unlabeled linear measurements; that is, where each sample comes from exactly one of the vectors, but we do not know which one. It is a classic problem, and the natural and empirically most popular approach to its solution has been the EM algorithm. As in other settings, this is prone to bad local minima; however, each iteration is very fast (alternating between guessing labels, and solving with those labels).    In this paper we provide a new initialization procedure for EM, based on finding the leading two eigenvectors of an appropriate matrix. We then show that with this, a re-sampled version of the EM algorithm provably converges to the correct vectors, under natural assumptions on the sampling distribution, and with nearly optimal (unimprovable) sample complexity. This provides not only the first characterization of EM’s performance, but also much lower sample complexity as compared to both standard (randomly initialized) EM, and other methods for this problem.",http://proceedings.mlr.press/v32/yia14.html,http://proceedings.mlr.press/v32/yia14.pdf,ICML
2589,2014,Coding for Random Projections,"Ping Li,         Michael Mitzenmacher,         Anshumali Shrivastava","The method of random projections has become  popular for large-scale applications in statistical learning, information retrieval, bio-informatics  and other applications.  Using a well-designed \textbfcoding scheme for the projected data, which determines the number of bits needed for each projected value and how to allocate these bits, can significantly improve the effectiveness of the algorithm, in storage cost as well as computational speed.   In this paper, we study a number of simple coding schemes, focusing on the task of similarity estimation and on an application to training linear classifiers. We demonstrate that \textbfuniform quantization outperforms the standard and influential method \citeProc:Datar_SCG04, which used a \em window-and-random offset scheme. Indeed, we argue that in many cases coding with just a small number of bits suffices.  Furthermore, we also  develop a \textbfnon-uniform 2-bit coding scheme that generally performs well in practice, as confirmed by our experiments on training linear support vector machines (SVM). Proofs and additional experiments  are available at \em arXiv:1308.2218.      In the context of using coded random projections for \textbfapproximate near neighbor search by building hash tables (\em arXiv:1403.8144) \citeReport:RPCodeLSH2014, we show that the step of random offset in \citeProc:Datar_SCG04 is  again not needed  and may hurt the performance. Furthermore, we show that, unless the target similarity level is high, it usually suffices to use only 1 or 2 bits to code each hashed value for this task. Section \refsec_LSH presents some experimental results for LSH.",http://proceedings.mlr.press/v32/lie14.html,http://proceedings.mlr.press/v32/lie14.pdf,ICML
2590,2014,Spectral Bandits for Smooth Graph Functions,"Michal Valko,         Remi Munos,         Branislav Kveton,         Tomáš Kocák","Smooth functions on graphs have wide applications in manifold and semi-supervised learning. In this paper, we study a bandit problem where the payoffs of arms are smooth on a graph. This framework is suitable for solving online learning problems that involve graphs, such as content-based recommendation. In this problem, each item we can recommend is a node and its expected rating is similar to its neighbors. The goal is to recommend items that have high expected ratings. We aim for the algorithms where the cumulative regret with respect to the optimal policy would not scale poorly with the number of nodes. In particular, we introduce the notion of an effective dimension, which is small in real-world graphs, and propose two algorithms for solving our problem that scale linearly and sublinearly in this dimension. Our experiments on real-world content recommendation problem show that a good estimator of user preferences for thousands of items can be learned from just tens of nodes evaluations.",http://proceedings.mlr.press/v32/valko14.html,http://proceedings.mlr.press/v32/valko14.pdf,ICML
2591,2014,Programming by Feedback,"Marc Schoenauer,         Riad Akrour,         Michele Sebag,         Jean-Christophe Souplet","This paper advocates a new ML-based programming framework, called Programming by Feedback (PF), which involves a sequence of interactions between the active computer and the user. The latter only provides preference judgments on pairs of solutions supplied by the active computer. The active computer involves two components: the learning component estimates the user’s utility function and accounts for the user’s  (possibly limited) competence; the optimization component explores the search space and returns the most appropriate candidate solution. A proof of principle of the approach is proposed, showing that PF requires a handful of interactions in order to solve some discrete and continuous benchmark problems.",http://proceedings.mlr.press/v32/schoenauer14.html,http://proceedings.mlr.press/v32/schoenauer14.pdf,ICML
2592,2014,Deep Generative Stochastic Networks Trainable by Backprop,"Yoshua Bengio,         Eric Laufer,         Guillaume Alain,         Jason Yosinski","We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution.  Because the transition distribution is a conditional distribution generally involving a small move, it has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. The theorems provided here generalize recent work on the probabilistic interpretation of denoising autoencoders and provide an interesting justification for dependency networks and generalized pseudolikelihood (along with defining an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent). GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest.  Successful experiments are conducted, validating these theoretical results, on two image datasets and with a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with backprop, without the need for layerwise pretraining.",http://proceedings.mlr.press/v32/bengio14.html,http://proceedings.mlr.press/v32/bengio14.pdf,ICML
2593,2014,Discrete Chebyshev Classifiers,"Elad Eban,         Elad Mezuman,         Amir Globerson","In large scale learning problems it is often easy to collect simple statistics of the data, but hard or impractical to store all the original data. A key question in this setting is how to construct classifiers based on such partial information. One traditional approach to the problem has been to use maximum entropy arguments to induce a complete distribution on variables from statistics. However, this approach essentially makes conditional independence assumptions about the distribution, and furthermore does not optimize prediction loss. Here we present a framework for discriminative learning given a set of statistics. Specifically, we address the case where all variables are discrete and we have access to various marginals.  Our approach minimizes the worst case hinge loss in this case, which upper bounds the generalization error. We show that for certain sets of statistics the problem is tractable, and in the general case can be approximated using MAP LP relaxations. Empirical results show that the method is competitive with other approaches that use the same input.",http://proceedings.mlr.press/v32/eban14.html,http://proceedings.mlr.press/v32/eban14.pdf,ICML
2594,2014,Covering Number for Efficient Heuristic-based POMDP Planning,"Zongzhang Zhang,         David Hsu,         Wee Sun Lee","The difficulty of POMDP planning depends on the size of the search space involved. Heuristics are often used to reduce the search space size and improve computational efficiency; however, there are few theoretical bounds on their effectiveness.  In this paper, we use the covering number to characterize the size of the search space reachable under heuristics and  connect the complexity of POMDP planning to the effectiveness of heuristics. With insights from the theoretical analysis, we have developed  a practical POMDP algorithm, Packing-Guided Value Iteration (PGVI). Empirically, PGVI is competitive with the state-of-the-art point-based POMDP algorithms on 65 small benchmark problems and outperforms them on 4 larger problems.",http://proceedings.mlr.press/v32/zhanga14.html,http://proceedings.mlr.press/v32/zhanga14.pdf,ICML
2595,2014,Local algorithms for interactive clustering,"Pranjal Awasthi,         Maria Balcan,         Konstantin Voevodski",We study the design of interactive clustering algorithms  for data sets satisfying natural stability  assumptions. Our algorithms start with any initial clustering  and only make local changes in each step; both are desirable features in many applications.  We show that in this constrained setting one can still design provably efficient algorithms that produce  accurate clusterings.  We also show that our algorithms perform well on real-world data.,http://proceedings.mlr.press/v32/awasthi14.html,http://proceedings.mlr.press/v32/awasthi14.pdf,ICML
2596,2014,Efficient Label Propagation,"Yasuhiro Fujiwara,         Go Irie","Label propagation is a popular graph-based semi-supervised learning framework.   So as to obtain the optimal labeling scores, the label propagation algorithm requires an inverse matrix which incurs the high computational cost of O(n^3+cn^2), where n and c are the numbers of data points and labels, respectively.   This paper proposes an efficient label propagation algorithm that guarantees exactly the same labeling results as those yielded by optimal labeling scores.   The key to our approach is to iteratively compute lower and upper bounds of labeling scores to prune unnecessary score computations.   This idea significantly reduces the computational cost to O(cnt) where t is the average number of iterations for each label and t << n in practice.   Experiments demonstrate the significant superiority of our algorithm over existing label propagation methods.",http://proceedings.mlr.press/v32/fujiwara14.html,http://proceedings.mlr.press/v32/fujiwara14.pdf,ICML
2597,2014,Active Transfer Learning under Model Shift,"Xuezhi Wang,         Tzu-Kuo Huang,         Jeff Schneider","Transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source task) but only very limited training data for a second task (the target task) that is similar but not identical to the first.  These algorithms use varying assumptions about the similarity between the tasks to carry information from the source to the target task.  Common assumptions are that only certain specific marginal or conditional distributions have changed while all else remains the same. Alternatively, if one has only the target task, but also has the ability to choose a limited amount of additional training data to collect, then active learning algorithms are used to make choices which will most improve performance on the target task. These algorithms may be combined into active transfer learning, but previous efforts have had to apply the two methods in sequence or use restrictive transfer assumptions.    We propose two transfer learning algorithms that allow changes in all marginal and conditional distributions but assume the changes are smooth in order to achieve transfer between the tasks.  We then propose an active learning algorithm for the second method that yields a combined active transfer learning algorithm.  We demonstrate the algorithms on synthetic functions and a real-world task on estimating the yield of vineyards from images of the grapes.",http://proceedings.mlr.press/v32/wangi14.html,http://proceedings.mlr.press/v32/wangi14.pdf,ICML
2598,2014,Stochastic Gradient Hamiltonian Monte Carlo,"Tianqi Chen,         Emily Fox,         Carlos Guestrin","Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals.  The popularity of such methods has grown significantly in recent years.  However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system-such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data.  In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad.  To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution.  Results on simulated data validate our theory.  We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.",http://proceedings.mlr.press/v32/cheni14.html,http://proceedings.mlr.press/v32/cheni14.pdf,ICML
2599,2014,A Bayesian Framework for Online Classifier Ensemble,"Qinxun Bai,         Henry Lam,         Stan Sclaroff","We propose a Bayesian framework for recursively estimating the classifier weights in online learning of a classifier ensemble. In contrast with past methods, such as stochastic gradient descent or online boosting, our framework estimates the weights in terms of evolving posterior distributions. For a specified class of loss functions, we show that it is possible to formulate a suitably defined likelihood function and hence use the posterior distribution as an approximation to the global empirical loss minimizer. If the stream of training data is sampled from a stationary process, we can also show that our framework admits a superior rate of convergence to the expected loss minimizer than is possible with standard stochastic gradient descent. In experiments with real-world datasets, our formulation often performs better than online boosting algorithms.",http://proceedings.mlr.press/v32/bai14.html,http://proceedings.mlr.press/v32/bai14.pdf,ICML
2600,2014,Latent Variable Copula Inference for Bundle Pricing from Retail Transaction Data,"Benjamin Letham,         Wei Sun,         Anshul Sheopuri","Bundle discounts are used by retailers in many industries. Optimal bundle pricing requires learning the joint distribution of consumer valuations for the items in the bundle, that is, how much they are willing to pay for each of the items. We suppose that a retailer has sales transaction data, and the corresponding consumer valuations are latent variables. We develop a statistically consistent and computationally tractable inference procedure for fitting a copula model over correlated valuations, using only sales transaction data for the individual items. Simulations and data experiments demonstrate consistency, scalability, and the importance of incorporating correlations in the joint distribution.",http://proceedings.mlr.press/v32/letham14.html,http://proceedings.mlr.press/v32/letham14.pdf,ICML
2601,2014,Wasserstein Propagation for Semi-Supervised Learning,"Justin Solomon,         Raif Rustamov,         Leonidas Guibas,         Adrian Butscher","Probability distributions and histograms are natural representations for product ratings, traffic measurements, and other data considered in many machine learning applications.  Thus, this paper introduces a technique for graph-based semi-supervised learning of histograms, derived from the theory of optimal transportation. Our method has several properties making it suitable for this application; in particular, its behavior can be characterized by the moments and shapes of the histograms at the labeled nodes. In addition, it can be used for histograms on non-standard domains like circles, revealing a strategy for manifold-valued semi-supervised learning. We also extend this technique to related problems such as smoothing distributions on graph nodes.",http://proceedings.mlr.press/v32/solomon14.html,http://proceedings.mlr.press/v32/solomon14.pdf,ICML
2602,2014,Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search,"Anshumali Shrivastava,         Ping Li","The query complexity of \em locality sensitive hashing (LSH) based similarity search is dominated by the number of hash evaluations, and this number grows with the data size \citeProc:Indyk_STOC98. In industrial applications such as search where the data are often high-dimensional and binary (e.g., text n-grams),  \em minwise hashing is widely adopted, which requires applying a large number  of permutations on the data. This is  costly in computation and energy-consumption.    In this paper, we propose a  hashing technique which generates all the necessary hash evaluations needed for similarity search, using  one single permutation.  The heart of the proposed hash function is a  “rotation” scheme which densifies the sparse sketches of \em one permutation hashing \citeProc:Li_Owen_Zhang_NIPS12 in an unbiased fashion thereby maintaining the LSH property. This makes the obtained sketches suitable for hash table construction. This idea of rotation presented in this paper could be of independent  interest  for densifying  other types of sparse sketches.     Using our proposed hashing method, the  query time of a (K,L)-parameterized LSH is reduced from  the typical O(dKL) complexity to merely O(KL+dL), where d is the  number of nonzeros of the data vector, K is the number of hashes in each hash table, and L is the number of hash tables.  Our experimental evaluation on real data confirms that the proposed scheme significantly reduces the query processing time over minwise hashing without loss in retrieval accuracies.",http://proceedings.mlr.press/v32/shrivastava14.html,http://proceedings.mlr.press/v32/shrivastava14.pdf,ICML
2603,2014,Linear Programming for Large-Scale Markov Decision Problems,"Alan Malek,         Yasin Abbasi-Yadkori,         Peter Bartlett","We consider the problem of controlling a Markov decision  process (MDP) with a large state space, so as to minimize average cost.  Since it is intractable to compete with the optimal policy for large  scale problems, we pursue the more modest goal of competing with a  low-dimensional family of policies. We use the dual linear programming  formulation of the MDP average cost problem, in which the variable is  a stationary distribution over state-action pairs, and we consider a  neighborhood of a low-dimensional subset of the set of stationary  distributions (defined in terms of state-action features) as  the comparison class.  We propose two techniques, one based on stochastic convex optimization,  and one based on constraint sampling. In both cases, we give bounds  that show that the performance of our algorithms approaches the best  achievable by any policy in the comparison class. Most importantly,  these results depend on the size of the comparison class, but not  on the size of the state space.  Preliminary experiments  show the effectiveness of the proposed algorithms in a queuing  application.",http://proceedings.mlr.press/v32/malek14.html,http://proceedings.mlr.press/v32/malek14.pdf,ICML
2604,2014,Efficient Algorithms for Robust One-bit Compressive Sensing,"Lijun Zhang,         Jinfeng Yi,         Rong Jin","While the conventional compressive sensing assumes measurements of infinite precision, one-bit compressive sensing considers an extreme setting where each measurement is quantized to just a single bit. In this paper, we study the vector recovery problem from noisy one-bit measurements, and develop two novel algorithms with formal theoretical guarantees. First, we propose a passive algorithm, which is very efficient in the sense it only needs to solve a convex optimization problem that has a closed-form solution. Despite the apparent simplicity, our theoretical analysis reveals that the proposed algorithm can recover both the exactly sparse and the approximately sparse vectors. In particular, for a sparse vector with s nonzero elements, the sample complexity is O(s \log n/ε^2), where n is the dimensionality and εis the recovery error. This result improves significantly over the previously best known sample complexity in the noisy setting, which is O(s \log n/ε^4). Second, in the case that the noise model is known, we develop an adaptive algorithm based on the principle of active learning. The key idea is to solicit the sign information only when it cannot be inferred from the current estimator. Compared with the passive algorithm, the adaptive one has a lower sample complexity if a high-precision solution is desired.",http://proceedings.mlr.press/v32/zhangc14.html,http://proceedings.mlr.press/v32/zhangc14.pdf,ICML
2605,2014,"Pursuit-Evasion Without Regret, with an Application to Trading","Lili Dworkin,         Michael Kearns,         Yuriy Nevmyvaka","We propose a state-based variant of the classical online learning problem of tracking the best expert. In our setting, the actions of the algorithm and experts correspond to local moves through a continuous and bounded state space. At each step, Nature chooses payoffs as a function of each player’s current position and action. Our model therefore integrates the problem of prediction with expert advice with the stateful formalisms of reinforcement learning. Traditional no-regret learning approaches no longer apply, but we propose a simple algorithm that provably achieves no-regret when the state space is any convex Euclidean region. Our algorithm combines techniques from online learning with results from the literature on pursuit-evasion games. We describe a quantitative trading application in which the convex region captures inventory risk constraints, and local moves limit market impact. Using historical market data, we show experimentally that our algorithm has a strong advantage over classic no-regret approaches.",http://proceedings.mlr.press/v32/dworkin14.html,http://proceedings.mlr.press/v32/dworkin14.pdf,ICML
2606,2014,Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization,"Xiaotong Yuan,         Ping Li,         Tong Zhang","Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantees and impressive numerical performance. In this paper, we generalize HTP from compressed sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard truncation step with or without debiasing. We prove that our method enjoys the strong guarantees analogous to HTP in terms of rate of convergence and parameter estimation accuracy. Numerical evidences show that our method is superior to the state-of-the-art greedy selection methods when applied to learning tasks of sparse logistic regression and sparse support vector machines.",http://proceedings.mlr.press/v32/yuan14.html,http://proceedings.mlr.press/v32/yuan14.pdf,ICML
2607,2014,Von Mises-Fisher Clustering Models,"Siddharth Gopal,         Yiming Yang","This paper proposes a suite of models for clustering high-dimensional data on a unit sphere based on Von Mises-Fisher (vMF) distribution and for discovering more intuitive clusters than existing approaches. The proposed models include  a) A Bayesian formulation of vMF mixture that enables information sharing among clusters,  b) a Hierarchical vMF mixture that provides multi-scale shrinkage and tree structured view of the data and c) a Temporal vMF mixture that captures evolution of clusters in temporal data.  For posterior inference, we develop fast variational methods  as well as collapsed Gibbs sampling techniques for all three models. Our experiments on six datasets provide strong empirical support in favour of vMF based clustering models over other popular tools such as K-means, Multinomial Mixtures and Latent Dirichlet Allocation.",http://proceedings.mlr.press/v32/gopal14.html,http://proceedings.mlr.press/v32/gopal14.pdf,ICML
2608,2014,Rank-One Matrix Pursuit for Matrix Completion,"Zheng Wang,         Ming-Jun Lai,         Zhaosong Lu,         Wei Fan,         Hasan Davulcu,         Jieping Ye","Low rank matrix completion has been applied successfully in a wide range of machine learning applications, such as collaborative filtering, image inpainting and Microarray data imputation. However, many existing algorithms are not scalable to large-scale problems, as they involve computing singular value decomposition. In this paper, we present an efficient and scalable algorithm for matrix completion. The key idea is to extend the well-known orthogonal matching pursuit from the vector case to the matrix case. In each iteration, we pursue a rank-one matrix basis generated by the top singular vector pair of the current approximation residual and update the weights for all rank-one matrices obtained up to the current iteration. We further propose a novel weight updating rule to reduce the time and storage complexity, making the proposed algorithm scalable to large matrices. We establish the linear convergence of the proposed algorithm. The fast convergence is achieved due to the proposed construction of matrix bases and the estimation of the weights. We empirically evaluate the proposed algorithm on many real-world large scale datasets. Results show that our algorithm is much more efficient than state-of-the-art matrix completion algorithms while achieving similar or better prediction performance.",http://proceedings.mlr.press/v32/wanga14.html,http://proceedings.mlr.press/v32/wanga14.pdf,ICML
2609,2014,Doubly Stochastic Variational Bayes for non-Conjugate Inference,"Michalis Titsias,         Miguel Lázaro-Gredilla",We propose a simple and effective variational inference algorithm based on stochastic optimisation   that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic approximation and allows for efficient use of gradient information from the model joint density. We demonstrate these properties using illustrative examples as well as in challenging and diverse Bayesian inference   problems such as variable selection in logistic regression and fully   Bayesian inference over kernel hyperparameters in Gaussian process regression.,http://proceedings.mlr.press/v32/titsias14.html,http://proceedings.mlr.press/v32/titsias14.pdf,ICML
2610,2014,Sample Efficient Reinforcement Learning with Gaussian Processes,"Robert Grande,         Thomas Walsh,         Jonathan How","This paper derives sample complexity results for using Gaussian Processes (GPs) in both model-based and model-free reinforcement learning (RL). We show that GPs are KWIK learnable, proving for the first time that a model-based RL approach using GPs, GP-Rmax, is sample efficient (PAC-MDP). However, we then show that previous approaches to model-free RL using GPs take an exponential number of steps to find an optimal policy, and are therefore not sample efficient. The third and main contribution is the introduction of a model-free RL algorithm using GPs, DGPQ, which is sample efficient and, in contrast to model-based algorithms, capable of acting in real time, as demonstrated on a five-dimensional aircraft simulator.",http://proceedings.mlr.press/v32/grande14.html,http://proceedings.mlr.press/v32/grande14.pdf,ICML
2611,2014,Efficient Learning of Mahalanobis Metrics for Ranking,"Daryl Lim,         Gert Lanckriet","We develop an efficient algorithm to learn a Mahalanobis distance metric by directly optimizing a ranking loss.  Our approach focuses on optimizing the top of the induced rankings, which is desirable in tasks such as visualization and nearest-neighbor retrieval.  We further develop and justify a simple technique to reduce training time significantly with minimal impact on performance.   Our proposed method significantly outperforms alternative methods on several real-world tasks, and can scale to large and high-dimensional data.",http://proceedings.mlr.press/v32/lim14.html,http://proceedings.mlr.press/v32/lim14.pdf,ICML
2612,2014,Local Ordinal Embedding,"Yoshikazu Terada,         Ulrike Luxburg","We study the problem of ordinal embedding: given a set of ordinal constraints of the form distance(i,j) < distance(k,l) for some_quadruples (i,j,k,l) of indices, the goal is to construct a point configuration \hat\bmx_1, ..., \hat\bmx_n in \R^p that preserves these constraints as well as possible. Our first contribution is to suggest a simple new algorithm for this problem, Soft Ordinal Embedding. The key feature of the algorithm is that it recovers not only the ordinal constraints, but even the density structure of the underlying data set. As our second contribution we prove that in the large sample limit it is enough to know “local ordinal information” in order to perfectly reconstruct a given point configuration. This leads to our Local Ordinal Embedding algorithm, which can also be used for graph drawing.",http://proceedings.mlr.press/v32/terada14.html,http://proceedings.mlr.press/v32/terada14.pdf,ICML
2613,2014,Making the Most of Bag of Words: Sentence Regularization with Alternating Direction Method of Multipliers,"Dani Yogatama,         Noah Smith","In many high-dimensional learning problems, only some parts of an observation are important to the prediction task; for example, the cues to correctly categorizing a document may lie in a handful of its sentences. We introduce a learning algorithm that exploits this intuition by encoding it in a regularizer.  Specifically, we apply the sparse overlapping group lasso with one group for every bundle of features occurring together in a training-data sentence, leading to thousands to millions of overlapping groups. We show how to efficiently solve the resulting optimization challenge using the alternating directions method of multipliers.  We find that the resulting method significantly outperforms competitive baselines (standard ridge, lasso, and elastic net regularizers) on a suite of real-world text categorization problems.",http://proceedings.mlr.press/v32/yogatama14.html,http://proceedings.mlr.press/v32/yogatama14.pdf,ICML
2614,2014,A Deep and Tractable Density Estimator,"Benigno Uria,         Iain Murray,         Hugo Larochelle","The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data  dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance.",http://proceedings.mlr.press/v32/uria14.html,http://proceedings.mlr.press/v32/uria14.pdf,ICML
2615,2014,Maximum Mean Discrepancy for Class Ratio Estimation: Convergence Bounds and Kernel Selection,"Arun Iyer,         Saketha Nath,         Sunita Sarawagi","In recent times, many real world applications have emerged that require estimates of class ratios in an unlabeled instance collection as opposed to labels of individual instances in the collection.  In this paper we investigate the use of maximum mean discrepancy (MMD) in a reproducing kernel Hilbert space (RKHS) for estimating such ratios. First, we theoretically analyze the MMD-based estimates. Our analysis establishes that, under some mild conditions, the estimate is statistically consistent. More importantly, it provides an upper bound on the error in the estimate in terms of intuitive geometric quantities like class separation and data spread. Next, we use the insights obtained from the theoretical analysis, to propose a novel convex formulation that automatically learns the kernel to be employed in the MMD-based estimation. We design an efficient cutting plane algorithm for solving this formulation.  Finally, we empirically compare our estimator with several existing methods, and show significantly improved performance under varying datasets, class ratios, and training sizes.",http://proceedings.mlr.press/v32/iyer14.html,http://proceedings.mlr.press/v32/iyer14.pdf,ICML
2616,2014,Marginal Structured SVM with Hidden Variables,"Wei Ping,         Qiang Liu,         Alex Ihler","In this work, we propose the marginal structured SVM (MSSVM) for structured prediction with hidden variables. MSSVM properly accounts for the uncertainty of hidden variables, and can significantly outperform the previously proposed latent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-art methods, especially when that uncertainty is large. Our method also results in a smoother objective function, making gradient-based optimization of MSSVMs converge significantly faster than for LSSVMs. We also show that our method consistently outperforms hidden conditional random fields (HCRFs; Quattoni et al. (2007)) on both simulated and real-world datasets. Furthermore, we propose a unified framework that includes both our and several other existing methods as special cases, and provides insights into the comparison of different models in practice.",http://proceedings.mlr.press/v32/ping14.html,http://proceedings.mlr.press/v32/ping14.pdf,ICML
2617,2014,Preserving Modes and Messages via Diverse Particle Selection,"Jason Pacheco,         Silvia Zuffi,         Michael Black,         Erik Sudderth","In applications of graphical models arising in domains such as computer vision and signal processing, we often seek the most likely configurations of high-dimensional, continuous variables.  We develop a particle-based max-product algorithm which maintains a diverse set of posterior mode hypotheses, and is robust to initialization.  At each iteration, the set of hypotheses at each node is augmented via stochastic proposals, and then reduced via an efficient selection algorithm.  The integer program underlying our optimization-based particle selection minimizes errors in subsequent max-product message updates.  This objective automatically encourages diversity in the maintained hypotheses, without requiring tuning of application-specific distances among hypotheses.  By avoiding the stochastic resampling steps underlying particle sum-product algorithms, we also avoid common degeneracies where particles collapse onto a single hypothesis.  Our approach significantly outperforms previous particle-based algorithms in experiments focusing on the estimation of human pose from single images.",http://proceedings.mlr.press/v32/pacheco14.html,http://proceedings.mlr.press/v32/pacheco14.pdf,ICML
2618,2014,Condensed Filter Tree for Cost-Sensitive Multi-Label Classification,"Chun-Liang Li,         Hsuan-Tien Lin","Different real-world applications of multi-label classification often demand different evaluation criteria. We formalize this demand with a general setup, cost-sensitive multi-label classification (CSMLC), which takes  the evaluation criteria into account during learning. Nevertheless, most existing algorithms can only focus on optimizing a few specific evaluation criteria, and cannot systematically deal with different ones. In this paper, we propose a novel algorithm, called condensed filter tree (CFT), for optimizing any criteria in CSMLC. CFT is derived from reducing CSMLC to the famous filter tree algorithm for cost-sensitive multi-class classification via constructing the label powerset. We successfully cope with the difficulty of having exponentially many extended-classes within the powerset for representation, training and prediction by carefully designing the tree structure and focusing on the key nodes. Experimental results across many real-world datasets validate that CFT is competitive with special purpose algorithms on special criteria and reaches better performance on general criteria.",http://proceedings.mlr.press/v32/lia14.html,http://proceedings.mlr.press/v32/lia14.pdf,ICML
2619,2014,Online Bayesian Passive-Aggressive Learning,"Tianlin Shi,         Jun Zhu","Online Passive-Aggressive (PA) learning is an effective framework for performing max-margin online learning. But the deterministic formulation and estimated single large-margin model could limit its capability in discovering descriptive structures underlying complex data. This paper presents online Bayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA and extends naturally to incorporate latent variables and perform nonparametric Bayesian inference, thus providing great flexibility for explorative analysis. We apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric methods to resolve the number of topics. Experimental results on real datasets show that our approaches significantly improve time efficiency while maintaining comparable results with the batch counterparts.",http://proceedings.mlr.press/v32/shi14.html,http://proceedings.mlr.press/v32/shi14.pdf,ICML
2620,2014,True Online TD(lambda),"Harm Seijen,         Rich Sutton","TD(lambda) is a core algorithm of modern reinforcement learning. Its appeal comes from its equivalence to a clear and conceptually simple forward view, and the fact that it can be implemented online in an inexpensive manner. However, the equivalence between TD(lambda) and the forward view is exact only for the off-line version of the algorithm (in which updates are made only at the end of each episode). In the online version of TD(lambda) (in which updates are made at each step, which generally performs better and is always used in applications) the match to the forward view is only approximate. In a sense this is unavoidable for the conventional forward view, as it itself presumes that the estimates are unchanging during an episode. In this paper we introduce a new forward view that takes into account the possibility of changing estimates and a new variant of TD(lambda) that exactly achieves it. Our algorithm uses a new form of eligibility trace similar to but different from conventional accumulating and replacing traces. The overall computational complexity is the same as TD(lambda), even when using function approximation. In our empirical comparisons, our algorithm outperformed TD(lambda) in all of its variations. It seems, by adhering more truly to the original goal of TD(lambda)—matching an intuitively clear forward view even in the online case—that we have found a new algorithm that simply improves on classical TD(lambda).",http://proceedings.mlr.press/v32/seijen14.html,http://proceedings.mlr.press/v32/seijen14.pdf,ICML
2621,2014,Optimal PAC Multiple Arm Identification with Applications to Crowdsourcing,"Yuan Zhou,         Xi Chen,         Jian Li","We study the problem of selecting K arms with the highest expected rewards in a stochastic N-armed bandit game.  Instead of using existing evaluation metrics  (e.g.,  misidentification probability or the metric in EXPLORE-K), we propose to use the aggregate regret, which is defined as the gap between the average reward of the optimal solution and that of our solution. Besides being a natural metric by itself, we argue that in many applications, such as our motivating example from crowdsourcing, the aggregate regret bound is more suitable. We propose a new PAC algorithm, which,  with  probability at least 1-δ, identifies a set of K arms with regret at most ε. We provide the sample complexity bound of our algorithm. To complement, we establish the  lower bound and show that the sample complexity of our algorithm matches the lower bound. Finally, we report experimental results on both synthetic and real data sets, which demonstrates the superior performance of the proposed algorithm.",http://proceedings.mlr.press/v32/zhoub14.html,http://proceedings.mlr.press/v32/zhoub14.pdf,ICML
2622,2014,Stochastic Neighbor Compression,"Matt Kusner,         Stephen Tyree,         Kilian Weinberger,         Kunal Agrawal","We present Stochastic Neighborhood Compression (SNC), an algorithm to compress a dataset for the purpose of k-nearest neighbor (kNN) classification. Given training data, SNC learns a much smaller synthetic data set, that minimizes the stochastic 1-nearest neighbor classification error on the training data. This approach has several appealing properties: due to its small size, the compressed set speeds up kNN testing drastically (up to several orders of magnitude, in our experiments); it makes the kNN classifier substantially more robust to label noise; on 4 of 7 data sets it yields lower test error than kNN on the entire training set, even at compression ratios as low as 2%; finally, the SNC compression leads to impressive speed ups over kNN even when kNN and SNC are both used with ball-tree data structures, hashing, and LMNN dimensionality reduction, demonstrating that it is complementary to existing state-of-the-art algorithms to speed up kNN classification and leads to substantial further improvements.",http://proceedings.mlr.press/v32/kusner14.html,http://proceedings.mlr.press/v32/kusner14.pdf,ICML
2623,2014,A PAC-Bayesian bound for Lifelong Learning,"Anastasia Pentina,         Christoph Lampert","Transfer learning has received a lot of attention in the machine learning community over the last years, and several effective algorithms have been developed. However, relatively little is known about their theoretical properties, especially in the setting of lifelong learning, where the goal is to transfer information to tasks for which no data have been observed so far.     In this work we study lifelong learning from a theoretical perspective. Our main result is a PAC-Bayesian generalization bound that offers a unified view on existing paradigms for transfer learning, such as the transfer of parameters or the transfer of low-dimensional representations. We also use the bound to derive two principled lifelong learning algorithms, and we show that these yield results comparable with existing methods.",http://proceedings.mlr.press/v32/pentina14.html,http://proceedings.mlr.press/v32/pentina14.pdf,ICML
2624,2014,Agnostic Bayesian Learning of Ensembles,"Alexandre Lacoste,         Mario Marchand,         François Laviolette,         Hugo Larochelle","We propose a method for producing ensembles of predictors based on holdout estimations of their generalization performances. This approach uses a prior directly on the performance of predictors taken from a finite set of candidates and attempts to infer which one is best. Using Bayesian inference, we can thus obtain a posterior that represents our uncertainty about that choice and construct a weighted ensemble of predictors accordingly. This approach has the advantage of not requiring that the predictors be probabilistic themselves, can deal with arbitrary measures of performance and does not assume that the data was actually generated from any of the predictors in the ensemble. Since the problem of finding the best (as opposed to the true) predictor among a class is known as agnostic PAC-learning, we refer to our method as agnostic Bayesian learning. We also propose a method to address the case where the performance estimate is obtained from k-fold cross validation. While being efficient and easily adjustable to any loss function, our experiments confirm that the agnostic Bayes approach is state of the art compared to common baselines such as model selection based on k-fold cross-validation or a linear combination of predictor outputs.",http://proceedings.mlr.press/v32/lacoste14.html,http://proceedings.mlr.press/v32/lacoste14.pdf,ICML
2625,2014,A new Q(lambda) with interim forward view and Monte Carlo equivalence,"Rich Sutton,         Ashique Rupam Mahmood,         Doina Precup,         Hado Hasselt","Q-learning, the most popular of reinforcement learning algorithms, has always included an extension to eligibility traces to enable more rapid learning and improved asymptotic performance on non-Markov problems. The lambda parameter smoothly shifts on-policy algorithms such as TD(lambda) and Sarsa(lambda) from a pure bootstrapping form (lambda=0) to a pure Monte Carlo form (lambda=1). In off-policy algorithms, including Q(lambda), GQ(lambda), and off-policy LSTD(lambda), the lambda parameter is intended to play the same role, but does not; on every exploratory action these algorithms bootstrap regardless of the value of lambda, and as a result they fail to approximate Monte Carlo learning when lambda=1. It may seem that this is inevitable for any online off-policy algorithm; if updates are made on each step on which the target policy is followed, then how could just the right updates be ‘un-made’ upon deviation from the target policy? In this paper, we introduce a new version of Q(lambda) that does exactly that, without significantly increased algorithmic complexity. En route to our new Q(lambda), we introduce a new derivation technique based on the forward-view/backward-view analysis familiar from TD(lambda) but extended to apply at every time step rather than only at the end of episodes. We apply this technique to derive first a new off-policy version of TD(lambda), called PTD(lambda), and then our new Q(lambda), called PQ(lambda).",http://proceedings.mlr.press/v32/sutton14.html,http://proceedings.mlr.press/v32/sutton14.pdf,ICML
2626,2014,Rectangular Tiling Process,"Masahiro Nakano,         Katsuhiko Ishiguro,         Akisato Kimura,         Takeshi Yamada,         Naonori Ueda","This paper proposes a novel stochastic process that represents the arbitrary rectangular partitioning of an infinite-dimensional matrix as the conditional projective limit. Rectangular partitioning is used in relational data analysis, and is classified into three types: regular grid, hierarchical, and arbitrary. Conventionally, a variety of probabilistic models have been advanced for the first two, including the product of Chinese restaurant processes and the Mondrian process. However, existing models for arbitrary partitioning are too complicated to permit the analysis of the statistical behaviors of models, which places very severe capability limits on relational data analysis. In this paper, we propose a new probabilistic model of arbitrary partitioning called the rectangular tiling process (RTP). Our model has a sound mathematical base in projective systems and infinite extension of conditional probabilities, and is capable of representing partitions of infinite elements as found in ordinary Bayesian nonparametric models.",http://proceedings.mlr.press/v32/nakano14.html,http://proceedings.mlr.press/v32/nakano14.pdf,ICML
2627,2014,Hierarchical Conditional Random Fields for Outlier Detection: An Application to Detecting Epileptogenic Cortical Malformations,"Bilal Ahmed,         Thomas Thesen,         Karen Blackmon,         Yijun Zhao,         Orrin Devinsky,         Ruben Kuzniecky,         Carla Brodley","We cast the problem of detecting and isolating regions of abnormal cortical tissue in the MRIs of epilepsy patients in an image segmentation framework. Employing a multiscale approach we divide the surface images into segments of different sizes and then classify each segment as being an outlier, by comparing it to the same region across controls. The final classification is obtained by fusing the outlier probabilities obtained at multiple scales using a tree-structured hierarchical conditional random field (HCRF). The proposed method correctly detects abnormal regions in 90% of patients whose abnormality was detected via routine visual inspection of their clinical MRI. More importantly, it detects abnormalities in 80% of patients whose abnormality escaped visual inspection by expert radiologists.",http://proceedings.mlr.press/v32/ahmed14.html,http://proceedings.mlr.press/v32/ahmed14.pdf,ICML
2628,2014,Standardized Mutual Information for Clustering Comparisons: One Step Further in Adjustment for Chance,"Simone Romano,         James Bailey,         Vinh Nguyen,         Karin Verspoor","Mutual information is a very popular measure for comparing clusterings. Previous work has shown that it is beneficial to make an adjustment for chance to this measure, by subtracting an expected value and normalizing via an upper bound. This yields the constant baseline property that enhances intuitiveness. In this paper, we argue that a further type of statistical adjustment for the mutual information is also beneficial - an adjustment to correct selection bias. This type of adjustment is useful when carrying out many clustering comparisons, to select one or more preferred clusterings. It reduces the tendency for the mutual information to choose clustering solutions i) with more clusters, or ii) induced on fewer data points, when compared to a reference one. We term our new adjusted measure the *standardized mutual information*. It requires computation of the variance of mutual information under a hypergeometric model of randomness, which is technically challenging. We derive an analytical formula for this variance and analyze its complexity. We then experimentally assess how our new measure can address selection bias and also increase interpretability. We recommend using the standardized mutual information when making multiple clustering comparisons in situations where the number of records is small compared to the number of clusters considered.",http://proceedings.mlr.press/v32/romano14.html,http://proceedings.mlr.press/v32/romano14.pdf,ICML
2629,2014,Saddle Points and Accelerated Perceptron Algorithms,"Adams Wei Yu,         Fatma Kilinc-Karzan,         Jaime Carbonell","In this paper, we consider the problem of finding a linear (binary) classifier or providing a near-infeasibility certificate if there is none. We bring a new perspective to addressing these two problems simultaneously in a single efficient process, by investigating a related Bilinear Saddle Point Problem (BSPP). More specifically, we show that a BSPP-based approach provides either a linear classifier or an ε-infeasibility certificate. We show that the accelerated primal-dual algorithm, Mirror Prox, can be used for this purpose and achieves the best known convergence rate of O(\sqrt\log n\overρ(A)) (O(\sqrt\log n\overε)), which is \emphalmost independent of the problem size, n. Our framework also solves kernelized and conic versions of the problem, with the same rate of convergence. We support our theoretical findings with an empirical study on synthetic and real data, highlighting the efficiency and numerical stability of our algorithms, especially on  large-scale instances.",http://proceedings.mlr.press/v32/yuc14.html,http://proceedings.mlr.press/v32/yuc14.pdf,ICML
2630,2014,Elementary Estimators for Sparse Covariance Matrices and other Structured Moments,"Eunho Yang,         Aurelie Lozano,         Pradeep Ravikumar","We consider the problem of estimating distributional parameters that are expected values of given feature functions. We are interested in recovery under high-dimensional regimes, where the number of variables p is potentially larger than the number of samples n, and where we need to impose structural constraints upon the parameters. In a natural distributional setting for this problem, the feature functions comprise the sufficient statistics of an exponential family, so that the problem would entail estimating structured moments of exponential family distributions. A special case of the above involves estimating the covariance matrix of a random vector, and where the natural distributional setting would correspond to the multivariate Gaussian distribution. Unlike the inverse covariance estimation case, we show that the regularized MLEs for covariance estimation, as well as natural Dantzig variants, are \emphnon-convex, even when the regularization functions themselves are convex; with the same holding for the general structured moment case. We propose a class of elementary convex estimators, that in many cases are available in \emphclosed-form, for estimating general structured moments. We then provide a unified statistical analysis of our class of estimators. Finally, we demonstrate the applicability of our class of estimators on real-world climatology and biology datasets.",http://proceedings.mlr.press/v32/yangd14.html,http://proceedings.mlr.press/v32/yangd14.pdf,ICML
2631,2014,Towards scaling up Markov chain Monte Carlo: an adaptive subsampling approach ,"Rémi Bardenet,         Arnaud Doucet,         Chris Holmes","Markov chain Monte Carlo (MCMC) methods are often deemed far too computationally intensive to be of any practical use for large datasets. This paper describes a methodology that aims to scale up the Metropolis-Hastings (MH) algorithm in this context. We propose an approximate implementation of the accept/reject step of MH that only requires evaluating the likelihood of a random subset of the data, yet is guaranteed to coincide with the accept/reject step based on the full dataset with a probability superior to a user-specified tolerance level. This adaptive subsampling technique is an alternative to the recent approach developed in (Korattikara et al, ICML’14), and it allows us to establish rigorously that the resulting approximate MH algorithm samples from a perturbed version of the target distribution of interest, whose total variation distance to this very target is controlled explicitly. We explore the benefits and limitations of this scheme on several examples.",http://proceedings.mlr.press/v32/bardenet14.html,http://proceedings.mlr.press/v32/bardenet14.pdf,ICML
2632,2014,GeNGA: A Generalization of Natural Gradient Ascent with Positive and Negative Convergence Results,Philip Thomas,"Natural gradient ascent (NGA) is a popular optimization method that uses a positive definite metric tensor. In many applications the metric tensor is only guaranteed to be positive semidefinite (e.g., when using the Fisher information matrix as the metric tensor), in which case NGA is not applicable. In our first contribution, we derive generalized natural gradient ascent (GeNGA), a generalization of NGA which allows for positive semidefinite non-smooth metric tensors. In our second contribution we show that, in standard settings, GeNGA and NGA can both be divergent. We then establish sufficient conditions to ensure that both achieve various forms of convergence. In our third contribution we show how several reinforcement learning methods that use NGA without positive definite metric tensors can be adapted to properly use GeNGA.",http://proceedings.mlr.press/v32/thomasb14.html,http://proceedings.mlr.press/v32/thomasb14.pdf,ICML
2633,2014,Two-Stage Metric Learning,"Jun Wang,         Ke Sun,         Fei Sha,         Stéphane Marchand-Maillet,         Alexandros Kalousis","In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric which presents unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM.",http://proceedings.mlr.press/v32/wangc14.html,http://proceedings.mlr.press/v32/wangc14.pdf,ICML
2634,2014,The Coherent Loss Function for Classification,"Wenzhuo Yang,         Melvyn Sim,         Huan Xu","A prediction rule in binary classification that aims to achieve the lowest probability of misclassification involves minimizing over a non-convex, 0-1 loss function, which is typically a computationally intractable optimization problem. To address the intractability, previous methods consider minimizing the cumulative loss – the sum of convex surrogates of the 0-1 loss of each sample. In this paper, we revisit this paradigm and develop instead an axiomatic framework by proposing a set of salient properties on functions for binary classification and then propose the coherent loss approach, which is a tractable upper-bound of the empirical classification error over the entire sample set. We show that the proposed approach yields a strictly tighter approximation to the empirical classification error than any convex cumulative loss approach while preserving the convexity of the underlying optimization problem, and this approach for binary classification also has a robustness interpretation which builds a connection to robust SVMs. The experimental results show that our approach outperforms the standard SVM when additional constraints are imposed.",http://proceedings.mlr.press/v32/yanga14.html,http://proceedings.mlr.press/v32/yanga14.pdf,ICML
2635,2014,Reducing Dueling Bandits to Cardinal Bandits,"Nir Ailon,         Zohar Karnin,         Thorsten Joachims","We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem is an online model of learning with ordinal feedback of the form “A is preferred to B” (as opposed to cardinal feedback like “A has value 2.5”), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences. In contrast to existing algorithms for the Dueling Bandits problem, our reductions – named \Doubler, \MultiSbm and \DoubleSbm – provide a generic schema for translating the extensive body of known results about conventional Multi-Armed Bandit algorithms to the Dueling Bandits setting.     For \Doubler and \MultiSbm we prove regret upper bounds in both finite and infinite settings, and conjecture about the performance of \DoubleSbm which empirically outperforms the other two as well as previous algorithms in our experiments.  In addition, we provide the first almost optimal regret bound in terms of second order terms, such as the differences between the values of the arms.",http://proceedings.mlr.press/v32/ailon14.html,http://proceedings.mlr.press/v32/ailon14.pdf,ICML
2636,2014,Efficient Continuous-Time Markov Chain Estimation,"Monir Hajiaghayi,         Bonnie Kirkpatrick,         Liangliang Wang,         Alexandre Bouchard-Côté","Many problems of practical interest rely on Continuous-time Markov chains (CTMCs) defined over combinatorial state spaces, rendering the computation of transition probabilities, and hence probabilistic inference, difficult or impossible with existing methods.  For problems with countably infinite states, where classical methods such as matrix exponentiation are not applicable,  the main alternative has been particle Markov chain Monte Carlo methods imputing both the holding times and sequences of visited states.    We propose a particle-based Monte Carlo approach where the holding times are marginalized analytically.  We demonstrate that in a range of realistic inferential setups, our scheme dramatically reduces the variance of the Monte Carlo approximation and yields more accurate parameter posterior approximations given a fixed computational budget. These experiments are performed on both synthetic and real datasets, drawing from two important examples of CTMCs having combinatorial state spaces: string-valued mutation models in phylogenetics and nucleic acid folding pathways.",http://proceedings.mlr.press/v32/hajiaghayi14.html,http://proceedings.mlr.press/v32/hajiaghayi14.pdf,ICML
2637,2014,"Margins, Kernels and Non-linear Smoothed Perceptrons","Aaditya Ramdas,         Javier Peña","We focus on the problem of finding a non-linear classification function that lies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of view (finding a perfect separator when one exists) and the dual point of view (giving a certificate of non-existence), with special focus on generalizations of two classical schemes - the Perceptron (primal) and Von-Neumann (dual) algorithms.   We cast our problem as one of maximizing the regularized normalized hard-margin (ρ) in an RKHS and use the Representer Theorem to  rephrase it in terms of a Mahalanobis dot-product/semi-norm associated with the kernel’s (normalized and signed) Gram matrix. We derive an accelerated smoothed algorithm with a convergence rate of \tfrac\sqrt \log nρ given n separable points, which is strikingly similar to the classical kernelized Perceptron algorithm whose rate is \tfrac1ρ^2. When no such classifier exists, we prove a version of Gordan’s separation theorem for RKHSs, and give a reinterpretation of negative margins. This allows us to give guarantees for a primal-dual algorithm that halts in \min{\tfrac\sqrt n|ρ|, \tfrac\sqrt nε} iterations with a perfect separator in the RKHS if the primal is feasible or a dual ε-certificate of near-infeasibility.",http://proceedings.mlr.press/v32/ramdas14.html,http://proceedings.mlr.press/v32/ramdas14.pdf,ICML
2638,2014,Modeling Correlated Arrival Events with Latent Semi-Markov Processes,"Wenzhao Lian,         Vinayak Rao,         Brian Eriksson,         Lawrence Carin","The analysis and characterization of correlated point process data has wide applications, ranging from biomedical research to network analysis. In this work, we model such data as generated by a latent collection of continuous-time binary semi-Markov processes, corresponding to external events appearing and disappearing. A continuous-time modeling framework is more appropriate for multichannel point process data than a binning approach requiring time discretization, and we show connections between our model and recent ideas from the discrete-time literature. We describe an efficient MCMC algorithm for posterior inference, and apply our ideas to both synthetic data and a real-world biometrics application.",http://proceedings.mlr.press/v32/lian14.html,http://proceedings.mlr.press/v32/lian14.pdf,ICML
2639,2014,Narrowing the Gap: Random Forests In Theory and In Practice,"Misha Denil,         David Matheson,         Nando De Freitas","Despite widespread interest and practical use, the theoretical properties of random forests are still not well understood. In this paper we contribute to this understanding in two ways. We present a new theoreti- cally tractable variant of random regression forests and prove that our algorithm is con- sistent. We also provide an empirical eval- uation, comparing our algorithm and other theoretically tractable random forest models to the random forest algorithm used in prac- tice. Our experiments provide insight into the relative importance of different simplifi- cations that theoreticians have made to ob- tain tractable models for analysis.",http://proceedings.mlr.press/v32/denil14.html,http://proceedings.mlr.press/v32/denil14.pdf,ICML
2640,2014,Coherent Matrix Completion,"Yudong Chen,         Srinadh Bhojanapalli,         Sujay Sanghavi,         Rachel Ward","Matrix completion concerns the recovery of a low-rank matrix from a subset of its revealed entries, and nuclear norm minimization has emerged as an effective surrogate for this combinatorial problem.  Here, we show that nuclear norm minimization can recover an arbitrary n \times n matrix of rank r from O(nr log^2(n)) revealed entries, provided that revealed entries are drawn proportionally to the local row and column coherences (closely related to leverage scores) of the underlying matrix.  Our results are order-optimal up to logarithmic factors, and extend existing results for nuclear norm minimization which require strong incoherence conditions on the types of matrices that can be recovered, due to assumed uniformly distributed revealed entries.  We further provide extensive numerical evidence that a proposed two-phase sampling algorithm can perform nearly as well as local-coherence sampling and without requiring a priori knowledge of the matrix coherence structure.  Finally, we apply our results to quantify how weighted nuclear norm minimization can improve on unweighted minimization given an arbitrary set of sampled entries.",http://proceedings.mlr.press/v32/chenc14.html,http://proceedings.mlr.press/v32/chenc14.pdf,ICML
2641,2014,On Robustness and Regularization of Structural Support Vector Machines,"Mohamad Ali Torkamani,         Daniel Lowd","Previous analysis of binary SVMs has demonstrated a deep connection between robustness to perturbations over uncertainty sets and regularization of the  weights.  In this paper, we explore the problem of learning robust  models for structured prediction problems.  We first formulate the problem  of learning robust structural SVMs when there are perturbations in  the feature space.  We consider two different classes of uncertainty sets for the perturbations: ellipsoidal uncertainty sets and polyhedral uncertainty sets. In both cases, we show that the robust optimization problem is equivalent to the non-robust formulation with an additional regularizer. For the ellipsoidal uncertainty set, the additional regularizer is based on the dual norm of the norm that constrains the ellipsoidal uncertainty. For the polyhedral uncertainty set, we show that the robust optimization problem is equivalent to adding a linear regularizer in a transformed weight space related to the linear constraints of the polyhedron. We also show that  these constraint sets can be combined and demonstrate a number of  interesting special cases.  This represents the first theoretical  analysis of robust optimization of structural support vector machines. Our experimental results show that our method outperforms the nonrobust structural SVMs on real world data when the test data distributions is drifted from the training data distribution.",http://proceedings.mlr.press/v32/torkamani14.html,http://proceedings.mlr.press/v32/torkamani14.pdf,ICML
2642,2014,Thompson Sampling for Complex Online Problems,"Aditya Gopalan,         Shie Mannor,         Yishay Mansour","We consider stochastic multi-armed bandit problems with complex actions over a set of basic arms, where the decision maker plays a complex action rather than a basic arm in each round. The reward of the complex action is some function of the basic arms’ rewards, and the feedback observed may not necessarily be the reward per-arm. For instance, when the complex actions are subsets of the arms, we may only observe the maximum reward over the chosen subset. Thus, feedback across complex actions may be coupled due to the nature of the reward function. We prove a frequentist regret bound for Thompson sampling in a very general setting involving parameter, action and observation spaces and a likelihood function over them. The bound holds for discretely-supported priors over the parameter space and without additional structural properties such as closed-form posteriors, conjugate prior structure or independence across arms. The regret bound scales logarithmically with time but, more importantly, with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards. As applications, we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms, including the first nontrivial regret bounds for nonlinear MAX reward feedback from subsets. Using particle filters for computing posterior distributions which lack an explicit closed-form, we present numerical results for the performance of Thompson sampling for subset-selection and job scheduling problems.",http://proceedings.mlr.press/v32/gopalan14.html,http://proceedings.mlr.press/v32/gopalan14.pdf,ICML
2643,2014,Near-Optimal Joint Object Matching via Convex Relaxation,"Yuxin Chen,         Leonidas Guibas,         Qixing Huang","Joint object matching aims at aggregating information from a large collection of similar instances (e.g. images, graphs, shapes) to improve the correspondences computed between pairs of objects, typically by exploiting global map compatibility. Despite some practical advances on this problem, from the theoretical point of view, the error-correction ability of existing algorithms are limited by a constant barrier — none of them can provably recover the correct solution when more than a constant fraction of input correspondences are corrupted. Moreover, prior approaches focus mostly on fully similar objects, while it is practically more demanding and realistic to match instances that are only partially similar to each other.      In this paper, we propose an algorithm to jointly match multiple objects that exhibit only partial similarities, where the provided pairwise feature correspondences can be densely corrupted. By encoding a consistent partial map collection into a 0-1 semidefinite matrix, we attempt recovery via a two-step procedure, that is, a spectral technique followed by a parameter-free convex program called MatchLift. Under a natural randomized model, MatchLift exhibits near-optimal error-correction ability, i.e. it guarantees the recovery of the ground-truth maps even when a dominant fraction of the inputs are randomly corrupted. We evaluate the proposed algorithm on various benchmark data sets including synthetic examples and real-world examples, all of which confirm the practical applicability of the proposed algorithm.",http://proceedings.mlr.press/v32/chend14.html,http://proceedings.mlr.press/v32/chend14.pdf,ICML
2644,2014,Hard-Margin Active Linear Regression,"Elad Hazan,         Zohar Karnin","We consider the fundamental problem of linear regression in which the designer can actively choose observations.   This model naturally captures various experiment design settings in medical experiments, ad placement problems and more. Whereas previous literature addresses the soft-margin or mean-square-error variants of the problem, we consider a natural machine learning hard-margin criterion. In this setting, we show that active learning admits significantly better sample complexity bounds than the passive learning counterpart, and give  efficient algorithms that attain near-optimal  bounds.",http://proceedings.mlr.press/v32/hazan14.html,http://proceedings.mlr.press/v32/hazan14.pdf,ICML
2645,2014,Fast Allocation of Gaussian Process Experts,"Trung Nguyen,         Edwin Bonilla","We propose a scalable nonparametric Bayesian regression model based on a mixture of Gaussian process (GP) experts  and the inducing points formalism underpinning sparse GP approximations. Each expert is augmented with a set of inducing points, and the allocation of data points to experts is defined probabilistically based on their proximity to the experts. This allocation mechanism enables a fast variational inference procedure for learning of the inducing inputs and hyperparameters of the experts. When using K experts, our method can  run K^2 times faster and use K^2 times less memory than popular sparse methods such as the FITC approximation. Furthermore, it is easy to parallelize and handles non-stationarity  straightforwardly. Our experiments show that on medium-sized datasets (of around 10^4 training points) it  trains up to 5 times faster than FITC while achieving comparable accuracy. On a large dataset  of 10^5 training points, our method significantly outperforms six  competitive baselines while requiring only a few hours of training.",http://proceedings.mlr.press/v32/nguyena14.html,http://proceedings.mlr.press/v32/nguyena14.pdf,ICML
2646,2014,Multiple Testing under Dependence via Semiparametric Graphical Models,"Jie Liu,         Chunming Zhang,         Elizabeth Burnside,         David Page","It has been shown that graphical models can be used to leverage the dependence in large-scale multiple testing problems with significantly improved performance (Sun & Cai, 2009; Liu et al., 2012). These graphical models are fully parametric and require that we know the parameterization of f1, the density function of the test statistic under the alternative hypothesis. However in practice, f1 is often heterogeneous, and cannot be estimated with a simple parametric distribution. We propose a novel semiparametric approach for multiple testing under dependence, which estimates f1 adaptively. This semiparametric approach exactly generalizes the local FDR procedure (Efron et al., 2001) and connects with the BH procedure (Benjamini & Hochberg, 1995). A variety of simulations show that our semiparametric approach outperforms classical procedures which assume independence and the parametric approaches which capture dependence.",http://proceedings.mlr.press/v32/liue14.html,http://proceedings.mlr.press/v32/liue14.pdf,ICML
2647,2014,The Falling Factorial Basis and Its Statistical Applications,"Yu-Xiang Wang,         Alex Smola,         Ryan Tibshirani","We study a novel spline-like basis, which we name the   \it falling factorial basis, bearing many similarities to the  classic truncated power basis.  The advantage of the falling factorial  basis is that it enables rapid, linear-time computations in basis  matrix multiplication and basis matrix inversion.  The falling  factorial functions are not actually splines, but are close enough  to splines that they provably retain some of the favorable properties  of the latter functions.  We examine their application in two  problems: trend filtering over arbitrary input points, and a  higher-order variant of the two-sample Kolmogorov-Smirnov test.",http://proceedings.mlr.press/v32/wange14.html,http://proceedings.mlr.press/v32/wange14.pdf,ICML
2648,2014,On the convergence of no-regret learning in selfish routing,"Walid Krichene,         Benjamin Drighès,         Alexandre Bayen","We study the repeated, non-atomic routing game, in which selfish players make a sequence of routing decisions. We consider a model in which players use regret-minimizing algorithms as the learning mechanism, and study the resulting dynamics. We are concerned in particular with the convergence to the set of Nash equilibria of the routing game. No-regret learning algorithms are known to guarantee convergence of a subsequence of population strategies. We are concerned with convergence of the actual sequence. We show that convergence holds for a large class of online learning algorithms, inspired from the continuous-time replicator dynamics. In particular, the discounted Hedge algorithm is proved to belong to this class, which guarantees its convergence.",http://proceedings.mlr.press/v32/krichene14.html,http://proceedings.mlr.press/v32/krichene14.pdf,ICML
2649,2014,Convergence rates for persistence diagram estimation in Topological Data Analysis,"Frédéric Chazal,         Marc Glisse,         Catherine Labruère,         Bertrand Michel","Computational topology  has recently seen an important development toward data analysis, giving birth to Topological Data Analysis. Persistent homology appears as a fundamental tool in this field. We show that  the use of persistent homology can be naturally considered in general statistical frameworks. We establish convergence rates of persistence diagrams associated to data randomly sampled from any compact metric space to a well defined limit diagram encoding the topological features of the support of the measure from which the data have been sampled. Our approach relies on a recent and deep stability result for persistence that allows to relate our problem to support estimation problems (with respect to the Gromov-Hausdorff distance). Some numerical experiments are performed in various contexts to illustrate our results.",http://proceedings.mlr.press/v32/chazal14.html,http://proceedings.mlr.press/v32/chazal14.pdf,ICML
2650,2014,Learning Modular Structures from Network Data and Node Variables,"Elham Azizi,         Edoardo Airoldi,         James Galagan","A standard technique for understanding underlying dependency structures among a set of variables posits a shared conditional probability distribution for the variables measured on individuals within a group. This approach is often referred to as module networks, where individuals are represented by nodes in a network, groups are termed modules, and the focus is on estimating the network structure among modules. However, estimation solely from node-specific variables can lead to spurious dependencies, and unverifiable structural assumptions are often used for regularization.  Here, we propose an extended model that leverages direct observations about the network in addition to node-specific variables. By integrating complementary data types, we avoid the need for structural assumptions. We illustrate theoretical and practical significance of the model and develop a reversible-jump MCMC learning procedure for learning modules and model parameters. We demonstrate the method accuracy in predicting modular structures from synthetic data and capability to learn regulatory modules in the  Mycobacterium tuberculosis gene regulatory network.",http://proceedings.mlr.press/v32/azizi14.html,http://proceedings.mlr.press/v32/azizi14.pdf,ICML
2651,2014,Automated inference of point of view from user interactions in collective intelligence venues,"Sanmay Das,         Allen Lavoie","Empirical evaluation of trust and manipulation in large-scale collective intelligence processes is challenging. The datasets involved are too large for thorough manual study, and current automated options are limited. We introduce a statistical framework which classifies point of view based on user interactions. The framework works on Web-scale datasets and is applicable to a wide variety of collective intelligence processes. It enables principled study of such issues as manipulation, trustworthiness of information, and potential bias. We demonstrate the model’s effectiveness in determining point of view on both synthetic data and a dataset of Wikipedia user interactions. We build a combined model of topics and points-of-view on the entire history of English Wikipedia, and show how it can be used to find potentially biased articles and visualize user interactions at a high level.",http://proceedings.mlr.press/v32/das14.html,http://proceedings.mlr.press/v32/das14.pdf,ICML
2652,2014,Making Fisher Discriminant Analysis Scalable,"Bojun Tu,         Zhihua Zhang,         Shusen Wang,         Hui Qian","The Fisher linear discriminant analysis (LDA) is a classical method for classification and dimension reduction jointly. A major limitation of the conventional LDA is a so-called singularity issue. Many LDA variants, especially two-stage methods such as PCA+LDA and LDA/QR,  were proposed to solve this issue. In the two-stage methods, an intermediate stage for dimension reduction is developed before  the actual LDA method works. These two-stage methods are scalable because they are an approximate alternative of the LDA method. However, there is no theoretical analysis on how well they approximate the conventional LDA problem. In this paper we present theoretical analysis on the approximation error of a two-stage algorithm. Accordingly, we develop a new two-stage algorithm. Furthermore, we resort to a random projection approach, making our algorithm scalable. We also provide an implemention on distributed system to handle large scale problems. Our algorithm takes LDA/QR as its special case, and outperforms PCA+LDA while having a similar scalability. We also generalize our algorithm to kernel discriminant analysis, a nonlinear version of the classical LDA. Extensive experiments show that our algorithms outperform PCA+LDA and have a similar scalability with it.",http://proceedings.mlr.press/v32/tu14.html,http://proceedings.mlr.press/v32/tu14.pdf,ICML
2653,2014,Large-margin  Weakly Supervised Dimensionality Reduction,"Chang Xu,         Dacheng Tao,         Chao Xu,         Yong Rui","This paper  studies dimensionality reduction in a weakly supervised setting, in which the preference relationship between examples is indicated by weak cues. A novel framework is proposed that integrates two aspects of the large margin principle (angle and distance), which simultaneously encourage angle consistency between preference pairs and maximize the distance between examples in preference pairs. Two specific algorithms are developed: an alternating direction method to learn a linear transformation matrix and a gradient boosting technique to optimize a non-linear transformation directly in the function space. Theoretical analysis demonstrates that the proposed large margin optimization criteria can strengthen and improve the robustness and generalization performance of preference learning algorithms on the obtained low-dimensional subspace. Experimental results on real-world datasets demonstrate the significance of studying dimensionality reduction in the weakly supervised setting and the effectiveness of the proposed framework.",http://proceedings.mlr.press/v32/xu14.html,http://proceedings.mlr.press/v32/xu14.pdf,ICML
2654,2014,Scalable Semidefinite Relaxation for Maximum A Posterior Estimation,"Qixing Huang,         Yuxin Chen,         Leonidas Guibas","Maximum a posteriori (MAP) inference over discrete Markov random fields is a central task spanning a wide spectrum of real-world applications but known to be NP-hard for general graphs. In this paper, we propose a novel semidefinite relaxation formulation (referred to as SDR) to estimate the MAP assignment. Algorithmically, we develop an accelerated variant of the alternating direction method of multipliers (referred to as SDPAD-LR) that can effectively exploit the special structure of SDR. Encouragingly, the proposed procedure allows solving SDR for large-scale problems,  e.g. problems comprising hundreds of thousands of variables with multiple states on a grid graph. Compared with prior SDP solvers, SDPAD-LR is capable of attaining comparable accuracy while exhibiting remarkably improved scalability. This contradicts the commonly held belief that semidefinite relaxation can only been applied on small-scale problems. We have evaluated the performance of SDR on various benchmark datasets including OPENGM2 and PIC. Experimental results demonstrate that for a broad class of problems, SDPAD-LR  outperforms state-of-the-art algorithms in producing better MAP assignments.",http://proceedings.mlr.press/v32/huang14.html,http://proceedings.mlr.press/v32/huang14.pdf,ICML
2655,2014,Boosting multi-step autoregressive forecasts,"Souhaib Ben Taieb,         Rob Hyndman","Multi-step forecasts can be produced recursively by iterating a one-step model, or directly using a specific model for each horizon. Choosing between these two strategies is not an easy task since it involves a trade-off between bias and estimation variance over the forecast horizon. Using a nonlinear machine learning model makes the tradeoff even more difficult. To address this issue, we propose a new forecasting strategy which boosts traditional recursive linear forecasts with a direct strategy using a boosting autoregression procedure at each horizon. First, we investigate the performance of the proposed strategy in terms of bias and variance decomposition of the error using simulated time series. Then, we evaluate the proposed strategy on real-world time series from two forecasting competitions. Overall, we obtain excellent performance with respect to the standard forecasting strategies.",http://proceedings.mlr.press/v32/taieb14.html,http://proceedings.mlr.press/v32/taieb14.pdf,ICML
2656,2014,Methods of Moments for Learning Stochastic Languages: Unified Presentation and Empirical Comparison,"Borja Balle,         William Hamilton,         Joelle Pineau","Probabilistic latent-variable models are a powerful tool for modelling structured data.  However, traditional expectation-maximization methods of learning such models are both computationally expensive and prone to local-minima. In contrast to these traditional methods, recently developed learning algorithms based upon the method of moments are both computationally efficient and provide strong statistical guarantees.  In this work, we provide a unified presentation and empirical comparison of three general moment-based methods in the context of modelling stochastic languages. By rephrasing these methods upon a common theoretical ground, introducing novel theoretical results where necessary, we provide a clear comparison, making explicit the statistical assumptions upon which each method relies. With this theoretical grounding, we then provide an in-depth empirical analysis of the methods on both real and synthetic data with the goal of elucidating performance trends and highlighting important implementation details.",http://proceedings.mlr.press/v32/balle14.html,http://proceedings.mlr.press/v32/balle14.pdf,ICML
2657,2014,Deep AutoRegressive Networks,"Karol Gregor,         Ivo Danihelka,         Andriy Mnih,         Charles Blundell,         Daan Wierstra","We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data.  Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling.  We derive an efficient approximate parameter estimation method based on the minimum  description length (MDL) principle,  which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference.   We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games.",http://proceedings.mlr.press/v32/gregor14.html,http://proceedings.mlr.press/v32/gregor14.pdf,ICML
2658,2014,Structured Prediction of Network Response,"Hongyu Su,         Aristides Gionis,         Juho Rousu","We introduce the following network response problem: given a complex network and an action, predict the subnetwork that responds to action, that is, which nodes perform the action and which directed edges relay the action to the adjacent nodes.     We approach the problem through max-margin structured learning, in which a compatibility score is learned between the actions and their activated  subnetworks. Thus, unlike the most popular influence network approaches, our method, called SPIN,  is context-sensitive, namely, the presence, the direction and the dynamics of influences depend on the properties of the actions.     The inference problems of finding the highest scoring as well as the worst margin violating networks, are proven to be NP-hard. To solve the problems, we present an approximate inference method through a semi-definite programming relaxation (SDP), as well as a more scalable greedy heuristic algorithm.    In our experiments, we demonstrate that taking advantage of the context given by the actions and the network structure leads SPIN to a markedly better predictive performance over competing methods.",http://proceedings.mlr.press/v32/su14.html,http://proceedings.mlr.press/v32/su14.pdf,ICML
2659,2014,Statistical-Computational Phase Transitions in Planted Models: The High-Dimensional Setting,"Yudong Chen,         Jiaming Xu","The planted models assume that a graph is generated from some unknown clusters by randomly placing edges between nodes according to their cluster memberships; the task is to recover the clusters given the graph. Special cases include planted clique, planted partition, planted densest subgraph and planted coloring. Of particular interest is the High-Dimensional setting where the number of clusters is allowed to grow with the number of nodes. We show that the space of model parameters can be partitioned into four disjoint regions corresponding to decreasing statistical and computational complexities: (1) the impossible regime, where all algorithms fail; (2) the hard regime, where the exponential-time Maximum Likelihood Estimator (MLE) succeeds, and no polynomial-time method is known; (3) the easy regime, where the polynomial-time convexified MLE succeeds; (4) the simple regime, where a simple counting/thresholding procedure succeeds. Moreover, each of these algorithms provably fails in the previous harder regimes. Our theorems establish the first minimax recovery results for the high-dimensional setting, and provide the best known guarantees for polynomial-time algorithms. Our results extend to the related problem of submatrix localization, a.k.a. bi-clustering. These results demonstrate the tradeoffs between statistical and computational considerations.",http://proceedings.mlr.press/v32/chene14.html,http://proceedings.mlr.press/v32/chene14.pdf,ICML
2660,2014,Elementary Estimators for High-Dimensional Linear Regression,"Eunho Yang,         Aurelie Lozano,         Pradeep Ravikumar","We consider the problem of structurally constrained high-dimensional linear regression. This has attracted considerable attention over the last decade, with state of the art statistical estimators based on solving regularized convex programs. While these typically non-smooth convex programs can be solved in polynomial time, scaling the state of the art optimization methods to very large-scale problems is an ongoing and rich area of research. In this paper, we attempt to address this scaling issue at the source, by asking whether one can build \emphsimpler possibly closed-form estimators, that yet come with statistical guarantees that are nonetheless comparable to regularized likelihood estimators! We answer this question in the affirmative, with variants of the classical ridge and OLS (ordinary least squares estimators) for linear regression. We analyze our estimators in the high-dimensional setting, and moreover provide empirical corroboration of its performance on simulated as well as real world microarray data.",http://proceedings.mlr.press/v32/yangc14.html,http://proceedings.mlr.press/v32/yangc14.pdf,ICML
2661,2014,Gaussian Processes for Bayesian Estimation in Ordinary Differential Equations,"David Barber,         Yali Wang","Bayesian parameter estimation in coupled ordinary differential equations (ODEs) is challenging due to the high computational cost of numerical integration. In gradient matching a separate data model is introduced with the property that its gradient can be calculated easily. Parameter estimation is achieved by requiring consistency between the gradients computed from the data model and those specified by the ODE. We propose a Gaussian process model that directly links state derivative information with system observations, simplifying previous approaches and providing a natural generative model.",http://proceedings.mlr.press/v32/barber14.html,http://proceedings.mlr.press/v32/barber14.pdf,ICML
2662,2014,Heavy-tailed regression with a generalized median-of-means,"Daniel Hsu,         Sivan Sabato","This work proposes a simple and computationally efficient estimator for  linear regression, and other smooth and strongly convex loss minimization  problems.  We prove loss approximation guarantees that hold for general distributions,  including those with heavy tails. All prior results only hold for estimators which  either assume bounded or subgaussian distributions,  require prior knowledge of distributional properties, or are not known to be computationally tractable.  In the special case of linear regression with possibly heavy-tailed responses and with bounded and well-conditioned covariates in d-dimensions, we show that a random sample of size  \tildeO(d\log(1/δ)) suffices to obtain a constant factor  approximation to the optimal loss with probability 1-δ, a minimax optimal sample complexity up to log factors.  The core technique used in the proposed estimator is a new generalization of  the median-of-means estimator to arbitrary metric spaces.",http://proceedings.mlr.press/v32/hsu14.html,http://proceedings.mlr.press/v32/hsu14.pdf,ICML
2663,2014,Exponential Family Matrix Completion under Structural Constraints,"Suriya Gunasekar,         Pradeep Ravikumar,         Joydeep Ghosh","We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low–rank, and the measurements consist of a subset, either of the exact individual entries,  or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin–tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data–types, such as skewed–continuous, count, binary, etc., (b) for heterogeneous noise models (beyond Gaussian), which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low–rank, such as block–sparsity, or a superposition structure of low–rank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for generalized matrix completion by considering a  matrix completion setting wherein the matrix entries are sampled from any member of the rich family of \textitexponential family distributions; and impose general structural constraints on the underlying matrix, as captured by a general regularizer \mathcalR(.). We propose a simple convex regularized M–estimator for the generalized framework, and provide a unified and novel statistical analysis for this general class of estimators. We finally corroborate our theoretical results on simulated datasets.",http://proceedings.mlr.press/v32/gunasekar14.html,http://proceedings.mlr.press/v32/gunasekar14.pdf,ICML
2664,2014,Active Learning of Parameterized Skills,"Bruno Da Silva,         George Konidaris,         Andrew Barto","We introduce a method for actively learning parameterized skills. Parameterized skills are flexible behaviors that can solve any task drawn from a distribution of parameterized reinforcement learning problems. Approaches to learning such skills have been proposed, but limited attention has been given to identifying which training tasks allow for rapid skill acquisition. We construct a non-parametric Bayesian model of skill performance and derive analytical expressions for a novel acquisition criterion capable of identifying tasks that maximize expected improvement in skill performance. We also introduce a spatiotemporal kernel tailored for non-stationary skill performance models. The proposed method is agnostic to policy and skill representation and scales independently of task dimensionality. We evaluate it on a non-linear simulated catapult control problem over arbitrarily mountainous terrains.",http://proceedings.mlr.press/v32/silva14.html,http://proceedings.mlr.press/v32/silva14.pdf,ICML
2665,2014,Stochastic Dual Coordinate Ascent with Alternating Direction Method of Multipliers,Taiji Suzuki,"We propose a new stochastic dual coordinate ascent technique  that can be applied to a wide range of regularized learning problems. Our method is based on  alternating direction method of multipliers (ADMM) to deal with complex regularization functions such as structured regularizations. Although the original ADMM is a batch method,  the proposed method offers a stochastic update rule where each iteration requires only one or few sample observations. Moreover, our method can naturally afford mini-batch update and it gives speed up of convergence. We show that, under mild assumptions, our method converges exponentially. The numerical experiments show that our method actually performs efficiently.",http://proceedings.mlr.press/v32/suzuki14.html,http://proceedings.mlr.press/v32/suzuki14.pdf,ICML
2666,2014,A Unifying View of Representer Theorems,"Andreas Argyriou,         Francesco Dinuzzo","It is known that the solution of regularization and interpolation problems with Hilbertian penalties can be expressed as a linear combination of the data. This very useful property, called the representer theorem, has been widely studied and applied to machine learning problems. Analogous optimality conditions have appeared in other contexts, notably in matrix regularization.  In this paper we propose a unified view, which generalizes the concept of representer theorems and extends necessary and sufficient conditions for such theorems to hold. Our main result shows a close connection between representer theorems and certain classes of regularization penalties, which we call orthomonotone functions.  This result not only subsumes previous representer theorems as  special cases but also yields a new class of optimality conditions, which goes beyond the classical linear combination of the data.  Moreover, orthomonotonicity   provides a useful criterion for testing whether a representer theorem  holds for a specific regularization problem.",http://proceedings.mlr.press/v32/argyriou14.html,http://proceedings.mlr.press/v32/argyriou14.pdf,ICML
2667,2014,Linear Time Solver for Primal SVM,"Feiping Nie,         Yizhen Huang,         Heng Huang","Support Vector Machines (SVM) is among the most popular classification techniques in machine learning, hence designing fast primal SVM algorithms for large-scale datasets is a hot topic in recent years. This paper presents a new L2-norm regularized primal SVM solver using Augmented Lagrange Multipliers, with linear-time computational cost for Lp-norm loss functions. The most computationally intensive steps (that determine the algorithmic complexity) of the proposed algorithm is purely and simply matrix-by-vector multiplication, which can be easily parallelized on a multi-core server for parallel computing. We implement and integrate our algorithm into the interfaces and framework of the well-known LibLinear software toolbox. Experiments show that our algorithm is with stable performance and on average faster than the state-of-the-art solvers such as SVMperf , Pegasos and the LibLinear that integrates the TRON, PCD and DCD algorithms.",http://proceedings.mlr.press/v32/niea14.html,http://proceedings.mlr.press/v32/niea14.pdf,ICML
2668,2014,Latent Bandits.,"Odalric-Ambrym Maillard,         Shie Mannor","We consider a multi-armed bandit problem where the reward distributions are indexed by two sets –one for arms, one for type– and can be partitioned into a small number of clusters according to the type. First, we consider the setting where all reward distributions are known and all types have the same underlying cluster, the type’s identity is, however, unknown. Second, we study the case  where types may come from different classes, which is significantly more challenging. Finally, we tackle the case where the reward distributions are completely unknown. In each setting, we introduce specific algorithms and derive non-trivial regret performance. Numerical experiments show that,  in the most challenging agnostic case, the proposed algorithm  achieves excellent performance in several difficult scenarios.",http://proceedings.mlr.press/v32/maillard14.html,http://proceedings.mlr.press/v32/maillard14.pdf,ICML
2669,2014,Memory Efficient Kernel Approximation,"Si Si,         Cho-Jui Hsieh,         Inderjit Dhillon","The scalability of kernel machines is a big challenge when facing millions of samples due to storage and computation issues for large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation algorithm – Memory Efficient Kernel Approximation (MEKA), which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the MNIST2M dataset with two-million samples, our method takes 550 seconds on a single machine using less than 500 MBytes memory to achieve 0.2313 test RMSE for kernel ridge regression, while standard Nyström approximation takes more than 2700 seconds and uses more than 2 GBytes memory on the same problem to achieve 0.2318 test RMSE.",http://proceedings.mlr.press/v32/si14.html,http://proceedings.mlr.press/v32/si14.pdf,ICML
2670,2014,Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget,"Anoop Korattikara,         Yutian Chen,         Max Welling","Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.",http://proceedings.mlr.press/v32/korattikara14.html,http://proceedings.mlr.press/v32/korattikara14.pdf,ICML
2671,2014,Forward-Backward Greedy Algorithms for General Convex Smooth Functions over A Cardinality Constraint,"Ji Liu,         Jieping Ye,         Ryohei Fujimaki","We consider forward-backward greedy algorithms for solving sparse feature selection problems with general convex smooth functions. A state-of-the-art greedy method, the Forward-Backward greedy algorithm (FoBa-obj) requires to solve a large number of optimization problems, thus it is not scalable for large-size problems. The FoBa-gdt algorithm, which uses the gradient information for feature selection at each forward iteration, significantly improves the efficiency of FoBa-obj. In this paper, we systematically analyze the theoretical properties of both algorithms. Our main contributions are: 1) We derive better theoretical bounds than existing analyses regarding FoBa-obj for general smooth convex functions; 2) We show that FoBa-gdt achieves the same theoretical performance as FoBa-obj under the same condition: restricted strong convexity condition. Our new bounds are consistent with the bounds of a special case (least squares) and fills a previously existing theoretical gap for general convex smooth functions; 3) We show that the restricted strong convexity condition is satisfied if the number of independent samples is more than \bark\log d where \bark is the sparsity number and d is the dimension of the variable; 4) We apply FoBa-gdt (with the conditional random field objective) to the sensor selection problem for human indoor activity recognition and our results show that FoBa-gdt outperforms other methods based on forward greedy selection and L1-regularization.",http://proceedings.mlr.press/v32/liub14.html,http://proceedings.mlr.press/v32/liub14.pdf,ICML
2672,2014,Filtering with Abstract Particles,"Jacob Steinhardt,         Percy Liang","Using particles, beam search and sequential Monte Carlo can approximate distributions in an extremely flexible manner. However, they can suffer from sparsity and inadequate coverage on large state spaces. We present a new filtering method that addresses this issue  by using “abstract particles” that each represent an entire region of the state space. These abstract particles are combined into a hierarchical decomposition, yielding a  representation that is both compact and flexible. Empirically, our method outperforms beam search and sequential Monte Carlo on both a text reconstruction task and a multiple object tracking task.",http://proceedings.mlr.press/v32/steinhardt14.html,http://proceedings.mlr.press/v32/steinhardt14.pdf,ICML
2673,2014,Understanding Protein Dynamics with L1-Regularized Reversible Hidden Markov Models,"Robert McGibbon,         Bharath Ramsundar,         Mohammad Sultan,         Gert Kiss,         Vijay Pande","We present a machine learning framework for modeling protein dynamics. Our  approach uses L1-regularized, reversible hidden Markov models to  understand large protein datasets generated via molecular dynamics  simulations. Our model is motivated by three design principles: (1) the requirement of massive scalability; (2) the need to adhere to relevant physical law; and (3) the necessity of providing accessible interpretations, critical for rational protein engineering and drug design. We present an EM algorithm for learning and introduce a model selection criteria based on the physical notion of relaxation timescales. We contrast our model with standard methods in biophysics and demonstrate improved robustness. We implement our algorithm on GPUs and apply the method to two large protein simulation datasets generated respectively on the NCSA Bluewaters supercomputer and the Folding@Home distributed computing network. Our analysis identifies the conformational dynamics of the ubiquitin protein responsible for signaling, and elucidates the stepwise activation mechanism of the c-Src kinase protein.",http://proceedings.mlr.press/v32/mcgibbon14.html,http://proceedings.mlr.press/v32/mcgibbon14.pdf,ICML
2674,2013,Gibbs Max-Margin Topic Models with Fast Sampling Algorithms,"Jun Zhu,         Ning Chen,         Hugh Perkins,         Bo Zhang","Existing max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents Gibbs max-margin supervised topic models by minimizing an expected margin loss, an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables, we develop simple and fast Gibbs sampling algorithms with no restricting assumptions and no need to solve SVM subproblems for both classification and regression. Empirical results demonstrate significant improvements on time efficiency. The classification performance is also significantly improved over competitors.",http://proceedings.mlr.press/v28/zhu13.html,http://proceedings.mlr.press/v28/zhu13.pdf,ICML
2675,2013,Topic Model Diagnostics: Assessing Domain Relevance via Topical Alignment,"Jason Chuang,         Sonal Gupta,         Christopher Manning,         Jeffrey Heer","The use of topic models to analyze domain-specific texts often requires manual validation of the latent topics to ensure they are meaningful. We introduce a framework to support large-scale assessment of topical relevance. We measure the correspondence between a set of latent topics and a set of reference concepts to quantify four types of topical misalignment: junk, fused, missing, and repeated topics. Our analysis compares 10,000 topic model variants to 200 expert-provided domain concepts, and demonstrates how our framework can inform choices of model parameters, inference algorithms, and intrinsic measures of topical quality.",http://proceedings.mlr.press/v28/chuang13.html,http://proceedings.mlr.press/v28/chuang13.pdf,ICML
2676,2013,No more pesky learning rates,"Tom Schaul,         Sixin Zhang,         Yann LeCun","The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained through systematic search, and effectively removes the need for learning rate tuning.",http://proceedings.mlr.press/v28/schaul13.html,http://proceedings.mlr.press/v28/schaul13.pdf,ICML
2677,2013,Enhanced statistical rankings  via  targeted data collection,"Braxton Osting,         Christoph Brune,         Stanley Osher","Given a graph where vertices represent alternatives and pairwise comparison data, y_ij, is given on the edges, the statistical ranking problem is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with pairwise comparisons. We study the dependence of the statistical ranking problem on the available pairwise data, i.e., pairs (i,j) for which the pairwise comparison data y_ij is known, and propose a framework to identify data which, when augmented with the current dataset, maximally increases the Fisher information of the ranking. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding an edge set on the graph (with a fixed number of edges) such that the  second eigenvalue of the graph Laplacian is maximal. This reduction of the data collection problem to a spectral graph-theoretic question is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating dataset and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking.",http://proceedings.mlr.press/v28/osting13.html,http://proceedings.mlr.press/v28/osting13.pdf,ICML
2678,2013,Consistency of Online Random Forests,"Misha Denil,         David Matheson,         Nando Freitas","As a testament to their success, the theory of random forests has long been outpaced by their application in practice. In this paper, we take a step towards narrowing this gap by providing a consistency result for online random forests.",http://proceedings.mlr.press/v28/denil13.html,http://proceedings.mlr.press/v28/denil13.pdf,ICML
2679,2013,MILEAGE: Multiple Instance LEArning with Global Embedding,"Dan Zhang,         Jingrui He,         Luo Si,         Richard Lawrence","Multiple Instance Learning (MIL) methods generally represent each example as a collection of  instances such that the features for local objects can be better captured, whereas traditional learning methods typically extract a global feature vector for each example as an integral part. However, there is limited research work on which of the two learning scenarios performs better. This paper proposes a novel framework – \emphMultiple Instance LEArning with  Global Embedding (MILEAGE), in which  the global feature vectors for traditional learning methods are integrated into the MIL setting.  MILEAGE can leverage the benefits derived from both learning settings. Within the proposed framework, a large margin method is formulated. In particular,  the proposed method adaptively  tunes the weights on the two different kinds of feature  representations (i.e., global and multiple instance) for each example and trains the classifier simultaneously. An alternative algorithm is proposed to solve the resulting optimization problem, which extends the bundle method to the non-convex case. Some important properties of the proposed method, such as the convergence rate and the generalization error rate, are analyzed. A series of  experiments  have been conducted to demonstrate the advantages of the proposed method over several state-of-the-art multiple instance and traditional learning methods.",http://proceedings.mlr.press/v28/zhang13a.html,http://proceedings.mlr.press/v28/zhang13a.pdf,ICML
2680,2013,Anytime Representation Learning,"Zhixiang Xu,         Matt Kusner,         Gao Huang,         Kilian Weinberger","Evaluation cost during test-time is becoming increasingly important as many real-world applications need fast evaluation (e.g. web search engines, email spam filtering) or use expensive features (e.g. medical diagnosis). We introduce Anytime Feature Representations (AFR), a novel algorithm that explicitly addresses this trade-off in the data representation rather than in the classifier. This enables us to turn conventional classifiers, in particular Support Vector Machines, into test-time cost sensitive anytime classifiers - combining the advantages of anytime learning and large-margin classification.",http://proceedings.mlr.press/v28/xu13b.html,http://proceedings.mlr.press/v28/xu13b.pdf,ICML
2681,2013,Nonparametric Mixture of Gaussian Processes with Constraints,"James Ross,         Jennifer Dy","Motivated by the need to identify new and clinically relevant categories of lung disease, we propose a novel clustering with constraints method using a Dirichlet process mixture of Gaussian processes in a variational Bayesian nonparametric framework. We claim that individuals should be grouped according to biological and/or genetic similarity regardless of their level of disease severity; therefore, we introduce a new way of looking at subtyping/clustering by recasting it in terms of discovering associations of individuals to disease trajectories (i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables). The nonparametric nature of our algorithm allows for learning the unknown number of meaningful trajectories. Additionally, we acknowledge the usefulness of expert guidance by providing for their input using must-link and cannot- link constraints. These constraints are encoded with Markov random fields. We also provide an efficient variational approach for performing inference on our model.",http://proceedings.mlr.press/v28/ross13a.html,http://proceedings.mlr.press/v28/ross13a.pdf,ICML
2682,2013,Domain Adaptation under Target and Conditional Shift,"Kun Zhang,         Bernhard Schölkopf,         Krikamol Muandet,         Zhikun Wang","Let X denote the feature and Y the target. We consider domain adaptation under three possible scenarios: (1) the marginal P_Y changes, while the conditional P_X|Y stays the same (\it target shift), (2) the marginal P_Y is fixed, while the conditional P_X|Y changes with certain constraints (\it conditional shift), and (3) the marginal P_Y changes, and the conditional P_X|Y changes with constraints (\it generalized target shift). Using background knowledge, causal interpretations allow us to determine the correct situation for a problem at hand. We exploit importance reweighting or sample transformation to find the learning machine that works well on test data, and propose to estimate the weights or transformations by \it reweighting or transforming training data to reproduce the covariate distribution on the test domain. Thanks to kernel embedding of conditional as well as marginal distributions, the proposed approaches avoid distribution estimation, and are applicable for high-dimensional problems. Numerical evaluations on synthetic and real-world datasets demonstrate the effectiveness of the proposed framework.",http://proceedings.mlr.press/v28/zhang13d.html,http://proceedings.mlr.press/v28/zhang13d.pdf,ICML
2683,2013,Covariate Shift in Hilbert Space: A Solution via Sorrogate Kernels,"Kai Zhang,         Vincent Zheng,         Qiaojun Wang,         James Kwok,         Qiang Yang,         Ivan Marsic","Covariate shift is a unconventional learning scenario in which training and testing data have different distributions. A general principle to solve the problem is to make the training data distribution similar to the test one, such that classifiers computed on the former generalizes well to the latter. Current approaches typically target on the sample distribution in the input space, however, for kernel-based learning methods, the algorithm performance depends directly on the geometry of the kernel-induced feature space. Motivated by this, we propose to match data distributions in the Hilbert space, which, given a pre-defined empirical kernel map, can be formulated as aligning kernel matrices across domains. In particular, to evaluate similarity of kernel matrices defined on arbitrarily different samples, the novel concept of surrogate kernel is introduced based on the Mercer's theorem. Our approach caters the model adaptation specifically to kernel-based learning mechanism, and demonstrates promising results on several real-world applications.",http://proceedings.mlr.press/v28/zhang13b.html,http://proceedings.mlr.press/v28/zhang13b.pdf,ICML
2684,2013,Convex formulations of radius-margin based Support Vector Machines,"Huyen Do,         Alexandros Kalousis","We consider Support Vector Machines (SVMs) learned together with linear transformations of the feature spaces on which they are applied. Under this scenario the radius of the smallest data enclosing sphere is no longer fixed. Therefore optimizing the SVM error bound by considering both the radius and the margin has the potential to deliver a tighter error bound.  In this paper we present two novel algorithms: R-SVM_μ^+—a SVM radius-margin based feature selection algorithm, and R-SVM^+ —  a metric learning-based SVM. We derive our algorithms by exploiting a new tighter approximation of the radius and a metric learning interpretation of SVM. Both optimize directly the radius-margin error bound using linear transformations. Unlike almost all existing radius-margin based SVM algorithms which are either non-convex or combinatorial, our algorithms are standard quadratic convex optimization problems with linear or quadratic constraints. We perform a number of experiments on benchmark datasets.   R-SVM_μ^+ exhibits excellent feature selection performance compared to the state-of-the-art feature selection methods, such as L_1-norm and elastic-net based methods.  R-SVM^+ achieves a significantly better classification performance compared to SVM and its other state-of-the-art variants. From the results it is clear that the incorporation of the radius, as a means to control the data spread, in the cost function has strong beneficial effects.",http://proceedings.mlr.press/v28/do13.html,http://proceedings.mlr.press/v28/do13.pdf,ICML
2685,2013,Solving Continuous POMDPs: Value Iteration with Incremental Learning of an Efficient Space Representation,"Sebastian Brechtel,         Tobias Gindele,         Rüdiger Dillmann","Discrete POMDPs of medium complexity can be approximately solved in reasonable time. However, most applications have a continuous and thus uncountably infinite state space. We propose the novel concept of learning a discrete representation of the continuous state space to solve the integrals in continuous POMDPs efficiently and generalize sparse calculations over the continuous space. The representation is iteratively refined as part of a novel Value Iteration step and does not depend on prior knowledge. Consistency for the learned generalization is asserted by a self-correction algorithm. The presented concept is implemented for continuous state and observation spaces based on Monte Carlo approximation to allow for arbitrary POMDP models. In an experimental comparison it yields higher values in significantly shorter time than state of the art algorithms and solves higher-dimensional problems.",http://proceedings.mlr.press/v28/brechtel13.html,http://proceedings.mlr.press/v28/brechtel13.pdf,ICML
2686,2013,Label Partitioning For Sublinear Ranking,"Jason Weston,         Ameesh Makadia,         Hector Yee","We consider the case of ranking a very large set of labels, items, or documents, which is common to information retrieval, recommendation, and large-scale annotation tasks. We present a general approach for converting an algorithm which has linear time in the size of the set to a sublinear one via label partitioning. Our method consists of learning an input partition and a label assignment  to each partition of the space such that precision at k is optimized, which is the loss function of interest in this setting. Experiments on large-scale ranking and recommendation tasks show that our method not only makes the original linear time algorithm computationally tractable, but can also improve its performance.",http://proceedings.mlr.press/v28/weston13.html,http://proceedings.mlr.press/v28/weston13.pdf,ICML
2687,2013,Learning Optimally Sparse Support Vector Machines,"Andrew Cotter,         Shai Shalev-Shwartz,         Nati Srebro","We show how to train SVMs with an optimal guarantee on the number of support vectors (up to constants), and with sample complexity and training runtime bounds matching the best known for kernel SVM optimization (i.e. without any additional asymptotic cost beyond standard SVM training). Our method is simple to implement and works well in practice.",http://proceedings.mlr.press/v28/cotter13.html,http://proceedings.mlr.press/v28/cotter13.pdf,ICML
2688,2013,Convex Adversarial Collective Classification,"MohamadAli Torkamani,         Daniel Lowd","In this paper, we present a novel method for robustly  performing collective classification in the presence of a malicious  adversary that can modify up to a fixed number of binary-valued  attributes.  Our method is formulated as a convex quadratic program  that guarantees optimal weights against a worst-case adversary in  polynomial time.  In addition to increased robustness against active  adversaries, this kind of adversarial regularization can also lead to  improved generalization even when no adversary is present.  In  experiments on real and simulated data, our method consistently  outperforms both non-adversarial and non-relational baselines.",http://proceedings.mlr.press/v28/torkamani13.html,http://proceedings.mlr.press/v28/torkamani13.pdf,ICML
2689,2013,On A Nonlinear Generalization of Sparse Coding and Dictionary Learning,"Jeffrey Ho,         Yuchen Xie,         Baba Vemuri","Existing dictionary learning algorithms are based on the  assumption that the data are vectors in an Euclidean vector  space, and the dictionary is learned from the training data using the vector space structure and its Euclidean metric. However, in many applications, features and data often originated from a Riemannian manifold that does not support a global linear (vector space) structure.  Furthermore, the extrinsic viewpoint of existing dictionary learning algorithms becomes inappropriate for modeling and incorporating the intrinsic geometry of the manifold that is potentially important and critical to the application. This paper proposes a novel framework for sparse coding and dictionary learning for data on a Riemannian manifold, and it shows that  the existing sparse coding and dictionary learning methods can be considered as special (Euclidean) cases of the more general framework proposed here. We show that both the dictionary and sparse coding can  be effectively computed for several important classes of Riemannian manifolds, and we validate the proposed method using two well-known classification problems in computer vision and medical imaging analysis.",http://proceedings.mlr.press/v28/ho13a.html,http://proceedings.mlr.press/v28/ho13a.pdf,ICML
2690,2013,Learning Convex QP Relaxations for Structured Prediction,"Jeremy Jancsary,         Sebastian Nowozin,         Carsten Rother","We introduce a new large margin approach to discriminative training of intractable discrete graphical models. Our approach builds on a convex quadratic programming relaxation of the MAP inference problem. The model parameters are trained directly within this restricted class of energy functions so as to optimize the predictions on the training data. We address the issue of how to parameterize the resulting model and point out its relation to existing approaches. The primary motivation behind our use of the QP relaxation is its computational efficiency; yet, empirically, its predictive accuracy compares favorably to more expensive approaches. This makes it an appealing choice for many practical tasks.",http://proceedings.mlr.press/v28/jancsary13.html,http://proceedings.mlr.press/v28/jancsary13.pdf,ICML
2691,2013,Fast Probabilistic Optimization from Noisy Gradients,Philipp Hennig,"Stochastic gradient descent remains popular in large-scale machine learning, on account of its very low computational cost and robustness to noise. However, gradient descent is only linearly efficient and not transformation invariant. Scaling by a local measure can substantially improve its performance. One natural choice of such a scale is the Hessian of the objective function: Were it available, it would turn linearly efficient gradient descent into the quadratically efficient Newton-Raphson optimization. Existing covariant methods, though, are either super-linearly expensive or do not address noise. Generalising recent results, this paper constructs a nonparametric Bayesian quasi-Newton algorithm that learns gradient and Hessian from noisy evaluations of the gradient. Importantly, the resulting algorithm, like stochastic gradient descent, has cost linear in the number of input dimensions.",http://proceedings.mlr.press/v28/hennig13.html,http://proceedings.mlr.press/v28/hennig13.pdf,ICML
2692,2013,Sequential Bayesian Search,"Zheng Wen,         Branislav Kveton,         Brian Eriksson,         Sandilya Bhamidipati","Millions of people search daily for movies, music, and books on the Internet. Unfortunately, non-personalized exploration of items can result in an infeasible number of costly interaction steps. We study the problem of efficient, repeated interactive search. In this problem, the user is navigated to the items of interest through a series of options and our objective is to learn a better search policy from past interactions with the user. We propose an efficient learning algorithm for solving the problem, sequential Bayesian search (SBS), and prove that it is Bayesian optimal. We also analyze the algorithm from the frequentist point of view and show that its regret is sublinear in the number of searches. Finally, we evaluate our method on a real-world movie discovery problem and show that it performs nearly optimally as the number of searches increases.",http://proceedings.mlr.press/v28/wen13.html,http://proceedings.mlr.press/v28/wen13.pdf,ICML
2693,2013,Cost-sensitive Multiclass Classification Risk Bounds,"Bernardo Ávila Pires,         Csaba Szepesvari,         Mohammad Ghavamzadeh","A commonly used approach to multiclass classification is to replace the 0-1 loss with a convex surrogate so as to make empirical risk minimization computationally tractable. Previous work has uncovered sufficient and necessary conditions for the consistency of the resulting procedures. In this paper, we strengthen these results by showing how the 0-1 excess loss of a predictor can be upper bounded as a function of the excess loss of the predictor measured using the convex surrogate. The bound is developed for the case of cost-sensitive multiclass classification and a convex surrogate loss that goes back to the work of  Lee, Lin and Wahba. The bounds are as easy to calculate as in binary classification. Furthermore, we also show that our analysis extends to the analysis of the recently introduced “Simplex Coding” scheme.",http://proceedings.mlr.press/v28/avilapires13.html,http://proceedings.mlr.press/v28/avilapires13.pdf,ICML
2694,2013,Efficient Semi-supervised and Active Learning of Disjunctions,"Nina Balcan,         Christopher Berlind,         Steven Ehrlich,         Yingyu Liang","We provide efficient algorithms for learning disjunctions in the semi-supervised setting under a natural regularity assumption introduced by (Balcan & Blum, 2005). We prove bounds on the sample complexity of our algorithms under a mild restriction on the data distribution. We also give an active learning algorithm with improved sample complexity and extend all our algorithms to the random classification noise setting.",http://proceedings.mlr.press/v28/balcan13.html,http://proceedings.mlr.press/v28/balcan13.pdf,ICML
2695,2013,Sharp Generalization Error Bounds for Randomly-projected Classifiers,"Robert Durrant,         Ata Kaban","We derive sharp bounds on the generalization error of a generic linear classifier trained by   empirical risk minimization on randomly-projected data. We make no restrictive assumptions   (such as sparsity or separability) on the data: Instead we use the fact that, in a classification setting, the question of interest is really ‘what is the effect of random projection on the predicted class labels?’ and we therefore derive the exact probability of ‘label flipping’ under Gaussian random projection in order to quantify this effect precisely in our bounds.",http://proceedings.mlr.press/v28/durrant13.html,http://proceedings.mlr.press/v28/durrant13.pdf,ICML
2696,2013,Local Deep Kernel Learning for Efficient Non-linear SVM Prediction,"Cijo Jose,         Prasoon Goyal,         Parv Aggrwal,         Manik Varma","Our objective is to speed up non-linear SVM prediction while maintaining classification accuracy above an acceptable limit. We generalize Localized Multiple Kernel Learning so as to learn a primal feature space embedding which is high dimensional, sparse and computationally deep. Primal based classification decouples prediction costs from the number of support vectors and our tree-structured features efficiently encode non-linearities while speeding up prediction exponentially over the state-of-the-art. We develop routines for optimizing over the space of tree-structured features and efficiently scale to problems with over half a million training points. Experiments on benchmark data sets reveal that our formulation can reduce prediction costs by more than three orders of magnitude in some cases with a moderate sacrifice in classification accuracy as compared to RBF-SVMs. Furthermore, our formulation leads to much better classification accuracies over leading methods.",http://proceedings.mlr.press/v28/jose13.html,http://proceedings.mlr.press/v28/jose13.pdf,ICML
2697,2013,Almost Optimal Exploration in Multi-Armed Bandits,"Zohar Karnin,         Tomer Koren,         Oren Somekh","We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplicative gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale applications. We present two novel, parameter-free algorithms for identifying the best arm, in two different settings: given a target confidence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doubly-logarithmic in the problem parameters. We corroborate our theoretical results with experiments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.",http://proceedings.mlr.press/v28/karnin13.html,http://proceedings.mlr.press/v28/karnin13.pdf,ICML
2698,2013,Learning from Human-Generated Lists,"Kwang-Sung Jun,         Jerry Zhu,         Burr Settles,         Timothy Rogers","Human-generated lists are a form of non-iid data with important applications in machine learning and cognitive psychology. We propose a generative model - sampling with reduced replacement (SWIRL) - for such lists. We discuss SWIRL’s relation to standard sampling paradigms, provide the maximum likelihood estimate for learning, and demonstrate its value with two real-world applications: (i) In a """"feature volunteering"""" task where non-experts spontaneously generate feature=>label pairs for text classification, SWIRL improves the accuracy of state-of-the-art feature-learning frameworks. (ii) In a """"verbal fluency"""" task where brain-damaged patients generate word lists when prompted with a category, SWIRL parameters align well with existing psychological theories, and our model can classify healthy people vs. patients from the lists they generate.",http://proceedings.mlr.press/v28/jun13.html,http://proceedings.mlr.press/v28/jun13.pdf,ICML
2699,2013,Guaranteed Sparse Recovery under Linear Transformation,"Ji Liu,         Lei Yuan,         Jieping Ye","We consider the following signal recovery problem: given a  measurement matrix Φ∈\mathbbR^n\times p and a noisy  observation vector c∈\mathbbR^n constructed from c =  Φθ^* + εwhere ε∈\mathbbR^n is the  noise vector whose entries follow i.i.d. centered sub-Gaussian  distribution, how to recover the signal θ^* if Dθ^* is  sparse \rca under a linear transformation D∈\mathbbR^m\times  p? One natural method using convex optimization is to solve the  following problem: \min_θ 1\over 2\|Φθ- c\|^2 +  λ\|Dθ\|_1. This paper provides an upper bound of the  estimate error and shows the consistency property of this method by  assuming that the design matrix Φis a Gaussian random matrix.  Specifically, we show 1) in the noiseless case, if the condition  number of D is bounded and the measurement number n≥Ω(s\log(p)) where s is the sparsity number, then the true  solution can be recovered with high probability; and 2) in the noisy  case, if the condition number of D is bounded and the measurement  increases faster than s\log(p), that is, s\log(p)=o(n), the  estimate error converges to zero with probability 1 when p and s  go to infinity. Our results are consistent with those for the  special case D=\boldI_p\times p (equivalently LASSO) and  improve the existing analysis. The condition number of D plays a  critical role in our analysis. We consider the condition numbers in  two cases including the fused LASSO and the random graph: the  condition number in the fused LASSO case is bounded by a constant,  while the condition number in the random graph case is bounded with  high probability if m\over p (i.e., #\textedge\over  #\textvertex\min_θ 1\over 2\|Φθ- c\|^2 +  λ\|Dθ\|_1. This paper provides an upper bound of the  estimate error and shows the consistency property of this method by  assuming that the design matrix Φis a Gaussian random matrix.  Specifically, we show 1) in the noiseless case, if the condition  number of D is bounded and the measurement number n≥Ω(s\log(p)) where s is the sparsity number, then the true  solution can be recovered with high probability; and 2) in the noisy  case, if the condition number of D is bounded and the measurement  increases faster than s\log(p), that is, s\log(p)=o(n), the  estimate error converges to zero with probability 1 when p and s  go to infinity. Our results are consistent with those for the  special case D=\boldI_p\times p (equivalently LASSO) and  improve the existing analysis. The condition number of D plays a  critical role in our analysis. We consider the condition numbers in  two cases including the fused LASSO and the random graph: the  condition number in the fused LASSO case is bounded by a constant,  while the condition number in the random graph case is bounded with  high probability if m\over p (i.e., #\textedge\over  #\textvertex) is larger than a certain constant. Numerical  simulations are consistent with our theoretical results.",http://proceedings.mlr.press/v28/liu13.html,http://proceedings.mlr.press/v28/liu13.pdf,ICML
2700,2013,Coco-Q: Learning in Stochastic Games with Side Payments,"Eric Sodomka,         Elizabeth Hilliard,         Michael Littman,         Amy Greenwald","Coco (""""cooperative/competitive"""") values are a solution concept for two-player normal-form games with transferable utility, when binding agreements and side payments between players are possible. In this paper, we show that coco values can also be defined for stochastic games and can be learned using a simple variant of Q-learning that is provably convergent. We provide a set of examples showing how the strategies learned by the Coco-Q algorithm relate to those learned by existing multiagent Q-learning algorithms.",http://proceedings.mlr.press/v28/sodomka13.html,http://proceedings.mlr.press/v28/sodomka13.pdf,ICML
2701,2013,"The lasso, persistence, and cross-validation","Darren Homrighausen,         Daniel McDonald","During the last fifteen years, the lasso procedure has been the target of a substantial amount of theoretical and applied research. Correspondingly, many results are known about its behavior for a fixed or optimally chosen smoothing parameter (given up to unknown constants). Much less, however, is known about the lasso’s behavior when the smoothing parameter is chosen in a data dependent way. To this end, we give the first result about the risk consistency of lasso when the smoothing parameter is chosen via cross-validation. We consider the high-dimensional setting wherein the number of predictors p=n^α, α>0 grows with the number of observations.",http://proceedings.mlr.press/v28/homrighausen13.html,http://proceedings.mlr.press/v28/homrighausen13.pdf,ICML
2702,2013,Learning and Selecting Features Jointly with Point-wise Gated Boltzmann Machines,"Kihyuk Sohn,         Guanyu Zhou,         Chansoo Lee,         Honglak Lee","Unsupervised feature learning has emerged as a promising tool in learning representations from unlabeled data. However, it is still challenging to learn useful high-level features when the data contains a significant amount of irrelevant patterns. Although feature selection can be used for such complex data, it may fail when we have to build a learning system from scratch (i.e., starting from the lack of useful raw features). To address this problem, we propose a point-wise gated Boltzmann machine, a unified generative model that combines feature learning and feature selection. Our model performs not only feature selection on learned high-level features (i.e., hidden units), but also dynamic feature selection on raw features (i.e., visible units) through a gating mechanism. For each example, the model can adaptively focus on a variable subset of visible nodes corresponding to the task-relevant patterns, while ignoring the visible units corresponding to the task-irrelevant patterns. In experiments, our method achieves improved performance over state-of-the-art in several visual recognition benchmarks.",http://proceedings.mlr.press/v28/sohn13.html,http://proceedings.mlr.press/v28/sohn13.pdf,ICML
2703,2013,Semi-supervised Clustering by Input Pattern Assisted Pairwise Similarity Matrix Completion,"Jinfeng Yi,         Lijun Zhang,         Rong Jin,         Qi Qian,         Anil Jain","Many semi-supervised clustering algorithms have been proposed to improve the clustering accuracy by effectively exploring the available side information that is usually in the form of pairwise constraints. Despite the progress, there are two main shortcomings of the existing semi-supervised clustering algorithms. First, they have to deal with non-convex optimization problems, leading to clustering results that are sensitive to the initialization. Second, none of these algorithms is equipped with theoretical guarantee regarding the clustering performance. We address these limitations by developing a framework for semi-supervised clustering based on \it input pattern assisted matrix completion. The key idea is to cast clustering into a matrix completion problem, and solve it efficiently by exploiting the correlation between input patterns and cluster assignments. Our analysis shows that under appropriate conditions, only O(\log n) pairwise constraints are needed to accurately recover the true cluster partition. We verify the effectiveness of the proposed algorithm by comparing it to the state-of-the-art semi-supervised clustering algorithms on several benchmark datasets.",http://proceedings.mlr.press/v28/yi13.html,http://proceedings.mlr.press/v28/yi13.pdf,ICML
2704,2013,A proximal Newton framework for composite minimization: Graph learning without Cholesky decompositions and matrix inversions,"Quoc Tran Dinh,         Anastasios Kyrillidis,         Volkan Cevher","We propose an algorithmic framework for convex minimization problems of composite functions with two terms:  a self-concordant part and a possibly nonsmooth regularization part.  Our method is a new proximal Newton algorithm with local quadratic convergence rate. As a specific problem instance, we consider sparse precision matrix estimation problems in graph learning. Via a careful dual formulation and a novel analytic step-size selection, we instantiate an algorithm within our framework for graph learning that avoids Cholesky decompositions and matrix inversions, making it attractive for parallel and distributed implementations.",http://proceedings.mlr.press/v28/trandinh13.html,http://proceedings.mlr.press/v28/trandinh13.pdf,ICML
2705,2013,Rounding Methods for Discrete Linear Classification,"Yann Chevaleyre,         Frédéerick Koriche,         Jean-daniel Zucker","Learning discrete linear functions is a notoriously difficult challenge. In this paper, the learning task is cast as combinatorial optimization problem: given a set of positive and negative feature vectors in the Euclidean space, the goal is to find a discrete linear function that minimizes the cumulative hinge loss of this training set. Since this problem is NP-hard, we propose two simple rounding algorithms that discretize the fractional solution of the problem. Generalization bounds are derived for two important classes of binary-weighted linear functions, by establishing the Rademacher complexity of these classes and proving approximation bounds for rounding methods. These methods are compared on both synthetic and real-world data.",http://proceedings.mlr.press/v28/chevaleyre13.html,http://proceedings.mlr.press/v28/chevaleyre13.pdf,ICML
2706,2013,Revisiting the Nystrom method for improved large-scale machine learning,"Alex Gittens,         Michael Mahoney","We reconsider randomized algorithms for the low-rank approximation of SPSD matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications.    Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods, and they point to differences between uniform and nonuniform sampling methods based on leverage scores.    We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds— e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error.",http://proceedings.mlr.press/v28/gittens13.html,http://proceedings.mlr.press/v28/gittens13.pdf,ICML
2707,2013,An Optimal Policy for Target Localization with Application to Electron Microscopy,"Raphael Sznitman,         Aurelien Lucchi,         Peter Frazier,         Bruno Jedynak,         Pascal Fua","This paper considers the task of finding a target location by making a limited number of sequential observations.  Each observation results from evaluating an imperfect classifier of a chosen cost and accuracy on an interval of chosen length and position.  Within a Bayesian framework, we study the problem of minimizing an objective that combines the entropy of the posterior distribution with the cost of the questions asked.  In this problem, we show that the one-step lookahead policy is Bayes-optimal for any arbitrary time horizon.  Moreover, this one-step lookahead policy is easy to compute and implement. We then use this policy in the context of localizing mitochondria in electron microscope images, and experimentally show that significant speed ups in acquisition can be gained, while maintaining near equal image quality at target locations, when compared to current policies.",http://proceedings.mlr.press/v28/sznitman13.html,http://proceedings.mlr.press/v28/sznitman13.pdf,ICML
2708,2013,Iterative Learning and Denoising in Convolutional Neural Associative Memories,"Amin Karbasi,         Amir Hesam Salavati,         Amin Shokrollahi","The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once. This is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third. More specifically, we devise an iterative algorithm that learns the redundancy among the patterns. The resulting network has a  retrieval capacity that is exponential in the size of the network. Lastly, by considering the local structures of the network, the asymptotic error correction performance  can be made linear in the size of the network.",http://proceedings.mlr.press/v28/karbasi13.html,http://proceedings.mlr.press/v28/karbasi13.pdf,ICML
2709,2013,Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization,"Stefano Ermon,         Carla Gomes,         Ashish Sabharwal,         Bart Selman","Integration is affected by the curse of dimensionality and quickly becomes intractable as the dimensionality of the problem grows. We propose a randomized algorithm that, with high probability, gives a constant-factor approximation of a general discrete integral defined over an exponentially large set. This algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function. As an application, we demonstrate that with a small number of MAP queries we can efficiently approximate the partition function of discrete graphical models, which can in turn be used, for instance, for marginal computation or model selection.",http://proceedings.mlr.press/v28/ermon13.html,http://proceedings.mlr.press/v28/ermon13.pdf,ICML
2710,2013,Learning Triggering Kernels for Multi-dimensional Hawkes Processes,"Ke Zhou,         Hongyuan Zha,         Le Song","How does the activity of one person affect that of another person? Does the strength of influence remain periodic or decay exponentially over time? In this paper, we study these critical questions in social network analysis quantitatively under the framework of multi-dimensional Hawkes processes. In particular, we focus on the nonparametric learning of the  triggering kernels, and propose an algorithm \sf MMEL that combines the idea of decoupling the parameters through constructing a tight upper-bound of the objective function and application of Euler-Lagrange equations for optimization in infinite dimensional functional space.    We show that the proposed method performs significantly better than alternatives in experiments on both synthetic and real world datasets.",http://proceedings.mlr.press/v28/zhou13.html,http://proceedings.mlr.press/v28/zhou13.pdf,ICML
2711,2013,Toward Optimal Stratification for Stratified Monte-Carlo Integration,"Alexandra Carpentier,         Rémi Munos","We consider the problem of adaptive stratified sampling for Monte Carlo integration of a function, given a finite number of function evaluations perturbed by noise. Here we address the problem of adapting simultaneously the number of samples into each stratum and the stratification itself. We show a tradeoff in the size of the partitioning. On the one hand it is important to refine the partition in areas where the observation noise or the function are heterogeneous in order to reduce this variability. But on the other hand, a too refined stratification makes it harder to assign the samples according to a near-optimal (oracle) allocation strategy. In this paper we provide an algorithm \em Monte-Carlo Upper-Lower Confidence Bound that selects online, among a large class of partitions, the partition that provides a near-optimal trade-off, and allocates the samples almost optimally on this partition.",http://proceedings.mlr.press/v28/carpentier13.html,http://proceedings.mlr.press/v28/carpentier13.pdf,ICML
2712,2013,Online Feature Selection for Model-based Reinforcement Learning,"Trung Nguyen,         Zhuoru Li,         Tomi Silander,         Tze Yun Leong","We propose a new framework for learning the world dynamics of feature-rich environments in model-based reinforcement learning. The main idea is formalized as a new, factored state-transition representation that supports efficient online-learning of the relevant features. We construct the transition models through predicting how the actions change the world. We introduce an online sparse coding learning technique for feature selection in high-dimensional spaces. We derive theoretical guarantees for our framework and empirically demonstrate its practicality in both simulated and real robotics domains.",http://proceedings.mlr.press/v28/nguyen13.html,http://proceedings.mlr.press/v28/nguyen13.pdf,ICML
2713,2013,Optimal rates for stochastic convex optimization under Tsybakov noise condition,"Aaditya Ramdas,         Aarti Singh","We focus on the problem of minimizing a convex function f over a convex set S given T queries to a stochastic first order oracle. We argue that the complexity of convex minimization is only determined by the rate of growth of the function around its minimum x^*_f,S, as quantified by a Tsybakov-like noise condition. Specifically, we prove that if f grows at least as fast as \|x-x^*_f,S\|^κaround its minimum, for some κ> 1, then the optimal rate of learning f(x^*_f,S) is  Θ(T^-\fracκ2κ-2). The classic rate Θ(1/\sqrt T) for convex functions and Θ(1/T) for strongly convex functions are special cases of our result for κ→∞and κ=2, and even faster rates are attained for 1 < κ< 2. We also derive tight bounds for the complexity of learning x_f,S^*, where the optimal rate is Θ(T^-\frac12κ-2). Interestingly, these precise rates also characterize the complexity of active learning and our results further strengthen the connections between the fields of active learning and convex optimization, both of which rely on feedback-driven queries.",http://proceedings.mlr.press/v28/ramdas13.html,http://proceedings.mlr.press/v28/ramdas13.pdf,ICML
2714,2013,That was fast! Speeding up NN search of high dimensional distributions.,"Emanuele Coviello,         Adeel Mumtaz,         Antoni Chan,         Gert Lanckriet","We present a data structure for fast nearest neighbor retrieval of generative models of documents based on KL divergence.  Our data structure, which shares some similarity with Bregman Ball Trees, consists of a hierarchical partition of a database,   and uses a novel branch and bound methodology for search.  The main technical contribution of the paper is a   novel and efficient algorithm  for deciding whether to explore nodes during backtracking, based on a variational approximation.  This reduces the number of computations per node, and overcomes the limitations of Bregman Ball Trees on high dimensional data.  In addition, our strategy is applicable also to probability distributions with hidden state variables, and is not limited to regular exponential family distributions.    Experiments demonstrate substantial speed-ups over both Bregman  Ball Trees and over brute force search, on both moderate and high dimensional histogram data.  In addition, experiments on linear dynamical systems demonstrate the flexibility of our approach to latent variable models.",http://proceedings.mlr.press/v28/coviello13.html,http://proceedings.mlr.press/v28/coviello13.pdf,ICML
2715,2013,The Cross-Entropy Method Optimizes for Quantiles,"Sergiu Goschin,         Ari Weinstein,         Michael Littman","Cross-entropy optimization (CE) has proven to be a powerful tool for search in control environments. In the basic scheme, a distribution over proposed solutions is repeatedly adapted by evaluating a sample of solutions and refocusing the distribution on a percentage of those with the highest scores.  We show that, in the kind of noisy evaluation environments that are common in decision-making domains, this percentage-based refocusing does not optimize the expected utility of solutions, but instead a quantile metric. We provide a variant of CE (Proportional CE) that effectively optimizes the expected value. We show using variants of established noisy environments that Proportional CE can be used in place of CE and can improve solution quality.",http://proceedings.mlr.press/v28/goschin13.html,http://proceedings.mlr.press/v28/goschin13.pdf,ICML
2716,2013,Topic Discovery through Data Dependent and Random Projections,"Weicong Ding,         Mohammad Hossein Rohban,         Prakash Ishwar,         Venkatesh Saligrama","We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms with provable guarantees based on data-dependent and random projections to identify novel words and associated topics. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and realworld datasets to demonstrate qualitative and quantitative merits of our scheme.",http://proceedings.mlr.press/v28/ding13.html,http://proceedings.mlr.press/v28/ding13.pdf,ICML
2717,2013,Deep Canonical Correlation Analysis,"Galen Andrew,         Raman Arora,         Jeff Bilmes,         Karen Livescu","We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation.   It can be viewed as a nonlinear extension of the linear method \emphcanonical correlation analysis (CCA).  It is an alternative to the nonparametric method \emphkernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances.  In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.",http://proceedings.mlr.press/v28/andrew13.html,http://proceedings.mlr.press/v28/andrew13.pdf,ICML
2718,2013,Maximum Variance Correction with Application to A* Search,"Wenlin Chen,         Kilian Weinberger,         Yixin Chen","In this paper we introduce Maximum Variance Correction (MVC), which finds large-scale feasible solutions to Maximum Variance Unfolding (MVU) by post-processing embeddings from any manifold learning algorithm. It increases the scale of MVU embeddings by several orders of magnitude and is naturally parallel. This unprecedented scalability opens up new avenues of applications for manifold learning, in particular the use of MVU embeddings as effective heuristics to speed-up A* search (Rayner et al. 2011).   We demonstrate that MVC embeddings lead to un-matched reductions in search time across several non-trivial A* benchmark search problems and bridge the gap between the manifold learning literature and one of its most promising high impact applications.",http://proceedings.mlr.press/v28/chen13c.html,http://proceedings.mlr.press/v28/chen13c.pdf,ICML
2719,2013,Online Latent Dirichlet Allocation with Infinite Vocabulary,"Ke Zhai,         Jordan Boyd-Graber","Topic models based on latent Dirichlet allocation (LDA) assume a predefined vocabulary a priori. This is reasonable in batch settings, but it is not reasonable when data are revealed over time, as is the case with streaming / online algorithms. To address this lacuna, we extend LDA by drawing topics from a Dirichlet process whose base distribution is a distribution over all strings rather than from a finite Dirichlet. We develop inference using online variational inference and because we only can consider a finite number of words for each truncated topic propose heuristics to dynamically organize, expand, and contract the set of words we consider in our vocabulary truncation. We show our model can successfully incorporate new words as it encounters new terms and that it performs better than online LDA in evaluations of topic quality and classification performance.",http://proceedings.mlr.press/v28/zhai13.html,http://proceedings.mlr.press/v28/zhai13.pdf,ICML
2720,2013,Copy or Coincidence? A Model for Detecting Social Influence and Duplication Events,"Lisa Friedland,         David Jensen,         Michael Lavine","In this paper, we analyze the task of inferring rare links between pairs of entities that seem too similar to have occurred by chance. Variations of this task appear in such diverse areas as social network analysis, security, fraud detection, and entity resolution. To address the task in a general form, we propose a simple, flexible mixture model in which most entities are generated independently from a distribution but a small number of pairs are constrained to be similar. We predict the true pairs using a likelihood ratio that trades off the entities’ similarity with their rarity. This method always outperforms using only similarity; however, with certain parameter settings, similarity turns out to be surprisingly competitive. Using real data, we apply the model to detect twins given their birth weights and to re-identify cell phone users based on distinctive usage patterns.",http://proceedings.mlr.press/v28/friedland13.html,http://proceedings.mlr.press/v28/friedland13.pdf,ICML
2721,2013,Markov Network Estimation From Multi-attribute Data,"Mladen Kolar,         Han Liu,         Eric Xing","Many real world network problems often concern multivariate nodal attributes such as image, textual, and multi-view feature vectors on nodes, rather than simple univariate nodal attributes. The existing graph estimation methods built on Gaussian graphical models and covariance selection algorithms can not handle such data, neither can the theories developed around such methods be directly applied. In this paper, we propose a new principled framework for estimating multi-attribute graphs. Instead of estimating the partial correlation as in current literature, our method estimates the partial canonical correlations that naturally accommodate complex nodal features.  Computationally, we provide an efficient algorithm which utilizes the multi-attribute structure. Theoretically, we provide sufficient conditions which guarantee consistent graph recovery. Extensive simulation studies demonstrate performance of our method under various conditions.",http://proceedings.mlr.press/v28/kolar13a.html,http://proceedings.mlr.press/v28/kolar13a.pdf,ICML
2722,2013,Convex Relaxations for Learning Bounded-Treewidth Decomposable Graphs,"K. S. Sesh Kumar,         Francis Bach","We consider the problem of learning the structure of undirected graphical models with bounded treewidth, within the maximum likelihood framework. This is an NP-hard problem and most approaches consider local search techniques. In this paper, we pose it as a combinatorial optimization problem, which is then relaxed to a convex optimization problem that involves searching over the forest and hyperforest polytopes with special structures. A supergradient method is used to solve the dual problem, with a run-time complexity of O(k^3 n^k+2 \log n) for each iteration, where n is the number of variables and k is a bound on the treewidth. We compare our approach to state-of-the-art methods on synthetic datasets and classical benchmarks, showing the gains of the novel convex approach.",http://proceedings.mlr.press/v28/kumar13c.html,http://proceedings.mlr.press/v28/kumar13c.pdf,ICML
2723,2013,Learning Hash Functions Using Column Generation,"Xi Li,         Guosheng Lin,         Chunhua Shen,         Anton Hengel,         Anthony Dick","Fast nearest neighbor searching is becoming  an increasingly important tool in solving many  large-scale problems. Recently a number of approaches  to learning data-dependent hash functions  have been developed. In this work,  we propose a column generation based method  for learning data-dependent hash functions on  the basis of proximity comparison information.  Given a set of triplets that encode the pairwise  proximity comparison information, our method  learns hash functions that preserve the relative  comparison relationships in the data as well as  possible within the large-margin learning framework.  The learning procedure is implemented  using column generation and hence is named  CGHash. At each iteration of the column generation  procedure, the best hash function is selected.  Unlike most other hashing methods, our  method generalizes to new data points naturally;  and has a training objective which is convex, thus  ensuring that the global optimum can be identified.  Experiments demonstrate that the proposed  method learns compact binary codes and that its  retrieval performance compares favorably with  state-of-the-art methods when tested on a few  benchmark datasets.",http://proceedings.mlr.press/v28/li13a.html,http://proceedings.mlr.press/v28/li13a.pdf,ICML
2724,2013,Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization,Martin Jaggi,"We provide stronger and more general primal-dual convergence results for Frank-Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimization, enabled by a simple framework of duality gap certificates. Our analysis also holds if the linear subproblems are only solved approximately (as well as if the gradients are inexact), and is proven to be worst-case optimal in the sparsity of the obtained solutions.    On the application side, this allows us to unify a large variety of existing sparse greedy methods, in particular for optimization over convex hulls of an atomic set, even if those sets can only be approximated, including sparse (or structured sparse) vectors or matrices, low-rank matrices, permutation matrices, or max-norm bounded matrices.    We present a new general framework for convex optimization over matrix factorizations, where every Frank-Wolfe iteration will consist of a low-rank update, and discuss the broad application areas of this approach.",http://proceedings.mlr.press/v28/jaggi13.html,http://proceedings.mlr.press/v28/jaggi13.pdf,ICML
2725,2013,Infinite Positive Semidefinite Tensor Factorization for Source Separation of Mixture Signals,"Kazuyoshi Yoshii,         Ryota Tomioka,         Daichi Mochihashi,         Masataka Goto","This paper presents a new class of tensor factorization called positive semidefinite tensor factorization (PSDTF) that decomposes a set of positive semidefinite (PSD) matrices into the convex combinations of fewer PSD basis matrices. PSDTF can be viewed as a natural extension of nonnegative matrix factorization. One of the main problems of PSDTF is that an appropriate number of bases should be given in advance. To solve this problem, we propose a nonparametric Bayesian model based on a gamma process that can instantiate only a limited number of necessary bases from the infinitely many bases assumed to exist. We derive a variational Bayesian algorithm for closed-form posterior inference and a multiplicative update rule for maximum-likelihood estimation. We evaluated PSDTF on both synthetic data and real music recordings to show its superiority.",http://proceedings.mlr.press/v28/yoshii13.html,http://proceedings.mlr.press/v28/yoshii13.pdf,ICML
2726,2013,The Most Generative Maximum Margin Bayesian Networks,"Robert Peharz,         Sebastian Tschiatschek,         Franz Pernkopf","Although discriminative learning in graphical models generally improves classification results, the generative semantics of the model are compromised.  In this paper, we introduce a novel approach of hybrid generative-discriminative learning for Bayesian networks.  We use an SVM-type large margin formulation for discriminative training, introducing a likelihood-weighted \ell^1-norm for the SVM-norm-penalization.  This simultaneously optimizes the data likelihood and therefore partly maintains the generative character of the model.  For many network structures, our method can be formulated as a convex problem, guaranteeing a globally optimal solution.  In terms of classification, the resulting models outperform state-of-the art generative and discriminative learning methods for Bayesian networks, and are comparable with linear and kernelized SVMs.  Furthermore, the models achieve likelihoods close to the maximum likelihood solution and show robust behavior in classification experiments with missing features.",http://proceedings.mlr.press/v28/peharz13.html,http://proceedings.mlr.press/v28/peharz13.pdf,ICML
2727,2013,Quantile Regression for Large-scale Applications,"Jiyan Yang,         Xiangrui Meng,         Michael Mahoney","Quantile regression is a method to estimate the quantiles of the   conditional distribution of a response variable, and as such it permits a   much more accurate portrayal of the relationship between the response variable   and observed covariates than methods such as Least-squares or   Least Absolute Deviations regression.  It can be expressed as a linear program,   and   interior-point methods can be used to find a solution for  moderately large problems.  Dealing with very large problems, \emphe.g., involving data up to and   beyond the terabyte regime, remains a challenge.  Here, we present a randomized algorithm that runs in time that is nearly   linear in the size of the input and that, with constant probability,   computes a (1+ε) approximate solution to an arbitrary quantile   regression problem.  Our algorithm computes a low-distortion subspace-preserving  embedding with respect to the loss function of quantile regression.  Our empirical evaluation illustrates that our algorithm is competitive with   the best previous work on small to medium-sized problems, and that   it can be implemented in MapReduce-like environments and    applied to terabyte-sized problems.",http://proceedings.mlr.press/v28/yang13f.html,http://proceedings.mlr.press/v28/yang13f.pdf,ICML
2728,2013,Approximate Inference in Collective Graphical Models,"Daniel Sheldon,         Tao Sun,         Akshat Kumar,         Tom Dietterich","We study the problem of approximate inference in collective graphical models (CGMs), which were recently introduced to model the problem of learning and inference with noisy aggregate observations. We first analyze the complexity of inference in CGMs: unlike inference in conventional graphical models, exact inference in CGMs is NP-hard even for tree-structured models. We then develop a tractable convex approximation to the NP-hard MAP inference problem in CGMs, and show how to use MAP inference for approximate marginal inference within the EM framework. We demonstrate empirically that these approximation techniques can reduce the computational cost of inference by two orders of magnitude and the cost of learning by at least an order of magnitude while providing solutions of equal or better quality.",http://proceedings.mlr.press/v28/sheldon13.html,http://proceedings.mlr.press/v28/sheldon13.pdf,ICML
2729,2013,Optimizing the F-Measure in Multi-Label Classification: Plug-in Rule Approach versus Structured Loss Minimization,"Krzysztof Dembczynski,         Arkadiusz Jachnik,         Wojciech Kotlowski,         Willem Waegeman,         Eyke Huellermeier","We compare the plug-in rule approach for optimizing the F-measure in multi-label classification with an approach based on structured loss minimization, such as the structured support vector machine (SSVM). Whereas the former derives an optimal prediction from a probabilistic model in a separate inference step, the latter seeks to optimize the F-measure directly during the training phase. We introduce a novel plug-in rule algorithm that estimates all parameters required for a Bayes-optimal prediction via a set of multinomial regression models, and we compare this algorithm with SSVMs in terms of computational complexity and statistical consistency. As a main theoretical result, we show that our plug-in rule algorithm is consistent, whereas the SSVM approaches are not. Finally, we present results of a large experimental study showing the benefits of the introduced algorithm.",http://proceedings.mlr.press/v28/dembczynski13.html,http://proceedings.mlr.press/v28/dembczynski13.pdf,ICML
2730,2013,Risk Bounds and Learning Algorithms for the Regression Approach to Structured Output Prediction,"Sébastien Giguère,         François Laviolette,         Mario Marchand,         Khadidja Sylla",We provide rigorous guarantees for the regression approach to structured output prediction. We show that the quadratic regression loss is a convex surrogate of the prediction loss when the output kernel satisfies some condition with respect to the prediction loss. We provide two upper bounds of the prediction risk that depend on the empirical quadratic risk of the predictor. The minimizer of the first bound is the predictor proposed by Cortes et al. (2007) while the minimizer of the second bound is a predictor that  has never been proposed so far. Both predictors are compared on practical tasks.,http://proceedings.mlr.press/v28/giguere13.html,http://proceedings.mlr.press/v28/giguere13.pdf,ICML
2731,2013,Large-Scale Learning with Less RAM via Randomization,"Daniel Golovin,         D. Sculley,         Brendan McMahan,         Michael Young","We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this reduces RAM usage by more than 50% during training and by up 95% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement per-coordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs.",http://proceedings.mlr.press/v28/golovin13.html,http://proceedings.mlr.press/v28/golovin13.pdf,ICML
2732,2013,Collective Stability in Structured Prediction: Generalization from One Example,"Ben London,         Bert Huang,         Ben Taskar,         Lise Getoor","Structured predictors enable joint inference over multiple interdependent output variables. These models are often trained on a small number of examples with large internal structure. Existing distribution-free generalization bounds do not guarantee generalization in this setting, though this contradicts a large body of empirical evidence from computer vision, natural language processing, social networks and other fields. In this paper, we identify a set of natural conditions – weak dependence, hypothesis complexity and a new measure, collective stability – that are sufficient for generalization from even a single example, without imposing an explicit generative model of the data. We then demonstrate that the complexity and stability conditions are satisfied by a broad class of models, including marginal inference in templated graphical models. We thus obtain uniform convergence rates that can decrease significantly faster than previous bounds, particularly when each structured example is sufficiently large and the number of training examples is constant, even one.",http://proceedings.mlr.press/v28/london13.html,http://proceedings.mlr.press/v28/london13.pdf,ICML
2733,2013,Bayesian Learning of Recursively Factored Environments,"Marc Bellemare,         Joel Veness,         Michael Bowling","Model-based reinforcement learning techniques have historically encountered a number of difficulties scaling up to large observation spaces. One promising approach has been to decompose the model learning task into a number of smaller, more manageable sub-problems by factoring the observation space. Typically, many different factorizations are possible, which can make it difficult to select an appropriate factorization without extensive testing. In this paper we introduce the class of recursively decomposable factorizations, and show how exact Bayesian inference can be used to efficiently guarantee predictive performance close to the best factorization in this class. We demonstrate the strength of this approach by presenting a collection of empirical results for 20 different Atari 2600 games.",http://proceedings.mlr.press/v28/bellemare13.html,http://proceedings.mlr.press/v28/bellemare13.pdf,ICML
2734,2013,On the importance of initialization and momentum in deep learning,"Ilya Sutskever,         James Martens,         George Dahl,         Geoffrey Hinton","Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.",http://proceedings.mlr.press/v28/sutskever13.html,http://proceedings.mlr.press/v28/sutskever13.pdf,ICML
2735,2013,General Functional Matrix Factorization Using Gradient Boosting,"Tianqi Chen,         Hang Li,         Qiang Yang,         Yong Yu","Matrix factorization is among the most successful techniques for collaborative filtering. One challenge of collaborative filtering is how to utilize available auxiliary information to improve prediction accuracy. In this paper, we study the  problem of utilizing auxiliary information as features of   factorization and propose formalizing the problem as general functional matrix factorization, whose model includes conventional matrix factorization models as its special cases. Moreover, we propose a gradient boosting based algorithm to efficiently solve the optimization problem. Finally, we give two specific algorithms for efficient feature function construction for two specific tasks. Our method can construct more suitable feature functions by searching in an infinite functional space based on training data and thus can yield better prediction accuracy. The experimental results demonstrate that the proposed method outperforms the baseline methods on three real-world datasets.",http://proceedings.mlr.press/v28/chen13e.html,http://proceedings.mlr.press/v28/chen13e.pdf,ICML
2736,2013,Multiple Identifications in Multi-Armed Bandits,"Séebastian Bubeck,         Tengyao Wang,         Nitin Viswanathan","We study the problem of identifying the top m arms in a multi-armed bandit game. Our proposed solution relies on a new algorithm based on successive rejects of the seemingly bad arms, and successive accepts of the good ones. This algorithmic contribution allows to tackle other multiple identifications settings that were previously out of reach. In particular we show that this idea of successive accepts and rejects applies to the multi-bandit best arm identification problem.",http://proceedings.mlr.press/v28/bubeck13.html,http://proceedings.mlr.press/v28/bubeck13.pdf,ICML
2737,2013,Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization,"Abhishek Kumar,         Vikas Sindhwani,         Prabhanjan Kambadur","The separability assumption (Arora et al., 2012; Donoho & Stodden, 2003) turns non-negative matrix factorization (NMF) into a tractable problem. Recently, a new class of provably-correct NMF algorithms have emerged under this assumption. In this paper, we reformulate the separable NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. From this geometric perspective, we derive new separable NMF algorithms that are highly scalable and empirically noise robust, and have several favorable properties in relation to existing methods. A parallel implementation of our algorithm scales excellently on shared and distributed-memory machines.",http://proceedings.mlr.press/v28/kumar13b.html,http://proceedings.mlr.press/v28/kumar13b.pdf,ICML
2738,2013,Safe Policy Iteration,"Matteo Pirotta,         Marcello Restelli,         Alessio Pecorino,         Daniele Calandriello","This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms.  When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur.  To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops.  We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy.  Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on some chain-walk domains and on the Blackjack card game.",http://proceedings.mlr.press/v28/pirotta13.html,http://proceedings.mlr.press/v28/pirotta13.pdf,ICML
2739,2013,Selective sampling algorithms for cost-sensitive multiclass prediction,Alekh Agarwal,"In this paper, we study the problem of active learning for cost-sensitive multiclass classification. We propose selective  sampling algorithms, which process the data in a streaming fashion,  querying only a subset of the labels. For these algorithms, we analyze the regret and label complexity when the labels are generated according to a generalized linear model. We establish that the gains of active learning over passive learning can range from none to exponentially large, based on a natural notion of margin. We also  present a safety guarantee to guard against model mismatch. Numerical  simulations show that our algorithms indeed obtain a low regret with a  small number of queries.",http://proceedings.mlr.press/v28/agarwal13.html,http://proceedings.mlr.press/v28/agarwal13.pdf,ICML
2740,2013,Collaborative hyperparameter tuning,"Rémi Bardenet,         Mátyás Brendel,         Balázs Kégl,         Michèle Sebag","Hyperparameter learning has traditionally been a manual task because of the limited number of trials. Today’s computing infrastructures allow bigger evaluation budgets, thus opening the way for algorithmic approaches. Recently, surrogate-based optimization was successfully applied to hyperparameter learning for deep belief networks and to WEKA classifiers. The methods combined brute force computational power with model building about the behavior of the error function in the hyperparameter space, and they could significantly improve on manual hyperparameter tuning. What may make experienced practitioners even better at hyperparameter optimization is their ability to generalize across similar learning problems. In this paper, we propose a generic method to incorporate knowledge from previous experiments when simultaneously tuning a learning algorithm on new problems at hand. To this end, we combine surrogate-based ranking and optimization techniques for surrogate-based collaborative tuning (SCoT). We demonstrate SCoT in two experiments where it outperforms standard tuning techniques and single-problem surrogate-based optimization.",http://proceedings.mlr.press/v28/bardenet13.html,http://proceedings.mlr.press/v28/bardenet13.pdf,ICML
2741,2013,Feature Multi-Selection among Subjective Features,"Sivan Sabato,         Adam Kalai","When dealing with subjective, noisy, or otherwise nebulous features, the “wisdom of crowds” suggests that one may benefit from multiple judgments of the same feature on the same object. We give theoretically-motivated """"feature multi-selection"""" algorithms that choose, among a large set of candidate features, not only which features to judge but how many times to judge each one. We demonstrate the effectiveness of this approach for linear regression on a crowdsourced learning task of predicting people’s height and weight from photos, using features such as """"gender"""" and  """"estimated weight"""" as well as culturally fraught ones such as """"attractive"""".",http://proceedings.mlr.press/v28/sabato13.html,http://proceedings.mlr.press/v28/sabato13.pdf,ICML
2742,2013,An Efficient Posterior Regularized Latent Variable Model for Interactive Sound Source Separation,"Nicholas Bryan,         Gautham Mysore","In applications such as audio denoising, music transcription, music remixing, and audio-based forensics, it is desirable to decompose a single-channel recording into its respective sources.  One of the current most effective class of methods to do so is based on non-negative matrix factorization and related latent variable models.  Such techniques, however, typically perform poorly when no isolated training data is given and do not allow user feedback to correct for poor results. To overcome these issues, we allow a user to interactively constrain a latent variable model by painting on a time-frequency display of sound to guide the learning process.  The annotations are used within the framework of posterior regularization to impose linear grouping constraints that would otherwise be difficult to achieve via standard priors.  For the constraints considered, an efficient expectation-maximization algorithm is derived with closed-form multiplicative updates, drawing connections to non-negative matrix factorization methods, and allowing for high-quality interactive-rate separation without explicit training data.",http://proceedings.mlr.press/v28/bryan13.html,http://proceedings.mlr.press/v28/bryan13.pdf,ICML
2743,2013,Optimization with First-Order Surrogate Functions,Julien Mairal,"In this paper, we study optimization methods consisting of iteratively minimizing surrogates of an objective function. By proposing several algorithmic variants and simple convergence analyses, we make two main contributions.  First, we provide a unified viewpoint for several first-order optimization techniques such as accelerated proximal gradient, block coordinate descent, or Frank-Wolfe algorithms.  Second, we introduce a new incremental scheme that experimentally matches or outperforms state-of-the-art solvers for large-scale optimization problems typically arising in machine learning.",http://proceedings.mlr.press/v28/mairal13.html,http://proceedings.mlr.press/v28/mairal13.pdf,ICML
2744,2013,Approximation properties of DBNs with binary hidden units and real-valued visible units,"Oswin Krause,         Asja Fischer,         Tobias Glasmachers,         Christian Igel","Deep belief networks (DBNs) can approximate any distribution over fixed-length binary vectors. However, DBNs are frequently applied to model real-valued data, and so far little is known about their representational power in this case.  We analyze the approximation properties of DBNs with two layers of binary hidden units and visible units with conditional distributions from the exponential family. It is shown that these DBNs can, under mild assumptions, model any additive mixture of distributions from the exponential family with independent variables. An arbitrarily good approximation in terms of Kullback-Leibler divergence of an m-dimensional mixture distribution with n components can be achieved by a DBN with m visible variables and n and n+1 hidden variables in the first and second hidden layer, respectively. Furthermore, relevant infinite mixtures can be approximated arbitrarily well by a DBN with a finite number of neurons. This includes the important special case of an infinite mixture of Gaussian distributions with fixed variance restricted to a compact domain, which in turn can approximate any strictly positive density over this domain.",http://proceedings.mlr.press/v28/krause13.html,http://proceedings.mlr.press/v28/krause13.pdf,ICML
2745,2013,Learning Connections in Financial Time Series,"Gartheeban Ganeshapillai,         John Guttag,         Andrew Lo","To reduce risk, investors seek assets that have high expected return and are unlikely to move in tandem. Correlation measures are generally used to quantify the connections between equities. The 2008 financial crisis, and its aftermath, demonstrated the need for a better way to quantify these connections. We present a machine learning-based method to build a connectedness matrix to address the shortcomings of correlation in capturing events such as large losses. Our method uses an unconstrained optimization to learn this matrix, while ensuring that the resulting matrix is positive semi-definite. We show that this matrix can be used to build portfolios that not only “beat the market,”  but also outperform optimal (i.e., minimum variance) portfolios.",http://proceedings.mlr.press/v28/ganeshapillai13.html,http://proceedings.mlr.press/v28/ganeshapillai13.pdf,ICML
2746,2013,Sparse coding for multitask and transfer learning,"Andreas Maurer,         Massi Pontil,         Bernardino Romera-Paredes","We investigate the use of sparse coding and dictionary learning in the context of multitask and transfer learning. The central assumption of our learning method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Numerical experiments on one synthetic and two real datasets show the advantage of our method over single task learning, a previous method based on orthogonal and dense representation of the tasks and a related method learning task grouping.",http://proceedings.mlr.press/v28/maurer13.html,http://proceedings.mlr.press/v28/maurer13.pdf,ICML
2747,2013,Message passing with l1 penalized KL minimization,"Yuan Qi,         Yandong Guo","Bayesian inference is often hampered by large computational expense.  As a generalization of belief propagation (BP), expectation propagation (EP) approximates exact Bayesian computation with efficient message passing updates. However, when an approximation family used by EP is far from exact posterior distributions, message passing may lead to poor approximation quality and suffer from divergence. To address this issue, we propose an  approximate inference method, relaxed expectation propagation(REP), based on a new divergence with a l1 penalty. Minimizing this penalized divergence adaptively relaxes EP’s moment matching requirement for message passing. We apply REP to Gaussian process classification and experimental results demonstrate significant improvement of REP over EP and alpha-divergence based power EP – in terms of algorithmic stability, estimation accuracy, and predictive performance. Furthermore, we develop relaxed belief propagation(RBP), a special case of REP, to conduct inference on discrete Markov random fields (MRFs). Our results show improved estimation accuracy of RBP over BP and fractional BP when interactions between MRF nodes are strong.",http://proceedings.mlr.press/v28/qi13.html,http://proceedings.mlr.press/v28/qi13.pdf,ICML
2748,2013,Predictable Dual-View Hashing,"Mohammad Rastegari,         Jonghyun Choi,         Shobeir Fakhraei,         Daume Hal,         Larry Davis","We propose a Predictable Dual-View Hashing (PDH) algorithm which embeds proximity of data samples in the original spaces. We create a cross-view hamming space with the ability to compare information from previously incomparable domains with a notion of ‘predictability’.   By performing comparative experimental analysis on two large datasets, PASCAL-Sentence and SUN-Attribute, we demonstrate the superiority of our method to the state-of-the-art dual-view binary code learning algorithms.",http://proceedings.mlr.press/v28/rastegari13.html,http://proceedings.mlr.press/v28/rastegari13.pdf,ICML
2749,2013,Squared-loss Mutual Information Regularization: A Novel Information-theoretic Approach to Semi-supervised Learning,"Gang Niu,         Wittawat Jitkrittum,         Bo Dai,         Hirotaka Hachiya,         Masashi Sugiyama","We propose squared-loss mutual information regularization (SMIR) for multi-class probabilistic classification, following the information maximization principle. SMIR is convex under mild conditions and thus improves the nonconvexity of mutual information regularization. It offers all of the following four abilities to semi-supervised algorithms: Analytical solution, out-of-sample/multi-class classification, and probabilistic output. Furthermore, novel generalization error bounds are derived. Experiments show SMIR compares favorably with state-of-the-art methods.",http://proceedings.mlr.press/v28/niu13.html,http://proceedings.mlr.press/v28/niu13.pdf,ICML
2750,2013,Kernelized Bayesian Matrix Factorization,"Mehmet Gönen,         Suleiman Khan,         Samuel Kaski","We extend kernelized matrix factorization with a fully Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the output matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (i) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas fully Bayesian treatments are not computationally feasible in the earlier approaches. (ii) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our method outperforms alternatives in predicting drug-protein interactions on two data sets. We then show that our framework can also be used for solving multilabel learning problems by considering samples and labels as the two domains where matrix factorization operates on. Our algorithm obtains the lowest Hamming loss values on 10 out of 14 multilabel classification data sets compared to five state-of-the-art multilabel learning algorithms.",http://proceedings.mlr.press/v28/gonen13a.html,http://proceedings.mlr.press/v28/gonen13a.pdf,ICML
2751,2013,Human Boosting,"Harsh Pareek,         Pradeep Ravikumar","Humans may be exceptional learners but they have biological limitations and moreover, inductive biases similar to machine learning algorithms. This puts limits on human learning ability and on the kinds of learning tasks humans can easily handle. In this paper, we consider the problem of “boosting” human learners to extend the learning ability of human learners and achieve improved performance on tasks which individual humans find difficult. We consider classification (category learning) tasks, propose a boosting algorithm for human learners and give theoretical justifications. We conduct experiments using Amazon’s Mechanical Turk on two synthetic datasets – a crosshair task with a nonlinear decision boundary and a gabor patch task with a linear boundary but which is inaccessible to human learners – and one real world dataset – the Opinion Spam detection task introduced in (Ott et al). Our results show that boosting human learners produces gains in accuracy and can overcome some fundamental limitations of human learners.",http://proceedings.mlr.press/v28/pareek13.html,http://proceedings.mlr.press/v28/pareek13.pdf,ICML
2752,2013,Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data,"Jan-Willem Meent,         Jonathan Bronson,         Frank Wood,         Ruben Gonzalez Jr.,         Chris Wiggins","We address the problem of analyzing sets of noisy time-varying signals that all report on the same process but confound straightforward analyses due to complex inter-signal heterogeneities and measurement artifacts.  In particular we consider single-molecule experiments which indirectly measure the distinct steps in a biomolecular process via observations of noisy time-dependent signals such as a fluorescence intensity or bead position. Straightforward hidden Markov model (HMM) analyses attempt to characterize such processes in terms of a set of conformational states, the transitions that can occur between these states, and the associated rates at which those transitions occur; but require ad-hoc post-processing steps to combine multiple signals.  Here we develop a hierarchically coupled HMM that allows experimentalists to deal with inter-signal variability in a principled and automatic way. Our approach is a generalized expectation maximization hyperparameter point estimation procedure with variational Bayes at the level of individual time series that learns an single interpretable representation of the overall data generating process.",http://proceedings.mlr.press/v28/willemvandemeent13.html,http://proceedings.mlr.press/v28/willemvandemeent13.pdf,ICML
2753,2013,Noisy and Missing Data Regression: Distribution-Oblivious Support Recovery,"Yudong Chen,         Constantine Caramanis","Many models for sparse regression typically assume that the covariates are known completely, and without noise. Particularly in high-dimensional applications, this is often not the case. Worse yet, even estimating statistics of the noise (the noise covariance) can be a central challenge. In this paper we develop a simple variant of orthogonal matching pursuit (OMP) for precisely this setting. We show that without knowledge of the noise covariance, our algorithm recovers the support, and we provide matching lower bounds that show that our algorithm performs at the minimax optimal rate. While simple, this is the first algorithm that (provably) recovers support in a noise-distribution-oblivious manner. When knowledge of the noise-covariance is available, our algorithm matches the best-known \ell^2-recovery bounds available. We show that these too are min-max optimal. Along the way, we also obtain improved performance guarantees for OMP for the standard sparse regression problem with Gaussian noise.",http://proceedings.mlr.press/v28/chen13d.html,http://proceedings.mlr.press/v28/chen13d.pdf,ICML
2754,2013,Dynamic Covariance Models for Multivariate Financial Time Series,"Yue Wu,         Jose Miguel Hernandez-Lobato,         Ghahramani Zoubin","The accurate prediction of time-changing covariances is an important problem in the modeling of multivariate financial data. However, some of the most popular models suffer from a) overfitting problems and multiple local optima, b) failure to capture shifts in market conditions and c) large computational costs. To address these problems we introduce a novel dynamic model for time-changing covariances. Over-fitting and local optima are avoided by following a Bayesian approach instead of computing point estimates. Changes in market conditions are captured by assuming a diffusion process in parameter values, and finally computationally efficient and scalable inference is performed using particle filters. Experiments with financial data show excellent performance of the proposed method with respect to current standard models.",http://proceedings.mlr.press/v28/wu13.html,http://proceedings.mlr.press/v28/wu13.pdf,ICML
2755,2013,Stability and Hypothesis Transfer Learning,"Ilja Kuzborskij,         Francesco Orabona","We consider the transfer learning scenario, where the learner does not have access to the source domain directly, but rather operates on the basis of hypotheses induced from it – the Hypothesis Transfer Learning (HTL) problem. Particularly, we conduct a theoretical analysis of HTL by considering the algorithmic stability of a class of HTL algorithms based on Regularized Least Squares with biased regularization. We show that the relatedness of source and target domains accelerates the convergence of the Leave-One-Out error to the generalization error, thus enabling the use of the Leave-One-Out error to find the optimal transfer parameters, even in the presence of a small training set. In case of unrelated domains we also suggest a theoretically principled way to prevent negative transfer, so that in the limit we recover the performance of the algorithm not using any knowledge from the source domain.",http://proceedings.mlr.press/v28/kuzborskij13.html,http://proceedings.mlr.press/v28/kuzborskij13.pdf,ICML
2756,2013,Domain Generalization via Invariant Feature Representation,"Krikamol Muandet,         David Balduzzi,         Bernhard Schölkopf","This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.",http://proceedings.mlr.press/v28/muandet13.html,http://proceedings.mlr.press/v28/muandet13.pdf,ICML
2757,2013,Estimating Unknown Sparsity in Compressed Sensing,Miles Lopes,"In the theory of compressed sensing (CS), the sparsity \|x\|_0 of the unknown signal x∈\R^p is commonly assumed to be a known parameter. However, it is typically unknown in practice. Due to the fact that many aspects of  CS depend on knowing \|x\|_0, it is important to estimate this parameter in a data-driven way. A second practical concern is that \|x\|_0 is a highly unstable function of x. In particular, for real signals with entries not exactly equal to 0, the value \|x\|_0=p is not a useful description of the effective number of coordinates. In this paper, we propose to estimate a stable measure of sparsity s(x):=\|x\|_1^2/\|x\|_2^2, which is a sharp lower bound on \|x\|_0. Our estimation procedure uses only a small number of linear measurements, does not rely on any sparsity assumptions, and requires very little computation. A confidence interval for s(x) is  provided, and its width is shown to have no dependence on the signal dimension p. Moreover, this result extends naturally to the matrix recovery setting, where a soft version of matrix rank can be estimated with analogous guarantees. Finally, we show that the use of randomized measurements is essential to estimating s(x). This is accomplished by proving that the minimax risk for estimating s(x) with deterministic measurements is large when n≪p.",http://proceedings.mlr.press/v28/lopes13.html,http://proceedings.mlr.press/v28/lopes13.pdf,ICML
2758,2013,Efficient Ranking from Pairwise Comparisons,"Fabian Wauthier,         Michael Jordan,         Nebojsa Jojic","The ranking of n objects based on pairwise comparisons is a core machine learning problem, arising in recommender systems, ad placement, player ranking, biological applications and others. In many practical situations the true pairwise comparisons cannot be actively measured, but a subset of all n(n-1)/2 comparisons is passively and noisily observed. Optimization algorithms (e.g., the SVM) could be used to predict a ranking with fixed expected Kendall tau distance, while achieving an Ω(n) lower bound on the corresponding sample complexity. However, due to their centralized structure they are difficult to extend to online or distributed settings. In this paper we show that much simpler algorithms can match the same Ω(n) lower bound in expectation. Furthermore, if an average of O(n\log(n)) binary comparisons are measured, then one algorithm recovers the true ranking in a uniform sense, while the other predicts the ranking more accurately near the top than the bottom. We discuss extensions to online and distributed ranking, with benefits over traditional alternatives.",http://proceedings.mlr.press/v28/wauthier13.html,http://proceedings.mlr.press/v28/wauthier13.pdf,ICML
2759,2013,Deep learning with COTS HPC systems,"Adam Coates,         Brody Huval,         Tao Wang,         David Wu,         Bryan Catanzaro,         Ng Andrew","Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features.  Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloud-like computing infrastructure and thousands of CPU cores.  In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI.  Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines.  As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.",http://proceedings.mlr.press/v28/coates13.html,http://proceedings.mlr.press/v28/coates13.pdf,ICML
2760,2013,Manifold Preserving Hierarchical Topic Models for Quantization and Approximation,"Minje Kim,         Paris Smaragdis","We present two complementary topic models to address the analysis of mixture data lying on manifolds. First, we propose a quantization method with an additional mid-layer latent variable, which selects only data points that best preserve the manifold structure of the input data. In order to address the case of modeling all the in-between parts of that manifold using this reduced representation of the input, we introduce a new model that provides a manifold-aware interpolation method. We demonstrate the advantages of these models with experiments on the hand-written digit recognition and the speech source separation tasks.",http://proceedings.mlr.press/v28/kim13a.html,http://proceedings.mlr.press/v28/kim13a.pdf,ICML
2761,2013,Constrained fractional set programs and their  application in local clustering and community detection,"Thomas Bühler,         Shyam Sundar Rangapuram,         Simon Setzer,         Matthias Hein","The (constrained) minimization of a ratio of set functions is a problem frequently occurring in clustering and community detection. As these optimization problems are typically NP-hard, one uses convex or spectral relaxations in practice. While these relaxations can be solved globally optimally, they are often too loose and thus lead to results far away from the optimum. In this paper we show that every constrained minimization problem of a ratio of non-negative set functions allows a tight relaxation into an unconstrained continuous optimization problem. This result leads to a flexible framework for solving constrained problems in network analysis. While a globally optimal solution for the resulting non-convex problem cannot be guaranteed, we outperform the loose convex or spectral relaxations by a large margin on constrained local clustering problems.",http://proceedings.mlr.press/v28/buhler13.html,http://proceedings.mlr.press/v28/buhler13.pdf,ICML
2762,2013,Strict Monotonicity of Sum of Squares Error  and Normalized Cut in the Lattice of Clusterings,Nicola Rebagliati,"Sum of Squares Error and Normalized Cut are two widely used clustering functional. It is known their minimum values are monotone with respect to the input number of clusters and this monotonicity does not allow for a simple automatic selection of a correct number of clusters. Here we study monotonicity not just on the minimizers but on the entire clustering lattice. We show the value of Sum of Squares Error is strictly monotone under the strict refinement relation of clusterings and we obtain data-dependent bounds on the difference between the value of a clustering and one of its refinements. Using analogous techniques we show the value of Normalized Cut is strictly anti-monotone. These results imply that even if we restrict our solutions to form a chain of clustering, like the one we get from hierarchical algorithms, we cannot rely on the functional values in order to choose the number of clusters. By using these results we get some data-dependent bounds on the difference of the values of any two clusterings.",http://proceedings.mlr.press/v28/rebagliati13.html,http://proceedings.mlr.press/v28/rebagliati13.pdf,ICML
2763,2013,Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation,"Hema Koppula,         Ashutosh Saxena","We consider the problem of detecting past activities as well as anticipating which activity will happen in the future and how. We start by modeling the rich spatio-temporal relations between human poses and objects (called affordances) using a conditional random field (CRF). However, because of the ambiguity in the temporal segmentation of the sub-activities that constitute an activity, in the past as well as in the future, multiple graph structures are possible. In this paper, we reason about these alternate possibilities by reasoning over multiple possible graph structures. We obtain them by approximating the graph with only additive features, which lends to efficient dynamic programming. Starting with this proposal graph structure, we then design moves to obtain several other likely graph structures. We then show that our approach improves the state-of-the-art significantly for detecting past activities as well as for anticipating future activities, on a dataset of 120 activity videos collected from four subjects.",http://proceedings.mlr.press/v28/koppula13.html,http://proceedings.mlr.press/v28/koppula13.pdf,ICML
2764,2013,O(logT) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions,"Lijun Zhang,         Tianbao Yang,         Rong Jin,         Xiaofei He","Traditional algorithms for stochastic optimization require projecting the solution at each iteration into a given domain to ensure its feasibility. When facing complex domains, such as the positive semidefinite cone, the projection operation can be expensive, leading to a high computational cost per iteration. In this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. The proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini-batches, extra-gradient, and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. We show, both in expectation and with a high probability, that when the objective function is both smooth and strongly convex, the proposed algorithm achieves the optimal O(1/T) rate of convergence with only O(logT) projections. Our empirical study verifies the theoretical result.",http://proceedings.mlr.press/v28/zhang13e.html,http://proceedings.mlr.press/v28/zhang13e.pdf,ICML
2765,2013,Sparse Uncorrelated Linear Discriminant Analysis,"Xiaowei Zhang,         Delin Chu","In this paper, we develop a novel approach for sparse uncorrelated linear discriminant analysis (ULDA). Our proposal is based on characterization of all solutions of the generalized ULDA. We incorporate sparsity into the ULDA transformation by seeking the solution with minimum \ell_1-norm from all minimum dimension solutions of the generalized ULDA. The problem is then formulated as a \ell_1-minimization problem and is solved by accelerated linearized Bregman method. Experiments on high-dimensional gene expression data demonstrate that our approach not only computes extremely sparse solutions but also performs well in classification. Experimental results also show that our approach can help for data visualization in low-dimensional space.",http://proceedings.mlr.press/v28/zhang13.html,http://proceedings.mlr.press/v28/zhang13.pdf,ICML
2766,2013,Mini-Batch Primal and Dual Methods for SVMs,"Martin Takac,         Avleen Bijral,         Peter Richtarik,         Nati Srebro","We address the issue of using mini-batches in stochastic  optimization of SVMs. We show that the same quantity, the  spectral norm of the data, controls the parallelization  speedup obtained for both primal stochastic subgradient descent(SGD) and stochastic dual coordinate ascent (SCDA) methods and use it to derive novel variants of mini-batched SDCA. Our guarantees for both methods are expressed in terms of the original nonsmooth primal problem based on the hinge-loss.",http://proceedings.mlr.press/v28/takac13.html,http://proceedings.mlr.press/v28/takac13.pdf,ICML
2767,2013,Thompson Sampling for Contextual Bandits with Linear Payoffs,"Shipra Agrawal,         Navin Goyal","Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied version of the contextual bandits problem. We prove a high probability regret bound of \tildeO(\fracd\sqrtε\sqrtT^1+ε) in time T for any ε∈(0,1), where d is the dimension of each context vector and εis a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of Ω(\sqrtdT) for this problem. This essentially solves the COLT open problem of Chapelle and Li [COLT 2012] regarding regret bounds for Thompson Sampling for contextual bandits problem with linear payoff functions.     Our version of Thompson sampling uses Gaussian prior and Gaussian likelihood function. Our novel martingale-based analysis techniques also allow easy extensions to the use of other distributions, satisfying certain general conditions.",http://proceedings.mlr.press/v28/agrawal13.html,http://proceedings.mlr.press/v28/agrawal13.pdf,ICML
2768,2013,Sparsity-Based Generalization Bounds for Predictive Sparse Coding,"Nishant Mehta,         Alexander Gray","The goal of predictive sparse coding is to learn a representation of examples as sparse linear combinations of elements from a dictionary, such that a learned hypothesis linear in the new representation performs well on a predictive task. Predictive sparse coding has demonstrated impressive performance on a variety of supervised tasks, but its generalization properties have not been studied. We establish the first generalization error bounds for predictive sparse coding, in the overcomplete setting, where the number of features k exceeds the original dimensionality d. The learning bound decays as (sqrt(d k/m)) with respect to d, k, and the size m of the training sample. It depends intimately on stability properties of the learned sparse encoder, as measured on the training sample. Consequently, we also present a fundamental stability result for the LASSO, a result that characterizes the stability of the sparse codes with respect to dictionary perturbations.",http://proceedings.mlr.press/v28/mehta13.html,http://proceedings.mlr.press/v28/mehta13.pdf,ICML
2769,2013,MAD-Bayes: MAP-based Asymptotic Derivations from Bayes,"Tamara Broderick,         Brian Kulis,         Michael Jordan","The classical mixture of Gaussians model is related to K-means via small-variance asymptotics: as the covariances of the Gaussians tend to zero, the negative log-likelihood of the mixture of Gaussians model approaches the K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis & Jordan (2012) used this observation to obtain a novel K-means-like algorithm from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead consider applying small-variance asymptotics directly to the posterior in Bayesian nonparametric models. This framework is independent of any specific Bayesian inference algorithm, and it has the major advantage that it generalizes immediately to a range of models beyond the DP mixture. To illustrate, we apply our framework to the feature learning setting, where the beta process and Indian buffet process provide an appropriate Bayesian nonparametric prior. We obtain a novel objective function that goes beyond clustering to learn (and penalize new) groupings for which we relax the mutual exclusivity and exhaustivity assumptions of clustering. We demonstrate several other algorithms, all of which are scalable and simple to implement. Empirical results demonstrate the benefits of the new framework.",http://proceedings.mlr.press/v28/broderick13.html,http://proceedings.mlr.press/v28/broderick13.pdf,ICML
2770,2013,Learning Heteroscedastic Models by Convex Programming under Group Sparsity,"Arnak Dalalyan,         Mohamed Hebiri,         Katia Meziani,         Joseph Salmon","Sparse estimation methods based on l1 relaxation,  such as the Lasso and the Dantzig  selector, require the knowledge of the variance  of the noise in order to properly tune the  regularization parameter. This constitutes a   major obstacle in applying these methods in  several frameworks, such as time series, random fields, inverse problems, for which noise  is rarely homoscedastic and the noise level is  hard to know in advance.  In this paper, we  propose a new approach to the joint estimation  of the conditional mean and the conditional  variance in a high-dimensional (auto-)  regression setting.   An attractive feature of the proposed estimator is   that it is efficiently computable even for very large   scale problems by solving a second-order cone program  (SOCP). We present theoretical analysis and  numerical results assessing the performance  of the proposed procedure.",http://proceedings.mlr.press/v28/dalalyan13.html,http://proceedings.mlr.press/v28/dalalyan13.pdf,ICML
2771,2013,Robust and Discriminative Self-Taught Learning,"Hua Wang,         Feiping Nie,         Heng Huang","The lack of training data is a common challenge in many machine learning problems, which is often tackled by semi-supervised learning methods or transfer learning methods. The former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks. However, these restrictions often cannot be satisfied. To address this, we propose a novel robust and discriminative self-taught learning approach to utilize any unlabeled data without the above restrictions. Our new approach employs a robust loss function to learn the dictionary, and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled data. We derive an efficient iterative algorithm to solve the optimization problem and rigorously prove its convergence. Promising results in extensive experiments have validated the proposed approach.",http://proceedings.mlr.press/v28/wang13g.html,http://proceedings.mlr.press/v28/wang13g.pdf,ICML
2772,2013,Distributed training of Large-scale Logistic models,"Siddharth Gopal,         Yiming Yang","Regularized Multinomial Logistic regression has emerged as one of the most common methods for performing data classification and analysis. With the advent of large-scale data it is common to find scenarios where the number of possible multinomial outcomes is large (in the order of thousands to tens of thousands). In such cases, the computational cost of training logistic models or even simply iterating through all the model parameters is prohibitively expensive. In this paper, we propose a training method for large-scale multinomial logistic models that breaks this bottleneck by enabling parallel optimization of the likelihood objective. Our experiments on large-scale datasets showed an order of magnitude reduction in training time.",http://proceedings.mlr.press/v28/gopal13.html,http://proceedings.mlr.press/v28/gopal13.pdf,ICML
2773,2013,Computation-Risk Tradeoffs for Covariance-Thresholded Regression,"Dinah Shender,         John Lafferty","We present a family of linear regression estimators that provides a fine-grained tradeoff between statistical accuracy and computational efficiency.  The estimators are based on hard thresholding of the sample covariance matrix entries together with l2-regularizion(ridge regression).  We analyze the predictive risk of this family of estimators as a function of the threshold and regularization parameter.  With appropriate parameter choices, the estimate is the solution to a sparse, diagonally dominant linear system, solvable in near-linear time.  Our analysis shows how the risk varies with the sparsity and regularization level, thus establishing a statistical estimation setting for which there is an explicit, smooth tradeoff between risk and computation.  Simulations are provided to support the theoretical analyses.",http://proceedings.mlr.press/v28/shender13.html,http://proceedings.mlr.press/v28/shender13.pdf,ICML
2774,2013,Gaussian Process Kernels for Pattern Discovery and Extrapolation,"Andrew Wilson,         Ryan Adams","Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation.  We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation.  These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture.  The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic.  We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data.  We also show that it is possible to reconstruct several popular standard covariances within our framework.",http://proceedings.mlr.press/v28/wilson13.html,http://proceedings.mlr.press/v28/wilson13.pdf,ICML
2775,2013,Adaptive Sparsity in Gaussian Graphical Models,"Eleanor Wong,         Suyash Awate,         P. Thomas Fletcher","An effective approach to structure learning and parameter estimation for Gaussian graphical models is to impose a sparsity prior, such as a Laplace prior, on the entries of the precision matrix. Such an approach involves a hyperparameter that must be tuned to control the amount of sparsity. In this paper, we introduce a parameter-free method for estimating a precision matrix with sparsity that adapts to the data automatically. We achieve this by formulating a hierarchical Bayesian model of the precision matrix with a non-informative Jeffreys’ hyperprior. We also naturally enforce the symmetry and positive-definiteness constraints on the precision matrix by parameterizing it with the Cholesky decomposition. Experiments on simulated and real (cell signaling) data demonstrate that the proposed approach not only automatically adapts the sparsity of the model, but it also results in improved estimates of the precision matrix compared to the Laplace prior model with sparsity parameter chosen by cross-validation.",http://proceedings.mlr.press/v28/wong13.html,http://proceedings.mlr.press/v28/wong13.pdf,ICML
2776,2013,A unifying framework for vector-valued manifold regularization and multi-view learning,"Minh Hà Quang,         Loris Bazzani,         Vittorio Murino","This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) formulation for the problem of learning an unknown functional dependency  between a structured input space and a structured output space, in the Semi-Supervised Learning setting. Our formulation includes as special cases Vector-valued Manifold Regularization and Multi-view Learning, thus provides in particular a unifying framework linking these two important learning approaches.  In the case of least square loss function, we provide a closed form solution with an efficient implementation. Numerical experiments on challenging multi-class categorization problems show that our multi-view learning formulation achieves results which are comparable with state of the art and are significantly better than single-view learning.",http://proceedings.mlr.press/v28/haquang13.html,http://proceedings.mlr.press/v28/haquang13.pdf,ICML
2777,2013,Distribution to Distribution Regression,"Junier Oliva,         Barnabas Poczos,         Jeff Schneider","We analyze ’Distribution to Distribution regression’ where one is regressing a mapping where both the covariate (inputs) and response (outputs) are distributions. No parameters on the input or output distributions are assumed, nor are any strong assumptions made on the measure from which input distributions are drawn from. We develop an estimator and derive an upper bound for the L2 risk; also, we show that when the effective dimension is small enough (as measured by the doubling dimension), then the risk converges to zero with a polynomial rate.",http://proceedings.mlr.press/v28/oliva13.html,http://proceedings.mlr.press/v28/oliva13.pdf,ICML
2778,2013,A Structural SVM Based Approach for Optimizing Partial AUC,"Harikrishna Narasimhan,         Shivani Agarwal","The area under the ROC curve (AUC) is a widely used performance measure in machine learning. Increasingly, however, in several applications, ranging from ranking and biometric screening to medical diagnosis, performance is measured not in terms of the full area under the ROC curve, but instead, in terms of the partial area under the ROC curve between two specified false positive rates. In this paper, we develop a structural SVM framework for directly optimizing the partial AUC between any two false positive rates. Our approach makes use of a cutting plane solver along the lines of the structural SVM based approach for optimizing the full AUC developed by Joachims (2005). Unlike the full AUC, where the combinatorial optimization problem needed to find the most violated constraint in the cutting plane solver can be decomposed easily to yield an efficient algorithm, the corresponding optimization problem in the case of partial AUC is harder to decompose. One of our key technical contributions is an efficient algorithm for solving this combinatorial optimization problem that has the same computational complexity as Joachims’ algorithm for optimizing the usual AUC. This allows us to efficiently optimize the partial AUC in any desired false positive range. We demonstrate the approach on a variety of real-world tasks.",http://proceedings.mlr.press/v28/narasimhan13.html,http://proceedings.mlr.press/v28/narasimhan13.pdf,ICML
2779,2013,Consistency versus Realizable H-Consistency for Multiclass Classification,"Phil Long,         Rocco Servedio","A consistent loss function for multiclass classification is one such  that for any source of labeled examples, any tuple  of scoring functions that  minimizes the expected loss will have classification accuracy close to that  of the Bayes optimal classifier. While consistency has been proposed as a  desirable property for multiclass loss functions, we  give experimental and theoretical results exhibiting a  sequence of linearly separable data sources with the following property:  a multiclass classification algorithm which optimizes a loss function  due to Crammer and Singer (which is known not to be consistent) produces  classifiers whose expected error goes to 0, while the expected error  of an algorithm which optimizes a generalization of the loss  function used by LogitBoost (a loss function which is known to be consistent)  is bounded below by a positive constant.    We identify a property of a loss function, realizable  consistency with respect to a restricted class of scoring functions,  that accounts for this difference.   As our main technical results we show  that the Crammer–Singer loss function is  realizable consistent for the class of linear scoring functions, while  the generalization of LogitBoost is not.  Our result for LogitBoost is  a special case of a more general theorem that applies to several other  loss functions that have been proposed for multiclass classification.",http://proceedings.mlr.press/v28/long13.html,http://proceedings.mlr.press/v28/long13.pdf,ICML
2780,2013,Stochastic Alternating Direction Method of Multipliers,"Hua Ouyang,         Niao He,         Long Tran,         Alexander Gray","The Alternating Direction Method of Multipliers (ADMM) has received lots of attention recently due to the tremendous demand from large-scale and data-distributed machine learning applications. In this paper, we present a stochastic setting for optimization problems with non-smooth composite objective functions. To solve this problem, we propose a stochastic ADMM algorithm. Our algorithm applies to a more general class of convex and nonsmooth objective functions, beyond the smooth and separable least squares loss used in lasso. We also demonstrate the rates of convergence for our algorithm under various structural assumptions of the stochastic function: O(1/\sqrtt) for convex functions and O(\log t/t) for strongly convex functions. Compared to previous literature, we establish the convergence rate of ADMM for convex problems in terms of both the objective value and the feasibility violation. A novel application named Graph-Guided SVM is proposed to demonstrate the usefulness of our algorithm.",http://proceedings.mlr.press/v28/ouyang13.html,http://proceedings.mlr.press/v28/ouyang13.pdf,ICML
2781,2013,Scaling Multidimensional Gaussian Processes using Projected Additive Approximations,"Elad Gilboa        Yunus Saatçi,         John Cunningham,         Elad Gilboa","Exact Gaussian Process (GP) regression has O(N^3) runtime for data size N, making it intractable for large N. Advances in GP scaling have not been extended to the multidimensional input setting, despite the preponderance of multidimensional applications.   This paper introduces and tests a novel method of projected additive approximation to multidimensional GPs. We thoroughly illustrate the power of this method on several datasets, achieving close performance to the naive Full GP at orders of magnitude less cost.",http://proceedings.mlr.press/v28/gilboa13.html,http://proceedings.mlr.press/v28/gilboa13.pdf,ICML
2782,2013,A Generalized Kernel Approach to Structured Output Learning,"Hachem Kadri,         Mohammad Ghavamzadeh,         Philippe Preux","We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. Our formulation overcomes the two main limitations of the original KDE approach, namely the decoupling between outputs  in the image space and the inability to use a joint feature space. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on three structured output problems, and compare it to the state-of-the art kernel-based structured output regression methods.",http://proceedings.mlr.press/v28/kadri13.html,http://proceedings.mlr.press/v28/kadri13.pdf,ICML
2783,2013,One-Pass AUC Optimization,"Wei Gao,         Rong Jin,         Shenghuo Zhu,         Zhi-Hua Zhou","AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires only going through the training data once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.",http://proceedings.mlr.press/v28/gao13.html,http://proceedings.mlr.press/v28/gao13.pdf,ICML
2784,2013,Noisy Sparse Subspace Clustering,"Yu-Xiang Wang,         Huan Xu","This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabelled input data points, which are assumed to lie in a union of low-dimensional subspaces.  We show that a modified version of SSC is \emphprovably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to the practical setting and provides justification to the success of SSC in a class of real applications.",http://proceedings.mlr.press/v28/wang13.html,http://proceedings.mlr.press/v28/wang13.pdf,ICML
2785,2013,Learning Multiple Behaviors from Unlabeled Demonstrations in a Latent Controller Space,"Javier Almingol,         Lui Montesano,         Manuel Lopes","In this paper we introduce a method to learn multiple behaviors in the form of motor primitives from an unlabeled dataset. One of the difficulties of this problem is that in the measurement space, behaviors can be very mixed, despite existing a latent representation where they can be easily separated.  We propose a mixture model based on Dirichlet Process (DP) to simultaneously cluster the observed time-series and recover a sparse representation of the behaviors using a Laplacian prior as the base measure of the DP. We show that for linear models, e.g potential functions generated by linear combinations of a large number of features, it is possible to compute analytically the marginal of the observations and derive an efficient sampler. The method is evaluated using robot behaviors and real data from human motion and compared to other techniques.",http://proceedings.mlr.press/v28/almingol13.html,http://proceedings.mlr.press/v28/almingol13.pdf,ICML
2786,2013,Exploiting Ontology Structures and Unlabeled Data for Learning,"Nina Balcan,         Avrim Blum,         Yishay Mansour","We present and analyze a theoretical model designed to understand and  explain the effectiveness of ontologies for learning multiple related  tasks from primarily unlabeled data.  We present both  information-theoretic results as well as efficient algorithms.    We show in this model that an ontology, which specifies the  relationships between multiple outputs, in some cases is sufficient  to completely learn a classification using a large unlabeled data  source.",http://proceedings.mlr.press/v28/balcan13a.html,http://proceedings.mlr.press/v28/balcan13a.pdf,ICML
2787,2013,Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing,"Huayan Wang,         Koller Daphne","Max-product (max-sum) message passing algorithms are widely used for MAP inference in MRFs. It has many variants sharing a common flavor of passing ""messages"" over some graph-object. Recent advances revealed that its convergent versions (such as MPLP, MSD, TRW-S) can be viewed as performing block coordinate descent (BCD) in a dual objective. That is, each BCD step achieves dual-optimal w.r.t. a block of dual variables (messages), thereby decreases the dual objective monotonically. However, most existing algorithms are limited to updating blocks selected in rather restricted ways. In this paper, we show a ""unified"" message passing algorithm that: (a) subsumes MPLP, MSD, and TRW-S as special cases when applied to their respective choices of dual objective and blocks, and (b) is able to perform BCD under much more flexible choices of blocks (including very large blocks) as well as the dual objective itself (that arise from an arbitrary dual decomposition).",http://proceedings.mlr.press/v28/wang13b.html,http://proceedings.mlr.press/v28/wang13b.pdf,ICML
2788,2013,Learning invariant features by harnessing the aperture problem,"Roland Memisevic,         Georgios Exarchakis","The energy model is a simple, biologically inspired approach to extracting relationships between images in tasks like stereopsis and motion analysis. We discuss how adding an extra pooling layer to the energy model makes it possible to learn encodings of transformations that are mostly invariant with respect to image content, and to learn encodings of images that are mostly invariant with respect to the observed transformations. We show how this makes it possible to learn 3D pose-invariant features of objects by watching videos of the objects. We test our approach on a dataset of videos derived from the NORB dataset.",http://proceedings.mlr.press/v28/memisevic13.html,http://proceedings.mlr.press/v28/memisevic13.pdf,ICML
2789,2013,Scale Invariant Conditional Dependence Measures,"Sashank J Reddi,         Barnabas Poczos","In this paper we develop new dependence and conditional dependence measures and provide their estimators. An attractive property of these measures and estimators is that they are invariant to any monotone increasing transformations of the random variables, which is important in many applications including feature selection. Under certain conditions we show the consistency of these estimators, derive upper bounds on their convergence rates, and show that the estimators do not suffer from the curse of dimensionality. However, when the conditions are less restrictive, we derive a lower bound which proves that in the worst case the convergence can be arbitrarily slow similarly to some other estimators. Numerical illustrations demonstrate the applicability of our method.",http://proceedings.mlr.press/v28/jreddi13.html,http://proceedings.mlr.press/v28/jreddi13.pdf,ICML
2790,2013,Fast Max-Margin Matrix Factorization with Data Augmentation,"Minjie Xu,         Jun Zhu,         Bo Zhang",Existing max-margin matrix factorization (M3F) methods either are computationally inefficient or need a model selection procedure to determine the number of latent factors. In this paper we present a probabilistic M3F model that admits a highly efficient Gibbs sampling algorithm through data augmentation. We further extend our approach to incorporate Bayesian nonparametrics and build accordingly a truncation-free nonparametric M3F model where the number of latent factors is literally unbounded and inferred from data. Empirical studies on two large real-world data sets verify the efficacy of our proposed methods.,http://proceedings.mlr.press/v28/xu13a.html,http://proceedings.mlr.press/v28/xu13a.pdf,ICML
2791,2013,Scaling the Indian Buffet Process via Submodular Maximization,"Colorado Reed,         Ghahramani Zoubin","Inference for latent feature models is inherently difficult as the inference space grows exponentially with the size of the input data and number of latent features. In this work, we use Kurihara & Wellings (2008)’s maximization-expectation framework to perform approximate MAP inference for linear-Gaussian latent feature models with an Indian Buffet Process (IBP) prior. This formulation yields a submodular function of the features that corresponds to a lower bound on the model evidence. By adding a constant to this function, we obtain a nonnegative submodular function that can be maximized via a greedy algorithm that obtains at least a 1/3-approximation to the optimal solution. Our inference method scales linearly with the size of the input data, and we show the efficacy of our method on the largest datasets currently analyzed using an IBP model.",http://proceedings.mlr.press/v28/reed13.html,http://proceedings.mlr.press/v28/reed13.pdf,ICML
2792,2013,Canonical Correlation Analysis based on Hilbert-Schmidt Independence Criterion and Centered Kernel Target Alignment,"Billy Chang,         Uwe Kruger,         Rafal Kustra,         Junping Zhang","Canonical correlation analysis (CCA) is a well established technique for identifying linear relationships among two variable sets.  Kernel CCA (KCCA) is the most notable nonlinear extension but it lacks interpretability and robustness against irrelevant features.  The aim of this article is to introduce two nonlinear CCA extensions that rely on the recently proposed Hilbert-Schmidt independence criterion and the centered kernel target alignment.  These extensions determine linear projections that provide maximally dependent projected data pairs.  The paper demonstrates that the use of linear projections allows removing irrelevant features, whilst extracting combinations of strongly associated features.  This is exemplified through a simulation and the analysis of recorded data that are available in the literature.",http://proceedings.mlr.press/v28/chang13.html,http://proceedings.mlr.press/v28/chang13.pdf,ICML
2793,2013,Multi-Class Classification with Maximum Margin Multiple Kernel,"Corinna Cortes,         Mehryar Mohri,         Afshin Rostamizadeh","We present a new algorithm for multi-class classification with multiple kernels. Our algorithm is based on a natural notion of the multi-class margin of a kernel. We show that larger values of this quantity guarantee the existence of an accurate multi-class predictor and also define a family of multiple kernel algorithms based on the maximization of the multi-class margin of a kernel (M^3K).  We present an extensive theoretical analysis in support of our algorithm, including novel multi-class Rademacher complexity margin bounds.  Finally, we also report the results of a series of experiments with several data sets, including comparisons where we improve upon the performance of state-of-the-art algorithms both in binary and multi-class classification with multiple kernels.",http://proceedings.mlr.press/v28/cortes13.html,http://proceedings.mlr.press/v28/cortes13.pdf,ICML
2794,2013,Intersecting singularities for multi-structured estimation,"Emile Richard,         Francis BACH,         Jean-Philippe Vert",We address the problem of designing a convex nonsmooth regularizer encouraging multiple structural effects simultaneously. Focusing on the inference of sparse and low-rank matrices we suggest a new complexity index and a convex penalty approximating it. The new penalty term can be written as the trace norm of a linear function of the matrix. By analyzing theoretical properties of this family of regularizers  we come up with oracle inequalities and compressed sensing results ensuring the quality of our regularized estimator. We also provide algorithms and supporting numerical experiments.,http://proceedings.mlr.press/v28/richard13.html,http://proceedings.mlr.press/v28/richard13.pdf,ICML
2795,2013,ELLA: An Efficient Lifelong Learning Algorithm,"Paul Ruvolo,         Eric Eaton","The problem of learning multiple consecutive tasks, known as lifelong learning, is of great importance to the creation of intelligent, general-purpose, and flexible machines.  In this paper, we develop a method for online multi-task learning in the lifelong learning setting.  The proposed Efficient Lifelong Learning Algorithm (ELLA) maintains a sparsely shared basis for all task models, transfers knowledge from the basis to learn each new task, and refines the basis over time to maximize performance across all tasks. We show that ELLA has strong connections to both online dictionary learning for sparse coding and state-of-the-art batch multi-task learning methods, and provide robust theoretical performance guarantees.  We show empirically that ELLA yields nearly identical performance to batch multi-task learning while learning tasks sequentially in three orders of magnitude (over 1,000x) less time.",http://proceedings.mlr.press/v28/ruvolo13.html,http://proceedings.mlr.press/v28/ruvolo13.pdf,ICML
2796,2013,Combinatorial Multi-Armed Bandit: General Framework and Applications,"Wei Chen,         Yajun Wang,         Yang Yuan","We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where simple arms with unknown istributions  form \em super arms. In each round, a super arm is played and the outcomes of its related simple arms are observed, which helps the selection of super arms in future rounds. The reward of the super arm depends on the outcomes of played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an (α,β)-approximation oracle that takes the  means of the distributions of arms and outputs a super arm that with probability βgenerates  an αfraction of the optimal expected reward. The objective of a CMAB algorithm is to minimize \em (α,β)-approximation regret, which is the difference in total expected reward between the αβfraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves O(\log n) regret, where n is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound for classical MAB problem up to a constant factor, and it significantly improves the regret bound in a recent paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage (PMC) for online advertising and social influence maximization for viral marketing, both having nonlinear reward structures.",http://proceedings.mlr.press/v28/chen13a.html,http://proceedings.mlr.press/v28/chen13a.pdf,ICML
2797,2013,Multi-Task Learning with Gaussian Matrix Generalized Inverse Gaussian Model,"Ming Yang,         Yingming Li,         Zhongfei Zhang","In this paper, we study the multi-task learning problem with a new perspective of considering the structure of the residue error matrix and the low-rank approximation to the task covariance matrix simultaneously. In particular, we first introduce the Matrix Generalized Inverse Gaussian (MGIG) prior and define a Gaussian Matrix Generalized Inverse Gaussian (GMGIG) model for low-rank approximation to the task covariance matrix. Through combining the GMGIG model with the residual error structure assumption, we propose the GMGIG regression model for multi-task learning. To make the computation tractable, we simultaneously use variational inference and sampling techniques. In particular, we propose two sampling strategies for computing the statistics of the MGIG distribution. Experiments show that this model is superior to the peer methods in regression and prediction.",http://proceedings.mlr.press/v28/yang13d.html,http://proceedings.mlr.press/v28/yang13d.pdf,ICML
2798,2013,Learning the beta-Divergence in Tweedie Compound Poisson Matrix Factorization Models,"Umut Simsekli,         Ali Taylan Cemgil,         Yusuf Kenan Yilmaz","In this study, we derive algorithms for estimating mixed β-divergences. Such cost functions are useful for Nonnegative Matrix and Tensor Factorization models with a compound Poisson observation model. Compound Poisson is a particular Tweedie model, an important special case of exponential dispersion models characterized by the fact that the variance is proportional to a power function of the mean. There are several well known matrix and tensor factorization algorithms that minimize the β-divergence; these estimate the mean parameter. The probabilistic interpretation gives us more flexibility and robustness by providing us additional tunable parameters such as power and dispersion. Estimation of the power parameter is useful for choosing a suitable divergence and estimation of dispersion is useful for data driven regularization and weighting in collective/coupled factorization of heterogeneous datasets. We present three inference algorithms for both estimating the factors and the additional parameters of the compound Poisson distribution. The methods are evaluated on two applications: modeling symbolic representations for polyphonic music and lyric prediction from audio features. Our conclusion is that the compound poisson based factorization models can be useful for sparse positive data.",http://proceedings.mlr.press/v28/simsekli13.html,http://proceedings.mlr.press/v28/simsekli13.pdf,ICML
2799,2013,Analogy-preserving Semantic Embedding for Visual Object Categorization,"Sung Ju Hwang,         Kristen Grauman,         Fei Sha","In multi-class categorization tasks, knowledge about the classes’ semantic relationships can provide valuable information beyond the class labels themselves.  However, existing techniques focus on preserving the semantic distances between classes (e.g., according to a given object taxonomy for visual recognition), limiting the influence to pairwise structures.  We propose to model \emphanalogies that reflect the relationships between multiple pairs of classes simultaneously, in the form “p is to q, as r is to s"""".  We translate semantic analogies into higher-order geometric constraints called \emphanalogical parallelograms, and use them in a novel convex regularizer for a discriminatively learned label embedding.   Furthermore, we show how to discover analogies from attribute-based class descriptions, and how to prioritize those likely to reduce inter-class confusion.  Evaluating our Analogy-preserving Semantic Embedding (ASE) on two visual recognition datasets, we demonstrate clear improvements over existing approaches, both in terms of recognition accuracy and analogy completion.",http://proceedings.mlr.press/v28/juhwang13.html,http://proceedings.mlr.press/v28/juhwang13.pdf,ICML
2800,2013,Planning by Prioritized Sweeping with Small Backups,"Harm Van Seijen,         Rich Sutton","Efficient planning plays a crucial role in model-based reinforcement learning. Traditionally, the main planning operation is a full backup based on the current estimates of the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we introduce a new planning backup that  uses only the current value of a single successor state and has a computation time independent of the number of successor states. This new backup, which we call a small backup, opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods. We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations.",http://proceedings.mlr.press/v28/vanseijen13.html,http://proceedings.mlr.press/v28/vanseijen13.pdf,ICML
2801,2013,Scalable Simple Random Sampling and Stratified Sampling,Xiangrui Meng,"Analyzing data sets of billions of records has now become a regular task in    many companies and institutions.    In the statistical analysis of those massive data sets, sampling generally    plays a very important role.    In this work, we describe a scalable simple random sampling algorithm, named    ScaSRS, which uses probabilistic thresholds to decide on the fly whether to    accept, reject, or wait-list an item independently of others.    We prove, with high probability, it succeeds and needs only O(\sqrtk)    storage, where k is the sample size.    ScaSRS extends naturally to a scalable stratified sampling algorithm, which is    favorable for heterogeneous data sets.    The proposed algorithms, when implemented in MapReduce, can effectively reduce    the size of intermediate output and greatly improve load balancing.    Empirical evaluation on large-scale data sets clearly demonstrates their    superiority.",http://proceedings.mlr.press/v28/meng13a.html,http://proceedings.mlr.press/v28/meng13a.pdf,ICML
2802,2013,Riemannian Similarity Learning,Li Cheng,"We consider a similarity-score based paradigm to address scenarios where either the class labels are only partially revealed during learning, or the training and testing data are drawn from heterogeneous sources. The learning problem is subsequently formulated as optimization over a bilinear form of fixed rank. Our paradigm bears similarity to metric learning, where the major difference lies in its aim of learning a rectangular similarity matrix, instead of a proper metric. We tackle this problem in a Riemannian optimization framework. In particular, we consider its applications in pairwise-based action recognition, and cross-domain image-based object recognition. In both applications, the proposed algorithm produces competitive performance on respective benchmark datasets.",http://proceedings.mlr.press/v28/cheng13.html,http://proceedings.mlr.press/v28/cheng13.pdf,ICML
2803,2013,Robust Sparse Regression under Adversarial Corruption,"Yudong Chen,         Constantine Caramanis,         Shie Mannor","We consider high dimensional sparse regression with arbitrary – possibly, severe or coordinated – errors in the covariates matrix. We are interested in understanding how many corruptions we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables), nor recent algorithms for dealing with stochastic noise or erasures, can provide guarantees on support recovery. As we show, neither can the natural brute force algorithm that takes exponential time to find the subset of data and support columns, that yields the smallest regression error.     We explore the power of a simple idea: replace the essential linear algebraic calculation – the inner product – with a robust counterpart that cannot be greatly affected by a controlled number of arbitrarily corrupted points: the trimmed inner product. We consider three popular algorithms in the uncorrupted setting: Thresholding Regression, Lasso, and the Dantzig selector, and show that the counterparts obtained using the trimmed inner product are provably robust.",http://proceedings.mlr.press/v28/chen13h.html,http://proceedings.mlr.press/v28/chen13h.pdf,ICML
2804,2013,Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines,"Kohei Ogawa,         Motoki Imamura,         Ichiro Takeuchi,         Masashi Sugiyama","The semi-supervised support vector machine (S3VM) is a maximum-margin classification algorithm based on both labeled and unlabeled data. Training S3VM involves either a combinatorial or non-convex optimization problem and thus finding the global optimal solution is intractable in practice. It has been demonstrated that a key to successfully find a good (local) solution of S3VM is to gradually increase the effect of unlabeled data, a la annealing. However, existing algorithms suffer from the trade-off between the resolution of annealing steps and the computation cost. In this paper, we go beyond this trade-off by proposing a novel training algorithm that efficiently performs annealing with an infinitesimal resolution. Through experiments, we demonstrate that the proposed infinitesimal annealing algorithm tends to produce better solutions with less computation time than existing approaches.",http://proceedings.mlr.press/v28/ogawa13a.html,http://proceedings.mlr.press/v28/ogawa13a.pdf,ICML
2805,2013,Mixture of Mutually Exciting Processes for Viral Diffusion,"Shuang-Hong Yang,         Hongyuan Zha","\emphDiffusion network inference and \emphmeme tracking have been two key challenges in viral diffusion. This paper shows that these two tasks can be addressed simultaneously with a probabilistic model involving a mixture of mutually exciting point processes. A fast learning algorithms is developed based on mean-field variational inference with budgeted diffusion bandwidth. The model is demonstrated with applications to the diffusion of viral texts in (1) online social networks (e.g., Twitter) and (2) the blogosphere on the Web.",http://proceedings.mlr.press/v28/yang13a.html,http://proceedings.mlr.press/v28/yang13a.pdf,ICML
2806,2013,Differentially Private Learning with Kernels,"Prateek Jain,         Abhradeep Thakurta","In this paper, we consider the problem of differentially private learning where access to the training features is through a kernel function only. Existing work on this problem is restricted to translation invariant kernels only, where (approximate) training features are available explicitly.  In fact, for general class of kernel functions and in general setting of releasing different private predictor (\w^*), the problem is impossible to solve \citeCMS11. In this work, we relax the problem setting into three different easier but practical settings. In our first problem setting, we consider an interactive model where the user sends its test set to a trusted learner who sends back differentially private predictions over the test points. This setting is prevalent in modern online systems like search engines, ad engines etc. In the second model, the learner sends back a differentially private version of the optimal parameter vector \w^* but requires access to a small subset of unlabeled test set beforehand. This also is a practical setting that involves two parties interacting through trusted third party. Our third model is similar to the traditional model, where learner is oblivious to the test set and needs to send a differentially private version of \w^*, but the kernels are restricted to efficiently computable functions over low-dimensional vector spaces. For each of the models, we derive differentially private learning algorithms with provable “utlity” or error bounds. Moreover, we  show that our methods can also be applied to the traditional setting of \cite Rubinstein09, CMS11. Here, our sample complexity bounds have only O(d^1/3) dependence on the dimensionality d while existing methods require O(d^1/2) samples to achieve same generalization error.",http://proceedings.mlr.press/v28/jain13.html,http://proceedings.mlr.press/v28/jain13.pdf,ICML
2807,2013,Dependent Normalized Random Measures,"Changyou Chen,         Vinayak Rao,         Wray Buntine,         Yee Whye Teh","In this paper we propose two constructions of dependent normalized random measures, a class of nonparametric priors over dependent probability measures. Our constructions, which we call mixed normalized random measures (MNRM) and thinned normalized random measures (TNRM), involve (respectively) weighting and thinning parts of a shared underlying Poisson process before combining them together. We show that both MNRM and TNRM are marginally normalized random measures, resulting in well understood theoretical properties. We develop marginal and slice samplers for both models, the latter necessary for inference in TNRM. In time-varying topic modelling experiments, both models exhibit superior performance over related dependent models such as the hierarchical Dirichlet process and the spatial normalized Gamma process.",http://proceedings.mlr.press/v28/chen13i.html,http://proceedings.mlr.press/v28/chen13i.pdf,ICML
2808,2013,Efficient Active Learning of Halfspaces: an Aggressive Approach,"Alon Gonen,         Sivan Sabato,         Shai Shalev-Shwartz","We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efficient and  practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches.  Our efficient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in significantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings.",http://proceedings.mlr.press/v28/gonen13.html,http://proceedings.mlr.press/v28/gonen13.pdf,ICML
2809,2013,A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems,"Pinghua Gong,         Changshui Zhang,         Zhaosong Lu,         Jianhua Huang,         Jieping Ye","Non-convex sparsity-inducing penalties have recently received considerable attentions in sparse learning. Recent theoretical investigations have demonstrated their superiority over the convex counterparts in several sparse learning settings. However, solving the non-convex optimization problems associated with non-convex penalties remains a big challenge. A commonly used approach is the Multi-Stage (MS) convex relaxation (or DC programming), which relaxes the original non-convex problem to a sequence of convex problems. This approach is usually not very practical for large-scale problems because its computational cost is a multiple of solving a single convex problem. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm to solve the nonconvex optimization problem for a large class of non-convex penalties. The GIST algorithm iteratively solves a proximal operator problem, which in turn has a closed-form solution for many commonly used penalties. At each outer iteration of the algorithm, we use a line search initialized by the Barzilai-Borwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets.",http://proceedings.mlr.press/v28/gong13a.html,http://proceedings.mlr.press/v28/gong13a.pdf,ICML
2810,2013,A non-IID Framework for Collaborative Filtering with Restricted Boltzmann Machines,"Kostadin Georgiev,         Preslav Nakov","We propose a framework for collaborative filtering based on Restricted Boltzmann Machines (RBM), which extends previous RBM-based approaches in several important directions. First, while previous RBM research has focused on modeling the correlation between item ratings, we model both user-user and item-item correlations in a unified hybrid non-IID framework. We further use real values in the visible layer as opposed to multinomial variables, thus taking advantage of the natural order between user-item ratings. Finally, we explore the potential of combining the original training data with data generated by the RBM-based model itself in a bootstrapping fashion. The evaluation on two MovieLens datasets (with 100K and 1M user-item ratings, respectively), shows that our RBM model rivals the best previously-proposed approaches.",http://proceedings.mlr.press/v28/georgiev13.html,http://proceedings.mlr.press/v28/georgiev13.pdf,ICML
2811,2013,Two-Sided Exponential Concentration Bounds for Bayes Error Rate and Shannon Entropy,"Jean Honorio,         Jaakkola Tommi","We provide a method that approximates the Bayes error rate and the Shannon entropy with high probability. The Bayes error rate approximation makes possible to build a classifier that polynomially approaches Bayes error rate. The Shannon entropy approximation provides provable performance guarantees for learning trees and Bayesian networks from continuous variables. Our results rely on some reasonable regularity conditions of the unknown probability distributions, and apply to bounded as well as unbounded variables.",http://proceedings.mlr.press/v28/honorio13.html,http://proceedings.mlr.press/v28/honorio13.pdf,ICML
2812,2013,Robust Regression on MapReduce,"Xiangrui Meng,         Michael Mahoney","Although the MapReduce framework is now the \emphde facto standard for   analyzing massive data sets, many algorithms (in particular, many   iterative algorithms popular in machine learning, optimization, and linear   algebra) are hard to fit into MapReduce.   Consider, \emphe.g., the \ell_p regression problem: given a matrix   A ∈\mathbbR^m \times n and a vector b ∈\mathbbR^m, find a   vector x^* ∈\mathbbR^n that minimizes f(x) = \|A x - b\|_p.   The widely-used \ell_2 regression, \emphi.e., linear least-squares, is   known to be highly sensitive to outliers; and choosing p ∈[1, 2) can   help improve robustness.  In this work, we propose an efficient algorithm for solving strongly   over-determined (m ≫n) robust \ell_p regression problems to moderate precision on   MapReduce.  Our empirical results on data up to the terabyte scale demonstrate   that our algorithm is a significant improvement over traditional iterative algorithms on MapReduce   for \ell_1 regression, even for a fairly small number   of iterations.   In addition, our proposed interior-point cutting-plane method can also be   extended to solving more general convex problems on MapReduce.",http://proceedings.mlr.press/v28/meng13b.html,http://proceedings.mlr.press/v28/meng13b.pdf,ICML
2813,2013,Unfolding Latent Tree Structures using 4th Order Tensors,"Mariya Ishteva,         Haesun Park,         Le Song","Discovering the latent structure from many observed variables is an important yet challenging learning task. Existing approaches for discovering latent structures often require the unknown number of hidden states as an input. In this paper, we propose a quartet based approach which is agnostic to this number. The key contribution is a novel rank characterization of the tensor associated with the marginal distribution of a quartet. This characterization allows us to design a nuclear norm based test for resolving quartet relations. We then use the quartet test as a subroutine in a divide-and-conquer algorithm for recovering the latent tree structure. Under mild conditions, the algorithm is consistent and its error probability decays exponentially with increasing sample size. We demonstrate that the proposed approach compares favorably to alternatives. In a real world stock dataset, it also discovers meaningful groupings of variables, and produces a model that fits the data better.",http://proceedings.mlr.press/v28/ishteva13.html,http://proceedings.mlr.press/v28/ishteva13.pdf,ICML
2814,2013,LDA Topic Model with Soft Assignment of Descriptors to Words,"Daphna Weinshall,         Gal Levi,         Dmitri Hanukaev","The LDA topic model is being used to model corpora of documents that can be represented by bags of words. Here we extend the LDA model to deal with documents that are represented more naturally by bags of continuous descriptors. Given a finite dictionary of words which are generative models of descriptors, our extended LDA model allows for the soft assignment of descriptors to (many) dictionary words. We derive variational inference and parameter estimation procedures for the extended model, which closely resemble those obtained for the original model, with two important differences: First, the histogram of word counts is replaced by a histogram of pseudo word counts, or sums of responsibilities over all descriptors. Second, parameter estimation now depends on the average covariance matrix between these pseudo-counts, reflecting the fact that with soft assignment words are not independent.    We use this approach to address novelty detection, where we seek to identify video events with low posterior probability. Video events are described by a generative dynamic texture model, from which we naturally derive a dictionary of generative words.  Using a benchmark dataset for novelty detection, we show a very significant improvement in the detection of novel events when using our extended LDA model with soft assignment to words as against hard assignment (the original model), achieving state of the art novelty detection results.",http://proceedings.mlr.press/v28/weinshall13.html,http://proceedings.mlr.press/v28/weinshall13.pdf,ICML
2815,2013,Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes,"Ohad Shamir,         Tong Zhang","Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines.  In this paper, we investigate the performance of SGD \emphwithout such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the \emphlast SGD iterate scales as O(\log(T)/\sqrtT) for non-smooth convex objective functions, and O(\log(T)/T) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in \citetRakhShaSri12arxiv is not as simple to implement). Finally, we provide some experimental illustrations.",http://proceedings.mlr.press/v28/shamir13.html,http://proceedings.mlr.press/v28/shamir13.pdf,ICML
2816,2013,The Sample-Complexity of General Reinforcement Learning,"Tor Lattimore,         Marcus Hutter,         Peter Sunehag","We study the sample-complexity of reinforcement learning in a general setting without  assuming ergodicity or finiteness of the environment. Instead, we define a topology  on the space of environments and show that  if an environment class is compact with respect to this topology then finite sample-complexity bounds are possible and give an  algorithm achieving these bounds. We also  show the existence of environment classes  that are non-compact where finite sample-complexity bounds are not achievable. A  lower bound is presented that matches the  upper bound except for logarithmic factors.",http://proceedings.mlr.press/v28/lattimore13.html,http://proceedings.mlr.press/v28/lattimore13.pdf,ICML
2817,2013,Large-Scale Bandit Problems and KWIK Learning,"Jacob Abernethy,         Kareem Amin,         Michael Kearns,         Moez Draief","We show that parametric multi-armed bandit (MAB) problems with large state and action spaces can be algorithmically reduced to the supervised learning model known as Knows What It Knows or KWIK learning. We give matching impossibility results showing that the KWIK learnability requirement cannot be replaced by weaker supervised learning assumptions. We provide such results in both the standard parametric MAB setting, as well as for a new model in which the action space is finite but growing with time.",http://proceedings.mlr.press/v28/abernethy13.html,http://proceedings.mlr.press/v28/abernethy13.pdf,ICML
2818,2013,Maxout Networks,"Ian Goodfellow,         David Warde-Farley,         Mehdi Mirza,         Aaron Courville,         Yoshua Bengio","We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.",http://proceedings.mlr.press/v28/goodfellow13.html,http://proceedings.mlr.press/v28/goodfellow13.pdf,ICML
2819,2013,Cost-Sensitive Tree of Classifiers,"Zhixiang Xu,         Matt Kusner,         Kilian Weinberger,         Minmin Chen","Recently, machine learning algorithms have successfully entered large-scale real-world industrial applications (e.g. search engines and email spam filters). Here, the CPU cost during test-time must be budgeted and accounted for. In this paper, we address the challenge of balancing test-time cost and the classifier accuracy in a principled fashion. The test-time cost of a classifier is often dominated by the computation required for feature extraction-which can vary drastically across features. We incorporate this extraction time by constructing a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. By only computing features for inputs that benefit from them the most, our cost-sensitive tree of classifiers can match the high accuracies of the current state-of-the-art at a small fraction of the computational cost.",http://proceedings.mlr.press/v28/xu13.html,http://proceedings.mlr.press/v28/xu13.pdf,ICML
2820,2013,Learning Policies for Contextual Submodular Prediction,"Stephane Ross,         Jiaji Zhou,         Yisong Yue,         Debadeepta Dey,         Drew Bagnell","Many prediction domains, such as ad placement, recommendation, trajectory prediction, and document summarization, require predicting a set or list of options. Such lists are often evaluated using submodular reward functions that measure both quality and diversity. We propose a simple, efficient, and provably near-optimal approach to optimizing such prediction problems based on no-regret learning. Our method leverages a surprising result from online submodular optimization: a single no-regret online learner can compete with an optimal sequence of predictions. Compared to previous work, which either learn a sequence of classifiers or rely on stronger assumptions such as realizability, we ensure both data-efficiency as well as performance guarantees in the fully agnostic setting. Experiments validate the efficiency and applicability of the approach on a wide range of problems including manipulator trajectory optimization, news recommendation and document summarization.",http://proceedings.mlr.press/v28/ross13b.html,http://proceedings.mlr.press/v28/ross13b.pdf,ICML
2821,2013,Guided Policy Search,"Sergey Levine,         Vladlen Koltun","Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.",http://proceedings.mlr.press/v28/levine13.html,http://proceedings.mlr.press/v28/levine13.pdf,ICML
2822,2013,Loss-Proportional Subsampling for Subsequent ERM,"Paul Mineiro,         Nikos Karampatziakis","We propose a sampling scheme suitable for reducing a data set prior to  selecting a hypothesis with minimum empirical risk.  The sampling only  considers a subset of the ultimate (unknown) hypothesis set, but can  nonetheless guarantee that the final excess risk will compare favorably  with utilizing the entire original data set. We demonstrate the practical  benefits of our approach on a large dataset which we subsample and  subsequently fit with boosted trees.",http://proceedings.mlr.press/v28/mineiro13.html,http://proceedings.mlr.press/v28/mineiro13.pdf,ICML
2823,2013,On learning parametric-output HMMs,"Aryeh Kontorovich,         Boaz Nadler,         Roi Weiss","We present a novel approach to learning an HMM whose outputs are distributed according to a parametric family. This is done by \em decoupling the learning task into two steps: first estimating the output parameters, and then estimating the hidden states transition probabilities. The first step is accomplished by fitting a mixture model to the output stationary distribution. Given the parameters of this mixture model, the second step is formulated as the solution of an easily solvable convex quadratic program. We provide an error analysis for the estimated transition probabilities and show they are robust to small perturbations in the estimates of the mixture parameters. Finally, we support our analysis with some encouraging empirical results.",http://proceedings.mlr.press/v28/kontorovich13.html,http://proceedings.mlr.press/v28/kontorovich13.pdf,ICML
2824,2013,Spectral Experts for Estimating Mixtures of Linear Regressions,"Arun Tejasvi Chaganty,         Percy Liang","Discriminative latent-variable models are typically learned using EM or gradient-based optimization, which suffer from local optima.  In this paper, we develop a new computationally efficient and provably consistent estimator for the mixture of linear regressions, a simple instance of discriminative latent-variable models.  Our approach relies on a low-rank linear regression to recover a symmetric tensor, which can be factorized into the parameters using the tensor power method.  We prove rates of convergence for our estimator and provide an empirical evaluation illustrating its strengths relative to local optimization (EM).",http://proceedings.mlr.press/v28/tejasvichaganty13.html,http://proceedings.mlr.press/v28/tejasvichaganty13.pdf,ICML
2825,2013,A New Frontier of Kernel Design for Structured Data,Kilho Shin,"Many kernels for discretely structured data in the literature are designed within the framework of the convolution kernel and its generalization, the mapping kernel. The two most important advantages to use this framework is an easy-to-check criteria of positive definiteness and efficient computation based on the dynamic programming methodology of the resulting kernels.  On the other hand, the recent theory of partitionable kernels reveals that the known kernels only take advantage of a very small portion of the potential of the framework.  In fact, we have good opportunities to find novel and important kernels in the unexplored area.  In this paper, we shed light on a novel important class of kernels within the framework: We give a mathematical characterization of the class, show a parametric method to optimize kernels of the class to specific problems, based on this characterization, and present some experimental results, which show the new kernels are promising in both accuracy and efficiency.",http://proceedings.mlr.press/v28/shin13.html,http://proceedings.mlr.press/v28/shin13.pdf,ICML
2826,2013,Characterizing the Representer Theorem,"Yaoliang Yu,         Hao Cheng,         Dale Schuurmans,         Csaba Szepesvari","The representer theorem assures that kernel methods retain optimality under penalized empirical risk minimization. While a sufficient condition on the form of the regularizer guaranteeing the representer theorem has been known since the initial development of kernel methods, necessary conditions have only been investigated recently. In this paper we completely characterize the necessary and sufficient conditions on the regularizer that ensure the representer theorem holds. The results are surprisingly simple yet broaden the conditions where the representer theorem is known to hold. Extension to the matrix domain is also addressed.",http://proceedings.mlr.press/v28/yu13.html,http://proceedings.mlr.press/v28/yu13.pdf,ICML
2827,2013,"Margins, Shrinkage, and Boosting",Matus Telgarsky,"This manuscript shows that AdaBoost and its immediate variants can produce approximately maximum margin classifiers simply by scaling their step size choices by a fixed small constant. In this way, when the unscaled step size is an optimal choice, these results provide guarantees for Friedman’s empirically successful “shrinkage” procedure for gradient boosting (Friedman, 2000).  Guarantees are also provided for a variety of other step sizes, affirming the intuition that increasingly regularized line searches provide improved margin guarantees. The results hold for the exponential loss and similar losses, most notably the logistic loss.",http://proceedings.mlr.press/v28/telgarsky13.html,http://proceedings.mlr.press/v28/telgarsky13.pdf,ICML
2828,2013,Nested Chinese Restaurant Franchise Process:  Applications to User Tracking and Document Modeling,"Amr Ahmed,         Liangjie Hong,         Alexander Smola","Much natural data is hierarchical in nature. Moreover, this hierarchy  is often shared between different instances. We introduce the  nested Chinese Restaurant Franchise Process as a means to obtain both  hierarchical tree-structured representations for objects, akin to (but more general than) the nested Chinese Restaurant Process while sharing their structure akin  to the Hierarchical Dirichlet Process.     Moreover, by decoupling the \emphstructure generating part of the  process from the components responsible for the observations, we are  able to apply the same statistical approach to a variety of user  generated data. In particular, we model the joint distribution of  microblogs and locations for Twitter for users. This leads to a 40%  reduction in location uncertainty relative to the best previously  published results. Moreover, we model documents from the NIPS papers  dataset, obtaining excellent perplexity relative to (hierarchical)  Pachinko allocation and LDA.",http://proceedings.mlr.press/v28/ahmed13.html,http://proceedings.mlr.press/v28/ahmed13.pdf,ICML
2829,2013,Learning Sparse Penalties for Change-point Detection using Max Margin Interval Regression,"Toby Hocking,         Guillem Rigaill,         Jean-Philippe Vert,         Francis Bach","In segmentation models, the number of change-points is typically    chosen using a penalized cost function.  In this work, we propose to    learn the penalty and its constants in databases of signals with    weak change-point annotations. We propose a convex relaxation for    the resulting interval regression problem, and solve it using    accelerated proximal gradient methods. We show that this method    achieves state-of-the-art change-point detection in a database of    annotated DNA copy number profiles from neuroblastoma tumors.",http://proceedings.mlr.press/v28/hocking13.html,http://proceedings.mlr.press/v28/hocking13.pdf,ICML
2830,2013,Fast Semidifferential-based Submodular Function Optimization,"Rishabh Iyer,         Stefanie Jegelka,         Jeff Bilmes","We present a practical and powerful new framework for both unconstrained and constrained submodular function optimization based on discrete semidifferentials (sub- and super-differentials). The resulting algorithms, which repeatedly compute and then efficiently optimize submodular semigradients, offer new and generalize many old methods for submodular optimization.  Our approach, moreover, takes steps towards providing a unifying paradigm applicable to both submodular minimization and maximization, problems that historically have been treated quite distinctly. The practicality of our algorithms is important since interest in submodularity, owing to its natural and wide applicability, has recently been in ascendance within machine learning.  We analyze theoretical properties of our algorithms for minimization and maximization, and show that many state-of-the-art maximization algorithms are special cases. Lastly, we complement our theoretical analyses with supporting empirical experiments.",http://proceedings.mlr.press/v28/iyer13.html,http://proceedings.mlr.press/v28/iyer13.pdf,ICML
2831,2013,Activized Learning with Uniform Classification Noise,"Liu Yang,         Steve Hanneke","We prove that for any VC class, it is possible to transform any passive learning algorithm into an active learning algorithm with strong asymptotic improvements in label complexity  for every nontrivial distribution satisfying a uniform classification noise condition.  This generalizes a similar result proven by  (Hanneke, 2009;2012) for the realizable case,  and is the first result establishing that such general improvement guarantees are possible  in the presence of restricted types of  classification noise.",http://proceedings.mlr.press/v28/yang13c.html,http://proceedings.mlr.press/v28/yang13c.pdf,ICML
2832,2013,Efficient Multi-label Classification with Many Labels,"Wei Bi,         James Kwok","Multi-label classification deals with the problem where each instance can be associated with a set of class labels. However, in many real-world applications, the number of class labels can be in the hundreds or even thousands, and existing multi-label classification methods often become computationally inefficient. In recent years, a number of remedies have been proposed. However, they are either based on simple dimension reduction techniques or involve expensive optimization problems. In this paper, we address this problem by selecting a small subset of class labels that can approximately span the original label space. This is performed by randomized sampling where the sampling probability of each class label reflects its importance among all the labels. Theoretical analysis shows that this randomized sampling approach is highly efficient. Experiments on a number of real-world multi-label datasets with many labels demonstrate the appealing performance and efficiency of the proposed algorithm.",http://proceedings.mlr.press/v28/bi13.html,http://proceedings.mlr.press/v28/bi13.pdf,ICML
2833,2013,Monochromatic Bi-Clustering,"Sharon Wulff,         Ruth Urner,         Shai Ben-David","We propose a natural cost function for the bi-clustering task, the monochromatic cost.  This cost function is suitable for detecting meaningful homogeneous bi-clusters based on categorical valued input matrices. Such tasks arise in many applications, such as the analysis of social networks and in systems-biology where researchers try to infer functional grouping of biological agents based on their pairwise interactions. We analyze the computational complexity of the resulting optimization problem. We present a polynomial time approximation algorithm for this bi-clustering task and complement this result by showing that finding (exact) optimal solutions is NP-hard. As far as we know, these are the first positive approximation guarantees  and formal NP-hardness results  for any bi-clustering optimization problem.  In addition, we show that our optimization problem can be efficiently  solved by deterministic annealing,  yielding a promising heuristic for large problem instances.",http://proceedings.mlr.press/v28/wulff13.html,http://proceedings.mlr.press/v28/wulff13.pdf,ICML
2834,2013,Hierarchical Regularization Cascade for Joint Learning,"Alon Zweig,         Daphna Weinshall","As the sheer volume of available benchmark datasets increases, the problem of joint learning of classifiers and knowledge-transfer between classifiers, becomes more and more relevant. We present a hierarchical approach which exploits information sharing among different classification tasks, in multi-task and multi-class settings. It engages a top-down iterative method, which begins by posing an optimization problem with an incentive for large scale sharing among all classes. This incentive to share is gradually decreased,until there is no sharing and all tasks are considered separately. The method therefore exploits different levels of sharing within a given group of related tasks, without having to make hard decisions about the grouping of tasks. In order to deal with large scale problems, with many tasks and many classes, we extend our batch approach to an online setting and provide regret analysis of the algorithm. We tested our approach extensively on synthetic and real datasets, showing significant improvement over baseline and state-of-the-art methods.",http://proceedings.mlr.press/v28/zweig13.html,http://proceedings.mlr.press/v28/zweig13.pdf,ICML
2835,2013,Subtle Topic Models and Discovering Subtly Manifested Software Concerns Automatically,"Mrinal Das,         Suparna Bhattacharya,         Chiranjib Bhattacharyya,         Gopinath Kanchi","In a recent pioneering approach LDA was used to discover cross cutting concerns(CCC) automatically from software codebases. LDA though successful in detecting prominent concerns, fails to detect many useful CCCs including ones that may be heavily executed but elude discovery because they do not have a strong prevalence in source-code. We pose this problem as that of discovering topics that rarely occur in individual documents, which we will refer to as subtle topics. Recently an interesting approach, namely focused topic models(FTM) was proposed for detecting rare topics. FTM, though successful in detecting topics which occur prominently in very few documents, is unable to detect subtle topics. Discovering subtle topics thus remains an important open problem. To address this issue we propose subtle topic models(STM). STM uses a generalized stick breaking process(GSBP) as a prior for defining multiple distributions over topics. This hierarchical structure on topics allows STM to discover rare topics beyond the capabilities of FTM. The associated inference is non-standard and is solved by exploiting the relationship between GSBP and generalized Dirichlet distribution. Empirical results show that STM is able to discover subtle CCC in two benchmark code-bases, a feat which is beyond the scope of existing topic models, thus demonstrating the potential of the model in automated concern discovery, a known difficult problem in Software Engineering. Furthermore it is observed that even in general text corpora STM outperforms the state of art in discovering subtle topics.",http://proceedings.mlr.press/v28/das13.html,http://proceedings.mlr.press/v28/das13.pdf,ICML
2836,2013,Mean Reversion with a Variance Threshold,"Marco Cuturi,         Alexandre D’Aspremont","Starting from a multivariate data set, we study several techniques to isolate affine combinations of the variables with a maximum amount of mean reversion, while constraining the variance to be larger than a given threshold. We show that many of the optimization problems arising in this context can be solved exactly using semidefinite programming and some variant of the \mathcalS-lemma. In finance, these methods are used to isolate statistical arbitrage opportunities, i.e. mean reverting portfolios with enough variance to overcome market friction. In a more general setting, mean reversion and its generalizations are also used as a proxy for stationarity, while variance simply measures signal strength.",http://proceedings.mlr.press/v28/cuturi13.html,http://proceedings.mlr.press/v28/cuturi13.pdf,ICML
2837,2013,Multi-View Clustering and Feature Learning via Structured Sparsity,"Hua Wang,         Feiping Nie,         Heng Huang","Combining information from various data sources has become an important research topic in machine learning with many scientific applications. Most previous studies employ kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of features in one source to an individual cluster of data can be varied, which makes the previous approaches ineffective. In this paper, we propose a novel multi-view learning model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint structured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoretical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods.",http://proceedings.mlr.press/v28/wang13c.html,http://proceedings.mlr.press/v28/wang13c.pdf,ICML
2838,2013,Fixed-Point Model For Structured Labeling,"Quannan Li,         Jingdong Wang,         David Wipf,         Zhuowen Tu","In this paper, we propose a simple but effective  solution to the structured labeling problem:  a fixed-point model. Recently, layered  models with sequential classifiers/regressors  have gained an increasing amount of interests  for structural prediction. Here, we design an  algorithm with a new perspective on layered  models; we aim to find a fixed-point function  with the structured labels being both the  output and the input. Our approach alleviates  the burden in learning multiple/different  classifiers in different layers. We devise a  training strategy for our method and provide  justifications for the fixed-point function  to be a contraction mapping. The learned  function captures rich contextual information  and is easy to train and test. On several  widely used benchmark datasets, the proposed  method observes significant improvement  in both performance and efficiency over  many state-of-the-art algorithms.",http://proceedings.mlr.press/v28/li13b.html,http://proceedings.mlr.press/v28/li13b.pdf,ICML
2839,2013,Non-Linear Stationary Subspace Analysis with Application to Video Classification,"Mahsa Baktashmotlagh,         Mehrtash Harandi,         Abbas Bigdeli,         Brian Lovell,         Mathieu Salzmann","Low-dimensional representations are key to the success of many video classification algorithms. However, the commonly-used dimensionality reduction techniques fail to account for the fact that only part of the signal is shared across all the videos in one class. As a consequence, the resulting representations contain instance-specific information, which introduces noise in the classification process. In this paper, we introduce Non-Linear Stationary Subspace Analysis: A method that overcomes this issue by explicitly separating the stationary parts of the video signal (i.e., the parts shared across all videos in one class), from its non-stationary parts (i.e., specific to individual videos). We demonstrate the effectiveness of our approach on action recognition, dynamic texture classification and scene recognition.",http://proceedings.mlr.press/v28/baktashmotlagh13.html,http://proceedings.mlr.press/v28/baktashmotlagh13.pdf,ICML
2840,2013,Quickly Boosting Decision Trees – Pruning Underachieving Features Early,"Ron Appel,         Thomas Fuchs,         Piotr Dollar,         Pietro Perona","Boosted decision trees are one of the most popular and successful learning techniques used today.  While exhibiting fast speeds at test time, relatively slow training makes them impractical for applications with real-time learning requirements. We propose a principled approach to overcome this drawback. We prove a bound on the error of a decision stump given its preliminary error on a subset of the training data; the bound may be used to prune unpromising features early on in the training process. We propose a fast training algorithm that exploits this bound, yielding speedups of an order of magnitude at no cost in the final performance of the classifier. Our method is not a new variant of Boosting; rather, it may be used in conjunction with existing Boosting algorithms and other sampling heuristics to achieve even greater speedups.",http://proceedings.mlr.press/v28/appel13.html,http://proceedings.mlr.press/v28/appel13.pdf,ICML
2841,2013,Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method,Taiji Suzuki,"We develop new stochastic optimization methods that are applicable to   a wide range of structured regularizations.  Basically our methods are combinations of   basic stochastic optimization techniques and Alternating Direction Multiplier Method (ADMM).  ADMM is a general framework for optimizing a composite function,  and has a wide range of applications.  We propose two types of online variants of ADMM,   which correspond to online proximal gradient descent and regularized dual averaging respectively.  The proposed algorithms are computationally efficient and easy to implement.  Our methods yield O(1/\sqrtT) convergence of the expected risk.  Moreover, the online proximal gradient descent type method yields   O(\log(T)/T) convergence for a strongly convex loss.  Numerical experiments show effectiveness of our methods in learning tasks with structured sparsity  such as overlapped group lasso.",http://proceedings.mlr.press/v28/suzuki13.html,http://proceedings.mlr.press/v28/suzuki13.pdf,ICML
2842,2013,Structure Discovery in Nonparametric Regression through Compositional Kernel Search,"David Duvenaud,         James Lloyd,         Roger Grosse,         Joshua Tenenbaum,         Ghahramani Zoubin","Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.",http://proceedings.mlr.press/v28/duvenaud13.html,http://proceedings.mlr.press/v28/duvenaud13.pdf,ICML
2843,2013,Natural Image Bases to Represent Neuroimaging Data,"Ashish Gupta,         Murat Ayhan,         Anthony Maida","Visual inspection of neuroimagery is susceptible to human eye limitations.  Computerized methods have been shown to be equally or more effective   than human clinicians in diagnosing dementia from neuroimages. Nevertheless,   much of the work involves the use of domain expertise to extract hand-crafted features. The key technique in this paper is the use of cross-domain features to represent MRI data.  We used a sparse autoencoder to learn a set of bases from natural images and   then applied convolution to extract features from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset.Using this new representation, we classify MRI instances into three categories: Alzheimer’s Disease (AD), Mild Cognitive Impairment (MCI) and Healthy Control (HC).Our approach, in spite of being very simple, achieved high classification performance,   which is competitive with or better than other approaches.",http://proceedings.mlr.press/v28/gupta13b.html,http://proceedings.mlr.press/v28/gupta13b.pdf,ICML
2844,2013,Fast Image Tagging,"Minmin Chen,         Alice Zheng,         Kilian Weinberger","Automatic image annotation is a difficult and highly relevant machine learning task. Recent advances have significantly improved the state-of-the-art in retrieval accuracy with algorithms based on nearest neighbor classification in carefully learned metric spaces. But this comes at a price of increased computational complexity during training and testing. We propose FastTag, a novel algorithm that achieves comparable results with two simple linear mappings that are co-regularized in a joint convex loss function. The loss function can be efficiently optimized in closed form updates, which allows us to incorporate a large number of image descriptors cheaply. On several standard real-world benchmark data sets, we demonstrate that FastTag matches the current state-of-the-art in tagging quality, yet reduces the training and testing times by several orders of magnitude and has lower asymptotic complexity.",http://proceedings.mlr.press/v28/chen13j.html,http://proceedings.mlr.press/v28/chen13j.pdf,ICML
2845,2013,Stochastic Simultaneous Optimistic Optimization,"Michal Valko,         Alexandra Carpentier,         Rémi Munos","We study the problem of global maximization of a function f given a finite number of evaluations perturbed by noise. We consider a very weak assumption on the function, namely that it is locally smooth (in some precise sense) with respect to some semi-metric, around one of its global maxima. Compared to previous works on bandits in general spaces (Kleinberg et al., 2008; Bubeck et al., 2011a) our algorithm does not require the knowledge of this semi-metric. Our algorithm, StoSOO, follows an optimistic strategy to iteratively construct upper confidence bounds over the hierarchical partitions of the function domain to decide which point to sample next. A finite-time analysis of StoSOO shows that it performs almost as well as the best specifically-tuned algorithms even though the local smoothness of the function is not known.",http://proceedings.mlr.press/v28/valko13.html,http://proceedings.mlr.press/v28/valko13.pdf,ICML
2846,2013,A Variational Approximation for Topic Modeling of Hierarchical Corpora,"Do-kyum Kim,         Geoffrey Voelker,         Lawrence Saul","We study the problem of topic modeling in corpora whose documents are organized in a multi-level hierarchy.  We explore a parametric approach to this problem, assuming that the number of topics is known or can be estimated by cross-validation.  The models we consider can be viewed as special (finite-dimensional) instances of hierarchical Dirichlet processes (HDPs).  For these models we show that there exists a simple variational approximation for probabilistic inference.  The approximation relies on a previously unexploited inequality that handles the conditional dependence between Dirichlet latent variables in adjacent levels of the model’s hierarchy.  We compare our approach to existing implementations of nonparametric HDPs.  On several benchmarks we find that our approach is faster than Gibbs sampling and able to learn more predictive models than existing variational methods.  Finally, we demonstrate the large-scale viability of our approach on two newly available corpora from researchers in computer security–one with 350,000 documents and over 6,000 internal subcategories, the other with a five-level deep hierarchy.",http://proceedings.mlr.press/v28/kim13.html,http://proceedings.mlr.press/v28/kim13.pdf,ICML
2847,2013,On the Statistical Consistency of Algorithms for Binary Classification under Class Imbalance,"Aditya Menon,         Harikrishna Narasimhan,         Shivani Agarwal,         Sanjay Chawla","Class imbalance situations, where one class is rare compared to the other, arise frequently in machine learning applications. It is well known that the usual misclassification error is ill-suited for measuring performance in such settings. A wide range of performance measures have been proposed for this problem, in machine learning as well as in data mining, artificial intelligence, and various applied fields. However, despite the large number of studies on this problem, little is understood about the statistical consistency of the algorithms proposed with respect to the performance measures of interest. In this paper, we study consistency with respect to one such performance measure, namely the arithmetic mean of the true positive and true negative rates (AM), and establish that some simple methods that have been used in practice, such as applying an empirically determined threshold to a suitable class probability estimate or performing an empirically balanced form of risk minimization, are in fact consistent with respect to the AM (under mild conditions on the underlying distribution). Our results employ balanced losses that have been used recently in analyses of ranking problems (Kotlowski et al., 2011) and build on recent results on consistent surrogates for cost-sensitive losses (Scott, 2012). Experimental results confirm our consistency theorems.",http://proceedings.mlr.press/v28/menon13a.html,http://proceedings.mlr.press/v28/menon13a.pdf,ICML
2848,2013,Precision-recall space to correct external indices for biclustering,"Blaise Hanczar,         Mohamed Nadif","Biclustering is a major tool of data mining in many domains and many algorithms have emerged in recent years. All these algorithms aim to obtain coherent biclusters and it is crucial to have a reliable procedure for their validation. We point out the problem of size bias in biclustering evaluation and show how it can lead to wrong conclusions in a comparative study. We present the theoretical corrections for all of the most popular measures in order to remove this bias. We introduce the corrected precision-recall space that combines the advantages of corrected measures, the ease of interpretation and visualization of uncorrected measures. Numerical experiments demonstrate the interest of our approach.",http://proceedings.mlr.press/v28/hanczar13.html,http://proceedings.mlr.press/v28/hanczar13.pdf,ICML
2849,2013,Discriminatively Activated Sparselets,"Ross Girshick,         Hyun Oh Song,         Trevor Darrell","Shared representations are highly appealing due to their potential  for gains in computational and statistical efficiency.  Compressing  a shared representation leads to greater computational savings, but  at the same time can severely decrease performance on a target task.  Recently, sparselets (Song et al., 2012) were introduced as a new  shared intermediate representation for multiclass object detection  with deformable part models (Felzenszwalb et al., 2010a), showing  significant speedup factors, but with a large decrease in task  performance.  In this paper we describe a new training framework  that learns which sparselets to activate in order to optimize a  discriminative objective, leading to larger speedup factors with  no decrease in task performance.  We first reformulate sparselets  in a general structured output prediction framework, then analyze  when sparselets lead to computational efficiency gains, and lastly  show experimental results on object detection and image classification  tasks.  Our experimental results demonstrate that discriminative  activation substantially outperforms the previous reconstructive  approach which, together with our structured output prediction  formulation, make sparselets broadly applicable and significantly  more effective.",http://proceedings.mlr.press/v28/girshick13.html,http://proceedings.mlr.press/v28/girshick13.pdf,ICML
2850,2013,A Practical Algorithm for Topic Modeling with Provable Guarantees,"Sanjeev Arora,         Rong Ge,         Yonatan Halpern,         David Mimno,         Ankur Moitra,         David Sontag,         Yichen Wu,         Michael Zhu","Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model learning have been based on a maximum likelihood objective. Efficient algorithms exist that attempt to approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for learning topic models that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster.",http://proceedings.mlr.press/v28/arora13.html,http://proceedings.mlr.press/v28/arora13.pdf,ICML
2851,2013,A Unified Robust Regression Model for Lasso-like Algorithms,"Wenzhuo Yang,         Huan Xu","We develop a unified robust linear regression model and show that it is equivalent to a general regularization framework to encourage sparse-like structure that contains group Lasso and fused Lasso as specific examples. This provides a robustness interpretation of these widely applied Lasso-like algorithms, and allows us to construct novel generalizations of Lasso-like algorithms by considering different uncertainty sets. Using this robustness interpretation, we present new sparsity results, and establish the statistical consistency of the proposed regularized linear regression. This work extends a classical result from Xu et al. (2010) that relates standard Lasso with robust linear regression to learning problems with more general sparse-like structures, and provides new robustness-based tools to to understand learning problems with sparse-like structures.",http://proceedings.mlr.press/v28/yang13e.html,http://proceedings.mlr.press/v28/yang13e.pdf,ICML
2852,2013,Connecting the Dots with Landmarks:  Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation,"Boqing Gong,         Kristen Grauman,         Fei Sha","Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available. In this paper, we propose a novel approach for learning such features. The central idea is to exploit the existence of landmarks, which are a subset of labeled data instances in the source domain that are distributed most similarly to the target domain. Our approach automatically discovers the landmarks and use them to bridge the source to the target by constructing provably easier auxiliary domain adaptation tasks. The solutions of those auxiliary tasks form the basis to compose invariant features for the original task. We show how this composition can be optimized discriminatively without requiring labels from the target domain. We validate the method on standard benchmark datasets for visual object recognition and sentiment analysis of text. Empirical results show the proposed method outperforms the state-of-the-art significantly.",http://proceedings.mlr.press/v28/gong13.html,http://proceedings.mlr.press/v28/gong13.pdf,ICML
2853,2013,Tensor Analyzers,"Yichuan Tang,         Ruslan Salakhutdinov,         Geoffrey Hinton","Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its additive nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact multiplicatively. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe an efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches. Tensor Analyzers can also accurately recognize a face under significant pose and illumination variations when given only one previous image of that face. We also show that Tensor Analyzers can be trained in an unsupervised, semi-supervised, or fully supervised settings.",http://proceedings.mlr.press/v28/tang13.html,http://proceedings.mlr.press/v28/tang13.pdf,ICML
2854,2013,Fast algorithms for sparse principal component analysis based on Rayleigh quotient iteration,Volodymyr Kuleshov,"We introduce new algorithms for sparse principal component analysis (sPCA), a variation of PCA which aims to represent data in a sparse low-dimensional basis. Our algorithms possess a cubic rate of convergence and can compute principal components with k non-zero elements at a cost of O(nk + k^3) flops per iteration. We observe in numerical experiments that these components are of equal or greater quality than ones obtained from current state-of-the-art techniques, but require between one and two orders of magnitude fewer flops to be computed. Conceptually, our approach generalizes the Rayleigh quotient iteration algorithm for computing eigenvectors, and can be interpreted as a type of second-order optimization method. We demonstrate the applicability of our algorithms on several datasets, including the STL-10 machine vision dataset and gene expression data.",http://proceedings.mlr.press/v28/kuleshov13.html,http://proceedings.mlr.press/v28/kuleshov13.pdf,ICML
2855,2013,Online Learning under Delayed Feedback,"Pooria Joulani,         Andras Gyorgy,         Csaba Szepesvari","Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.",http://proceedings.mlr.press/v28/joulani13.html,http://proceedings.mlr.press/v28/joulani13.pdf,ICML
2856,2013,Optimal Regret Bounds for  Selecting the State Representation in Reinforcement Learning,"Odalric-Ambrym Maillard,         Phuong Nguyen,         Ronald Ortner,         Daniil Ryabko","We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows   an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order O(T^2/3) with an additive term constant yet exponential in some characteristics of the optimal MDP.  We propose an algorithm whose regret after T time steps is O(\sqrtT),  with all constants reasonably small.  This is optimal in T since O(\sqrtT) is the optimal regret in the setting of learning in a (single discrete) MDP.",http://proceedings.mlr.press/v28/maillard13.html,http://proceedings.mlr.press/v28/maillard13.pdf,ICML
2857,2013,On the difficulty of training recurrent neural networks,"Razvan Pascanu,         Tomas Mikolov,         Yoshua Bengio","There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.",http://proceedings.mlr.press/v28/pascanu13.html,http://proceedings.mlr.press/v28/pascanu13.pdf,ICML
2858,2013,Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures,"James Bergstra,         Daniel Yamins,         David Cox","Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method’s full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned.     In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included.  A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric.  Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures.",http://proceedings.mlr.press/v28/bergstra13.html,http://proceedings.mlr.press/v28/bergstra13.pdf,ICML
2859,2013,"Algebraic classifiers: a generic approach to fast cross-validation, online training, and parallel training",Michael Izbicki,"We use abstract algebra to derive new algorithms for fast cross-validation, online learning, and parallel learning.  To use these algorithms on a classification model, we must show that the model has appropriate algebraic structure.  It is easy to give algebraic structure to some models, and we do this explicitly for Bayesian classifiers and a novel variation of decision stumps called HomStumps.  But not all classifiers have an obvious structure, so we introduce the Free HomTrainer.  This can be used to give a “generic” algebraic structure to any classifier.  We use the Free HomTrainer to give algebraic structure to bagging and boosting.  In so doing, we derive novel online and parallel algorithms, and present the first fast cross-validation schemes for these classifiers.",http://proceedings.mlr.press/v28/izbicki13.html,http://proceedings.mlr.press/v28/izbicki13.pdf,ICML
2860,2013,A Spectral Learning Approach to Range-Only SLAM,"Byron Boots,         Geoff Gordon","We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences.  This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with MHT and with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, our approach does not need to linearize a transition or measurement model. We provide a theoretical analysis of our method, including finite-sample error bounds.  Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost.",http://proceedings.mlr.press/v28/boots13.html,http://proceedings.mlr.press/v28/boots13.pdf,ICML
2861,2013,Modeling Musical Influence with Topic Models,"Uri Shalit,         Daphna Weinshall,         Gal Chechik","The role of musical influence has long been debated by scholars  and critics in the humanities, but never in a data-driven way.  In this work we approach the question of influence by applying topic-modeling tools (Blei & Lafferty, 2006; Gerrish & Blei, 2010) to a dataset of 24941 songs by 9222 artists, from the years 1922 to 2010. We find the models to be significantly correlated with a human-curated influence measure, and to clearly outperform a baseline method. Further using the learned model to study properties of influence, we find that musical influence and musical innovation are not monotonically correlated. However, we do find that the most influential songs were more innovative during two time periods: the early 1970’s and the mid 1990’s.",http://proceedings.mlr.press/v28/shalit13.html,http://proceedings.mlr.press/v28/shalit13.pdf,ICML
2862,2013,The Extended Parameter Filter,"Yusuf Bugra Erol,         Lei Li,         Bharath Ramsundar,         Russell Stuart","The parameters of temporal models, such as dynamic Bayesian networks, may be modelled in a Bayesian context as static or atemporal variables that influence transition probabilities at every time step. Particle filters fail for models that include such variables, while methods that use Gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence. Storvik devised a method for incremental computation of exact sufficient statistics that, for some cases, reduces the per-sample cost to a constant.  In this paper, we demonstrate a connection between Storvik’s filter and a Kalman filter in parameter space and establish more general conditions under which Storvik’s filter works. Drawing on an analogy to the extended Kalman filter, we develop and analyze, both theoretically and experimentally, a Taylor approximation to the parameter posterior that allows Storvik’s method to be applied to a broader class of models. Our experiments on both synthetic examples and real applications show improvement over existing methods.",http://proceedings.mlr.press/v28/bugraerol13.html,http://proceedings.mlr.press/v28/bugraerol13.pdf,ICML
2863,2013,Adaptive Task Assignment for Crowdsourced Classification,"Chien-Ju Ho,         Shahin Jabbari,         Jennifer Wortman Vaughan","Crowdsourcing markets have gained popularity as a tool for inexpensively collecting data from diverse populations of workers. Classification tasks, in which workers provide labels (such as “offensive” or “not offensive”) for instances (such as websites), are among the most common tasks posted, but due to a mix of human error and the overwhelming prevalence of spam, the labels collected are often noisy. This problem is typically addressed by collecting labels for each instance from multiple workers and combining them in a clever way. However, the question of how to choose which tasks to assign to each worker is often overlooked. We investigate the problem of task assignment and label inference for heterogeneous classification tasks. By applying online primal-dual techniques, we derive a provably near-optimal adaptive assignment algorithm. We show that adaptively assigning workers to tasks can lead to more accurate predictions at a lower cost when the available workers are diverse.",http://proceedings.mlr.press/v28/ho13.html,http://proceedings.mlr.press/v28/ho13.pdf,ICML
2864,2013,Local Low-Rank Matrix Approximation,"Joonseok Lee,         Seungyeon Kim,         Guy Lebanon,         Yoram Singer","Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks.",http://proceedings.mlr.press/v28/lee13.html,http://proceedings.mlr.press/v28/lee13.pdf,ICML
2865,2013,An Adaptive Learning Rate for Stochastic Variational Inference,"Rajesh Ranganath,         Chong Wang,         Blei David,         Eric Xing","Stochastic variational inference finds good posterior approximations of probabilistic models with very large data sets.  It optimizes the variational objective with stochastic optimization, following noisy estimates of the natural gradient.  Operationally, stochastic inference iteratively subsamples from the data, analyzes the subsample, and updates parameters with a decreasing learning rate. However, the algorithm is sensitive to that rate, which usually requires hand-tuning to each application. We solve this problem by developing an adaptive learning rate for stochastic inference.  Our method requires no tuning and is easily implemented with computations already made in the algorithm.  We demonstrate our approach with latent Dirichlet allocation applied to three large text corpora.  Inference with the adaptive learning rate converges faster and to a better approximation than the best settings of hand-tuned rates.",http://proceedings.mlr.press/v28/ranganath13.html,http://proceedings.mlr.press/v28/ranganath13.pdf,ICML
2866,2013,The Bigraphical Lasso,"Alfredo Kalaitzis,         John Lafferty,         Neil D. Lawrence,         Shuheng Zhou","The i.i.d. assumption in machine learning is endemic, but often flawed. Complex data sets exhibit partial correlations between both instances and features. A model specifying both types of correlation can have a number of parameters that scales quadratically with the number of features and data points. We introduce the bigraphical lasso, an estimator for precision matrices of matrix-normals based on the Cartesian product of graphs. A prominent product in spectral graph theory, this structure has appealing properties for regression, enhanced sparsity and interpretability. To deal with the parameter explosion we introduce L1 penalties and fit the model through a flip-flop algorithm that results in a linear number of lasso regressions.",http://proceedings.mlr.press/v28/kalaitzis13.html,http://proceedings.mlr.press/v28/kalaitzis13.pdf,ICML
2867,2013,Bayesian Games for Adversarial Regression Problems,"Michael Großhans,         Christoph Sawade,         Michael Brückner,         Tobias Scheffer","We study regression problems in which an adversary can exercise some control over the data generation process. Learner and adversary have conflicting but not necessarily perfectly antagonistic objectives. We study the case in which the learner is not fully informed about the adversary’s objective; instead, any knowledge of the learner about parameters of the adversary’s goal may be reflected in a Bayesian prior. We model this problem as a Bayesian game, and characterize conditions under which a unique Bayesian equilibrium point exists. We experimentally compare the Bayesian equilibrium strategy to the Nash equilibrium strategy, the minimax strategy, and regular linear regression.",http://proceedings.mlr.press/v28/grosshans13.html,http://proceedings.mlr.press/v28/grosshans13.pdf,ICML
2868,2013,Active Learning for Multi-Objective Optimization,"Marcela Zuluaga,         Guillaume Sergent,         Andreas Krause,         Markus Püschel","In many fields one encounters the challenge of identifying, out of a pool of possible designs, those that simultaneously optimize multiple objectives. This means that usually there is not one optimal design but an entire set of Pareto-optimal ones with optimal tradeoffs in the objectives. In many applications, evaluating one design is expensive; thus, an exhaustive search for the Pareto-optimal set is unfeasible. To address this challenge, we propose the Pareto Active Learning (PAL) algorithm, which intelligently samples the design space to predict the Pareto-optimal set. Key features of PAL include (1) modeling the objectives as samples from a Gaussian process distribution to capture structure and accommodate noisy evaluation; (2) a method to carefully choose the next design to evaluate to maximize progress; and (3) the ability to control prediction accuracy and sampling cost. We provide theoretical bounds on PAL’s sampling cost required to achieve a desired accuracy. Further, we show an experimental evaluation on three real-world data sets. The results show PAL’s effectiveness; in particular it improves significantly over a state-of-the-art evolutionary algorithm, saving in many cases about 33%.",http://proceedings.mlr.press/v28/zuluaga13.html,http://proceedings.mlr.press/v28/zuluaga13.pdf,ICML
2869,2013,A Fast and Exact Energy Minimization Algorithm for Cycle MRFs,"Huayan Wang,         Koller Daphne","The presence of cycles gives rise to the difficulty in performing inference for MRFs. Handling cycles efficiently would greatly enhance our ability to tackle general MRFs. In particular, for dual decomposition of energy minimization (MAP inference), using cycle subproblems leads   to a much tighter relaxation than using trees, but solving the cycle subproblems turns out to be the bottleneck.  In this paper, we present a fast and exact algorithm for energy minimization in cycle MRFs, which can be used as a subroutine in tackling general MRFs. Our method builds on junction-tree message passing, with a large portion of the message entries pruned for efficiency. The pruning conditions fully exploit the structure of a cycle. Experimental results show that our algorithm is more than an order of magnitude faster than other state-of-the-art fast inference methods, and it performs consistently well in several different real problems.",http://proceedings.mlr.press/v28/wang13f.html,http://proceedings.mlr.press/v28/wang13f.pdf,ICML
2870,2013,Modelling Sparse Dynamical Systems with Compressed Predictive State Representations,"William L. Hamilton,         Mahdi Milani Fard,         Joelle Pineau","Efficiently learning accurate models of dynamical systems is of central importance for developing rational agents that can succeed in a wide range of challenging domains. The difficulty of this learning problem is particularly acute in settings with large observation spaces and partial observability. We present a new algorithm, called Compressed Predictive State Representation (CPSR), for learning models of high-dimensional partially observable uncontrolled dynamical systems from small sample sets. The algorithm, which extends previous work on Predictive State Representations, exploits a particular sparse structure present in many domains. This sparse structure is used to compress information during learning, allowing for an increase in both the efficiency and predictive power. The compression technique also relieves the burden of domain specific feature selection and allows for domains with extremely large discrete observation spaces to be efficiently modelled. We present empirical results showing that the algorithm is able to build accurate models more efficiently than its uncompressed counterparts, and provide theoretical results on the accuracy of the learned compressed model.",http://proceedings.mlr.press/v28/hamilton13.html,http://proceedings.mlr.press/v28/hamilton13.pdf,ICML
2871,2013,A PAC-Bayesian Approach for Domain Adaptation with Specialization to Linear Classifiers,"Pascal Germain,         Amaury Habrard,         François Laviolette,         Emilie Morvant","We provide a first PAC-Bayesian analysis for domain adaptation (DA) which arises when the learning and test distributions differ. It relies on a novel distribution pseudodistance based on a disagreement averaging. Using this measure, we derive a PAC-Bayesian DA bound for the stochastic Gibbs classifier. This bound has the advantage of being directly optimizable for any hypothesis space. We specialize it to linear classifiers, and design a learning algorithm which shows interesting results on a synthetic problem and on a popular sentiment annotation task. This opens the door to tackling DA tasks by making use of all the PAC-Bayesian tools.",http://proceedings.mlr.press/v28/germain13.html,http://proceedings.mlr.press/v28/germain13.pdf,ICML
2872,2013,Gated Autoencoders with Tied Input Weights,"Droniou Alain,         Sigaud Olivier","The semantic interpretation of images is one of the core applications of deep learning. Several techniques have been recently proposed to model the relation between two images, with application to pose estimation, action recognition or invariant object recognition. Among these techniques, higher-order Boltzmann machines or relational autoencoders consider projections of the images on different subspaces and intermediate layers act as transformation specific detectors. In this work, we extend the mathematical study of (Memisevic, 2012b) to show that it is possible to use a unique projection for both images in a way that turns intermediate layers as spectrum encoders of transformations. We show that this results in networks that are easier to tune and have greater generalization capabilities.",http://proceedings.mlr.press/v28/alain13.html,http://proceedings.mlr.press/v28/alain13.pdf,ICML
2873,2013,Top-down particle filtering for Bayesian decision trees,"Balaji Lakshminarayanan,         Daniel Roy,         Yee Whye Teh","Decision tree learning is a popular approach for classification and regression in machine learning and statistics, and Bayesian formulations - which introduce a prior distribution over decision trees, and formulate learning as posterior inference given data - have been shown to produce competitive performance. Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing Bayesian algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection thereof) iteratively via local Monte Carlo modifications to the structure of the tree, e.g., using Markov chain Monte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that instead works in a top-down manner, mimicking the behavior and speed of classic algorithms. We demonstrate empirically that our approach delivers accuracy comparable to the most popular MCMC method, but operates more than an order of magnitude faster, and thus represents a better computation-accuracy tradeoff.",http://proceedings.mlr.press/v28/lakshminarayanan13.html,http://proceedings.mlr.press/v28/lakshminarayanan13.pdf,ICML
2874,2013,A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning,"Arash Afkanpour,         András György,         Csaba Szepesvari,         Michael Bowling","We consider the problem of simultaneously learning to linearly combine a very large number of kernels and learn a good predictor based on the learnt kernel. When the number of kernels d to be combined is very large, multiple kernel learning methods whose computational cost scales linearly in d are intractable. We propose a randomized version of the mirror descent algorithm to overcome this issue, under the objective of minimizing the group p-norm penalized empirical risk. The key to achieve the required exponential speed-up is the computationally efficient construction of low-variance estimates of the gradient. We propose importance sampling based estimates, and find that the ideal distribution samples a coordinate with a probability proportional to the magnitude of the corresponding gradient. We show that in the case of learning the coefficients of a polynomial kernel, the combinatorial structure of the base kernels to be combined allows sampling from this distribution in O(\log(d)) time, making the total computational cost of the method to achieve an epsilon-optimal solution to be O(\log(d)/epsilon^2), thereby allowing our method to operate for very large values of d. Experiments with simulated and real data confirm that the new algorithm is computationally more efficient than its state-of-the-art alternatives.",http://proceedings.mlr.press/v28/afkanpour13.html,http://proceedings.mlr.press/v28/afkanpour13.pdf,ICML
2875,2013,Parsing epileptic events using a Markov switching process model for correlated time series,"Drausin Wulsin,         Emily Fox,         Brian Litt","Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in addition to full-blown clinical seizures. We believe the relationship between these two classes of events—something not previously studied quantitatively—could yield important insights into the nature and intrinsic dynamics of seizures. A goal of our work is to parse these complex epileptic events into distinct dynamic regimes.  A challenge posed by the intracranial EEG (iEEG) data we study is the fact that the number and placement of electrodes can vary between patients.  We develop a Bayesian nonparametric Markov switching process that allows for (i) shared dynamic regimes between a variable numbers of channels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary of dynamic regimes.  We encode a sparse and changing set of dependencies between the channels using a Markov-switching Gaussian graphical model for the innovations process driving the channel dynamics. We demonstrate the importance of this model in parsing and out-of-sample predictions of iEEG data.  We show that our model produces intuitive state assignments that can help automate clinical analysis of seizures and enable the comparison of sub-clinical bursts and full clinical seizures.",http://proceedings.mlr.press/v28/wulsin13.html,http://proceedings.mlr.press/v28/wulsin13.pdf,ICML
2876,2013,Learning Fair Representations,"Rich Zemel,         Yu Wu,         Kevin Swersky,         Toni Pitassi,         Cynthia Dwork","We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.",http://proceedings.mlr.press/v28/zemel13.html,http://proceedings.mlr.press/v28/zemel13.pdf,ICML
2877,2013,Joint Transfer and Batch-mode Active Learning,"Rita Chattopadhyay,         Wei Fan,         Ian Davidson,         Sethuraman Panchanathan,         Jieping Ye","Active learning and transfer learning are two different methodologies that address the common  problem of insufficient labels. Transfer learning addresses this problem by using the knowledge gained from a related and already labeled data source, whereas active learning focuses on selecting a small set of informative samples  for manual annotation. Recently, there has been much interest in developing frameworks that combine both transfer and active learning  methodologies. A few such frameworks reported in literature perform transfer and active learning in two separate stages. In this work, we present an integrated framework that performs transfer and active learning simultaneously by solving a  single convex optimization problem. The proposed framework computes the weights of source domain data and selects the samples from the target domain data simultaneously, by minimizing a common objective of reducing distribution difference between the data set consisting of reweighted source and the queried target domain data and the set of unlabeled target domain data. Comprehensive experiments on three real world data sets demonstrate that the proposed method improves the classification accuracy by 5% to  10% over the existing two-stage approach",http://proceedings.mlr.press/v28/chattopadhyay13.html,http://proceedings.mlr.press/v28/chattopadhyay13.pdf,ICML
2878,2013,Better Rates for Any Adversarial Deterministic MDP,"Ofer Dekel,         Elad Hazan","We consider regret minimization in adversarial deterministic Markov  Decision Processes (ADMDPs) with bandit feedback. We devise a new  algorithm that pushes the state-of-the-art forward in two ways: First,  it attains a regret of O(T^2/3) with respect to the best fixed  policy in hindsight, whereas the previous best regret bound was  O(T^3/4). Second, the algorithm and its analysis are compatible  with any feasible ADMDP graph topology, while all previous approaches  required additional restrictions on the graph topology.",http://proceedings.mlr.press/v28/dekel13.html,http://proceedings.mlr.press/v28/dekel13.pdf,ICML
2879,2013,SADA: A General Framework to Support Robust Causation Discovery,"Ruichu Cai,         Zhenjie Zhang,         Zhifeng Hao","Causality discovery without manipulation is considered a crucial problem to a variety of applications, such as genetic therapy. The state-of-the-art solutions, e.g. LiNGAM, return accurate results when the number of labeled samples is larger than the number of variables. These approaches are thus applicable only when large numbers of samples are available or the problem domain is sufficiently small. Motivated by the observations of the local sparsity properties on causal structures, we propose a general Split-and-Merge strategy, named SADA, to enhance the scalability of a wide class of causality discovery algorithms. SADA is able to accurately identify the causal variables, even when the sample size is significantly smaller than the number of variables. In SADA, the variables are partitioned into subsets, by finding cuts on the sparse probabilistic graphical model over the variables. By running mainstream causation discovery algorithms, e.g. LiNGAM, on the subproblems, complete causality can be reconstructed by combining all the partial results. SADA benefits from the recursive division technique, since each small subproblem generates more accurate result under the same number of samples. We theoretically prove that SADA always reduces the scale of problems without significant sacrifice on result accuracy, depending only on the local sparsity condition over the variables. Experiments on real-world datasets verify the improvements on scalability and accuracy by applying SADA on top of existing causation algorithms.",http://proceedings.mlr.press/v28/cai13.html,http://proceedings.mlr.press/v28/cai13.pdf,ICML
2880,2013,Infinite Markov-Switching Maximum Entropy Discrimination Machines,Sotirios Chatzis,"In this paper, we present a method that combines the merits of Bayesian  nonparametrics, specifically stick-breaking priors, and large-margin  kernel machines in the context of sequential data classification.  The proposed model postulates a set of (theoretically) infinite interdependent  large-margin classifiers as model components, that robustly capture  local nonlinearity of complex data. The postulated large-margin classifiers  are connected in the context of a Markov-switching construction that  allows for capturing complex temporal dynamics in the modeled datasets.  Appropriate stick-breaking priors are imposed over the component switching  mechanism of our model to allow for data-driven determination of the  optimal number of component large-margin classifiers, under a standard  nonparametric Bayesian inference scheme. Efficient model training  is performed under the maximum entropy discrimination (MED) framework,  which integrates the large-margin principle with Bayesian posterior  inference. We evaluate our method using several real-world datasets,  and compare it to state-of-the-art alternatives.",http://proceedings.mlr.press/v28/chatzis13.html,http://proceedings.mlr.press/v28/chatzis13.pdf,ICML
2881,2013,Concurrent Reinforcement Learning from Customer Interactions,"David Silver,         Leonard Newnham,         David Barker,         Suzanne Weller,         Jason McFall","In this paper, we explore applications in which a company interacts concurrently with many customers. The company has an objective function, such as maximising revenue, customer satisfaction, or customer loyalty, which depends primarily on the sequence of interactions between company and customer. A key aspect of this setting is that interactions with different customers occur in parallel. As a result, it is imperative to learn online from partial interaction sequences, so that information acquired from one customer is efficiently assimilated and applied in subsequent interactions with other customers. We present the first framework for concurrent reinforcement learning, using a variant of temporal-difference learning to learn efficiently from partial interaction sequences.   We evaluate our algorithms in two large-scale test-beds for online and email interaction respectively, generated from a database of 300,000 customer records.",http://proceedings.mlr.press/v28/silver13.html,http://proceedings.mlr.press/v28/silver13.pdf,ICML
2882,2013,Regularization of Neural Networks using DropConnect,"Li Wan,         Matthew Zeiler,         Sixin Zhang,         Yann Le Cun,         Rob Fergus","We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.",http://proceedings.mlr.press/v28/wan13.html,http://proceedings.mlr.press/v28/wan13.pdf,ICML
2883,2013,Expensive Function Optimization with Stochastic Binary Outcomes,"Matthew Tesch,         Jeff Schneider,         Howie Choset","Real world systems often have parameterized controllers which can be tuned to improve performance. Bayesian optimization methods provide for efficient optimization of these controllers, so as to reduce the number of required experiments on the expensive physical system. In this paper we address Bayesian optimization in the setting where performance is only observed through a stochastic binary outcome – success or failure of the experiment. Unlike bandit problems, the goal is to maximize the system performance after this offline training phase rather than minimize regret during training.  In this work we define the stochastic binary optimization problem and propose an approach using an adaptation of Gaussian Processes for classification that presents a Bayesian optimization framework for this problem.  We propose an experiment selection metric for this setting based on expected improvement.  We demonstrate the algorithm’s performance on synthetic problems and on a real snake robot learning to move over an obstacle.",http://proceedings.mlr.press/v28/tesch13.html,http://proceedings.mlr.press/v28/tesch13.pdf,ICML
2884,2013,Gossip-based distributed stochastic bandit algorithms,"Balazs Szorenyi,         Robert Busa-Fekete,         Istvan Hegedus,         Robert Ormandi,         Mark Jelasity,         Balazs Kegl","The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called exploitation-exploration dilemma in various bandit setups. At the same time, significantly less effort has been devoted to adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines, or peer-to-peer (P2P) environments, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms is available in each peer. In every iteration each peer can pull one arm independently of the other peers, and then some limited communication is possible with a few random other peers.  As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability of playing a suboptimal arm at a peer in iteration t = Ω( \log N ) is proportional to 1/(Nt) where N denotes the number of peers.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network.",http://proceedings.mlr.press/v28/szorenyi13.html,http://proceedings.mlr.press/v28/szorenyi13.pdf,ICML
2885,2013,Spectral Compressed Sensing via Structured Matrix Completion,"Yuxin Chen,         Yuejie Chi","The paper studies the problem of recovering a spectrally sparse object from a small number of time domain samples. Specifically, the object of interest with ambient dimension n is assumed to be a mixture of r complex multi-dimensional sinusoids, while the underlying frequencies can assume any value in the unit disk. Conventional compressed sensing paradigms suffer from the \em basis mismatch issue when imposing a discrete dictionary on the Fourier representation. To address this problem,  we develop a novel nonparametric algorithm, called enhanced matrix completion (EMaC), based on structured matrix completion. The algorithm starts by converting the data into a low-rank enhanced form with multi-fold Hankel structure, then attempts recovery via nuclear norm minimization. Under mild incoherence conditions, EMaC allows perfect recovery as soon as the number of samples exceeds the order of \mathcalO(r\log^2 n). We also show that, in many instances, accurate completion of a low-rank multi-fold Hankel matrix is possible when the number of observed entries is proportional to the information theoretical limits (except for a logarithmic gap). The robustness of EMaC against bounded noise and its applicability to super resolution are further demonstrated by numerical experiments.",http://proceedings.mlr.press/v28/chen13g.html,http://proceedings.mlr.press/v28/chen13g.pdf,ICML
2886,2013,Smooth Operators,"Steffen Grunewalder,         Gretton Arthur,         John Shawe-Taylor","We develop a generic approach to form smooth versions of basic mathematical operations like multiplication, composition, change of measure, and conditional expectation, among others.   Operations which result in functions outside the reproducing kernel Hilbert space (such as the product of two RKHS functions) are approximated via a natural cost function, such that the solution is guaranteed to be in the targeted RKHS. This approximation problem is reduced to a regression problem using an adjoint trick, and solved in a vector-valued RKHS, consisting of continuous, linear, smooth operators which map from an input, real-valued RKHS to the desired target RKHS. Important constraints, such as an almost everywhere positive density, can be enforced or approximated naturally in this framework, using convex constraints on the operators. Finally, smooth operators can be composed to accomplish more complex machine learning tasks, such as the sum rule and kernelized approximate Bayesian inference, where state-of-the-art convergence rates are obtained.",http://proceedings.mlr.press/v28/grunewalder13.html,http://proceedings.mlr.press/v28/grunewalder13.pdf,ICML
2887,2013,Forecastable Component Analysis,Georg Goerg,"I introduce Forecastable Component Analysis (ForeCA), a novel dimension reduction technique for temporally dependent signals. Based on a new forecastability measure, ForeCA finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space. I present a converging algorithm with a fast eigenvector solution. Applications to financial and macro-economic time series show that ForeCA can successfully discover informative structure, which can be used for forecasting as well as classification. The R package ForeCA accompanies this work and is publicly available on CRAN.",http://proceedings.mlr.press/v28/goerg13.html,http://proceedings.mlr.press/v28/goerg13.pdf,ICML
2888,2013,Sparse PCA through Low-rank Approximations,"Dimitris Papailiopoulos,         Alexandros Dimakis,         Stavros Korokythakis","We introduce a novel algorithm that computes the k-sparse principal component of a positive semidefinite matrix A.  Our algorithm is combinatorial and operates by examining a discrete set of special vectors lying in a low-dimensional eigen-subspace of A.  We obtain provable approximation guarantees that depend on the spectral profile of the matrix: the faster the eigenvalue decay, the better the quality of our approximation.  For example, if the eigenvalues of A follow a power-law decay, we obtain a polynomial-time approximation   algorithm for any desired accuracy.   We implement our algorithm and test it on multiple artificial and real data sets. Due to   a feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes.   Our experimental evaluation shows that our scheme is nearly optimal   while finding very sparse vectors.  We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms   in all tested data sets.",http://proceedings.mlr.press/v28/papailiopoulos13.html,http://proceedings.mlr.press/v28/papailiopoulos13.pdf,ICML
2889,2013,Saving Evaluation Time for the Decision Function in Boosting: Representation and Reordering Base Learner,"Peng Sun,         Jie Zhou","For a well trained Boosting classifier, we are interested in how to save the testing time, i.e., to make the decision without evaluating all the base learners. To address this problem, in previous work the base learners are sequentially calculated and early stopping is allowed if the decision function has been confident enough to output its value. In such a chain structure, the order of base learners is critical: better order can lead to less evaluation time.    In this paper, we present a novel method for ordering. We base our discussion on the data structure representing Boosting’s decision function. Viewing the decision function a boolean expression, we propose a Binary Valued Tree for its representation. As a secondary contribution, such a representation unifies the work by previous researchers and helps devise new representation. Also, its connection to Binary Decision Diagram(BDD) is discussed.",http://proceedings.mlr.press/v28/sun13.html,http://proceedings.mlr.press/v28/sun13.pdf,ICML
2890,2013,Online Kernel Learning with a Near Optimal Sparsity Bound,"Lijun Zhang,         Jinfeng Yi,         Rong Jin,         Ming Lin,         Xiaofei He","In this work, we focus on Online Sparse Kernel Learning that aims to online learn a kernel classifier with a bounded number of support vectors. Although many online learning algorithms have been proposed to learn a sparse kernel classifier, most of them fail to bound the number of support vectors used by the final solution which is the average of the intermediate kernel classifiers generated by online algorithms. The key idea of the proposed algorithm is to measure the difficulty in correctly classifying a training example by the derivative of a smooth loss function, and give a more chance to a difficult example to be a support vector than an easy one via a sampling scheme. Our analysis shows that when the loss function is smooth, the proposed algorithm yields similar performance guarantee as the standard online learning algorithm but with a near optimal number of support vectors (up to a poly(lnT) factor). Our empirical study shows promising performance of the proposed algorithm compared to the state-of-the-art algorithms for online sparse kernel learning.",http://proceedings.mlr.press/v28/zhang13c.html,http://proceedings.mlr.press/v28/zhang13c.pdf,ICML
2891,2013,\proptoSVM for Learning with Label Proportions,"Felix Yu,         Dong Liu,         Sanjiv Kumar,         Jebara Tony,         Shih-Fu Chang","We study the problem of learning with label proportions in which the training data is provided in groups and only the proportion of each class in each group is known. We propose a new method called proportion-SVM, or \proptoSVM, which explicitly models the latent unknown instance labels together with the known group label proportions in a large-margin framework. Unlike the existing works, our approach avoids making restrictive assumptions about the data. The \proptoSVM model leads to a non-convex integer programming problem. In order to solve it efficiently, we propose two algorithms: one based on simple alternating optimization and the other based on a convex relaxation. Extensive experiments on standard datasets show that \proptoSVM outperforms the state-of-the-art, especially for larger group sizes.",http://proceedings.mlr.press/v28/yu13a.html,http://proceedings.mlr.press/v28/yu13a.pdf,ICML
2892,2013,Inference algorithms for pattern-based CRFs on sequence data,"Rustem Takhanov,         Vladimir Kolmogorov","We consider \em Conditional Random Fields (CRFs) with pattern-based potentials  defined on a chain. In this model the energy of a string (labeling) x_1\ldots x_n  is the sum of terms over intervals [i,j] where each term is non-zero only if the substring x_i\ldots x_j  equals a prespecified pattern α. Such CRFs can be naturally applied to many sequence tagging problems. We present efficient algorithms for the three standard inference tasks in a CRF, namely  computing (i) the partition function, (ii) marginals, and (iii) computing the MAP.  Their complexities are respectively  O(n L), O(n L \ell_\max) and  O(n L \min{|D|,\log (\ell_\max + 1)})  where L is the combined length of input patterns, \ell_\max is the maximum length of a pattern,  and D is the input alphabet.  This improves on the previous algorithms of \citeYe:NIPS09 whose complexities are respectively O(n L |D|),  O\left(n |Γ| L^2 \ell_\max^2\right) and O(n L |D|),  where |Γ| is the number of input patterns. In addition, we give an efficient algorithm for sampling,  and revisit the case of MAP with non-positive weights. Finally, we apply pattern-based CRFs to the problem of the protein dihedral angles prediction.",http://proceedings.mlr.press/v28/takhanov13.html,http://proceedings.mlr.press/v28/takhanov13.pdf,ICML
2893,2013,Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations,"Krishnakumar Balasubramanian,         Kai Yu,         Guy Lebanon","We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via nonparametric kernel smoothing. We provide generalization bounds for dictionary learning  using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the  speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach could be used for improving semisupervised sparse coding.",http://proceedings.mlr.press/v28/balasubramanian13.html,http://proceedings.mlr.press/v28/balasubramanian13.pdf,ICML
2894,2013,Temporal Difference Methods for the Variance of the Reward To Go,"Aviv Tamar,         Dotan Di Castro,         Shie Mannor","In this paper we extend temporal difference policy evaluation algorithms to performance criteria that include the variance of the cumulative reward. Such criteria are useful for risk management, and are important in domains such as finance and process control. We propose variants of both TD(0) and LSTD(λ) with linear function approximation, prove their convergence, and demonstrate their utility in a 4-dimensional continuous state space problem.",http://proceedings.mlr.press/v28/tamar13.html,http://proceedings.mlr.press/v28/tamar13.pdf,ICML
2895,2013,Algorithms for Direct 0–1 Loss Optimization in Binary Classification,"Tan Nguyen,         Scott Sanner","While convex losses for binary classification are attractive due to the existence of numerous (provably) efficient methods for finding their global optima, they are sensitive to outliers.  On the other hand, while the non-convex 0–1 loss is robust to outliers, it is NP-hard to optimize and thus rarely directly optimized in practice.  In this paper, however, we do just that: we explore a variety of practical methods for direct (approximate) optimization of the 0–1 loss based on branch and bound search, combinatorial search, and coordinate descent on smooth, differentiable relaxations of 0–1 loss. Empirically, we compare our proposed algorithms to logistic regression, SVM, and the Bayes point machine showing that the proposed 0–1 loss optimization algorithms perform at least as well and offer a clear advantage in the presence of outliers.  To this end, we believe this work reiterates the importance of 0–1 loss and its robustness properties while challenging the notion that it is difficult to directly optimize.",http://proceedings.mlr.press/v28/nguyen13a.html,http://proceedings.mlr.press/v28/nguyen13a.pdf,ICML
2896,2013,Exact Rule Learning via Boolean Compressed Sensing,"Dmitry Malioutov,         Kush Varshney","We propose an interpretable rule-based classification system based on ideas from Boolean compressed sensing. We represent the problem of learning individual conjunctive clauses or individual disjunctive clauses as a Boolean group testing problem, and apply a novel linear programming relaxation to find solutions. We derive results for exact rule recovery which parallel the conditions for exact recovery of sparse signals in the compressed sensing literature: although the general rule recovery problem is NP-hard, under some conditions on the Boolean ‘sensing’ matrix, the rule can be recovered exactly. This is an exciting development in rule learning where most prior work focused on heuristic solutions. Furthermore we construct rule sets from these learned clauses using set covering and boosting.  We show competitive classification accuracy using the proposed approach.",http://proceedings.mlr.press/v28/malioutov13.html,http://proceedings.mlr.press/v28/malioutov13.pdf,ICML
2897,2013,Near-optimal Batch Mode Active Learning and Adaptive Submodular Optimization,"Yuxin Chen,         Andreas Krause","Active learning can lead to a dramatic reduction in labeling effort. However, in many practical implementations (such as crowdsourcing,  surveys, high-throughput experimental design), it is preferable to query labels for batches of examples to be labelled in parallel. While several heuristics have been proposed for batch-mode active learning, little is known about their theoretical performance.    We consider batch mode active learning and more general information-parallel stochastic optimization problems that exhibit adaptive submodularity, a natural diminishing returns condition. We prove that for such problems, a simple greedy strategy is competitive with the optimal batch-mode policy. In some cases, surprisingly, the use of batches incurs competitively low cost, even when compared to a fully sequential strategy. We demonstrate the effectiveness of our approach on batch-mode active learning tasks, where it outperforms the state of the art, as well as the novel problem of multi-stage influence maximization in social networks.",http://proceedings.mlr.press/v28/chen13b.html,http://proceedings.mlr.press/v28/chen13b.pdf,ICML
2898,2013,Robust Structural Metric Learning,"Daryl Lim,         Gert Lanckriet,         Brian McFee","Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking.  However, when the input data contains a large portion of non-informative features, existing methods fail to identify the relevant features, and performance degrades accordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation, while optimizing for structured ranking output prediction.  Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methods in both high- and low-noise settings.",http://proceedings.mlr.press/v28/lim13.html,http://proceedings.mlr.press/v28/lim13.pdf,ICML
2899,2013,Breaking the Small Cluster Barrier of Graph Clustering,"Nir Ailon,         Yudong Chen,         Huan Xu","This paper investigates graph clustering in the planted cluster model in the   presence of  \em small clusters. Traditional results dictate that for an   algorithm to provably correctly recover the clusters, \em all clusters must be   sufficiently large (in particular, \tildeΩ(\sqrtn) where n is the number   of nodes of the graph). We show that this is not really a restriction: by a more refined   analysis of the trace-norm based matrix recovery approach proposed in (Jalali et al. 2011) and (Chen et al. 2012), we prove that small clusters, under certain mild assuptions, do not hinder recovery of large ones.  Based on this result, we further devise an iterative algorithm   to recover \em almost all clusters via a “peeling strategy”, i.e., recover large clusters   first, leading to a reduced problem, and repeat this procedure.   These results are extended to the    \em partial observation setting, in which only a (chosen) part of the graph is observed.  The peeling strategy gives rise to an active learning algorithm, in which   edges adjacent to smaller clusters are queried more often as large clusters are learned  (and removed).  Our findings are supported by experiments.    From a high level, this paper sheds novel insights on high-dimesional statistics and   learning structured data, by presenting a structured matrix learning problem for which  a one shot convex relaxation approach necessarily fails, but a carefully constructed sequence of convex relaxations  does the job.",http://proceedings.mlr.press/v28/ailon13.html,http://proceedings.mlr.press/v28/ailon13.pdf,ICML
2900,2013,A Local Algorithm for Finding Well-Connected Clusters,"Zeyuan Allen Zhu,         Silvio Lattanzi,         Vahab Mirrokni","Motivated by applications of large-scale graph clustering, we study random-walk-based LOCAL algorithms whose running times depend only on the size of the output cluster, rather than the entire graph. In particular, we develop a method with better theoretical guarantee compared to all previous work, both in terms of the clustering accuracy and the conductance of the output set. We also prove that our analysis is tight, and perform empirical evaluation to support our theory on both synthetic and real data.    More specifically, our method outperforms prior work when the cluster is WELL-CONNECTED. In fact, the better it is well-connected inside, the more significant improvement we can obtain. Our results shed light on why in practice some random-walk-based algorithms perform better than its previous theory, and help guide future research about local clustering.",http://proceedings.mlr.press/v28/allenzhu13.html,http://proceedings.mlr.press/v28/allenzhu13.pdf,ICML
2901,2013,Fast dropout training,"Sida Wang,         Christopher Manning","Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance.  Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective.  This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability.  We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.",http://proceedings.mlr.press/v28/wang13a.html,http://proceedings.mlr.press/v28/wang13a.pdf,ICML
2902,2013,Learning an Internal Dynamics Model from Control Demonstration,"Matthew Golub,         Steven Chase,         Byron Yu","Much work in optimal control and inverse control has assumed that the controller has perfect knowledge of plant dynamics.  However, if the controller is a human or animal subject, the subject’s internal dynamics model may differ from the true plant dynamics.  Here, we consider the problem of learning the subject’s internal model from demonstrations of control and knowledge of task goals.  Due to sensory feedback delay, the subject uses an internal model to generate an internal prediction of the current plant state, which may differ from the actual plant state.  We develop a probabilistic framework and exact EM algorithm to jointly estimate the internal model, internal state trajectories, and feedback delay. We applied this framework to demonstrations by a nonhuman primate of brain-machine interface (BMI) control. We discovered that the subject’s internal model deviated from the true BMI plant dynamics and provided significantly better explanation of the recorded neural control signals than did the true plant dynamics.",http://proceedings.mlr.press/v28/golub13.html,http://proceedings.mlr.press/v28/golub13.pdf,ICML
2903,2013,Stable Coactive Learning via Perturbation,"Karthik Raman,         Thorsten Joachims,         Pannaga Shivaswamy,         Tobias Schnabel","Coactive Learning is a model of interaction between a learning system (e.g. search engine) and its human users, wherein the system learns from (typically implicit) user feedback during operational use. User feedback takes the form of preferences, and recent work has introduced online algorithms that learn from this weak feedback. However, we show that these algorithms can be unstable and ineffective in real-world settings where biases and noise in the feedback are significant. In this paper, we propose the first coactive learning algorithm that can learn robustly despite bias and noise. In particular, we explore how presenting users with slightly perturbed objects (e.g., rankings) can stabilize the learning process. We theoretically validate the algorithm by proving bounds on the average regret. We also provide extensive empirical evidence on benchmarks and from a live search engine user study, showing that the new algorithm substantially outperforms existing methods.",http://proceedings.mlr.press/v28/raman13.html,http://proceedings.mlr.press/v28/raman13.pdf,ICML
2904,2013,Learning the Structure of Sum-Product Networks,"Robert Gens,         Domingos Pedro","Sum-product networks (SPNs) are a new class of deep probabilistic models. SPNs can have unbounded treewidth but inference in them is always tractable. An SPN is either a univariate distribution, a product of SPNs over disjoint variables, or a weighted sum of SPNs over the same variables. We propose the first algorithm for learning the structure of SPNs that takes full advantage of their expressiveness. At each step, the algorithm attempts to divide the current variables into approximately independent subsets. If successful, it returns the product of recursive calls on the subsets; otherwise it returns the sum of recursive calls on subsets of similar instances from the current training set. A comprehensive empirical study shows that the learned SPNs are typically comparable to graphical models in likelihood but superior in inference speed and accuracy.",http://proceedings.mlr.press/v28/gens13.html,http://proceedings.mlr.press/v28/gens13.pdf,ICML
2905,2013,On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions,"Purushottam Kar,         Bharath Sriperumbudur,         Prateek Jain,         Harish Karnick","In this paper, we study the generalization properties of online learning based stochastic methods for supervised learning problems where the loss function is dependent on more than one training sample (e.g., metric learning, ranking). We present a generic decoupling technique that enables us to provide Rademacher complexity-based generalization error bounds. Our bounds are in general tighter than those obtained by Wang et al. (COLT 2012) for the same problem. Using our decoupling technique, we are further able to obtain fast convergence rates for strongly con-vex pairwise loss functions. We are also able to analyze a class of memory efficient on-line learning algorithms for pairwise learning problems that use only a bounded subset of past training samples to update the hypothesis at each step. Finally, in order to complement our generalization bounds, we propose a novel memory efficient online learning algorithm for higher order learning problems with bounded regret guarantees.",http://proceedings.mlr.press/v28/kar13.html,http://proceedings.mlr.press/v28/kar13.pdf,ICML
2906,2013,Estimation of Causal Peer Influence Effects,"Panos Toulis,         Edward Kao","The broad adoption of social media has generated interest in leveraging peer influence for inducing desired user behavior. Quantifying the causal effect of peer influence presents technical challenges, however, including how to deal with social interference, complex response functions and network uncertainty. In this paper, we extend potential outcomes to allow for interference, we introduce well-defined causal estimands of peer-influence, and we develop two estimation procedures: a frequentist procedure relying on a sequential randomization design that requires knowledge of the network but operates under complicated response functions, and a Bayesian procedure which accounts for network uncertainty but relies on a linear response assumption to increase estimation precision. Our results show the advantages and disadvantages of the proposed methods in a number of situations.",http://proceedings.mlr.press/v28/toulis13.html,http://proceedings.mlr.press/v28/toulis13.pdf,ICML
2907,2013,Fastfood - Computing Hilbert Space Expansions in loglinear time,"Quoc Le,         Tamas Sarlos,         Alexander Smola","Fast nonlinear function classes are crucial for nonparametric estimation, such as in kernel methods. This paper proposes an improvement to random kitchen sinks that offers significantly faster computation in log-linear time without sacrificing accuracy. Furthermore, we show how one may adjust the regularization properties of the kernel simply by changing the spectral distribution of the projection matrix. We provide experimental results which show that even for for moderately small problems we already achieve two orders of magnitude faster computation and three orders of magnitude lower memory footprint.",http://proceedings.mlr.press/v28/le13.html,http://proceedings.mlr.press/v28/le13.pdf,ICML
2908,2013,Vanishing Component Analysis,"Roi Livni,         David Lehavi,         Sagi Schein,         Hila Nachliely,         Shai Shalev-Shwartz,         Amir Globerson","The vanishing ideal of a set of n points S, is the set of all polynomials that attain the value of zero on all the points in S. Such ideals can be compactly represented using a small set of polynomials known as generators of the ideal. Here we describe and analyze an efficient procedure that constructs a set of generators of a vanishing ideal. Our procedure is numerically stable, and can be used to find approximately vanishing polynomials.  The resulting polynomials capture nonlinear structure in data, and can for example be used within supervised learning. Empirical comparison with kernel methods show that our method constructs more compact classifiers with comparable accuracy.",http://proceedings.mlr.press/v28/livni13.html,http://proceedings.mlr.press/v28/livni13.pdf,ICML
2909,2013,Sparse projections onto the simplex,"Anastasios Kyrillidis,         Stephen Becker,         Volkan Cevher,         Christoph Koch","Most learning methods with rank or sparsity constraints use convex relaxations, which lead to optimization with the nuclear norm or the \ell_1-norm. However, several important learning applications cannot benefit from this approach as they feature these convex norms as constraints in addition to the non-convex rank and sparsity constraints. In this setting, we derive efficient sparse projections onto the simplex and its extension, and illustrate how to use them to solve high-dimensional learning problems in quantum tomography, sparse density estimation and portfolio selection with non-convex constraints.",http://proceedings.mlr.press/v28/kyrillidis13.html,http://proceedings.mlr.press/v28/kyrillidis13.pdf,ICML
2910,2013,One-Bit Compressed Sensing: Provable Support and Vector Recovery,"Sivakant Gopi,         Praneeth Netrapalli,         Prateek Jain,         Aditya Nori","In this paper, we study the problem of one-bit compressed sensing (1-bit CS), where the goal is to design a measurement matrix A and a recovery algorithm s.t. a k-sparse vector \x^* can be efficiently recovered back from signed linear measurements, i.e., b=\sign(A\x^*). This is an important problem in the signal acquisition area and has several learning applications as well, e.g., multi-label classification \citeHsuKLZ10. We study this problem in two settings: a) support recovery: recover \supp(\x^*), b) approximate vector recovery: recover a unit vector \hx s.t. || \hatx-\x^*/||\x^*|| ||_2≤ε. For support recovery, we propose two novel and efficient solutions based on two combinatorial structures: union free family of sets and expanders. In contrast to  existing methods for  support recovery, our methods are universal i.e. a single measurement matrix A can recover almost all the signals. For approximate recovery, we propose the first  method to recover sparse vector using a near optimal number of measurements.  We also empirically demonstrate  effectiveness of our algorithms; we show that our algorithms are able to recover signals with smaller number of measurements than several existing methods.",http://proceedings.mlr.press/v28/gopi13.html,http://proceedings.mlr.press/v28/gopi13.pdf,ICML
2911,2013,A Machine Learning Framework for Programming by Example,"Aditya Menon,         Omer Tamuz,         Sumit Gulwani,         Butler Lampson,         Adam Kalai","Learning programs is a timely and interesting challenge. In Programming by Example (PBE), a system attempts to infer a program from input and output examples alone, by searching for a composition of some set of base functions. We show how machine learning can be used to speed up this seemingly hopeless search problem, by learning weights that relate textual features describing the provided input-output examples to plausible sub-components of a program. This generic learning framework lets us address problems beyond the scope of earlier PBE systems. Experiments on a prototype implementation show that learning improves search and ranking on a variety of text processing tasks found on help forums.",http://proceedings.mlr.press/v28/menon13.html,http://proceedings.mlr.press/v28/menon13.pdf,ICML
2912,2013,Feature Selection in High-Dimensional Classification,"Mladen Kolar,         Han Liu","High-dimensional discriminant analysis is of fundamental importance in multivariate statistics. Existing theoretical results sharply characterize different procedures, providing sharp convergence results for the classification risk, as well as the l2 convergence results to the discriminative rule. However, sharp theoretical results for the problem of variable selection have not been established, even though model interpretation is of importance in many scientific domains. In this paper, we  bridge this gap by providing sharp sufficient conditions for consistent variable selection using the ROAD estimator (Fan et al., 2010). Our results provide novel theoretical insights for the ROAD estimator. Sufficient conditions are complemented by the necessary information theoretic limits on variable selection in high-dimensional discriminant analysis. This complementary result also establishes optimality of the ROAD estimator for a certain family of problems.",http://proceedings.mlr.press/v28/kolar13.html,http://proceedings.mlr.press/v28/kolar13.pdf,ICML
2913,2013,Stochastic k-Neighborhood Selection for Supervised and Unsupervised Learning,"Daniel Tarlow,         Kevin Swersky,         Laurent Charlin,         Ilya Sutskever,         Rich Zemel","Neighborhood Components Analysis (NCA) is a popular method for  learning a distance metric to be used within a k-nearest neighbors  (kNN) classifier.    A key assumption built into the model is that each point  stochastically selects a single neighbor, which  makes the model well-justified only for kNN with k=1.  However, kNN classifiers with k>1 are more robust and usually   preferred in practice.     Here we present kNCA, which generalizes NCA by  learning distance metrics that are appropriate for  kNN with arbitrary k.  The main technical contribution is showing  how to efficiently compute and optimize the expected  accuracy of a kNN classifier.  We apply similar ideas in an unsupervised  setting to yield kSNE and ktSNE, generalizations of  Stochastic Neighbor Embedding (SNE, tSNE) that operate on  neighborhoods of size k, which provide an axis of control over  embeddings that allow for more homogeneous and interpretable regions.  Empirically, we show that kNCA often improves classification accuracy over  state of the art methods, produces qualitative  differences in the embeddings as k is varied, and is more robust with  respect to label noise.",http://proceedings.mlr.press/v28/tarlow13.html,http://proceedings.mlr.press/v28/tarlow13.pdf,ICML
2914,2013,Principal Component Analysis on non-Gaussian Dependent Data,"Fang Han,         Han Liu","In this paper, we analyze the performance of a semiparametric principal component analysis named Copula Component Analysis (COCA) (Han & Liu, 2012) when the data are dependent. The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. We study the scenario where the observations are drawn from non-i.i.d. processes (mm-dependency or a more general ϕ\phi-mixing case). We show that COCA can allow weak dependence. In particular, we provide the generalization bounds of convergence for both support recovery and parameter estimation of COCA for the dependent data. We provide explicit sufficient conditions on the degree of dependence, under which the parametric rate can be maintained. To our knowledge, this is the first work analyzing the theoretical performance of PCA for the dependent data in high dimensional settings. Our results strictly generalize the analysis in Han & Liu (2012) and the techniques we used have the separate interest for analyzing a variety of other multivariate statistical methods.",http://proceedings.mlr.press/v28/han13.html,http://proceedings.mlr.press/v28/han13.pdf,ICML
2915,2013,Fast Dual Variational Inference for Non-Conjugate Latent Gaussian Models,"Mohammad Emtiyaz Khan,         Aleksandr Aravkin,         Michael Friedlander,         Matthias Seeger","Latent Gaussian models (LGMs) are widely used in statistics and machine learning.  Bayesian inference in non-conjugate LGM is difficult due to intractable integrals involving the Gaussian prior and non-conjugate likelihoods.  Algorithms based on Variational Gaussian (VG) approximations are widely employed since they strike a favorable balance between accuracy, generality, speed, and ease of use.  However, the structure of optimization problems associated with them    remains poorly understood, and standard solvers take too long to converge.  In this paper, we derive a novel dual variational inference approach, which exploits the convexity property of the VG approximations.   The implications of our approach is that we obtain an algorithm that solves a convex optimization problem, reduces the number of variational parameters, and converges much faster than previous methods.  Using real world data, we demonstrate these advantages on a variety of LGMs including Gaussian process classification and latent Gaussian Markov random fields.",http://proceedings.mlr.press/v28/emtiyazkhan13.html,http://proceedings.mlr.press/v28/emtiyazkhan13.pdf,ICML
2916,2013,Learning with Marginalized Corrupted Features,"Laurens Maaten,         Minmin Chen,         Stephen Tyree,         Kilian Weinberger","The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on very large (infinite) training data sets that capture all variations in the data distribution. In the case of finite training data, an effective solution is to extend the training set with artificially created examples – which, however, is also computationally costly. We propose to corrupt training examples with noise from known distributions within the exponential family and present a novel learning algorithm, called marginalized corrupted features (MCF), that trains robust predictors by minimizing the expected value of the loss function under the corrupting distribution – essentially learning with infinitely many (corrupted) training examples. We show empirically on a variety of data sets that MCF classifiers can be trained efficiently, may generalize substantially better to test data, and are more robust to feature deletion at test time.",http://proceedings.mlr.press/v28/vandermaaten13.html,http://proceedings.mlr.press/v28/vandermaaten13.pdf,ICML
2917,2013,Entropic Affinities: Properties and Efficient Numerical Computation,"Max Vladymyrov,         Miguel Carreira-Perpinan","Gaussian affinities are commonly used in graph-based methods such as spectral clustering or nonlinear embedding. Hinton and Roweis (2003) introduced a way to set the scale individually for each point so that it has a distribution over neighbors with a desired perplexity, or effective number of neighbors. This gives very good affinities that adapt locally to the data but are harder to compute. We study the mathematical properties of these “entropic affinities” and show that they implicitly define a continuously differentiable function in the input space and give bounds for it. We then devise a fast algorithm to compute the widths and affinities, based on robustified, quickly convergent root-finding methods combined with a tree- or density-based initialization scheme that exploits the slowly-varying behavior of this function. This algorithm is nearly optimal and much more accurate and fast than the existing bisection-based approach, particularly with large datasets, as we show with image and text data.",http://proceedings.mlr.press/v28/vladymyrov13.html,http://proceedings.mlr.press/v28/vladymyrov13.pdf,ICML
2918,2013,Safe Screening of Non-Support Vectors in Pathwise SVM Computation,"Kohei Ogawa,         Yoshiki Suzuki,         Ichiro Takeuchi","In this paper, we claim that some of the non-support vectors (non-SVs) that have no influence on the classifier can be screened out prior to the training phase in pathwise SVM computation scenario, in which one is asked to train a sequence (or path) of SVM classifiers for different regularization parameters. Based on a recently proposed framework so-called safe screening rule, we derive a rule for screening out non-SVs in advance, and discuss how we can exploit the advantage of the rule in pathwise SVM computation scenario. Experiments indicate that our approach often substantially reduce the total pathwise computation cost.",http://proceedings.mlr.press/v28/ogawa13b.html,http://proceedings.mlr.press/v28/ogawa13b.pdf,ICML
2919,2013,Efficient Dimensionality Reduction for  Canonical Correlation Analysis,"Haim Avron,         Christos Boutsidis,         Sivan Toledo,         Anastasios Zouzias","We present a fast algorithm for approximate Canonical Correlation Analysis (CCA). Given a pair of tall-and-thin matrices, the proposed algorithm first employs a randomized  dimensionality reduction transform to reduce the size of the input matrices, and then applies any standard CCA algorithm to the new pair of matrices. The algorithm computes an approximate CCA to the original pair of matrices with provable guarantees, while requiring asymptotically less operations than the state-of-the-art exact algorithms.",http://proceedings.mlr.press/v28/avron13.html,http://proceedings.mlr.press/v28/avron13.pdf,ICML
2920,2013,Average Reward Optimization Objective In Partially Observable Domains,"Yuri Grinberg,         Doina Precup","We consider the problem of average reward optimization in domains with partial observability, within the modeling framework of linear predictive state representations (PSRs). The key to average-reward computation is to have a well-defined stationary behavior of a system, so the required averages can be computed. If, additionally, the stationary behavior varies smoothly with changes in policy parameters, average-reward control through policy search also becomes a possibility. In this paper, we show that PSRs have a well-behaved stationary distribution, which is a rational function of policy parameters.  Based on this result, we define a related reward process particularly suitable for average reward optimization, and analyze its properties. We show that in such a predictive state reward process, the average reward is a rational function of the policy parameters, whose complexity depends on the dimension of the underlying linear PSR. This result suggests that average reward-based policy search methods can be effective when the dimension of the system is small, even when the system representation in the POMDP framework requires many hidden states. We provide illustrative examples of this type.",http://proceedings.mlr.press/v28/grinberg13.html,http://proceedings.mlr.press/v28/grinberg13.pdf,ICML
2921,2013,Scalable Optimization of Neighbor Embedding for Visualization,"Zhirong Yang,         Jaakko Peltonen,         Samuel Kaski","Neighbor embedding (NE) methods have found their use in data visualization but are limited in big data analysis tasks due to their O(n^2) complexity for n data samples. We demonstrate that the obvious approach of subsampling produces inferior results and propose a generic approximated optimization technique that reduces the NE optimization cost to O(n log n). The technique is based on realizing that in visualization the embedding space is necessarily very low-dimensional (2D or 3D), and hence efficient approximations developed for n-body force calculations can be applied. In gradient-based NE algorithms the gradient for an individual point decomposes into “forces” exerted by the other points. The contributions of close-by points need to be computed individually but far-away points can be approximated by their “center of mass”, rapidly computable by applying a recursive decomposition of the visualization space into quadrants. The new algorithm brings a significant speed-up for medium-size data, and brings “big data” within reach of visualization.",http://proceedings.mlr.press/v28/yang13b.html,http://proceedings.mlr.press/v28/yang13b.pdf,ICML
2922,2013,Parallel Markov Chain Monte Carlo for Nonparametric Mixture Models,"Sinead Williamson,         Avinava Dubey,         Eric Xing","Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to perform MCMC using the correct equilibrium distribution, in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods.",http://proceedings.mlr.press/v28/williamson13.html,http://proceedings.mlr.press/v28/williamson13.pdf,ICML
2923,2013,Ellipsoidal Multiple Instance Learning,"Gabriel Krummenacher,         Cheng Soon Ong,         Joachim Buhmann","We propose a large margin method for asymmetric learning with ellipsoids, called eMIL, suited to multiple instance learning (MIL). We derive the distance between ellipsoids and the hyperplane, generalising the standard support vector machine. Negative bags in MIL contain only negative instances, and we treat them akin to uncertain observations in the robust optimisation framework. However, our method allows positive bags to cross the margin, since it is not known which instances within are positive.  We show that representing bags as ellipsoids under the introduced distance is the most robust solution when treating a bag as a random variable with finite mean and covariance. Two algorithms are derived to solve the resulting non-convex optimization problem: a concave-convex procedure and a quasi-Newton method. Our method achieves competitive results on benchmark datasets. We introduce a MIL dataset from a real world application of detecting wheel defects from multiple partial observations, and show that eMIL outperforms competing approaches.",http://proceedings.mlr.press/v28/krummenacher13.html,http://proceedings.mlr.press/v28/krummenacher13.pdf,ICML
2924,2013,Spectral Learning of Hidden Markov Models from Dynamic and Static Data,"Tzu-Kuo Huang,         Jeff Schneider","We develop spectral learning algorithms for Hidden Markov Models  that learn not only from time series, or dynamic data but also  static data drawn independently from the HMM’s stationary distribution.  This is motivated by the fact that static, orderless snapshots are usually easier to obtain than time series in quite a few dynamic modeling tasks. Building on existing spectral learning algorithms, our methods solve convex optimization problems minimizing squared loss on the dynamic data plus a regularization term on the static data. Experiments on synthetic and  real human activities data demonstrate better prediction by the proposed method than existing spectral algorithms.",http://proceedings.mlr.press/v28/huang13.html,http://proceedings.mlr.press/v28/huang13.pdf,ICML
2925,2013,Near-Optimal Bounds for Cross-Validation via Loss Stability,"Ravi Kumar,         Daniel Lokshtanov,         Sergei Vassilvitskii,         Andrea Vattani",Multi-fold cross-validation is an established practice to estimate the error rate   of a learning algorithm.  Quantifying the variance reduction gains due to cross-validation   has been challenging due to the inherent correlations introduced by the folds.    In this work we introduce a new and weak measure of stability called \emphloss stability  and relate the cross-validation performance to loss stability; we also establish that this   relationship is near-optimal.  Our work thus quantitatively improves the current  best bounds on cross-validation.,http://proceedings.mlr.press/v28/kumar13a.html,http://proceedings.mlr.press/v28/kumar13a.pdf,ICML
2926,2013,Better Mixing via Deep Representations,"Yoshua Bengio,         Gregoire Mesnil,         Yann Dauphin,         Salah Rifai","It has been hypothesized, and supported with experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation.  We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce Markov chains that mix faster between modes. Consequently, mixing between modes would be more efficient at higher levels of representation.  To better understand this, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels.  The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing between modes and interpolating between samples.",http://proceedings.mlr.press/v28/bengio13.html,http://proceedings.mlr.press/v28/bengio13.pdf,ICML
2927,2013,Thurstonian Boltzmann Machines: Learning from Multiple Inequalities,"Truyen Tran,         Dinh Phung,         Svetha Venkatesh","We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture that can naturally incorporate a wide range of data inputs at the same time. Our motivation rests in the Thurstonian view that many discrete data types can be considered as being generated from a subset of underlying latent continuous variables, and in the observation that each realisation of a discrete type imposes certain inequalities on those variables. Thus learning and inference in TBM reduce to making sense of a set of inequalities. Our proposed TBM naturally supports the following types: Gaussian, intervals, censored, binary, categorical, muticategorical, ordinal, (in)-complete rank with and without ties. We demonstrate the versatility and capacity of the proposed model on three applications of very different natures; namely handwritten digit recognition, collaborative filtering and complex social survey analysis.",http://proceedings.mlr.press/v28/tran13.html,http://proceedings.mlr.press/v28/tran13.pdf,ICML
2928,2013,ABC Reinforcement Learning,"Christos Dimitrakakis,         Nikolaos Tziortziotis","We introduce a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The advantage is that we only require a prior distribution on a class of simulators. This is useful when a probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows  the use of any Bayesian reinforcement learning technique in this case. It can be seen as an extension of simulation methods to both planning and inference.  We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is sound.",http://proceedings.mlr.press/v28/dimitrakakis13.html,http://proceedings.mlr.press/v28/dimitrakakis13.pdf,ICML
2929,2013,Generic Exploration and K-armed Voting Bandits,"Tanguy Urvoy,         Fabrice Clerot,         Raphael Féraud,         Sami Naamane","We study a stochastic online learning scheme with partial feedback where the utility of decisions is only observable through an estimation of the environment parameters. We propose a generic pure-exploration algorithm, able to cope with various utility functions from multi-armed bandits settings to dueling bandits. The primary application of this setting is to offer a natural generalization of dueling bandits for situations where the environment parameters reflect the idiosyncratic preferences of a mixed crowd.",http://proceedings.mlr.press/v28/urvoy13.html,http://proceedings.mlr.press/v28/urvoy13.pdf,ICML
2930,2013,On autoencoder scoring,"Hanna Kamyshanska,         Roland Memisevic","Autoencoders are popular feature learning models because they are conceptually simple, easy to train and allow for efficient inference and training. Recent work has shown how certain autoencoders can assign an unnormalized “score” to data which measures how well the autoencoder can represent the data. Scores are commonly computed by using training criteria that relate the autoencoder to a probabilistic model, such as the Restricted Boltzmann Machine. In this paper we show how an autoencoder can assign meaningful scores to data independently of training procedure and without reference to any probabilistic model, by interpreting it as a dynamical system. We discuss how, and under which conditions, running the dynamical system can be viewed as performing gradient descent in an energy function, which in turn allows us to derive a score via integration. We also show how one can combine multiple, unnormalized scores into a generative classifier.",http://proceedings.mlr.press/v28/kamyshanska13.html,http://proceedings.mlr.press/v28/kamyshanska13.pdf,ICML
2931,2013,Modeling Temporal Evolution and Multiscale Structure in Networks,"Tue Herlau,         Morten Mørup,         Mikkel Schmidt",Many real-world networks exhibit both temporal evolution and multiscale structure.  We propose a model for temporally correlated multifurcating hierarchies in complex networks which jointly capture both effects. We use the Gibbs fragmentation tree as prior over multifurcating trees and a change-point model to account for the temporal evolution of each vertex.  We demonstrate that our model is able to infer time-varying multiscale structure in synthetic as well as three real world time-evolving complex networks.  Our modeling of the temporal evolution of hierarchies brings new insights into the changing roles and position of entities and possibilities for better understanding these dynamic complex systems.,http://proceedings.mlr.press/v28/herlau13.html,http://proceedings.mlr.press/v28/herlau13.pdf,ICML
2932,2013,Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model,"Min Xiao,         Yuhong Guo","In this paper, we propose to address the problem of domain adaptation for sequence labeling tasks via distributed representation learning by using a log-bilinear language adaptation model. The proposed neural probabilistic language model simultaneously models two different but related data distributions in the source and target domains   based on induced distributed representations, which encode both generalizable and domain-specific latent features. We then use the learned dense real-valued representation as   augmenting features for natural language processing systems. We empirically evaluate the proposed learning technique on WSJ and MEDLINE domains with POS tagging systems, and on WSJ and Brown corpora with syntactic chunking and name entity recognition systems. Our primary results show that the proposed domain adaptation method outperforms a number comparison methods for cross domain sequence labeling tasks.",http://proceedings.mlr.press/v28/xiao13.html,http://proceedings.mlr.press/v28/xiao13.pdf,ICML
2933,2013,Adaptive Hamiltonian and Riemann Manifold Monte Carlo,"Ziyu Wang,         Shakir Mohamed,         Nando Freitas","In this paper we address the widely-experienced difficulty in tuning Hamiltonian-based Monte Carlo samplers. We develop an algorithm that allows for the adaptation of Hamiltonian and Riemann manifold Hamiltonian Monte Carlo samplers using Bayesian optimization that allows for infinite adaptation of the parameters of these samplers. We show that the resulting sampling algorithms are ergodic, and demonstrate on several models and data sets that the use of our adaptive algorithms makes it is easy to obtain more efficient samplers, in some precluding the need for more complex models. Hamiltonian-based Monte Carlo samplers are widely known to be an excellent choice of MCMC method, and we aim with this paper to remove a key obstacle towards the more widespread use of these samplers in practice.",http://proceedings.mlr.press/v28/wang13e.html,http://proceedings.mlr.press/v28/wang13e.pdf,ICML
2934,2013,Direct Modeling of Complex Invariances for Visual Object Features,Ka Yu Hui,"View-invariant object representations created from feature pooling networks have been widely adopted in state-of-the-art visual recognition systems. Recently, the research community seeks to improve these view-invariant representations further by additional invariance and receptive field learning, or by taking on the challenge of processing massive amounts of learning data. In this paper we consider an alternate strategy of directly modeling complex invariances of object features. While this may sound like a naive and inferior approach, our experiments show that this approach can achieve competitive and state-of-the-art accuracy on visual recognition data sets such as CIFAR-10 and STL-10. We present an highly applicable dictionary learning algorithm on complex invariances that can be used in most feature pooling network settings. It also has the merits of simplicity and requires no additional tuning. We also discuss the implication of our experiment results concerning recent observations on the usefulness of pre-trained features, and the role of direct invariance modeling in invariance learning.",http://proceedings.mlr.press/v28/yuhui13.html,http://proceedings.mlr.press/v28/yuhui13.pdf,ICML
2935,2013,The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification,"Ofir Pele,         Ben Taskar,         Amir Globerson,         Michael Werman","Linear classiffers are much faster to learn and test than non-linear ones. On the other hand, non-linear kernels offer improved performance, albeit at the increased cost of training kernel classiffers. To use non-linear mappings with efficient linear learning algorithms, explicit embeddings that approximate popular kernels have recently been proposed. However, the embedding process itself is often costly and the results are usually less accurate than kernel methods. In this work we propose a non-linear feature map that is both very efficient, but at the same time highly expressive. The method is based on discretization and interpolation of individual features values and feature pairs. The discretization allows us to model different regions of the feature space separately, while the interpolation preserves the original continuous values. Using this embedding is strictly more general than a linear model and as efficient as the second-order polynomial explicit feature map. An extensive empirical evaluation shows that our method consistently signiffcantly outperforms other methods, including a wide range of kernels. This is in contrast to other proposed embeddings that were faster than kernel methods, but with lower accuracy.",http://proceedings.mlr.press/v28/pele13.html,http://proceedings.mlr.press/v28/pele13.pdf,ICML
2936,2013,Exploring the Mind: Integrating Questionnaires and fMRI,"Esther Salazar,         Ryan Bogdan,         Adam Gorka,         Ahmad Hariri,         Lawrence Carin","A new model is developed for joint analysis of ordered, categorical, real and count data. The ordered and categorical data are answers to questionnaires, the (word) count data correspond to the text questions from the questionnaires, and the real data correspond to fMRI responses for each subject. The Bayesian model employs the von Mises distribution in a novel manner to infer sparse graphical models jointly across people, questions, fMRI stimuli and brain region, with this integrated within a new matrix factorization based on latent binary features. The model is compared with simpler alternatives on two real datasets. We also demonstrate the ability to predict the response of the brain to visual stimuli (as measured by fMRI), based on knowledge of how the associated person answered classical questionnaires.",http://proceedings.mlr.press/v28/salazar13.html,http://proceedings.mlr.press/v28/salazar13.pdf,ICML
2937,2013,Tree-Independent Dual-Tree Algorithms,"Ryan Curtin,         William March,         Parikshit Ram,         David Anderson,         Alexander Gray,         Charles Isbell","Dual-tree algorithms are a widely used class of branch-and-bound algorithms.  Unfortunately, developing dual-tree algorithms for use with different trees and problems is often complex and burdensome.  We introduce a four-part logical split: the tree, the traversal, the point-to-point base case, and the pruning rule.  We provide a meta-algorithm which allows development of dual-tree algorithms in a tree-independent manner and easy extension to entirely new types of trees.  Representations are provided  for five common algorithms; for k-nearest neighbor search, this leads to a novel, tighter pruning bound. The meta-algorithm also allows  straightforward extensions to massively parallel settings.",http://proceedings.mlr.press/v28/curtin13.html,http://proceedings.mlr.press/v28/curtin13.pdf,ICML
2938,2013,"Sparse Gaussian Conditional Random Fields: Algorithms, Theory, and Application to Energy Forecasting","Matt Wytock,         Zico Kolter","This paper considers the sparse Gaussian conditional random field, a discriminative extension of sparse inverse covariance estimation, where we use convex methods to learn a high-dimensional conditional distribution of outputs given inputs. The model has been proposed by multiple researchers within the past year, yet previous papers have been substantially limited in their analysis of the method and in the ability to solve large-scale problems.  In this paper, we make three contributions: 1) we develop a second-order active-set method which is several orders of magnitude faster that previously proposed optimization approaches for this problem 2) we analyze the model from a theoretical standpoint, improving upon past bounds with convergence rates that depend logarithmically on the data dimension, and 3) we apply the method to large-scale energy forecasting problems, demonstrating state-of-the-art performance on two real-world tasks.",http://proceedings.mlr.press/v28/wytock13.html,http://proceedings.mlr.press/v28/wytock13.pdf,ICML
2939,2013,Hierarchical Tensor Decomposition of Latent Tree Graphical Models,"Le Song,         Mariya Ishteva,         Ankur Parikh,         Eric Xing,         Haesun Park","We approach the problem of estimating the parameters of a latent tree graphical model from a hierarchical tensor decomposition point of view. In this new view, the marginal probability table of the observed variables in a latent tree is treated as a tensor, and we show that: (i) the latent variables induce low rank structures in various matricizations of the tensor; (ii) this collection of low rank matricizations induce a hierarchical low rank decomposition of the tensor. Exploiting these properties, we derive an optimization problem for estimating the parameters of a latent tree graphical model, i.e., hierarchical decomposion of a tensor which minimizes the Frobenius norm of the difference between the original tensor and its decomposition.    When the latent tree graphical models are correctly specified, we show that a global optimum of the optimization problem can be obtained via a recursive decomposition algorithm. This algorithm recovers previous spectral algorithms for hidden Markov models (Hsu et al., 2009; Foster et al., 2012) and latent tree graphical models (Parikh et al., 2011; Song et al., 2011) as special cases, elucidating the global objective these algorithms are optimizing for. When the latent tree graphical models are misspecified, we derive a better decomposition based on our framework, and provide approximation guarantee for this new estimator. In both synthetic and real world data, this new estimator significantly improves over the-state-of-the-art.",http://proceedings.mlr.press/v28/song13.html,http://proceedings.mlr.press/v28/song13.pdf,ICML
2940,2013,Max-Margin Multiple-Instance Dictionary Learning,"Xinggang Wang,         Baoyuan Wang,         Xiang Bai,         Wenyu Liu,         Zhuowen Tu","Dictionary learning has became an increasingly important task in machine learning, as it is fundamental to the representation problem. A number of emerging techniques specifically include a codebook learning step, in which a critical knowledge abstraction process is carried out. Existing approaches in dictionary (codebook) learning are either generative (unsupervised e.g. k-means) or discriminative (supervised e.g. extremely randomized forests). In this paper, we propose a multiple instance learning (MIL) strategy (along the line of weakly supervised learning) for dictionary learning. Each code is represented by a classifier, such as a linear SVM, which naturally performs metric fusion for multi-channel features. We design a formulation to simultaneously learn mixtures of codes by maximizing classification margins in MIL.  State-of-the-art results are observed in image classification benchmarks based on the learned codebooks, which observe both compactness and effectiveness.",http://proceedings.mlr.press/v28/wang13d.html,http://proceedings.mlr.press/v28/wang13d.pdf,ICML
2941,2013,Simple Sparsification Improves Sparse Denoising Autoencoders in Denoising Highly Corrupted Images,Kyunghyun Cho,"Recently Burger et al. (2012) and Xie et al. (2012) proposed to use a denoising autoencoder (DAE) for denoising noisy images. They showed that a plain, deep DAE can denoise noisy images as well as the conventional methods such as BM3D and KSVD. Both of them approached image denoising by denoising small, image patches of a larger image and combining them to form a clean image. In this setting, it is usual to use the encoder of the DAE to obtain the latent representation and subsequently apply the decoder to get the clean patch. We propose that a simple sparsification of the latent representation found by the encoder improves denoising performance, when the DAE was trained with sparsity regularization. The experiments confirm that the proposed sparsification indeed helps both denoising a small image patch and denoising a larger image consisting of those patches. Furthermore, it is found out that the proposed method improves even classification performance when test samples are corrupted with noise.",http://proceedings.mlr.press/v28/cho13.html,http://proceedings.mlr.press/v28/cho13.pdf,ICML
2942,2013,Transition Matrix Estimation in High Dimensional Time Series,"Fang Han,         Han Liu","In this paper, we propose a new method in estimating transition matrices of high dimensional vector autoregressive (VAR) models. Here the data are assumed to come from a stationary Gaussian VAR time series. By formulating the problem as a linear program, we provide a new approach to conduct inference on such models. In theory, under a doubly asymptotic framework in which both the sample size T and dimensionality d of the time series can increase, we provide explicit rates of convergence between the estimator and the population transition matrix under different matrix norms. Our results show that the spectral norm of the transition matrix plays a pivotal role in determining the final rates of convergence. This is the first work analyzing the estimation of transition matrices under a high dimensional doubly asymptotic framework. Experiments are conducted on both synthetic and real-world stock data to demonstrate the effectiveness of the proposed method compared with the existing methods. The results of this paper have broad impact on different applications, including finance, genomics, and brain imaging.",http://proceedings.mlr.press/v28/han13a.html,http://proceedings.mlr.press/v28/han13a.pdf,ICML
2943,2013,On Compact Codes for Spatially Pooled Features,"Yangqing Jia,         Oriol Vinyals,         Trevor Darrell","Feature encoding with an overcomplete dictionary has demonstrated good performance in many applications, especially computer vision. In this paper we analyze the classification accuracy with respect to dictionary size by linking the encoding stage to kernel methods and \nystrom sampling, and obtain useful bounds on accuracy as a function of size. The \nystrom method also inspires us to revisit dictionary learning from local patches, and we propose to learn the dictionary in an end-to-end fashion taking into account pooling, a common computational layer in vision. We validate our contribution by showing how the derived bounds are able to explain the observed behavior of multiple datasets, and show that the pooling aware method efficiently reduces the dictionary size by a factor of two for a given accuracy.",http://proceedings.mlr.press/v28/jia13.html,http://proceedings.mlr.press/v28/jia13.pdf,ICML
2944,2013,Multiple-source cross-validation,"Krzysztof Geras,         Charles Sutton","Cross-validation is an essential tool in machine learning and statistics. The typical procedure, in which data points are randomly assigned to one of the test sets, makes an implicit assumption that the data are exchangeable. A common case in which this does not hold is when the data come from multiple sources, in the sense used in transfer learning. In this case it is common to arrange the cross-validation procedure in a way that takes the source structure into account. Although common in practice, this procedure does not appear to have been theoretically analysed. We present new estimators of the variance of the cross-validation, both in the multiple-source setting and in the standard iid setting. These new estimators allow for much more accurate confidence intervals and hypothesis tests to compare algorithms.",http://proceedings.mlr.press/v28/geras13.html,http://proceedings.mlr.press/v28/geras13.pdf,ICML
2945,2013,Learning Linear Bayesian Networks with Latent Variables,"Animashree Anandkumar,         Daniel Hsu,         Adel Javanmard,         Sham Kakade","This work considers the problem of learning linear Bayesian networks when  some of the variables are unobserved.  Identifiability and efficient recovery from low-order observable moments  are established under a novel graphical constraint.  The constraint concerns the expansion properties of the underlying directed  acyclic graph (DAG) between observed and unobserved variables in the  network, and it is satisfied by many natural families of DAGs that include  multi-level DAGs, DAGs with effective depth one, as well as certain  families of polytrees.",http://proceedings.mlr.press/v28/anandkumar13.html,http://proceedings.mlr.press/v28/anandkumar13.pdf,ICML
2946,2013,Dynamical Models and tracking regret in online convex programming,"Eric Hall,         Rebecca Willett","This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that scale with comparator’s deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and existing tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed dynamic mirror descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network.",http://proceedings.mlr.press/v28/hall13.html,http://proceedings.mlr.press/v28/hall13.pdf,ICML
2947,2013,Factorial Multi-Task Learning : A Bayesian Nonparametric Approach,"Sunil Gupta,         Dinh Phung,         Svetha Venkatesh","Multi-task learning is a paradigm shown to improve the performance of related tasks through their joint learning. However, for real-world data, it is usually difficult to assess the task relatedness and joint learning with unrelated tasks may lead to serious performance degradations. To this end, we propose a framework that groups the tasks based on their relatedness in a low dimensional subspace and allows a varying degree of relatedness among tasks by sharing the subspace bases across the groups. This provides the flexibility of no sharing when two sets of tasks are unrelated and partial/total sharing when the tasks are related. Importantly, the number of task-groups and the subspace dimensionality are automatically inferred from the data. This feature keeps the model beyond a specific set of parameters. To realize our framework, we present a novel Bayesian nonparametric prior that extends the traditional hierarchical beta process prior using a Dirichlet process to permit potentially infinite number of child beta processes. We apply our model for multi-task regression and classification applications. Experimental results using several synthetic and real-world datasets show the superiority of our model to other recent state-of-the-art multi-task learning methods.",http://proceedings.mlr.press/v28/gupta13a.html,http://proceedings.mlr.press/v28/gupta13a.pdf,ICML
2948,2013,Optimistic Knowledge Gradient Policy for Optimal Budget Allocation in Crowdsourcing,"Xi Chen,         Qihang Lin,         Dengyong Zhou","In real crowdsourcing applications, each label from a crowd usually comes  with a certain cost. Given a pre- fixed amount of budget, since different tasks have different ambiguities and different workers have different expertises, we want to  find an optimal way to allocate the budget among instance-worker pairs such that the overall label quality can be maximized. To address this issue, we start from the simplest setting in which all workers are assumed to be perfect. We formulate the problem as a Bayesian Markov Decision Process (MDP). Using the dynamic programming (DP) algorithm, one can obtain the optimal allocation policy for a given budget. However,  DP is computationally intractable. To solve the computational challenge, we propose a novel approximate policy which is called optimistic knowledge gradient. It is practically efficient while theoretically its consistency can be guaranteed.  We then extend  the MDP framework to deal with inhomogeneous workers and tasks with contextual information available.  The experiments on both simulated and real data  demonstrate the superiority of our method.",http://proceedings.mlr.press/v28/chen13f.html,http://proceedings.mlr.press/v28/chen13f.pdf,ICML
2949,2013,Efficient Sparse Group Feature Selection via Nonconvex Optimization,"Shuo Xiang,         Xiaoshen Tong,         Jieping Ye","Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) computationally, we introduce a nonconvex sparse group feature selection model and present an efficient optimization algorithm, of which the key step is a projection with two coupled constraints; (2) statistically, we show that the proposed model can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved. Numerical results on synthetic and real-world data suggest that the proposed nonconvex method compares favorably against its competitors, thus achieving desired goal of delivering high performance.",http://proceedings.mlr.press/v28/xiang13.html,http://proceedings.mlr.press/v28/xiang13.pdf,ICML
2950,2013,Parameter Learning and Convergent Inference for Dense Random Fields,"Philipp Kraehenbuehl,         Vladlen Koltun","Dense random fields are models in which all pairs of variables are directly connected by pairwise potentials. It has recently been shown that mean field inference in dense random fields can be performed efficiently and that these models enable significant accuracy gains in computer vision applications. However, parameter estimation for dense random fields is still poorly understood. In this paper, we present an efficient algorithm for learning parameters in dense random fields. All parameters are estimated jointly, thus capturing dependencies between them. We show that gradients of a variety of loss functions over the mean field marginals can be computed efficiently. The resulting algorithm learns parameters that directly optimize the performance of mean field inference in the model.  As a supporting result, we present an efficient inference algorithm for dense random fields that is guaranteed to converge.",http://proceedings.mlr.press/v28/kraehenbuehl13.html,http://proceedings.mlr.press/v28/kraehenbuehl13.pdf,ICML
2951,2013,Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks,"Creighton Heaukulani,         Zoubin Ghahramani","Current Bayesian models for dynamic social network data have focused on modelling the influence of evolving unobserved structure on observed social interactions. However, an understanding of how observed social relationships from the past affect future unobserved structure in the network has been neglected.  In this paper, we introduce a new probabilistic model for capturing this phenomenon, which we call latent feature propagation, in social networks.  We demonstrate our model’s capability for inferring such latent structure in varying types of social network datasets, and experimental studies show this structure achieves higher predictive performance on link prediction and forecasting tasks.",http://proceedings.mlr.press/v28/heaukulani13.html,http://proceedings.mlr.press/v28/heaukulani13.pdf,ICML
